diff -Naur a/arch/alpha/include/asm/spinlock_types.h b/arch/alpha/include/asm/spinlock_types.h
--- a/arch/alpha/include/asm/spinlock_types.h	2020-11-23 13:47:50.209052667 +0200
+++ b/arch/alpha/include/asm/spinlock_types.h	2021-07-14 15:38:50.966284421 +0300
@@ -2,10 +2,6 @@
 #ifndef _ALPHA_SPINLOCK_TYPES_H
 #define _ALPHA_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Naur a/arch/arm/include/asm/spinlock_types.h b/arch/arm/include/asm/spinlock_types.h
--- a/arch/arm/include/asm/spinlock_types.h	2020-11-23 13:47:51.909086361 +0200
+++ b/arch/arm/include/asm/spinlock_types.h	2021-07-14 15:38:51.782278498 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 #define TICKET_SHIFT	16
 
 typedef struct {
diff -Naur a/arch/arm/include/asm/switch_to.h b/arch/arm/include/asm/switch_to.h
--- a/arch/arm/include/asm/switch_to.h	2020-11-23 13:47:51.913086441 +0200
+++ b/arch/arm/include/asm/switch_to.h	2021-07-14 15:38:51.782278498 +0300
@@ -4,6 +4,13 @@
 
 #include <linux/thread_info.h>
 
+#if defined CONFIG_PREEMPT_RT && defined CONFIG_HIGHMEM
+void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p);
+#else
+static inline void
+switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
+#endif
+
 /*
  * For v7 SMP cores running a preemptible kernel we may be pre-empted
  * during a TLB maintenance operation, so execute an inner-shareable dsb
@@ -26,6 +33,7 @@
 #define switch_to(prev,next,last)					\
 do {									\
 	__complete_pending_tlbi();					\
+	switch_kmaps(prev, next);					\
 	last = __switch_to(prev,task_thread_info(prev), task_thread_info(next));	\
 } while (0)
 
diff -Naur a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
--- a/arch/arm/include/asm/thread_info.h	2020-11-23 13:47:51.917086520 +0200
+++ b/arch/arm/include/asm/thread_info.h	2021-07-14 15:38:51.782278498 +0300
@@ -46,6 +46,7 @@
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	int			preempt_lazy_count; /* 0 => preemptable, <0 => bug */
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
 	__u32			cpu;		/* cpu */
@@ -134,7 +135,8 @@
 #define TIF_SYSCALL_TRACE	4	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	5	/* syscall auditing active */
 #define TIF_SYSCALL_TRACEPOINT	6	/* syscall tracepoint instrumentation */
-#define TIF_SECCOMP		7	/* seccomp syscall filtering active */
+#define TIF_NEED_RESCHED_LAZY	7
+#define TIF_SECCOMP		8	/* seccomp syscall filtering active */
 
 #define TIF_USING_IWMMXT	17
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
@@ -143,6 +145,7 @@
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
@@ -158,7 +161,8 @@
  * Change these and you break ASM code in entry-common.S
  */
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
-				 _TIF_NOTIFY_RESUME | _TIF_UPROBE)
+				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
+				 _TIF_NEED_RESCHED_LAZY)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_ARM_THREAD_INFO_H */
diff -Naur a/arch/arm/Kconfig b/arch/arm/Kconfig
--- a/arch/arm/Kconfig	2020-11-23 13:47:50.565059722 +0200
+++ b/arch/arm/Kconfig	2021-07-14 15:38:51.074283637 +0300
@@ -31,6 +31,7 @@
 	select ARCH_OPTIONAL_KERNEL_RWX if ARCH_HAS_STRICT_KERNEL_RWX
 	select ARCH_OPTIONAL_KERNEL_RWX_DEFAULT if CPU_V7
 	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_SUPPORTS_RT if HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_CMPXCHG_LOCKREF
 	select ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT if MMU
@@ -64,7 +65,7 @@
 	select HARDIRQS_SW_RESEND
 	select HAVE_ARCH_AUDITSYSCALL if AEABI && !OABI_COMPAT
 	select HAVE_ARCH_BITREVERSE if (CPU_32v7M || CPU_32v7) && !CPU_32v6
-	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU
+	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU && !PREEMPT_RT
 	select HAVE_ARCH_KGDB if !CPU_ENDIAN_BE32 && MMU
 	select HAVE_ARCH_MMAP_RND_BITS if MMU
 	select HAVE_ARCH_SECCOMP_FILTER if AEABI && !OABI_COMPAT
@@ -102,6 +103,7 @@
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select MMU_GATHER_RCU_TABLE_FREE if SMP && ARM_LPAE
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_RSEQ
@@ -117,6 +119,7 @@
 	select OLD_SIGSUSPEND3
 	select PCI_SYSCALL if PCI
 	select PERF_USE_VMALLOC
+	select HAVE_POSIX_CPU_TIMERS_TASK_WORK if !KVM
 	select RTC_LIB
 	select SYS_SUPPORTS_APM_EMULATION
 	# Above selects are sorted alphabetically; please add new ones
diff -Naur a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
--- a/arch/arm/kernel/asm-offsets.c	2020-11-23 13:47:51.957087313 +0200
+++ b/arch/arm/kernel/asm-offsets.c	2021-07-14 15:38:51.834278121 +0300
@@ -41,6 +41,7 @@
   BLANK();
   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
+  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
   DEFINE(TI_CPU,		offsetof(struct thread_info, cpu));
diff -Naur a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
--- a/arch/arm/kernel/entry-armv.S	2020-11-23 13:47:51.965087471 +0200
+++ b/arch/arm/kernel/entry-armv.S	2021-07-14 15:38:51.834278121 +0300
@@ -206,11 +206,18 @@
 
 #ifdef CONFIG_PREEMPTION
 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
-	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
 	teq	r8, #0				@ if preempt count != 0
+	bne	1f				@ return from exeption
+	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
+	tst	r0, #_TIF_NEED_RESCHED		@ if NEED_RESCHED is set
+	blne	svc_preempt			@ preempt!
+
+	ldr	r8, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r8, #0				@ if preempt lazy count != 0
 	movne	r0, #0				@ force flags to 0
-	tst	r0, #_TIF_NEED_RESCHED
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	blne	svc_preempt
+1:
 #endif
 
 	svc_exit r5, irq = 1			@ return from exception
@@ -225,8 +232,14 @@
 1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
 	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
 	tst	r0, #_TIF_NEED_RESCHED
+	bne	1b
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	reteq	r8				@ go again
-	b	1b
+	ldr	r0, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r0, #0				@ if preempt lazy count != 0
+	beq	1b
+	ret	r8				@ go again
+
 #endif
 
 __und_fault:
diff -Naur a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
--- a/arch/arm/kernel/entry-common.S	2020-11-23 13:47:51.965087471 +0200
+++ b/arch/arm/kernel/entry-common.S	2021-07-14 15:38:51.834278121 +0300
@@ -53,7 +53,9 @@
 	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
-	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
+	tst	r1, #((_TIF_SYSCALL_WORK | _TIF_WORK_MASK) & ~_TIF_SECCOMP)
+	bne	fast_work_pending
+	tst	r1, #_TIF_SECCOMP
 	bne	fast_work_pending
 
 
@@ -90,8 +92,11 @@
 	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
-	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
+	tst	r1, #((_TIF_SYSCALL_WORK | _TIF_WORK_MASK) & ~_TIF_SECCOMP)
+	bne	do_slower_path
+	tst	r1, #_TIF_SECCOMP
 	beq	no_work_pending
+do_slower_path:
  UNWIND(.fnend		)
 ENDPROC(ret_fast_syscall)
 
diff -Naur a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
--- a/arch/arm/kernel/signal.c	2020-11-23 13:47:51.989087947 +0200
+++ b/arch/arm/kernel/signal.c	2021-07-14 15:38:51.834278121 +0300
@@ -649,7 +649,8 @@
 	 */
 	trace_hardirqs_off();
 	do {
-		if (likely(thread_flags & _TIF_NEED_RESCHED)) {
+		if (likely(thread_flags & (_TIF_NEED_RESCHED |
+					   _TIF_NEED_RESCHED_LAZY))) {
 			schedule();
 		} else {
 			if (unlikely(!user_mode(regs)))
diff -Naur a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
--- a/arch/arm/kernel/smp.c	2020-11-23 13:47:51.989087947 +0200
+++ b/arch/arm/kernel/smp.c	2021-07-14 15:38:51.834278121 +0300
@@ -680,11 +680,9 @@
 		break;
 
 	case IPI_CPU_BACKTRACE:
-		printk_nmi_enter();
 		irq_enter();
 		nmi_cpu_backtrace(regs);
 		irq_exit();
-		printk_nmi_exit();
 		break;
 
 	default:
diff -Naur a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
--- a/arch/arm/mm/fault.c	2020-11-23 13:47:52.821104446 +0200
+++ b/arch/arm/mm/fault.c	2021-07-14 15:38:52.294274785 +0300
@@ -400,6 +400,9 @@
 	if (addr < TASK_SIZE)
 		return do_page_fault(addr, fsr, regs);
 
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	if (user_mode(regs))
 		goto bad_area;
 
@@ -470,6 +473,9 @@
 static int
 do_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	do_bad_area(addr, fsr, regs);
 	return 0;
 }
diff -Naur a/arch/arm/mm/highmem.c b/arch/arm/mm/highmem.c
--- a/arch/arm/mm/highmem.c	2020-11-23 13:47:52.825104524 +0200
+++ b/arch/arm/mm/highmem.c	2021-07-14 15:38:52.294274785 +0300
@@ -31,8 +31,14 @@
 	return *ptep;
 }
 
+static unsigned int fixmap_idx(int type)
+{
+	return FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+}
+
 void *kmap_atomic_high_prot(struct page *page, pgprot_t prot)
 {
+	pte_t pte = mk_pte(page, kmap_prot);
 	unsigned int idx;
 	unsigned long vaddr;
 	void *kmap;
@@ -53,7 +59,7 @@
 
 	type = kmap_atomic_idx_push();
 
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+	idx = fixmap_idx(type);
 	vaddr = __fix_to_virt(idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
 	/*
@@ -62,12 +68,15 @@
 	 */
 	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
 #endif
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
 	/*
 	 * When debugging is off, kunmap_atomic leaves the previous mapping
 	 * in place, so the contained TLB flush ensures the TLB is updated
 	 * with the new mapping.
 	 */
-	set_fixmap_pte(idx, mk_pte(page, prot));
+	set_fixmap_pte(idx, pte);
 
 	return (void *)vaddr;
 }
@@ -80,16 +89,19 @@
 
 	if (kvaddr >= (void *)FIXADDR_START) {
 		type = kmap_atomic_idx();
-		idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+		idx = fixmap_idx(type);
 
 		if (cache_is_vivt())
 			__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);
+#ifdef CONFIG_PREEMPT_RT
+		current->kmap_pte[type] = __pte(0);
+#endif
 #ifdef CONFIG_DEBUG_HIGHMEM
 		BUG_ON(vaddr != __fix_to_virt(idx));
-		set_fixmap_pte(idx, __pte(0));
 #else
 		(void) idx;  /* to kill a warning */
 #endif
+		set_fixmap_pte(idx, __pte(0));
 		kmap_atomic_idx_pop();
 	} else if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
 		/* this address was obtained through kmap_high_get() */
@@ -100,22 +112,51 @@
 
 void *kmap_atomic_pfn(unsigned long pfn)
 {
+	pte_t pte = pfn_pte(pfn, kmap_prot);
 	unsigned long vaddr;
 	int idx, type;
 	struct page *page = pfn_to_page(pfn);
 
-	preempt_disable();
+	migrate_disable();
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
 
 	type = kmap_atomic_idx_push();
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+	idx = fixmap_idx(type);
 	vaddr = __fix_to_virt(idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
 	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
 #endif
-	set_fixmap_pte(idx, pfn_pte(pfn, kmap_prot));
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_fixmap_pte(idx, pte);
 
 	return (void *)vaddr;
 }
+
+#if defined CONFIG_PREEMPT_RT
+void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	int i;
+
+	/*
+	 * Clear @prev's kmap_atomic mappings
+	 */
+	for (i = 0; i < prev_p->kmap_idx; i++) {
+		int idx = fixmap_idx(i);
+
+		set_fixmap_pte(idx, __pte(0));
+	}
+	/*
+	 * Restore @next_p's kmap_atomic mappings
+	 */
+	for (i = 0; i < next_p->kmap_idx; i++) {
+		int idx = fixmap_idx(i);
+
+		if (!pte_none(next_p->kmap_pte[i]))
+			set_fixmap_pte(idx, next_p->kmap_pte[i]);
+	}
+}
+#endif
diff -Naur a/arch/arm/tools/syscall.tbl b/arch/arm/tools/syscall.tbl
--- a/arch/arm/tools/syscall.tbl	2020-11-23 13:47:52.929106587 +0200
+++ b/arch/arm/tools/syscall.tbl	2021-07-14 15:38:52.342274438 +0300
@@ -453,3 +453,13 @@
 437	common	openat2				sys_openat2
 438	common	pidfd_getfd			sys_pidfd_getfd
 439	common	faccessat2			sys_faccessat2
+# rtnet
+1436     common  socket_rtnet                    sys_socket_rtnet
+1437     common  bind_rtnet                      sys_bind_rtnet
+1438     common  recvmsg_rtnet                   sys_recvmsg_rtnet
+1439     common  sendto_rtnet                    sys_sendto_rtnet
+1440     common  recvfrom_rtnet                  sys_recvfrom_rtnet
+1441     common  sendmsg_rtnet                   sys_sendmsg_rtnet
+1442     common  select_rtnet                    sys_select_rtnet
+1443     common  poll_rtnet                      sys_poll_rtnet
+
diff -Naur a/arch/arm64/include/asm/preempt.h b/arch/arm64/include/asm/preempt.h
--- a/arch/arm64/include/asm/preempt.h	2020-11-23 13:47:53.513118171 +0200
+++ b/arch/arm64/include/asm/preempt.h	2021-07-14 15:38:52.582272698 +0300
@@ -70,17 +70,43 @@
 	 * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
 	 * pair.
 	 */
-	return !pc || !READ_ONCE(ti->preempt_count);
+	if (!pc || !READ_ONCE(ti->preempt_count))
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if ((pc & ~PREEMPT_NEED_RESCHED))
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
 }
 
 static inline bool should_resched(int preempt_offset)
 {
+#ifdef CONFIG_PREEMPT_LAZY
+	u64 pc = READ_ONCE(current_thread_info()->preempt_count);
+	if (pc == preempt_offset)
+		return true;
+
+	if ((pc & ~PREEMPT_NEED_RESCHED) != preempt_offset)
+		return false;
+
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
 	u64 pc = READ_ONCE(current_thread_info()->preempt_count);
 	return pc == preempt_offset;
+#endif
 }
 
 #ifdef CONFIG_PREEMPTION
 void preempt_schedule(void);
+#ifdef CONFIG_PREEMPT_RT
+void preempt_schedule_lock(void);
+#endif
 #define __preempt_schedule() preempt_schedule()
 void preempt_schedule_notrace(void);
 #define __preempt_schedule_notrace() preempt_schedule_notrace()
diff -Naur a/arch/arm64/include/asm/spinlock_types.h b/arch/arm64/include/asm/spinlock_types.h
--- a/arch/arm64/include/asm/spinlock_types.h	2020-11-23 13:47:53.521118329 +0200
+++ b/arch/arm64/include/asm/spinlock_types.h	2021-07-14 15:38:52.586272669 +0300
@@ -5,10 +5,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__ASM_SPINLOCK_H)
-# error "please don't include this file directly"
-#endif
-
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
 
diff -Naur a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
--- a/arch/arm64/include/asm/thread_info.h	2020-11-23 13:47:53.525118408 +0200
+++ b/arch/arm64/include/asm/thread_info.h	2021-07-14 15:38:52.586272669 +0300
@@ -29,6 +29,7 @@
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
+	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
 	union {
 		u64		preempt_count;	/* 0 => preemptible, <0 => bug */
 		struct {
@@ -67,6 +68,7 @@
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
 #define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
 #define TIF_FSCHECK		5	/* Check FS is USER_DS on return */
+#define TIF_NEED_RESCHED_LAZY	6
 #define TIF_SYSCALL_TRACE	8	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	9	/* syscall auditing */
 #define TIF_SYSCALL_TRACEPOINT	10	/* syscall tracepoint for ftrace */
@@ -93,14 +95,16 @@
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_FSCHECK		(1 << TIF_FSCHECK)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 #define _TIF_SVE		(1 << TIF_SVE)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \
-				 _TIF_UPROBE | _TIF_FSCHECK)
+				 _TIF_UPROBE | _TIF_FSCHECK | _TIF_NEED_RESCHED_LAZY)
 
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
 				 _TIF_SYSCALL_EMU)
diff -Naur a/arch/arm64/include/generated/asm/bugs.h b/arch/arm64/include/generated/asm/bugs.h
--- a/arch/arm64/include/generated/asm/bugs.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/bugs.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/bugs.h>
diff -Naur a/arch/arm64/include/generated/asm/delay.h b/arch/arm64/include/generated/asm/delay.h
--- a/arch/arm64/include/generated/asm/delay.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/delay.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/delay.h>
diff -Naur a/arch/arm64/include/generated/asm/div64.h b/arch/arm64/include/generated/asm/div64.h
--- a/arch/arm64/include/generated/asm/div64.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/div64.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/div64.h>
diff -Naur a/arch/arm64/include/generated/asm/dma-contiguous.h b/arch/arm64/include/generated/asm/dma-contiguous.h
--- a/arch/arm64/include/generated/asm/dma-contiguous.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/dma-contiguous.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/dma-contiguous.h>
diff -Naur a/arch/arm64/include/generated/asm/dma.h b/arch/arm64/include/generated/asm/dma.h
--- a/arch/arm64/include/generated/asm/dma.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/dma.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/dma.h>
diff -Naur a/arch/arm64/include/generated/asm/dma-mapping.h b/arch/arm64/include/generated/asm/dma-mapping.h
--- a/arch/arm64/include/generated/asm/dma-mapping.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/dma-mapping.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/dma-mapping.h>
diff -Naur a/arch/arm64/include/generated/asm/early_ioremap.h b/arch/arm64/include/generated/asm/early_ioremap.h
--- a/arch/arm64/include/generated/asm/early_ioremap.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/early_ioremap.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/early_ioremap.h>
diff -Naur a/arch/arm64/include/generated/asm/emergency-restart.h b/arch/arm64/include/generated/asm/emergency-restart.h
--- a/arch/arm64/include/generated/asm/emergency-restart.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/emergency-restart.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/emergency-restart.h>
diff -Naur a/arch/arm64/include/generated/asm/hw_irq.h b/arch/arm64/include/generated/asm/hw_irq.h
--- a/arch/arm64/include/generated/asm/hw_irq.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/hw_irq.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/hw_irq.h>
diff -Naur a/arch/arm64/include/generated/asm/irq_regs.h b/arch/arm64/include/generated/asm/irq_regs.h
--- a/arch/arm64/include/generated/asm/irq_regs.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/irq_regs.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/irq_regs.h>
diff -Naur a/arch/arm64/include/generated/asm/kdebug.h b/arch/arm64/include/generated/asm/kdebug.h
--- a/arch/arm64/include/generated/asm/kdebug.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/kdebug.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/kdebug.h>
diff -Naur a/arch/arm64/include/generated/asm/kmap_types.h b/arch/arm64/include/generated/asm/kmap_types.h
--- a/arch/arm64/include/generated/asm/kmap_types.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/kmap_types.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/kmap_types.h>
diff -Naur a/arch/arm64/include/generated/asm/local64.h b/arch/arm64/include/generated/asm/local64.h
--- a/arch/arm64/include/generated/asm/local64.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/local64.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/local64.h>
diff -Naur a/arch/arm64/include/generated/asm/local.h b/arch/arm64/include/generated/asm/local.h
--- a/arch/arm64/include/generated/asm/local.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/local.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/local.h>
diff -Naur a/arch/arm64/include/generated/asm/mcs_spinlock.h b/arch/arm64/include/generated/asm/mcs_spinlock.h
--- a/arch/arm64/include/generated/asm/mcs_spinlock.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/mcs_spinlock.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/mcs_spinlock.h>
diff -Naur a/arch/arm64/include/generated/asm/mm-arch-hooks.h b/arch/arm64/include/generated/asm/mm-arch-hooks.h
--- a/arch/arm64/include/generated/asm/mm-arch-hooks.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/mm-arch-hooks.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/mm-arch-hooks.h>
diff -Naur a/arch/arm64/include/generated/asm/mmiowb.h b/arch/arm64/include/generated/asm/mmiowb.h
--- a/arch/arm64/include/generated/asm/mmiowb.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/mmiowb.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/mmiowb.h>
diff -Naur a/arch/arm64/include/generated/asm/msi.h b/arch/arm64/include/generated/asm/msi.h
--- a/arch/arm64/include/generated/asm/msi.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/msi.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/msi.h>
diff -Naur a/arch/arm64/include/generated/asm/qrwlock.h b/arch/arm64/include/generated/asm/qrwlock.h
--- a/arch/arm64/include/generated/asm/qrwlock.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/qrwlock.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/qrwlock.h>
diff -Naur a/arch/arm64/include/generated/asm/qspinlock.h b/arch/arm64/include/generated/asm/qspinlock.h
--- a/arch/arm64/include/generated/asm/qspinlock.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/qspinlock.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/qspinlock.h>
diff -Naur a/arch/arm64/include/generated/asm/rwonce.h b/arch/arm64/include/generated/asm/rwonce.h
--- a/arch/arm64/include/generated/asm/rwonce.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/rwonce.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/rwonce.h>
diff -Naur a/arch/arm64/include/generated/asm/serial.h b/arch/arm64/include/generated/asm/serial.h
--- a/arch/arm64/include/generated/asm/serial.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/serial.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/serial.h>
diff -Naur a/arch/arm64/include/generated/asm/set_memory.h b/arch/arm64/include/generated/asm/set_memory.h
--- a/arch/arm64/include/generated/asm/set_memory.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/set_memory.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/set_memory.h>
diff -Naur a/arch/arm64/include/generated/asm/switch_to.h b/arch/arm64/include/generated/asm/switch_to.h
--- a/arch/arm64/include/generated/asm/switch_to.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/switch_to.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/switch_to.h>
diff -Naur a/arch/arm64/include/generated/asm/trace_clock.h b/arch/arm64/include/generated/asm/trace_clock.h
--- a/arch/arm64/include/generated/asm/trace_clock.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/trace_clock.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/trace_clock.h>
diff -Naur a/arch/arm64/include/generated/asm/unaligned.h b/arch/arm64/include/generated/asm/unaligned.h
--- a/arch/arm64/include/generated/asm/unaligned.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/unaligned.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/unaligned.h>
diff -Naur a/arch/arm64/include/generated/asm/user.h b/arch/arm64/include/generated/asm/user.h
--- a/arch/arm64/include/generated/asm/user.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/user.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/user.h>
diff -Naur a/arch/arm64/include/generated/asm/vga.h b/arch/arm64/include/generated/asm/vga.h
--- a/arch/arm64/include/generated/asm/vga.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/asm/vga.h	2021-07-14 15:38:52.598272582 +0300
@@ -0,0 +1 @@
+#include <asm-generic/vga.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/errno.h b/arch/arm64/include/generated/uapi/asm/errno.h
--- a/arch/arm64/include/generated/uapi/asm/errno.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/errno.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/errno.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/ioctl.h b/arch/arm64/include/generated/uapi/asm/ioctl.h
--- a/arch/arm64/include/generated/uapi/asm/ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/ioctl.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/ioctl.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/ioctls.h b/arch/arm64/include/generated/uapi/asm/ioctls.h
--- a/arch/arm64/include/generated/uapi/asm/ioctls.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/ioctls.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/ioctls.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/ipcbuf.h b/arch/arm64/include/generated/uapi/asm/ipcbuf.h
--- a/arch/arm64/include/generated/uapi/asm/ipcbuf.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/ipcbuf.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/ipcbuf.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/kvm_para.h b/arch/arm64/include/generated/uapi/asm/kvm_para.h
--- a/arch/arm64/include/generated/uapi/asm/kvm_para.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/kvm_para.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/kvm_para.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/msgbuf.h b/arch/arm64/include/generated/uapi/asm/msgbuf.h
--- a/arch/arm64/include/generated/uapi/asm/msgbuf.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/msgbuf.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/msgbuf.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/poll.h b/arch/arm64/include/generated/uapi/asm/poll.h
--- a/arch/arm64/include/generated/uapi/asm/poll.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/poll.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/poll.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/resource.h b/arch/arm64/include/generated/uapi/asm/resource.h
--- a/arch/arm64/include/generated/uapi/asm/resource.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/resource.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/resource.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/sembuf.h b/arch/arm64/include/generated/uapi/asm/sembuf.h
--- a/arch/arm64/include/generated/uapi/asm/sembuf.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/sembuf.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/sembuf.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/shmbuf.h b/arch/arm64/include/generated/uapi/asm/shmbuf.h
--- a/arch/arm64/include/generated/uapi/asm/shmbuf.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/shmbuf.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/shmbuf.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/siginfo.h b/arch/arm64/include/generated/uapi/asm/siginfo.h
--- a/arch/arm64/include/generated/uapi/asm/siginfo.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/siginfo.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/siginfo.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/socket.h b/arch/arm64/include/generated/uapi/asm/socket.h
--- a/arch/arm64/include/generated/uapi/asm/socket.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/socket.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/socket.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/sockios.h b/arch/arm64/include/generated/uapi/asm/sockios.h
--- a/arch/arm64/include/generated/uapi/asm/sockios.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/sockios.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/sockios.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/stat.h b/arch/arm64/include/generated/uapi/asm/stat.h
--- a/arch/arm64/include/generated/uapi/asm/stat.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/stat.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/stat.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/swab.h b/arch/arm64/include/generated/uapi/asm/swab.h
--- a/arch/arm64/include/generated/uapi/asm/swab.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/swab.h	2021-07-14 15:38:52.590272640 +0300
@@ -0,0 +1 @@
+#include <asm-generic/swab.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/termbits.h b/arch/arm64/include/generated/uapi/asm/termbits.h
--- a/arch/arm64/include/generated/uapi/asm/termbits.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/termbits.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/termbits.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/termios.h b/arch/arm64/include/generated/uapi/asm/termios.h
--- a/arch/arm64/include/generated/uapi/asm/termios.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/termios.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/termios.h>
diff -Naur a/arch/arm64/include/generated/uapi/asm/types.h b/arch/arm64/include/generated/uapi/asm/types.h
--- a/arch/arm64/include/generated/uapi/asm/types.h	1970-01-01 02:00:00.000000000 +0200
+++ b/arch/arm64/include/generated/uapi/asm/types.h	2021-07-14 15:38:52.594272611 +0300
@@ -0,0 +1 @@
+#include <asm-generic/types.h>
diff -Naur a/arch/arm64/Kconfig b/arch/arm64/Kconfig
--- a/arch/arm64/Kconfig	2020-11-23 13:47:52.945106905 +0200
+++ b/arch/arm64/Kconfig	2021-07-14 15:38:52.354274350 +0300
@@ -75,6 +75,7 @@
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_SUPPORTS_INT128 if CC_HAS_INT128 && (GCC_VERSION >= 50000 || CC_IS_CLANG)
 	select ARCH_SUPPORTS_NUMA_BALANCING
+	select ARCH_SUPPORTS_RT if HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select ARCH_WANT_COMPAT_IPC_PARSE_VERSION if COMPAT
 	select ARCH_WANT_DEFAULT_BPF_JIT
 	select ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT
@@ -169,6 +170,7 @@
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_FUNCTION_ARG_ACCESS_API
 	select HAVE_FUTEX_CMPXCHG if FUTEX
@@ -190,6 +192,7 @@
 	select PCI_DOMAINS_GENERIC if PCI
 	select PCI_ECAM if (ACPI && PCI)
 	select PCI_SYSCALL if PCI
+	select HAVE_POSIX_CPU_TIMERS_TASK_WORK if !KVM
 	select POWER_RESET
 	select POWER_SUPPLY
 	select SPARSE_IRQ
diff -Naur a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
--- a/arch/arm64/kernel/asm-offsets.c	2020-11-23 13:47:53.549118884 +0200
+++ b/arch/arm64/kernel/asm-offsets.c	2021-07-14 15:38:52.626272379 +0300
@@ -30,6 +30,7 @@
   BLANK();
   DEFINE(TSK_TI_FLAGS,		offsetof(struct task_struct, thread_info.flags));
   DEFINE(TSK_TI_PREEMPT,	offsetof(struct task_struct, thread_info.preempt_count));
+  DEFINE(TSK_TI_PREEMPT_LAZY,	offsetof(struct task_struct, thread_info.preempt_lazy_count));
   DEFINE(TSK_TI_ADDR_LIMIT,	offsetof(struct task_struct, thread_info.addr_limit));
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
   DEFINE(TSK_TI_TTBR0,		offsetof(struct task_struct, thread_info.ttbr0));
diff -Naur a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
--- a/arch/arm64/kernel/entry.S	2020-11-23 13:47:53.561119123 +0200
+++ b/arch/arm64/kernel/entry.S	2021-07-14 15:38:52.626272379 +0300
@@ -624,9 +624,18 @@
 	mrs	x0, daif
 	orr	x24, x24, x0
 alternative_else_nop_endif
-	cbnz	x24, 1f				// preempt count != 0 || NMI return path
-	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
+
+	cbz	x24, 1f					// (need_resched + count) == 0
+	cbnz	w24, 2f					// count != 0
+
+	ldr	w24, [tsk, #TSK_TI_PREEMPT_LAZY]	// get preempt lazy count
+	cbnz	w24, 2f					// preempt lazy count != 0
+
+	ldr	x0, [tsk, #TSK_TI_FLAGS]		// get flags
+	tbz	x0, #TIF_NEED_RESCHED_LAZY, 2f		// needs rescheduling?
 1:
+	bl	arm64_preempt_schedule_irq		// irq en/disable is done inside
+2:
 #endif
 
 #ifdef CONFIG_ARM64_PSEUDO_NMI
diff -Naur a/arch/arm64/kernel/fpsimd.c b/arch/arm64/kernel/fpsimd.c
--- a/arch/arm64/kernel/fpsimd.c	2020-11-23 13:47:53.561119123 +0200
+++ b/arch/arm64/kernel/fpsimd.c	2021-07-14 15:38:52.626272379 +0300
@@ -224,6 +224,16 @@
 	__sve_free(task);
 }
 
+static void *sve_free_atomic(struct task_struct *task)
+{
+	void *sve_state = task->thread.sve_state;
+
+	WARN_ON(test_tsk_thread_flag(task, TIF_SVE));
+
+	task->thread.sve_state = NULL;
+	return sve_state;
+}
+
 /*
  * TIF_SVE controls whether a task can use SVE without trapping while
  * in userspace, and also the way a task's FPSIMD/SVE state is stored
@@ -1020,6 +1030,7 @@
 void fpsimd_flush_thread(void)
 {
 	int vl, supported_vl;
+	void *mem = NULL;
 
 	if (!system_supports_fpsimd())
 		return;
@@ -1032,7 +1043,7 @@
 
 	if (system_supports_sve()) {
 		clear_thread_flag(TIF_SVE);
-		sve_free(current);
+		mem = sve_free_atomic(current);
 
 		/*
 		 * Reset the task vector length as required.
@@ -1066,6 +1077,7 @@
 	}
 
 	put_cpu_fpsimd_context();
+	kfree(mem);
 }
 
 /*
diff -Naur a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
--- a/arch/arm64/kernel/signal.c	2020-11-23 13:47:53.589119678 +0200
+++ b/arch/arm64/kernel/signal.c	2021-07-14 15:38:52.626272379 +0300
@@ -921,7 +921,7 @@
 		/* Check valid user FS if needed */
 		addr_limit_user_check();
 
-		if (thread_flags & _TIF_NEED_RESCHED) {
+		if (thread_flags & _TIF_NEED_RESCHED_MASK) {
 			/* Unmask Debug and SError for the next task */
 			local_daif_restore(DAIF_PROCCTX_NOIRQ);
 
diff -Naur a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
--- a/arch/arm64/kvm/arm.c	2020-11-23 13:47:53.605119995 +0200
+++ b/arch/arm64/kvm/arm.c	2021-07-14 15:38:52.646272234 +0300
@@ -681,7 +681,7 @@
 		 * involves poking the GIC, which must be done in a
 		 * non-preemptible context.
 		 */
-		preempt_disable();
+		migrate_disable();
 
 		kvm_pmu_flush_hwstate(vcpu);
 
@@ -730,7 +730,7 @@
 				kvm_timer_sync_user(vcpu);
 			kvm_vgic_sync_hwstate(vcpu);
 			local_irq_enable();
-			preempt_enable();
+			migrate_enable();
 			continue;
 		}
 
@@ -802,7 +802,7 @@
 		/* Exit types that need handling before we can be preempted */
 		handle_exit_early(vcpu, ret);
 
-		preempt_enable();
+		migrate_enable();
 
 		ret = handle_exit(vcpu, ret);
 	}
diff -Naur a/arch/hexagon/include/asm/spinlock_types.h b/arch/hexagon/include/asm/spinlock_types.h
--- a/arch/hexagon/include/asm/spinlock_types.h	2020-11-23 13:47:53.829124438 +0200
+++ b/arch/hexagon/include/asm/spinlock_types.h	2021-07-14 15:38:52.762271394 +0300
@@ -8,10 +8,6 @@
 #ifndef _ASM_SPINLOCK_TYPES_H
 #define _ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Naur a/arch/ia64/include/asm/spinlock_types.h b/arch/ia64/include/asm/spinlock_types.h
--- a/arch/ia64/include/asm/spinlock_types.h	2020-11-23 13:47:53.897125787 +0200
+++ b/arch/ia64/include/asm/spinlock_types.h	2021-07-14 15:38:52.814271016 +0300
@@ -2,10 +2,6 @@
 #ifndef _ASM_IA64_SPINLOCK_TYPES_H
 #define _ASM_IA64_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Naur a/arch/Kconfig b/arch/Kconfig
--- a/arch/Kconfig	2020-11-23 13:47:50.113050765 +0200
+++ b/arch/Kconfig	2021-07-14 15:38:50.934284654 +0300
@@ -34,6 +34,7 @@
 	tristate "OProfile system profiling"
 	depends on PROFILING
 	depends on HAVE_OPROFILE
+	depends on !PREEMPT_RT
 	select RING_BUFFER
 	select RING_BUFFER_ALLOW_SWAP
 	help
@@ -414,6 +415,13 @@
 	bool
 	depends on MMU_GATHER_TABLE_FREE
 
+config ARCH_WANT_IRQS_OFF_ACTIVATE_MM
+	bool
+	help
+	  Temporary select until all architectures can be converted to have
+	  irqs disabled over activate_mm. Architectures that do IPI based TLB
+	  shootdowns should enable this.
+
 config ARCH_HAVE_NMI_SAFE_CMPXCHG
 	bool
 
diff -Naur a/arch/mips/Kconfig b/arch/mips/Kconfig
--- a/arch/mips/Kconfig	2020-11-23 13:47:54.421136183 +0200
+++ b/arch/mips/Kconfig	2021-07-14 15:38:53.302267484 +0300
@@ -2653,7 +2653,7 @@
 #
 config HIGHMEM
 	bool "High Memory Support"
-	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA
+	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA && !PREEMPT_RT
 
 config CPU_SUPPORTS_HIGHMEM
 	bool
diff -Naur a/arch/powerpc/include/asm/spinlock_types.h b/arch/powerpc/include/asm/spinlock_types.h
--- a/arch/powerpc/include/asm/spinlock_types.h	2020-11-23 13:47:56.053168571 +0200
+++ b/arch/powerpc/include/asm/spinlock_types.h	2021-07-14 15:38:53.890263229 +0300
@@ -2,10 +2,6 @@
 #ifndef _ASM_POWERPC_SPINLOCK_TYPES_H
 #define _ASM_POWERPC_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 #ifdef CONFIG_PPC_QUEUED_SPINLOCKS
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
diff -Naur a/arch/powerpc/include/asm/stackprotector.h b/arch/powerpc/include/asm/stackprotector.h
--- a/arch/powerpc/include/asm/stackprotector.h	2020-11-23 13:47:56.057168651 +0200
+++ b/arch/powerpc/include/asm/stackprotector.h	2021-07-14 15:38:53.890263229 +0300
@@ -24,7 +24,11 @@
 	unsigned long canary;
 
 	/* Try to get a semi random initial value. */
+#ifdef CONFIG_PREEMPT_RT
+	canary = (unsigned long)&canary;
+#else
 	canary = get_random_canary();
+#endif
 	canary ^= mftb();
 	canary ^= LINUX_VERSION_CODE;
 	canary &= CANARY_MASK;
diff -Naur a/arch/powerpc/include/asm/thread_info.h b/arch/powerpc/include/asm/thread_info.h
--- a/arch/powerpc/include/asm/thread_info.h	2020-11-23 13:47:56.061168730 +0200
+++ b/arch/powerpc/include/asm/thread_info.h	2021-07-14 15:38:53.890263229 +0300
@@ -48,6 +48,8 @@
 struct thread_info {
 	int		preempt_count;		/* 0 => preemptable,
 						   <0 => BUG */
+	int             preempt_lazy_count;	/* 0 => preemptable,
+						   <0 => BUG */
 	unsigned long	local_flags;		/* private flags for thread */
 #ifdef CONFIG_LIVEPATCH
 	unsigned long *livepatch_sp;
@@ -98,11 +100,12 @@
 #define TIF_SINGLESTEP		8	/* singlestepping active */
 #define TIF_NOHZ		9	/* in adaptive nohz mode */
 #define TIF_SECCOMP		10	/* secure computing */
-#define TIF_RESTOREALL		11	/* Restore all regs (implies NOERROR) */
-#define TIF_NOERROR		12	/* Force successful syscall return */
+
+#define TIF_NEED_RESCHED_LAZY	11	/* lazy rescheduling necessary */
+#define TIF_SYSCALL_TRACEPOINT	12	/* syscall tracepoint instrumentation */
+
 #define TIF_NOTIFY_RESUME	13	/* callback before returning to user */
 #define TIF_UPROBE		14	/* breakpointed or single-stepping */
-#define TIF_SYSCALL_TRACEPOINT	15	/* syscall tracepoint instrumentation */
 #define TIF_EMULATE_STACK_STORE	16	/* Is an instruction emulation
 						for stack store? */
 #define TIF_MEMDIE		17	/* is terminating due to OOM killer */
@@ -111,6 +114,9 @@
 #endif
 #define TIF_POLLING_NRFLAG	19	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_32BIT		20	/* 32 bit binary */
+#define TIF_RESTOREALL		21	/* Restore all regs (implies NOERROR) */
+#define TIF_NOERROR		22	/* Force successful syscall return */
+
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -130,6 +136,7 @@
 #define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 #define _TIF_EMULATE_STACK_STORE	(1<<TIF_EMULATE_STACK_STORE)
 #define _TIF_NOHZ		(1<<TIF_NOHZ)
+#define _TIF_NEED_RESCHED_LAZY	(1<<TIF_NEED_RESCHED_LAZY)
 #define _TIF_FSCHECK		(1<<TIF_FSCHECK)
 #define _TIF_SYSCALL_EMU	(1<<TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_DOTRACE	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
@@ -139,8 +146,9 @@
 #define _TIF_USER_WORK_MASK	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | \
 				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
 				 _TIF_RESTORE_TM | _TIF_PATCH_PENDING | \
-				 _TIF_FSCHECK)
+				 _TIF_FSCHECK | _TIF_NEED_RESCHED_LAZY)
 #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 
 /* Bits in local_flags */
 /* Don't move TLF_NAPPING without adjusting the code in entry_32.S */
diff -Naur a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
--- a/arch/powerpc/Kconfig	2020-11-23 13:47:55.673161029 +0200
+++ b/arch/powerpc/Kconfig	2021-07-14 15:38:54.186261090 +0300
@@ -143,6 +143,7 @@
 	select ARCH_MIGHT_HAVE_PC_SERIO
 	select ARCH_OPTIONAL_KERNEL_RWX		if ARCH_HAS_STRICT_KERNEL_RWX
 	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_SUPPORTS_RT if HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_CMPXCHG_LOCKREF		if PPC64
 	select ARCH_USE_QUEUED_RWLOCKS		if PPC_QUEUED_SPINLOCKS
@@ -225,6 +226,7 @@
 	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI && !HAVE_HARDLOCKUP_DETECTOR_ARCH
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select MMU_GATHER_RCU_TABLE_FREE
 	select MMU_GATHER_PAGE_SIZE
 	select HAVE_REGS_AND_STACK_ACCESS_API
@@ -246,6 +248,7 @@
 	select OLD_SIGSUSPEND
 	select PCI_DOMAINS			if PCI
 	select PCI_SYSCALL			if PCI
+	select HAVE_POSIX_CPU_TIMERS_TASK_WORK	if !KVM
 	select PPC_DAWR				if PPC64
 	select RTC_LIB
 	select SPARSE_IRQ
@@ -403,7 +406,7 @@
 
 config HIGHMEM
 	bool "High memory support"
-	depends on PPC32
+	depends on PPC32 && !PREEMPT_RT
 
 source "kernel/Kconfig.hz"
 
diff -Naur a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
--- a/arch/powerpc/kernel/asm-offsets.c	2020-11-23 13:47:56.089169286 +0200
+++ b/arch/powerpc/kernel/asm-offsets.c	2021-07-14 15:38:53.954262766 +0300
@@ -188,6 +188,7 @@
 	OFFSET(TI_FLAGS, thread_info, flags);
 	OFFSET(TI_LOCAL_FLAGS, thread_info, local_flags);
 	OFFSET(TI_PREEMPT, thread_info, preempt_count);
+	OFFSET(TI_PREEMPT_LAZY, thread_info, preempt_lazy_count);
 
 #ifdef CONFIG_PPC64
 	OFFSET(DCACHEL1BLOCKSIZE, ppc64_caches, l1d.block_size);
diff -Naur a/arch/powerpc/kernel/entry_32.S b/arch/powerpc/kernel/entry_32.S
--- a/arch/powerpc/kernel/entry_32.S	2020-11-23 13:47:56.105169604 +0200
+++ b/arch/powerpc/kernel/entry_32.S	2021-07-14 15:38:53.954262766 +0300
@@ -415,7 +415,9 @@
 	mtmsr	r10
 	lwz	r9,TI_FLAGS(r2)
 	li	r8,-MAX_ERRNO
-	andi.	r0,r9,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
+	lis	r0,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@h
+	ori	r0,r0, (_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@l
+	and.	r0,r9,r0
 	bne-	syscall_exit_work
 	cmplw	0,r3,r8
 	blt+	syscall_exit_cont
@@ -532,13 +534,13 @@
 	b	syscall_dotrace_cont
 
 syscall_exit_work:
-	andi.	r0,r9,_TIF_RESTOREALL
+	andis.	r0,r9,_TIF_RESTOREALL@h
 	beq+	0f
 	REST_NVGPRS(r1)
 	b	2f
 0:	cmplw	0,r3,r8
 	blt+	1f
-	andi.	r0,r9,_TIF_NOERROR
+	andis.	r0,r9,_TIF_NOERROR@h
 	bne-	1f
 	lwz	r11,_CCR(r1)			/* Load CR */
 	neg	r3,r3
@@ -547,12 +549,12 @@
 
 1:	stw	r6,RESULT(r1)	/* Save result */
 	stw	r3,GPR3(r1)	/* Update return value */
-2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)
+2:	andis.	r0,r9,(_TIF_PERSYSCALL_MASK)@h
 	beq	4f
 
 	/* Clear per-syscall TIF flags if any are set.  */
 
-	li	r11,_TIF_PERSYSCALL_MASK
+	lis	r11,(_TIF_PERSYSCALL_MASK)@h
 	addi	r12,r2,TI_FLAGS
 3:	lwarx	r8,0,r12
 	andc	r8,r8,r11
@@ -942,7 +944,14 @@
 	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
 	bne	restore_kuap
 	andi.	r8,r8,_TIF_NEED_RESCHED
+	bne+	1f
+	lwz	r0,TI_PREEMPT_LAZY(r2)
+	cmpwi	0,r0,0          /* if non-zero, just restore regs and return */
+	bne	restore_kuap
+	lwz	r0,TI_FLAGS(r2)
+	andi.	r0,r0,_TIF_NEED_RESCHED_LAZY
 	beq+	restore_kuap
+1:
 	lwz	r3,_MSR(r1)
 	andi.	r0,r3,MSR_EE	/* interrupts off? */
 	beq	restore_kuap	/* don't schedule if so */
@@ -1265,7 +1274,7 @@
 #endif /* !(CONFIG_4xx || CONFIG_BOOKE) */
 
 do_work:			/* r10 contains MSR_KERNEL here */
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	beq	do_user_signal
 
 do_resched:			/* r10 contains MSR_KERNEL here */
@@ -1286,7 +1295,7 @@
 	SYNC
 	mtmsr	r10		/* disable interrupts */
 	lwz	r9,TI_FLAGS(r2)
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	bne-	do_resched
 	andi.	r0,r9,_TIF_USER_WORK_MASK
 	beq	restore_user
diff -Naur a/arch/powerpc/kernel/exceptions-64e.S b/arch/powerpc/kernel/exceptions-64e.S
--- a/arch/powerpc/kernel/exceptions-64e.S	2020-11-23 13:47:56.109169683 +0200
+++ b/arch/powerpc/kernel/exceptions-64e.S	2021-07-14 15:38:53.958262738 +0300
@@ -1081,7 +1081,7 @@
 	li	r10, -1
 	mtspr	SPRN_DBSR,r10
 	b	restore
-1:	andi.	r0,r4,_TIF_NEED_RESCHED
+1:	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
 	beq	2f
 	bl	restore_interrupts
 	SCHEDULE_USER
@@ -1133,12 +1133,20 @@
 	bne-	0b
 1:
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	/* Check if we need to preempt */
+	lwz	r8,TI_PREEMPT(r9)
+	cmpwi	0,r8,0		/* if non-zero, just restore regs and return */
+	bne	restore
 	andi.	r0,r4,_TIF_NEED_RESCHED
+	bne+	check_count
+
+	andi.	r0,r4,_TIF_NEED_RESCHED_LAZY
 	beq+	restore
+	lwz	r8,TI_PREEMPT_LAZY(r9)
+
 	/* Check that preempt_count() == 0 and interrupts are enabled */
-	lwz	r8,TI_PREEMPT(r9)
+check_count:
 	cmpwi	cr0,r8,0
 	bne	restore
 	ld	r0,SOFTE(r1)
@@ -1159,7 +1167,7 @@
 	 * interrupted after loading SRR0/1.
 	 */
 	wrteei	0
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 restore:
 	/*
diff -Naur a/arch/powerpc/kernel/irq.c b/arch/powerpc/kernel/irq.c
--- a/arch/powerpc/kernel/irq.c	2020-11-23 13:47:56.129170080 +0200
+++ b/arch/powerpc/kernel/irq.c	2021-07-14 15:38:53.958262738 +0300
@@ -784,10 +784,12 @@
 void *softirq_ctx[NR_CPUS] __read_mostly;
 void *hardirq_ctx[NR_CPUS] __read_mostly;
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	call_do_softirq(softirq_ctx[smp_processor_id()]);
 }
+#endif
 
 irq_hw_number_t virq_to_hw(unsigned int virq)
 {
diff -Naur a/arch/powerpc/kernel/misc_32.S b/arch/powerpc/kernel/misc_32.S
--- a/arch/powerpc/kernel/misc_32.S	2020-11-23 13:47:56.137170239 +0200
+++ b/arch/powerpc/kernel/misc_32.S	2021-07-14 15:38:53.958262738 +0300
@@ -31,6 +31,7 @@
  * We store the saved ksp_limit in the unused part
  * of the STACK_FRAME_OVERHEAD
  */
+#ifndef CONFIG_PREEMPT_RT
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	stw	r0,4(r1)
@@ -46,6 +47,7 @@
 	stw	r10,THREAD+KSP_LIMIT(r2)
 	mtlr	r0
 	blr
+#endif
 
 /*
  * void call_do_irq(struct pt_regs *regs, void *sp);
diff -Naur a/arch/powerpc/kernel/misc_64.S b/arch/powerpc/kernel/misc_64.S
--- a/arch/powerpc/kernel/misc_64.S	2020-11-23 13:47:56.137170239 +0200
+++ b/arch/powerpc/kernel/misc_64.S	2021-07-14 15:38:53.958262738 +0300
@@ -27,6 +27,7 @@
 
 	.text
 
+#ifndef CONFIG_PREEMPT_RT
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	std	r0,16(r1)
@@ -37,6 +38,7 @@
 	ld	r0,16(r1)
 	mtlr	r0
 	blr
+#endif
 
 _GLOBAL(call_do_irq)
 	mflr	r0
diff -Naur a/arch/powerpc/kernel/syscall_64.c b/arch/powerpc/kernel/syscall_64.c
--- a/arch/powerpc/kernel/syscall_64.c	2020-11-23 13:47:56.177171033 +0200
+++ b/arch/powerpc/kernel/syscall_64.c	2021-07-14 15:38:53.958262738 +0300
@@ -193,7 +193,7 @@
 	ti_flags = READ_ONCE(*ti_flagsp);
 	while (unlikely(ti_flags & (_TIF_USER_WORK_MASK & ~_TIF_RESTORE_TM))) {
 		local_irq_enable();
-		if (ti_flags & _TIF_NEED_RESCHED) {
+		if (ti_flags & _TIF_NEED_RESCHED_MASK) {
 			schedule();
 		} else {
 			/*
@@ -277,7 +277,7 @@
 	ti_flags = READ_ONCE(*ti_flagsp);
 	while (unlikely(ti_flags & (_TIF_USER_WORK_MASK & ~_TIF_RESTORE_TM))) {
 		local_irq_enable(); /* returning to user: may enable */
-		if (ti_flags & _TIF_NEED_RESCHED) {
+		if (ti_flags & _TIF_NEED_RESCHED_MASK) {
 			schedule();
 		} else {
 			if (ti_flags & _TIF_SIGPENDING)
@@ -361,11 +361,14 @@
 		/* Returning to a kernel context with local irqs enabled. */
 		WARN_ON_ONCE(!(regs->msr & MSR_EE));
 again:
-		if (IS_ENABLED(CONFIG_PREEMPT)) {
+		if (IS_ENABLED(CONFIG_PREEMPTION)) {
 			/* Return to preemptible kernel context */
 			if (unlikely(*ti_flagsp & _TIF_NEED_RESCHED)) {
 				if (preempt_count() == 0)
 					preempt_schedule_irq();
+			} else if (unlikely(*ti_flagsp & _TIF_NEED_RESCHED_LAZY)) {
+				if (current_thread_info()->preempt_lazy_count == 0)
+					preempt_schedule_irq();
 			}
 		}
 
diff -Naur a/arch/powerpc/kernel/traps.c b/arch/powerpc/kernel/traps.c
--- a/arch/powerpc/kernel/traps.c	2020-11-23 13:47:56.185171192 +0200
+++ b/arch/powerpc/kernel/traps.c	2021-07-14 15:38:53.958262738 +0300
@@ -170,7 +170,6 @@
 
 extern void panic_flush_kmsg_end(void)
 {
-	printk_safe_flush_on_panic();
 	kmsg_dump(KMSG_DUMP_PANIC);
 	bust_spinlocks(0);
 	debug_locks_off();
@@ -260,12 +259,17 @@
 
 static int __die(const char *str, struct pt_regs *regs, long err)
 {
+	const char *pr = "";
+
 	printk("Oops: %s, sig: %ld [#%d]\n", str, err, ++die_counter);
 
+	if (IS_ENABLED(CONFIG_PREEMPTION))
+		pr = IS_ENABLED(CONFIG_PREEMPT_RT) ? " PREEMPT_RT" : " PREEMPT";
+
 	printk("%s PAGE_SIZE=%luK%s%s%s%s%s%s %s\n",
 	       IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN) ? "LE" : "BE",
 	       PAGE_SIZE / 1024, get_mmu_str(),
-	       IS_ENABLED(CONFIG_PREEMPT) ? " PREEMPT" : "",
+	       pr,
 	       IS_ENABLED(CONFIG_SMP) ? " SMP" : "",
 	       IS_ENABLED(CONFIG_SMP) ? (" NR_CPUS=" __stringify(NR_CPUS)) : "",
 	       debug_pagealloc_enabled() ? " DEBUG_PAGEALLOC" : "",
diff -Naur a/arch/powerpc/kernel/watchdog.c b/arch/powerpc/kernel/watchdog.c
--- a/arch/powerpc/kernel/watchdog.c	2020-11-23 13:47:56.197171430 +0200
+++ b/arch/powerpc/kernel/watchdog.c	2021-07-14 15:38:53.958262738 +0300
@@ -181,11 +181,6 @@
 
 	wd_smp_unlock(&flags);
 
-	printk_safe_flush();
-	/*
-	 * printk_safe_flush() seems to require another print
-	 * before anything actually goes out to console.
-	 */
 	if (sysctl_hardlockup_all_cpu_backtrace)
 		trigger_allbutself_cpu_backtrace();
 
diff -Naur a/arch/powerpc/kexec/crash.c b/arch/powerpc/kexec/crash.c
--- a/arch/powerpc/kexec/crash.c	2020-11-23 13:47:56.197171430 +0200
+++ b/arch/powerpc/kexec/crash.c	2021-07-14 15:38:53.962262708 +0300
@@ -311,9 +311,6 @@
 	unsigned int i;
 	int (*old_handler)(struct pt_regs *regs);
 
-	/* Avoid hardlocking with irresponsive CPU holding logbuf_lock */
-	printk_nmi_enter();
-
 	/*
 	 * This function is only called after the system
 	 * has panicked or is otherwise in a critical state.
diff -Naur a/arch/powerpc/kvm/Kconfig b/arch/powerpc/kvm/Kconfig
--- a/arch/powerpc/kvm/Kconfig	2020-11-23 13:47:56.201171510 +0200
+++ b/arch/powerpc/kvm/Kconfig	2021-07-14 15:38:53.986262535 +0300
@@ -178,6 +178,7 @@
 config KVM_MPIC
 	bool "KVM in-kernel MPIC emulation"
 	depends on KVM && E500
+	depends on !PREEMPT_RT
 	select HAVE_KVM_IRQCHIP
 	select HAVE_KVM_IRQFD
 	select HAVE_KVM_IRQ_ROUTING
diff -Naur a/arch/powerpc/platforms/pseries/iommu.c b/arch/powerpc/platforms/pseries/iommu.c
--- a/arch/powerpc/platforms/pseries/iommu.c	2020-11-23 13:47:56.589179213 +0200
+++ b/arch/powerpc/platforms/pseries/iommu.c	2021-07-14 15:38:54.158261292 +0300
@@ -24,6 +24,7 @@
 #include <linux/of.h>
 #include <linux/iommu.h>
 #include <linux/rculist.h>
+#include <linux/local_lock.h>
 #include <asm/io.h>
 #include <asm/prom.h>
 #include <asm/rtas.h>
@@ -177,6 +178,7 @@
 }
 
 static DEFINE_PER_CPU(__be64 *, tce_page);
+static DEFINE_LOCAL_IRQ_LOCK(tcp_page_lock);
 
 static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 				     long npages, unsigned long uaddr,
@@ -198,7 +200,8 @@
 		                           direction, attrs);
 	}
 
-	local_irq_save(flags);	/* to protect tcep and the page behind it */
+	/* to protect tcep and the page behind it */
+	local_lock_irqsave(tcp_page_lock, flags);
 
 	tcep = __this_cpu_read(tce_page);
 
@@ -209,7 +212,7 @@
 		tcep = (__be64 *)__get_free_page(GFP_ATOMIC);
 		/* If allocation fails, fall back to the loop implementation */
 		if (!tcep) {
-			local_irq_restore(flags);
+			local_unlock_irqrestore(tcp_page_lock, flags);
 			return tce_build_pSeriesLP(tbl->it_index, tcenum,
 					tbl->it_page_shift,
 					npages, uaddr, direction, attrs);
@@ -244,7 +247,7 @@
 		tcenum += limit;
 	} while (npages > 0 && !rc);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(tcp_page_lock, flags);
 
 	if (unlikely(rc == H_NOT_ENOUGH_RESOURCES)) {
 		ret = (int)rc;
@@ -415,13 +418,14 @@
 				DMA_BIDIRECTIONAL, 0);
 	}
 
-	local_irq_disable();	/* to protect tcep and the page behind it */
+	/* to protect tcep and the page behind it */
+	local_lock_irq(tcp_page_lock);
 	tcep = __this_cpu_read(tce_page);
 
 	if (!tcep) {
 		tcep = (__be64 *)__get_free_page(GFP_ATOMIC);
 		if (!tcep) {
-			local_irq_enable();
+			local_unlock_irq(tcp_page_lock);
 			return -ENOMEM;
 		}
 		__this_cpu_write(tce_page, tcep);
@@ -467,7 +471,7 @@
 
 	/* error cleanup: caller will clear whole range */
 
-	local_irq_enable();
+	local_unlock_irq(tcp_page_lock);
 	return rc;
 }
 
diff -Naur a/arch/s390/include/asm/spinlock_types.h b/arch/s390/include/asm/spinlock_types.h
--- a/arch/s390/include/asm/spinlock_types.h	2020-11-23 13:47:56.813183660 +0200
+++ b/arch/s390/include/asm/spinlock_types.h	2021-07-14 15:38:54.298260281 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	int lock;
 } __attribute__ ((aligned (4))) arch_spinlock_t;
diff -Naur a/arch/sh/include/asm/spinlock_types.h b/arch/sh/include/asm/spinlock_types.h
--- a/arch/sh/include/asm/spinlock_types.h	2020-11-23 13:47:57.085189061 +0200
+++ b/arch/sh/include/asm/spinlock_types.h	2021-07-14 15:38:54.438259269 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SH_SPINLOCK_TYPES_H
 #define __ASM_SH_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Naur a/arch/sh/kernel/irq.c b/arch/sh/kernel/irq.c
--- a/arch/sh/kernel/irq.c	2020-11-23 13:47:57.193191205 +0200
+++ b/arch/sh/kernel/irq.c	2021-07-14 15:38:54.510258749 +0300
@@ -148,6 +148,7 @@
 	hardirq_ctx[cpu] = NULL;
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	struct thread_info *curctx;
@@ -175,6 +176,7 @@
 		  "r5", "r6", "r7", "r8", "r9", "r15", "t", "pr"
 	);
 }
+#endif
 #else
 static inline void handle_one_irq(unsigned int irq)
 {
diff -Naur a/arch/sparc/kernel/irq_64.c b/arch/sparc/kernel/irq_64.c
--- a/arch/sparc/kernel/irq_64.c	2020-11-23 13:47:57.461196528 +0200
+++ b/arch/sparc/kernel/irq_64.c	2021-07-14 15:38:54.650257738 +0300
@@ -854,6 +854,7 @@
 	set_irq_regs(old_regs);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	void *orig_sp, *sp = softirq_stack[smp_processor_id()];
@@ -868,6 +869,7 @@
 	__asm__ __volatile__("mov %0, %%sp"
 			     : : "r" (orig_sp));
 }
+#endif
 
 #ifdef CONFIG_HOTPLUG_CPU
 void fixup_irqs(void)
diff -Naur a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
--- a/arch/x86/crypto/aesni-intel_glue.c	2020-11-23 13:47:57.749202248 +0200
+++ b/arch/x86/crypto/aesni-intel_glue.c	2021-07-14 15:38:54.778256814 +0300
@@ -376,14 +376,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -398,14 +398,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -420,14 +420,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -442,14 +442,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -497,18 +497,20 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+		kernel_fpu_begin();
 		aesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			              nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 	if (walk.nbytes) {
+		kernel_fpu_begin();
 		ctr_crypt_final(ctx, &walk);
+		kernel_fpu_end();
 		err = skcipher_walk_done(&walk, 0);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
diff -Naur a/arch/x86/crypto/cast5_avx_glue.c b/arch/x86/crypto/cast5_avx_glue.c
--- a/arch/x86/crypto/cast5_avx_glue.c	2020-11-23 13:47:57.761202486 +0200
+++ b/arch/x86/crypto/cast5_avx_glue.c	2021-07-14 15:38:54.778256814 +0300
@@ -46,7 +46,7 @@
 
 static int ecb_crypt(struct skcipher_request *req, bool enc)
 {
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
 	struct skcipher_walk walk;
@@ -61,7 +61,7 @@
 		u8 *wsrc = walk.src.virt.addr;
 		u8 *wdst = walk.dst.virt.addr;
 
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 
 		/* Process multi-block batch */
 		if (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {
@@ -90,10 +90,9 @@
 		} while (nbytes >= bsize);
 
 done:
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -197,7 +196,7 @@
 {
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
@@ -205,12 +204,11 @@
 	err = skcipher_walk_virt(&walk, req, false);
 
 	while ((nbytes = walk.nbytes)) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 		nbytes = __cbc_decrypt(ctx, &walk);
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -277,7 +275,7 @@
 {
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
@@ -285,13 +283,12 @@
 	err = skcipher_walk_virt(&walk, req, false);
 
 	while ((nbytes = walk.nbytes) >= CAST5_BLOCK_SIZE) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 		nbytes = __ctr_crypt(&walk, ctx);
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	cast5_fpu_end(fpu_enabled);
-
 	if (walk.nbytes) {
 		ctr_crypt_final(&walk, ctx);
 		err = skcipher_walk_done(&walk, 0);
diff -Naur a/arch/x86/crypto/glue_helper.c b/arch/x86/crypto/glue_helper.c
--- a/arch/x86/crypto/glue_helper.c	2020-11-23 13:47:57.885204948 +0200
+++ b/arch/x86/crypto/glue_helper.c	2021-07-14 15:38:54.778256814 +0300
@@ -24,7 +24,7 @@
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -37,7 +37,7 @@
 		unsigned int i;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 		for (i = 0; i < gctx->num_funcs; i++) {
 			func_bytes = bsize * gctx->funcs[i].num_blocks;
 
@@ -55,10 +55,9 @@
 			if (nbytes < bsize)
 				break;
 		}
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_ecb_req_128bit);
@@ -101,7 +100,7 @@
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -115,7 +114,7 @@
 		u128 last_iv;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 		/* Start of the last block. */
 		src += nbytes / bsize - 1;
 		dst += nbytes / bsize - 1;
@@ -148,10 +147,10 @@
 done:
 		u128_xor(dst, dst, (u128 *)walk.iv);
 		*(u128 *)walk.iv = last_iv;
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_cbc_decrypt_req_128bit);
@@ -162,7 +161,7 @@
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -176,7 +175,7 @@
 		le128 ctrblk;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 
 		be128_to_le128(&ctrblk, (be128 *)walk.iv);
 
@@ -202,11 +201,10 @@
 		}
 
 		le128_to_be128((be128 *)walk.iv, &ctrblk);
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
-
 	if (nbytes) {
 		le128 ctrblk;
 		u128 tmp;
@@ -306,8 +304,14 @@
 	tweak_fn(tweak_ctx, walk.iv, walk.iv);
 
 	while (nbytes) {
+		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+					     &walk, fpu_enabled,
+					     nbytes < bsize ? bsize : nbytes);
 		nbytes = __glue_xts_req_128bit(gctx, crypt_ctx, &walk);
 
+		glue_fpu_end(fpu_enabled);
+		fpu_enabled = false;
+
 		err = skcipher_walk_done(&walk, nbytes);
 		nbytes = walk.nbytes;
 	}
diff -Naur a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
--- a/arch/x86/include/asm/fpu/api.h	2020-11-23 13:47:58.033207888 +0200
+++ b/arch/x86/include/asm/fpu/api.h	2021-07-14 15:38:54.826256467 +0300
@@ -23,6 +23,7 @@
 extern void kernel_fpu_end(void);
 extern bool irq_fpu_usable(void);
 extern void fpregs_mark_activate(void);
+extern void kernel_fpu_resched(void);
 
 /*
  * Use fpregs_lock() while editing CPU's FPU registers or fpu->state.
@@ -33,12 +34,18 @@
 static inline void fpregs_lock(void)
 {
 	preempt_disable();
-	local_bh_disable();
+	/*
+	 * On RT disabling preemption is good enough because bottom halfs
+	 * are always running in thread context.
+	 */
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_bh_disable();
 }
 
 static inline void fpregs_unlock(void)
 {
-	local_bh_enable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_bh_enable();
 	preempt_enable();
 }
 
diff -Naur a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h
--- a/arch/x86/include/asm/preempt.h	2020-11-23 13:47:58.105209318 +0200
+++ b/arch/x86/include/asm/preempt.h	2021-07-14 15:38:54.894255975 +0300
@@ -89,20 +89,54 @@
  * a decrement which hits zero means we have no preempt_count and should
  * reschedule.
  */
-static __always_inline bool __preempt_count_dec_and_test(void)
+static __always_inline bool ____preempt_count_dec_and_test(void)
 {
 	return GEN_UNARY_RMWcc("decl", __preempt_count, e, __percpu_arg([var]));
 }
 
+static __always_inline bool __preempt_count_dec_and_test(void)
+{
+	if (____preempt_count_dec_and_test())
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if (preempt_count())
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
+}
+
 /*
  * Returns true when we need to resched and can (barring IRQ state).
  */
 static __always_inline bool should_resched(int preempt_offset)
 {
+#ifdef CONFIG_PREEMPT_LAZY
+	u32 tmp;
+	tmp = raw_cpu_read_4(__preempt_count);
+	if (tmp == preempt_offset)
+		return true;
+
+	/* preempt count == 0 ? */
+	tmp &= ~PREEMPT_NEED_RESCHED;
+	if (tmp != preempt_offset)
+		return false;
+	/* XXX PREEMPT_LOCK_OFFSET */
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
 	return unlikely(raw_cpu_read_4(__preempt_count) == preempt_offset);
+#endif
 }
 
 #ifdef CONFIG_PREEMPTION
+#ifdef CONFIG_PREEMPT_RT
+  extern void preempt_schedule_lock(void);
+#endif
   extern asmlinkage void preempt_schedule_thunk(void);
 # define __preempt_schedule() \
 	asm volatile ("call preempt_schedule_thunk" : ASM_CALL_CONSTRAINT)
diff -Naur a/arch/x86/include/asm/signal.h b/arch/x86/include/asm/signal.h
--- a/arch/x86/include/asm/signal.h	2020-11-23 13:47:58.117209557 +0200
+++ b/arch/x86/include/asm/signal.h	2021-07-14 15:38:54.894255975 +0300
@@ -28,6 +28,19 @@
 #define SA_IA32_ABI	0x02000000u
 #define SA_X32_ABI	0x01000000u
 
+/*
+ * Because some traps use the IST stack, we must keep preemption
+ * disabled while calling do_trap(), but do_trap() may call
+ * force_sig_info() which will grab the signal spin_locks for the
+ * task, which in PREEMPT_RT are mutexes.  By defining
+ * ARCH_RT_DELAYS_SIGNAL_SEND the force_sig_info() will set
+ * TIF_NOTIFY_RESUME and set up the signal to be sent on exit of the
+ * trap.
+ */
+#if defined(CONFIG_PREEMPT_RT)
+#define ARCH_RT_DELAYS_SIGNAL_SEND
+#endif
+
 #ifndef CONFIG_COMPAT
 typedef sigset_t compat_sigset_t;
 #endif
diff -Naur a/arch/x86/include/asm/stackprotector.h b/arch/x86/include/asm/stackprotector.h
--- a/arch/x86/include/asm/stackprotector.h	2020-11-23 13:47:58.121209636 +0200
+++ b/arch/x86/include/asm/stackprotector.h	2021-07-14 15:38:54.894255975 +0300
@@ -65,7 +65,7 @@
  */
 static __always_inline void boot_init_stack_canary(void)
 {
-	u64 canary;
+	u64 canary = 0;
 	u64 tsc;
 
 #ifdef CONFIG_X86_64
@@ -76,8 +76,14 @@
 	 * of randomness. The TSC only matters for very early init,
 	 * there it already has some randomness on most systems. Later
 	 * on during the bootup the random pool has true entropy too.
+	 * For preempt-rt we need to weaken the randomness a bit, as
+	 * we can't call into the random generator from atomic context
+	 * due to locking constraints. We just leave canary
+	 * uninitialized and use the TSC based randomness on top of it.
 	 */
+#ifndef CONFIG_PREEMPT_RT
 	get_random_bytes(&canary, sizeof(canary));
+#endif
 	tsc = rdtsc();
 	canary += tsc + (tsc << 32UL);
 	canary &= CANARY_MASK;
diff -Naur a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
--- a/arch/x86/include/asm/thread_info.h	2020-11-23 13:47:58.129209795 +0200
+++ b/arch/x86/include/asm/thread_info.h	2021-07-14 15:38:54.894255975 +0300
@@ -56,17 +56,24 @@
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	u32			status;		/* thread synchronous flags */
+	int			preempt_lazy_count;	/* 0 => lazy preemptable
+							  <0 => BUG */
 };
 
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.flags		= 0,			\
+	.preempt_lazy_count = 0,		\
 }
 
 #else /* !__ASSEMBLY__ */
 
 #include <asm/asm-offsets.h>
 
+#define GET_THREAD_INFO(reg) \
+	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \
+	_ASM_SUB $(THREAD_SIZE),reg ;
+
 #endif
 
 /*
@@ -93,6 +100,7 @@
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_SLD			18	/* Restore split lock detection on context switch */
+#define TIF_NEED_RESCHED_LAZY	19	/* lazy rescheduling necessary */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
@@ -123,6 +131,7 @@
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_SLD		(1 << TIF_SLD)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
@@ -156,6 +165,8 @@
 
 #define _TIF_WORK_CTXSW_NEXT	(_TIF_WORK_CTXSW)
 
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
+
 #define STACK_WARN		(THREAD_SIZE/8)
 
 /*
diff -Naur a/arch/x86/Kconfig b/arch/x86/Kconfig
--- a/arch/x86/Kconfig	2020-11-23 13:47:57.705201374 +0200
+++ b/arch/x86/Kconfig	2021-07-14 15:38:54.906255889 +0300
@@ -92,6 +92,7 @@
 	select ARCH_SUPPORTS_ACPI
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
+	select ARCH_SUPPORTS_RT
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_QUEUED_RWLOCKS
 	select ARCH_USE_QUEUED_SPINLOCKS
@@ -208,6 +209,7 @@
 	select HAVE_PCI
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select MMU_GATHER_RCU_TABLE_FREE		if PARAVIRT
 	select HAVE_POSIX_CPU_TIMERS_TASK_WORK
 	select HAVE_REGS_AND_STACK_ACCESS_API
diff -Naur a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c
--- a/arch/x86/kernel/cpu/mshyperv.c	2020-11-23 13:47:58.301213212 +0200
+++ b/arch/x86/kernel/cpu/mshyperv.c	2021-07-14 15:38:54.934255687 +0300
@@ -75,11 +75,12 @@
 DEFINE_IDTENTRY_SYSVEC(sysvec_hyperv_stimer0)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 
 	inc_irq_stat(hyperv_stimer0_count);
 	if (hv_stimer0_handler)
 		hv_stimer0_handler();
-	add_interrupt_randomness(HYPERV_STIMER0_VECTOR, 0);
+	add_interrupt_randomness(HYPERV_STIMER0_VECTOR, 0, ip);
 	ack_APIC_irq();
 
 	set_irq_regs(old_regs);
diff -Naur a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
--- a/arch/x86/kernel/fpu/core.c	2020-11-23 13:47:58.337213927 +0200
+++ b/arch/x86/kernel/fpu/core.c	2021-07-14 15:38:54.942255629 +0300
@@ -158,6 +158,18 @@
 }
 EXPORT_SYMBOL_GPL(kernel_fpu_end);
 
+void kernel_fpu_resched(void)
+{
+	WARN_ON_FPU(!this_cpu_read(in_kernel_fpu));
+
+	if (should_resched(PREEMPT_OFFSET)) {
+		kernel_fpu_end();
+		cond_resched();
+		kernel_fpu_begin();
+	}
+}
+EXPORT_SYMBOL_GPL(kernel_fpu_resched);
+
 /*
  * Save the FPU state (mark it for reload if necessary):
  *
diff -Naur a/arch/x86/kernel/irq_32.c b/arch/x86/kernel/irq_32.c
--- a/arch/x86/kernel/irq_32.c	2020-11-23 13:47:58.361214404 +0200
+++ b/arch/x86/kernel/irq_32.c	2021-07-14 15:38:54.974255398 +0300
@@ -131,6 +131,7 @@
 	return 0;
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	struct irq_stack *irqstk;
@@ -147,6 +148,7 @@
 
 	call_on_stack(__do_softirq, isp);
 }
+#endif
 
 void __handle_irq(struct irq_desc *desc, struct pt_regs *regs)
 {
diff -Naur a/arch/x86/kernel/irq_64.c b/arch/x86/kernel/irq_64.c
--- a/arch/x86/kernel/irq_64.c	2020-11-23 13:47:58.361214404 +0200
+++ b/arch/x86/kernel/irq_64.c	2021-07-14 15:38:54.974255398 +0300
@@ -72,7 +72,9 @@
 	return map_irq_stack(cpu);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	run_on_irqstack_cond(__do_softirq, NULL);
 }
+#endif
diff -Naur a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c
--- a/arch/x86/kernel/process_32.c	2020-11-23 13:47:58.385214881 +0200
+++ b/arch/x86/kernel/process_32.c	2021-07-14 15:38:54.974255398 +0300
@@ -38,6 +38,7 @@
 #include <linux/io.h>
 #include <linux/kdebug.h>
 #include <linux/syscalls.h>
+#include <linux/highmem.h>
 
 #include <asm/ldt.h>
 #include <asm/processor.h>
@@ -126,6 +127,35 @@
 }
 EXPORT_SYMBOL_GPL(start_thread);
 
+#ifdef CONFIG_PREEMPT_RT
+static void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	int i;
+
+	/*
+	 * Clear @prev's kmap_atomic mappings
+	 */
+	for (i = 0; i < prev_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
+		pte_t *ptep = kmap_pte - idx;
+
+		kpte_clear_flush(ptep, __fix_to_virt(FIX_KMAP_BEGIN + idx));
+	}
+	/*
+	 * Restore @next_p's kmap_atomic mappings
+	 */
+	for (i = 0; i < next_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
+
+		if (!pte_none(next_p->kmap_pte[i]))
+			set_pte(kmap_pte - idx, next_p->kmap_pte[i]);
+	}
+}
+#else
+static inline void
+switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
+#endif
+
 
 /*
  *	switch_to(x,y) should switch tasks from x to y.
@@ -187,6 +217,8 @@
 
 	switch_to_extra(prev_p, next_p);
 
+	switch_kmaps(prev_p, next_p);
+
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so
diff -Naur a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
--- a/arch/x86/kernel/tsc.c	2020-11-23 13:47:58.405215278 +0200
+++ b/arch/x86/kernel/tsc.c	2021-07-14 15:38:54.974255398 +0300
@@ -54,7 +54,7 @@
 
 struct cyc2ns {
 	struct cyc2ns_data data[2];	/*  0 + 2*16 = 32 */
-	seqcount_t	   seq;		/* 32 + 4    = 36 */
+	seqcount_latch_t   seq;		/* 32 + 4    = 36 */
 
 }; /* fits one cacheline */
 
@@ -73,14 +73,14 @@
 	preempt_disable_notrace();
 
 	do {
-		seq = this_cpu_read(cyc2ns.seq.sequence);
+		seq = this_cpu_read(cyc2ns.seq.seqcount.sequence);
 		idx = seq & 1;
 
 		data->cyc2ns_offset = this_cpu_read(cyc2ns.data[idx].cyc2ns_offset);
 		data->cyc2ns_mul    = this_cpu_read(cyc2ns.data[idx].cyc2ns_mul);
 		data->cyc2ns_shift  = this_cpu_read(cyc2ns.data[idx].cyc2ns_shift);
 
-	} while (unlikely(seq != this_cpu_read(cyc2ns.seq.sequence)));
+	} while (unlikely(seq != this_cpu_read(cyc2ns.seq.seqcount.sequence)));
 }
 
 __always_inline void cyc2ns_read_end(void)
@@ -186,7 +186,7 @@
 {
 	struct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);
 
-	seqcount_init(&c2n->seq);
+	seqcount_latch_init(&c2n->seq);
 	__set_cyc2ns_scale(tsc_khz, smp_processor_id(), rdtsc());
 }
 
@@ -203,7 +203,7 @@
 
 	for_each_possible_cpu(cpu) {
 		if (cpu != this_cpu) {
-			seqcount_init(&c2n->seq);
+			seqcount_latch_init(&c2n->seq);
 			c2n = per_cpu_ptr(&cyc2ns, cpu);
 			c2n->data[0] = data[0];
 			c2n->data[1] = data[1];
diff -Naur a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
--- a/arch/x86/kvm/x86.c	2020-11-23 13:47:58.509217344 +0200
+++ b/arch/x86/kvm/x86.c	2021-07-14 15:38:54.998255224 +0300
@@ -7513,6 +7513,14 @@
 		goto out;
 	}
 
+#ifdef CONFIG_PREEMPT_RT
+	if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {
+		pr_err("RT requires X86_FEATURE_CONSTANT_TSC\n");
+		r = -EOPNOTSUPP;
+		goto out;
+	}
+#endif
+
 	r = -ENOMEM;
 	x86_fpu_cache = kmem_cache_create("x86_fpu", sizeof(struct fpu),
 					  __alignof__(struct fpu), SLAB_ACCOUNT,
diff -Naur a/arch/x86/mm/highmem_32.c b/arch/x86/mm/highmem_32.c
--- a/arch/x86/mm/highmem_32.c	2020-11-23 13:47:58.561218377 +0200
+++ b/arch/x86/mm/highmem_32.c	2021-07-14 15:38:55.038254936 +0300
@@ -8,12 +8,17 @@
 {
 	unsigned long vaddr;
 	int idx, type;
+	pte_t pte;
 
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR*smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 	BUG_ON(!pte_none(*(kmap_pte-idx)));
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
+	pte = mk_pte(page, prot);
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_pte(kmap_pte-idx, pte);
 	arch_flush_lazy_mmu_mode();
 
 	return (void *)vaddr;
@@ -50,6 +55,9 @@
 		 * is a bad idea also, in case the page changes cacheability
 		 * attributes or becomes a protected page in a hypervisor.
 		 */
+#ifdef CONFIG_PREEMPT_RT
+		current->kmap_pte[type] = __pte(0);
+#endif
 		kpte_clear_flush(kmap_pte-idx, vaddr);
 		kmap_atomic_idx_pop();
 		arch_flush_lazy_mmu_mode();
diff -Naur a/arch/x86/mm/iomap_32.c b/arch/x86/mm/iomap_32.c
--- a/arch/x86/mm/iomap_32.c	2020-11-23 13:47:58.565218456 +0200
+++ b/arch/x86/mm/iomap_32.c	2021-07-14 15:38:55.038254936 +0300
@@ -46,16 +46,22 @@
 
 void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 {
+	pte_t pte = pfn_pte(pfn, prot);
 	unsigned long vaddr;
 	int idx, type;
 
-	preempt_disable();
+	migrate_disable();
 	pagefault_disable();
 
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR * smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
+	WARN_ON(!pte_none(*(kmap_pte - idx)));
+
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_pte(kmap_pte - idx, pte);
 	arch_flush_lazy_mmu_mode();
 
 	return (void *)vaddr;
@@ -106,11 +112,14 @@
 		 * is a bad idea also, in case the page changes cacheability
 		 * attributes or becomes a protected page in a hypervisor.
 		 */
+#ifdef CONFIG_PREEMPT_RT
+		current->kmap_pte[type] = __pte(0);
+#endif
 		kpte_clear_flush(kmap_pte-idx, vaddr);
 		kmap_atomic_idx_pop();
 	}
 
 	pagefault_enable();
-	preempt_enable();
+	migrate_enable();
 }
 EXPORT_SYMBOL_GPL(iounmap_atomic);
diff -Naur a/arch/xtensa/include/asm/spinlock_types.h b/arch/xtensa/include/asm/spinlock_types.h
--- a/arch/xtensa/include/asm/spinlock_types.h	2020-11-23 13:47:58.777222668 +0200
+++ b/arch/xtensa/include/asm/spinlock_types.h	2021-07-14 15:38:55.146254158 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__ASM_SPINLOCK_H)
-# error "please don't include this file directly"
-#endif
-
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
 
diff -Naur a/block/blk-mq.c b/block/blk-mq.c
--- a/block/blk-mq.c	2020-11-23 13:47:58.909225290 +0200
+++ b/block/blk-mq.c	2021-07-14 15:38:55.218253638 +0300
@@ -604,6 +604,7 @@
 	if (list->next == &rq->ipi_list)
 		raise_softirq_irqoff(BLOCK_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 static int blk_softirq_cpu_dead(unsigned int cpu)
@@ -617,6 +618,7 @@
 			 this_cpu_ptr(&blk_cpu_done));
 	raise_softirq_irqoff(BLOCK_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 	return 0;
 }
@@ -1603,14 +1605,14 @@
 		return;
 
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
-		int cpu = get_cpu();
+		int cpu = get_cpu_light();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
 			__blk_mq_run_hw_queue(hctx);
-			put_cpu();
+			put_cpu_light();
 			return;
 		}
 
-		put_cpu();
+		put_cpu_light();
 	}
 
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
diff -Naur a/crypto/cryptd.c b/crypto/cryptd.c
--- a/crypto/cryptd.c	2020-11-23 13:47:58.993226959 +0200
+++ b/crypto/cryptd.c	2021-07-14 15:38:55.314252946 +0300
@@ -36,6 +36,7 @@
 struct cryptd_cpu_queue {
 	struct crypto_queue queue;
 	struct work_struct work;
+	spinlock_t qlock;
 };
 
 struct cryptd_queue {
@@ -105,6 +106,7 @@
 		cpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);
 		crypto_init_queue(&cpu_queue->queue, max_cpu_qlen);
 		INIT_WORK(&cpu_queue->work, cryptd_queue_worker);
+		spin_lock_init(&cpu_queue->qlock);
 	}
 	pr_info("cryptd: max_cpu_qlen set to %d\n", max_cpu_qlen);
 	return 0;
@@ -129,8 +131,10 @@
 	struct cryptd_cpu_queue *cpu_queue;
 	refcount_t *refcnt;
 
-	cpu = get_cpu();
-	cpu_queue = this_cpu_ptr(queue->cpu_queue);
+	cpu_queue = raw_cpu_ptr(queue->cpu_queue);
+	spin_lock_bh(&cpu_queue->qlock);
+	cpu = smp_processor_id();
+
 	err = crypto_enqueue_request(&cpu_queue->queue, request);
 
 	refcnt = crypto_tfm_ctx(request->tfm);
@@ -146,7 +150,7 @@
 	refcount_inc(refcnt);
 
 out_put_cpu:
-	put_cpu();
+	spin_unlock_bh(&cpu_queue->qlock);
 
 	return err;
 }
@@ -162,16 +166,11 @@
 	cpu_queue = container_of(work, struct cryptd_cpu_queue, work);
 	/*
 	 * Only handle one request at a time to avoid hogging crypto workqueue.
-	 * preempt_disable/enable is used to prevent being preempted by
-	 * cryptd_enqueue_request(). local_bh_disable/enable is used to prevent
-	 * cryptd_enqueue_request() being accessed from software interrupts.
 	 */
-	local_bh_disable();
-	preempt_disable();
+	spin_lock_bh(&cpu_queue->qlock);
 	backlog = crypto_get_backlog(&cpu_queue->queue);
 	req = crypto_dequeue_request(&cpu_queue->queue);
-	preempt_enable();
-	local_bh_enable();
+	spin_unlock_bh(&cpu_queue->qlock);
 
 	if (!req)
 		return;
diff -Naur a/Documentation/locking/seqlock.rst b/Documentation/locking/seqlock.rst
--- a/Documentation/locking/seqlock.rst	2020-11-23 13:47:49.285034360 +0200
+++ b/Documentation/locking/seqlock.rst	2021-07-14 15:38:50.482287940 +0300
@@ -139,6 +139,24 @@
 
 Read path: same as in :ref:`seqcount_t`.
 
+
+.. _seqcount_latch_t:
+
+Latch sequence counters (``seqcount_latch_t``)
+----------------------------------------------
+
+Latch sequence counters are a multiversion concurrency control mechanism
+where the embedded seqcount_t counter even/odd value is used to switch
+between two copies of protected data. This allows the sequence counter
+read path to safely interrupt its own write side critical section.
+
+Use seqcount_latch_t when the write side sections cannot be protected
+from interruption by readers. This is typically the case when the read
+side can be invoked from NMI handlers.
+
+Check `raw_write_seqcount_latch()` for more information.
+
+
 .. _seqlock_t:
 
 Sequential locks (``seqlock_t``)
diff -Naur a/Documentation/printk-ringbuffer.txt b/Documentation/printk-ringbuffer.txt
--- a/Documentation/printk-ringbuffer.txt	1970-01-01 02:00:00.000000000 +0200
+++ b/Documentation/printk-ringbuffer.txt	2021-07-14 15:38:49.130297780 +0300
@@ -0,0 +1,377 @@
+struct printk_ringbuffer
+------------------------
+John Ogness <john.ogness@linutronix.de>
+
+Overview
+~~~~~~~~
+As the name suggests, this ring buffer was implemented specifically to serve
+the needs of the printk() infrastructure. The ring buffer itself is not
+specific to printk and could be used for other purposes. _However_, the
+requirements and semantics of printk are rather unique. If you intend to use
+this ring buffer for anything other than printk, you need to be very clear on
+its features, behavior, and pitfalls.
+
+Features
+^^^^^^^^
+The printk ring buffer has the following features:
+
+- single global buffer
+- resides in initialized data section (available at early boot)
+- lockless readers
+- supports multiple writers
+- supports multiple non-consuming readers
+- safe from any context (including NMI)
+- groups bytes into variable length blocks (referenced by entries)
+- entries tagged with sequence numbers
+
+Behavior
+^^^^^^^^
+Since the printk ring buffer readers are lockless, there exists no
+synchronization between readers and writers. Basically writers are the tasks
+in control and may overwrite any and all committed data at any time and from
+any context. For this reason readers can miss entries if they are overwritten
+before the reader was able to access the data. The reader API implementation
+is such that reader access to entries is atomic, so there is no risk of
+readers having to deal with partial or corrupt data. Also, entries are
+tagged with sequence numbers so readers can recognize if entries were missed.
+
+Writing to the ring buffer consists of 2 steps. First a writer must reserve
+an entry of desired size. After this step the writer has exclusive access
+to the memory region. Once the data has been written to memory, it needs to
+be committed to the ring buffer. After this step the entry has been inserted
+into the ring buffer and assigned an appropriate sequence number.
+
+Once committed, a writer must no longer access the data directly. This is
+because the data may have been overwritten and no longer exists. If a
+writer must access the data, it should either keep a private copy before
+committing the entry or use the reader API to gain access to the data.
+
+Because of how the data backend is implemented, entries that have been
+reserved but not yet committed act as barriers, preventing future writers
+from filling the ring buffer beyond the location of the reserved but not
+yet committed entry region. For this reason it is *important* that writers
+perform both reserve and commit as quickly as possible. Also, be aware that
+preemption and local interrupts are disabled and writing to the ring buffer
+is processor-reentrant locked during the reserve/commit window. Writers in
+NMI contexts can still preempt any other writers, but as long as these
+writers do not write a large amount of data with respect to the ring buffer
+size, this should not become an issue.
+
+API
+~~~
+
+Declaration
+^^^^^^^^^^^
+The printk ring buffer can be instantiated as a static structure:
+
+ /* declare a static struct printk_ringbuffer */
+ #define DECLARE_STATIC_PRINTKRB(name, szbits, cpulockptr)
+
+The value of szbits specifies the size of the ring buffer in bits. The
+cpulockptr field is a pointer to a prb_cpulock struct that is used to
+perform processor-reentrant spin locking for the writers. It is specified
+externally because it may be used for multiple ring buffers (or other
+code) to synchronize writers without risk of deadlock.
+
+Here is an example of a declaration of a printk ring buffer specifying a
+32KB (2^15) ring buffer:
+
+....
+DECLARE_STATIC_PRINTKRB_CPULOCK(rb_cpulock);
+DECLARE_STATIC_PRINTKRB(rb, 15, &rb_cpulock);
+....
+
+If writers will be using multiple ring buffers and the ordering of that usage
+is not clear, the same prb_cpulock should be used for both ring buffers.
+
+Writer API
+^^^^^^^^^^
+The writer API consists of 2 functions. The first is to reserve an entry in
+the ring buffer, the second is to commit that data to the ring buffer. The
+reserved entry information is stored within a provided `struct prb_handle`.
+
+ /* reserve an entry */
+ char *prb_reserve(struct prb_handle *h, struct printk_ringbuffer *rb,
+                   unsigned int size);
+
+ /* commit a reserved entry to the ring buffer */
+ void prb_commit(struct prb_handle *h);
+
+Here is an example of a function to write data to a ring buffer:
+
+....
+int write_data(struct printk_ringbuffer *rb, char *data, int size)
+{
+    struct prb_handle h;
+    char *buf;
+
+    buf = prb_reserve(&h, rb, size);
+    if (!buf)
+        return -1;
+    memcpy(buf, data, size);
+    prb_commit(&h);
+
+    return 0;
+}
+....
+
+Pitfalls
+++++++++
+Be aware that prb_reserve() can fail. A retry might be successful, but it
+depends entirely on whether or not the next part of the ring buffer to
+overwrite belongs to reserved but not yet committed entries of other writers.
+Writers can use the prb_inc_lost() function to allow readers to notice that a
+message was lost.
+
+Reader API
+^^^^^^^^^^
+The reader API utilizes a `struct prb_iterator` to track the reader's
+position in the ring buffer.
+
+ /* declare a pre-initialized static iterator for a ring buffer */
+ #define DECLARE_STATIC_PRINTKRB_ITER(name, rbaddr)
+
+ /* initialize iterator for a ring buffer (if static macro NOT used) */
+ void prb_iter_init(struct prb_iterator *iter,
+                    struct printk_ringbuffer *rb, u64 *seq);
+
+ /* make a deep copy of an iterator */
+ void prb_iter_copy(struct prb_iterator *dest,
+                    struct prb_iterator *src);
+
+ /* non-blocking, advance to next entry (and read the data) */
+ int prb_iter_next(struct prb_iterator *iter, char *buf,
+                   int size, u64 *seq);
+
+ /* blocking, advance to next entry (and read the data) */
+ int prb_iter_wait_next(struct prb_iterator *iter, char *buf,
+                        int size, u64 *seq);
+
+ /* position iterator at the entry seq */
+ int prb_iter_seek(struct prb_iterator *iter, u64 seq);
+
+ /* read data at current position */
+ int prb_iter_data(struct prb_iterator *iter, char *buf,
+                   int size, u64 *seq);
+
+Typically prb_iter_data() is not needed because the data can be retrieved
+directly with prb_iter_next().
+
+Here is an example of a non-blocking function that will read all the data in
+a ring buffer:
+
+....
+void read_all_data(struct printk_ringbuffer *rb, char *buf, int size)
+{
+    struct prb_iterator iter;
+    u64 prev_seq = 0;
+    u64 seq;
+    int ret;
+
+    prb_iter_init(&iter, rb, NULL);
+
+    for (;;) {
+        ret = prb_iter_next(&iter, buf, size, &seq);
+        if (ret > 0) {
+            if (seq != ++prev_seq) {
+                /* "seq - prev_seq" entries missed */
+                prev_seq = seq;
+            }
+            /* process buf here */
+        } else if (ret == 0) {
+            /* hit the end, done */
+            break;
+        } else if (ret < 0) {
+            /*
+             * iterator is invalid, a writer overtook us, reset the
+             * iterator and keep going, entries were missed
+             */
+            prb_iter_init(&iter, rb, NULL);
+        }
+    }
+}
+....
+
+Pitfalls
+++++++++
+The reader's iterator can become invalid at any time because the reader was
+overtaken by a writer. Typically the reader should reset the iterator back
+to the current oldest entry (which will be newer than the entry the reader
+was at) and continue, noting the number of entries that were missed.
+
+Utility API
+^^^^^^^^^^^
+Several functions are available as convenience for external code.
+
+ /* query the size of the data buffer */
+ int prb_buffer_size(struct printk_ringbuffer *rb);
+
+ /* skip a seq number to signify a lost record */
+ void prb_inc_lost(struct printk_ringbuffer *rb);
+
+ /* processor-reentrant spin lock */
+ void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store);
+
+ /* processor-reentrant spin unlock */
+ void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store);
+
+Pitfalls
+++++++++
+Although the value returned by prb_buffer_size() does represent an absolute
+upper bound, the amount of data that can be stored within the ring buffer
+is actually less because of the additional storage space of a header for each
+entry.
+
+The prb_lock() and prb_unlock() functions can be used to synchronize between
+ring buffer writers and other external activities. The function of a
+processor-reentrant spin lock is to disable preemption and local interrupts
+and synchronize against other processors. It does *not* protect against
+multiple contexts of a single processor, i.e NMI.
+
+Implementation
+~~~~~~~~~~~~~~
+This section describes several of the implementation concepts and details to
+help developers better understand the code.
+
+Entries
+^^^^^^^
+All ring buffer data is stored within a single static byte array. The reason
+for this is to ensure that any pointers to the data (past and present) will
+always point to valid memory. This is important because the lockless readers
+may be preempted for long periods of time and when they resume may be working
+with expired pointers.
+
+Entries are identified by start index and size. (The start index plus size
+is the start index of the next entry.) The start index is not simply an
+offset into the byte array, but rather a logical position (lpos) that maps
+directly to byte array offsets.
+
+For example, for a byte array of 1000, an entry may have have a start index
+of 100. Another entry may have a start index of 1100. And yet another 2100.
+All of these entry are pointing to the same memory region, but only the most
+recent entry is valid. The other entries are pointing to valid memory, but
+represent entries that have been overwritten.
+
+Note that due to overflowing, the most recent entry is not necessarily the one
+with the highest lpos value. Indeed, the printk ring buffer initializes its
+data such that an overflow happens relatively quickly in order to validate the
+handling of this situation. The implementation assumes that an lpos (unsigned
+long) will never completely wrap while a reader is preempted. If this were to
+become an issue, the seq number (which never wraps) could be used to increase
+the robustness of handling this situation.
+
+Buffer Wrapping
+^^^^^^^^^^^^^^^
+If an entry starts near the end of the byte array but would extend beyond it,
+a special terminating entry (size = -1) is inserted into the byte array and
+the real entry is placed at the beginning of the byte array. This can waste
+space at the end of the byte array, but simplifies the implementation by
+allowing writers to always work with contiguous buffers.
+
+Note that the size field is the first 4 bytes of the entry header. Also note
+that calc_next() always ensures that there are at least 4 bytes left at the
+end of the byte array to allow room for a terminating entry.
+
+Ring Buffer Pointers
+^^^^^^^^^^^^^^^^^^^^
+Three pointers (lpos values) are used to manage the ring buffer:
+
+ - _tail_: points to the oldest entry
+ - _head_: points to where the next new committed entry will be
+ - _reserve_: points to where the next new reserved entry will be
+
+These pointers always maintain a logical ordering:
+
+ tail <= head <= reserve
+
+The reserve pointer moves forward when a writer reserves a new entry. The
+head pointer moves forward when a writer commits a new entry.
+
+The reserve pointer cannot overwrite the tail pointer in a wrap situation. In
+such a situation, the tail pointer must be "pushed forward", thus
+invalidating that oldest entry. Readers identify if they are accessing a
+valid entry by ensuring their entry pointer is `>= tail && < head`.
+
+If the tail pointer is equal to the head pointer, it cannot be pushed and any
+reserve operation will fail. The only resolution is for writers to commit
+their reserved entries.
+
+Processor-Reentrant Locking
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+The purpose of the processor-reentrant locking is to limit the interruption
+scenarios of writers to 2 contexts. This allows for a simplified
+implementation where:
+
+- The reserve/commit window only exists on 1 processor at a time. A reserve
+  can never fail due to uncommitted entries of other processors.
+
+- When committing entries, it is trivial to handle the situation when
+  subsequent entries have already been committed, i.e. managing the head
+  pointer.
+
+Performance
+~~~~~~~~~~~
+Some basic tests were performed on a quad Intel(R) Xeon(R) CPU E5-2697 v4 at
+2.30GHz (36 cores / 72 threads). All tests involved writing a total of
+32,000,000 records at an average of 33 bytes each. Each writer was pinned to
+its own CPU and would write as fast as it could until a total of 32,000,000
+records were written. All tests involved 2 readers that were both pinned
+together to another CPU. Each reader would read as fast as it could and track
+how many of the 32,000,000 records it could read. All tests used a ring buffer
+of 16KB in size, which holds around 350 records (header + data for each
+entry).
+
+The only difference between the tests is the number of writers (and thus also
+the number of records per writer). As more writers are added, the time to
+write a record increases. This is because data pointers, modified via cmpxchg,
+and global data access in general become more contended.
+
+1 writer
+^^^^^^^^
+ runtime: 0m 18s
+ reader1: 16219900/32000000 (50%) records
+ reader2: 16141582/32000000 (50%) records
+
+2 writers
+^^^^^^^^^
+ runtime: 0m 32s
+ reader1: 16327957/32000000 (51%) records
+ reader2: 16313988/32000000 (50%) records
+
+4 writers
+^^^^^^^^^
+ runtime: 0m 42s
+ reader1: 16421642/32000000 (51%) records
+ reader2: 16417224/32000000 (51%) records
+
+8 writers
+^^^^^^^^^
+ runtime: 0m 43s
+ reader1: 16418300/32000000 (51%) records
+ reader2: 16432222/32000000 (51%) records
+
+16 writers
+^^^^^^^^^^
+ runtime: 0m 54s
+ reader1: 16539189/32000000 (51%) records
+ reader2: 16542711/32000000 (51%) records
+
+32 writers
+^^^^^^^^^^
+ runtime: 1m 13s
+ reader1: 16731808/32000000 (52%) records
+ reader2: 16735119/32000000 (52%) records
+
+Comments
+^^^^^^^^
+It is particularly interesting to compare/contrast the 1-writer and 32-writer
+tests. Despite the writing of the 32,000,000 records taking over 4 times
+longer, the readers (which perform no cmpxchg) were still unable to keep up.
+This shows that the memory contention between the increasing number of CPUs
+also has a dramatic effect on readers.
+
+It should also be noted that in all cases each reader was able to read >=50%
+of the records. This means that a single reader would have been able to keep
+up with the writer(s) in all cases, becoming slightly easier as more writers
+are added. This was the purpose of pinning 2 readers to 1 CPU: to observe how
+maximum reader performance changes.
diff -Naur a/Documentation/RCU/checklist.rst b/Documentation/RCU/checklist.rst
--- a/Documentation/RCU/checklist.rst	2020-11-23 13:47:46.828985727 +0200
+++ b/Documentation/RCU/checklist.rst	2021-07-14 15:38:49.150297634 +0300
@@ -214,8 +214,8 @@
 	the rest of the system.
 
 7.	As of v4.20, a given kernel implements only one RCU flavor,
-	which is RCU-sched for PREEMPT=n and RCU-preempt for PREEMPT=y.
-	If the updater uses call_rcu() or synchronize_rcu(),
+	which is RCU-sched for PREEMPTION=n and RCU-preempt for
+	PREEMPTION=y. If the updater uses call_rcu() or synchronize_rcu(),
 	then the corresponding readers my use rcu_read_lock() and
 	rcu_read_unlock(), rcu_read_lock_bh() and rcu_read_unlock_bh(),
 	or any pair of primitives that disables and re-enables preemption,
diff -Naur a/Documentation/RCU/stallwarn.rst b/Documentation/RCU/stallwarn.rst
--- a/Documentation/RCU/stallwarn.rst	2020-11-23 13:47:46.832985806 +0200
+++ b/Documentation/RCU/stallwarn.rst	2021-07-14 15:38:49.150297634 +0300
@@ -25,7 +25,7 @@
 
 -	A CPU looping with bottom halves disabled.
 
--	For !CONFIG_PREEMPT kernels, a CPU looping anywhere in the kernel
+-	For !CONFIG_PREEMPTION kernels, a CPU looping anywhere in the kernel
 	without invoking schedule().  If the looping in the kernel is
 	really expected and desirable behavior, you might need to add
 	some calls to cond_resched().
@@ -44,7 +44,7 @@
 	result in the ``rcu_.*kthread starved for`` console-log message,
 	which will include additional debugging information.
 
--	A CPU-bound real-time task in a CONFIG_PREEMPT kernel, which might
+-	A CPU-bound real-time task in a CONFIG_PREEMPTION kernel, which might
 	happen to preempt a low-priority task in the middle of an RCU
 	read-side critical section.   This is especially damaging if
 	that low-priority task is not permitted to run on any other CPU,
diff -Naur a/drivers/block/zram/zram_drv.c b/drivers/block/zram/zram_drv.c
--- a/drivers/block/zram/zram_drv.c	2020-11-23 13:48:00.001246990 +0200
+++ b/drivers/block/zram/zram_drv.c	2021-07-14 15:38:55.594250927 +0300
@@ -56,6 +56,40 @@
 static int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,
 				u32 index, int offset, struct bio *bio);
 
+#ifdef CONFIG_PREEMPT_RT
+static void zram_meta_init_table_locks(struct zram *zram, size_t num_pages)
+{
+	size_t index;
+
+	for (index = 0; index < num_pages; index++)
+		spin_lock_init(&zram->table[index].lock);
+}
+
+static int zram_slot_trylock(struct zram *zram, u32 index)
+{
+	int ret;
+
+	ret = spin_trylock(&zram->table[index].lock);
+	if (ret)
+		__set_bit(ZRAM_LOCK, &zram->table[index].flags);
+	return ret;
+}
+
+static void zram_slot_lock(struct zram *zram, u32 index)
+{
+	spin_lock(&zram->table[index].lock);
+	__set_bit(ZRAM_LOCK, &zram->table[index].flags);
+}
+
+static void zram_slot_unlock(struct zram *zram, u32 index)
+{
+	__clear_bit(ZRAM_LOCK, &zram->table[index].flags);
+	spin_unlock(&zram->table[index].lock);
+}
+
+#else
+
+static void zram_meta_init_table_locks(struct zram *zram, size_t num_pages) { }
 
 static int zram_slot_trylock(struct zram *zram, u32 index)
 {
@@ -71,6 +105,7 @@
 {
 	bit_spin_unlock(ZRAM_LOCK, &zram->table[index].flags);
 }
+#endif
 
 static inline bool init_done(struct zram *zram)
 {
@@ -1158,6 +1193,7 @@
 
 	if (!huge_class_size)
 		huge_class_size = zs_huge_class_size(zram->mem_pool);
+	zram_meta_init_table_locks(zram, num_pages);
 	return true;
 }
 
@@ -1220,6 +1256,7 @@
 	unsigned long handle;
 	unsigned int size;
 	void *src, *dst;
+	struct zcomp_strm *zstrm;
 
 	zram_slot_lock(zram, index);
 	if (zram_test_flag(zram, index, ZRAM_WB)) {
@@ -1250,6 +1287,7 @@
 
 	size = zram_get_obj_size(zram, index);
 
+	zstrm = zcomp_stream_get(zram->comp);
 	src = zs_map_object(zram->mem_pool, handle, ZS_MM_RO);
 	if (size == PAGE_SIZE) {
 		dst = kmap_atomic(page);
@@ -1257,14 +1295,13 @@
 		kunmap_atomic(dst);
 		ret = 0;
 	} else {
-		struct zcomp_strm *zstrm = zcomp_stream_get(zram->comp);
 
 		dst = kmap_atomic(page);
 		ret = zcomp_decompress(zstrm, src, size, dst);
 		kunmap_atomic(dst);
-		zcomp_stream_put(zram->comp);
 	}
 	zs_unmap_object(zram->mem_pool, handle);
+	zcomp_stream_put(zram->comp);
 	zram_slot_unlock(zram, index);
 
 	/* Should NEVER happen. Return bio error if it does. */
diff -Naur a/drivers/block/zram/zram_drv.h b/drivers/block/zram/zram_drv.h
--- a/drivers/block/zram/zram_drv.h	2020-11-23 13:48:00.001246990 +0200
+++ b/drivers/block/zram/zram_drv.h	2021-07-14 15:38:55.594250927 +0300
@@ -63,6 +63,7 @@
 		unsigned long element;
 	};
 	unsigned long flags;
+	spinlock_t lock;
 #ifdef CONFIG_ZRAM_MEMORY_TRACKING
 	ktime_t ac_time;
 #endif
diff -Naur a/drivers/char/random.c b/drivers/char/random.c
--- a/drivers/char/random.c	2020-11-23 13:48:00.137249693 +0200
+++ b/drivers/char/random.c	2021-07-14 15:38:55.650250523 +0300
@@ -1252,28 +1252,27 @@
 	return *ptr;
 }
 
-void add_interrupt_randomness(int irq, int irq_flags)
+void add_interrupt_randomness(int irq, int irq_flags, __u64 ip)
 {
 	struct entropy_store	*r;
 	struct fast_pool	*fast_pool = this_cpu_ptr(&irq_randomness);
-	struct pt_regs		*regs = get_irq_regs();
 	unsigned long		now = jiffies;
 	cycles_t		cycles = random_get_entropy();
 	__u32			c_high, j_high;
-	__u64			ip;
 	unsigned long		seed;
 	int			credit = 0;
 
 	if (cycles == 0)
-		cycles = get_reg(fast_pool, regs);
+		cycles = get_reg(fast_pool, NULL);
 	c_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;
 	j_high = (sizeof(now) > 4) ? now >> 32 : 0;
 	fast_pool->pool[0] ^= cycles ^ j_high ^ irq;
 	fast_pool->pool[1] ^= now ^ c_high;
-	ip = regs ? instruction_pointer(regs) : _RET_IP_;
+	if (!ip)
+		ip = _RET_IP_;
 	fast_pool->pool[2] ^= ip;
 	fast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :
-		get_reg(fast_pool, regs);
+		get_reg(fast_pool, NULL);
 
 	fast_mix(fast_pool);
 	add_interrupt_bench(cycles);
diff -Naur a/drivers/char/tpm/tpm-dev-common.c b/drivers/char/tpm/tpm-dev-common.c
--- a/drivers/char/tpm/tpm-dev-common.c	2020-11-23 13:48:00.213251204 +0200
+++ b/drivers/char/tpm/tpm-dev-common.c	2021-07-14 15:38:55.690250234 +0300
@@ -20,7 +20,6 @@
 #include "tpm-dev.h"
 
 static struct workqueue_struct *tpm_dev_wq;
-static DEFINE_MUTEX(tpm_dev_wq_lock);
 
 static ssize_t tpm_dev_transmit(struct tpm_chip *chip, struct tpm_space *space,
 				u8 *buf, size_t bufsiz)
diff -Naur a/drivers/char/tpm/tpm_tis.c b/drivers/char/tpm/tpm_tis.c
--- a/drivers/char/tpm/tpm_tis.c	2020-11-23 13:48:00.225251442 +0200
+++ b/drivers/char/tpm/tpm_tis.c	2021-07-14 15:38:55.690250234 +0300
@@ -49,6 +49,31 @@
 	return container_of(data, struct tpm_tis_tcg_phy, priv);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Flushes previous write operations to chip so that a subsequent
+ * ioread*()s won't stall a cpu.
+ */
+static inline void tpm_tis_flush(void __iomem *iobase)
+{
+	ioread8(iobase + TPM_ACCESS(0));
+}
+#else
+#define tpm_tis_flush(iobase) do { } while (0)
+#endif
+
+static inline void tpm_tis_iowrite8(u8 b, void __iomem *iobase, u32 addr)
+{
+	iowrite8(b, iobase + addr);
+	tpm_tis_flush(iobase);
+}
+
+static inline void tpm_tis_iowrite32(u32 b, void __iomem *iobase, u32 addr)
+{
+	iowrite32(b, iobase + addr);
+	tpm_tis_flush(iobase);
+}
+
 static bool interrupts = true;
 module_param(interrupts, bool, 0444);
 MODULE_PARM_DESC(interrupts, "Enable interrupts");
@@ -146,7 +171,7 @@
 	struct tpm_tis_tcg_phy *phy = to_tpm_tis_tcg_phy(data);
 
 	while (len--)
-		iowrite8(*value++, phy->iobase + addr);
+		tpm_tis_iowrite8(*value++, phy->iobase, addr);
 
 	return 0;
 }
@@ -173,7 +198,7 @@
 {
 	struct tpm_tis_tcg_phy *phy = to_tpm_tis_tcg_phy(data);
 
-	iowrite32(value, phy->iobase + addr);
+	tpm_tis_iowrite32(value, phy->iobase, addr);
 
 	return 0;
 }
diff -Naur a/drivers/firmware/efi/efi.c b/drivers/firmware/efi/efi.c
--- a/drivers/firmware/efi/efi.c	2020-11-23 13:48:01.961285955 +0200
+++ b/drivers/firmware/efi/efi.c	2021-07-14 15:38:56.374245305 +0300
@@ -62,7 +62,7 @@
 
 struct workqueue_struct *efi_rts_wq;
 
-static bool disable_runtime;
+static bool disable_runtime = IS_ENABLED(CONFIG_PREEMPT_RT);
 static int __init setup_noefi(char *arg)
 {
 	disable_runtime = true;
@@ -93,6 +93,9 @@
 	if (parse_option_str(str, "noruntime"))
 		disable_runtime = true;
 
+	if (parse_option_str(str, "runtime"))
+		disable_runtime = false;
+
 	if (parse_option_str(str, "nosoftreserve"))
 		set_bit(EFI_MEM_NO_SOFT_RESERVE, &efi.flags);
 
diff -Naur a/drivers/gpu/drm/i915/display/intel_sprite.c b/drivers/gpu/drm/i915/display/intel_sprite.c
--- a/drivers/gpu/drm/i915/display/intel_sprite.c	2020-11-23 13:48:09.853443086 +0200
+++ b/drivers/gpu/drm/i915/display/intel_sprite.c	2021-07-14 15:38:57.858234629 +0300
@@ -118,7 +118,8 @@
 			"PSR idle timed out 0x%x, atomic update may fail\n",
 			psr_status);
 
-	local_irq_disable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_irq_disable();
 
 	crtc->debug.min_vbl = min;
 	crtc->debug.max_vbl = max;
@@ -143,11 +144,13 @@
 			break;
 		}
 
-		local_irq_enable();
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+			local_irq_enable();
 
 		timeout = schedule_timeout(timeout);
 
-		local_irq_disable();
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+			local_irq_disable();
 	}
 
 	finish_wait(wq, &wait);
@@ -180,7 +183,8 @@
 	return;
 
 irq_disable:
-	local_irq_disable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_irq_disable();
 }
 
 /**
@@ -218,7 +222,8 @@
 		new_crtc_state->uapi.event = NULL;
 	}
 
-	local_irq_enable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		local_irq_enable();
 
 	if (intel_vgpu_active(dev_priv))
 		return;
diff -Naur a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c	2020-11-23 13:48:09.917444361 +0200
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c	2021-07-14 15:38:57.922234169 +0300
@@ -59,9 +59,10 @@
 
 static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 {
-	unsigned long flags;
+	unsigned long flags = 0;
 
-	local_irq_save(flags);
+	if (!force_irqthreads)
+		local_irq_save(flags);
 	mutex_acquire(&ce->timeline->mutex.dep_map, 2, 0, _THIS_IP_);
 
 	return flags;
@@ -71,7 +72,8 @@
 					  unsigned long flags)
 {
 	mutex_release(&ce->timeline->mutex.dep_map, _THIS_IP_);
-	local_irq_restore(flags);
+	if (!force_irqthreads)
+		local_irq_restore(flags);
 }
 
 #else
diff -Naur a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
--- a/drivers/gpu/drm/i915/i915_irq.c	2020-11-23 13:48:09.633438701 +0200
+++ b/drivers/gpu/drm/i915/i915_irq.c	2021-07-14 15:38:57.986233709 +0300
@@ -865,6 +865,7 @@
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -916,6 +917,7 @@
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 
diff -Naur a/drivers/gpu/drm/i915/i915_trace.h b/drivers/gpu/drm/i915/i915_trace.h
--- a/drivers/gpu/drm/i915/i915_trace.h	2020-11-23 13:48:09.681439658 +0200
+++ b/drivers/gpu/drm/i915/i915_trace.h	2021-07-14 15:38:57.986233709 +0300
@@ -2,6 +2,10 @@
 #if !defined(_I915_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
 #define _I915_TRACE_H_
 
+#ifdef CONFIG_PREEMPT_RT
+#define NOTRACE
+#endif
+
 #include <linux/stringify.h>
 #include <linux/types.h>
 #include <linux/tracepoint.h>
@@ -778,7 +782,7 @@
 	    TP_ARGS(rq)
 );
 
-#if defined(CONFIG_DRM_I915_LOW_LEVEL_TRACEPOINTS)
+#if defined(CONFIG_DRM_I915_LOW_LEVEL_TRACEPOINTS) && !defined(NOTRACE)
 DEFINE_EVENT(i915_request, i915_request_submit,
 	     TP_PROTO(struct i915_request *rq),
 	     TP_ARGS(rq)
diff -Naur a/drivers/gpu/drm/radeon/radeon_display.c b/drivers/gpu/drm/radeon/radeon_display.c
--- a/drivers/gpu/drm/radeon/radeon_display.c	2020-11-23 13:48:11.353472990 +0200
+++ b/drivers/gpu/drm/radeon/radeon_display.c	2021-07-14 15:38:58.566229546 +0300
@@ -1822,6 +1822,7 @@
 	struct radeon_device *rdev = dev->dev_private;
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -1914,6 +1915,7 @@
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	/* Decode into vertical and horizontal scanout position. */
 	*vpos = position & 0x1fff;
diff -Naur a/drivers/hv/hyperv_vmbus.h b/drivers/hv/hyperv_vmbus.h
--- a/drivers/hv/hyperv_vmbus.h	2020-11-23 13:48:12.053486949 +0200
+++ b/drivers/hv/hyperv_vmbus.h	2021-07-14 15:38:58.826227680 +0300
@@ -18,6 +18,7 @@
 #include <linux/atomic.h>
 #include <linux/hyperv.h>
 #include <linux/interrupt.h>
+#include <linux/irq.h>
 
 #include "hv_trace.h"
 
diff -Naur a/drivers/hv/vmbus_drv.c b/drivers/hv/vmbus_drv.c
--- a/drivers/hv/vmbus_drv.c	2020-11-23 13:48:12.057487029 +0200
+++ b/drivers/hv/vmbus_drv.c	2021-07-14 15:38:58.826227680 +0300
@@ -22,6 +22,7 @@
 #include <linux/clockchips.h>
 #include <linux/cpu.h>
 #include <linux/sched/task_stack.h>
+#include <linux/irq.h>
 
 #include <linux/delay.h>
 #include <linux/notifier.h>
@@ -1303,6 +1304,8 @@
 	void *page_addr = hv_cpu->synic_event_page;
 	struct hv_message *msg;
 	union hv_synic_event_flags *event;
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 	bool handled = false;
 
 	if (unlikely(page_addr == NULL))
@@ -1347,7 +1350,7 @@
 			tasklet_schedule(&hv_cpu->msg_dpc);
 	}
 
-	add_interrupt_randomness(HYPERVISOR_CALLBACK_VECTOR, 0);
+	add_interrupt_randomness(HYPERVISOR_CALLBACK_VECTOR, 0, ip);
 }
 
 /*
diff -Naur a/drivers/leds/trigger/Kconfig b/drivers/leds/trigger/Kconfig
--- a/drivers/leds/trigger/Kconfig	2020-11-23 13:48:14.197529723 +0200
+++ b/drivers/leds/trigger/Kconfig	2021-07-14 15:38:59.602222117 +0300
@@ -64,6 +64,7 @@
 
 config LEDS_TRIGGER_CPU
 	bool "LED CPU Trigger"
+	depends on !PREEMPT_RT
 	help
 	  This allows LEDs to be controlled by active CPUs. This shows
 	  the active CPUs across an array of LEDs so you can see which
diff -Naur a/drivers/md/raid5.c b/drivers/md/raid5.c
--- a/drivers/md/raid5.c	2020-11-23 13:48:14.401533794 +0200
+++ b/drivers/md/raid5.c	2021-07-14 15:38:59.682221544 +0300
@@ -2077,8 +2077,9 @@
 	struct raid5_percpu *percpu;
 	unsigned long cpu;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	percpu = per_cpu_ptr(conf->percpu, cpu);
+	spin_lock(&percpu->lock);
 	if (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {
 		ops_run_biofill(sh);
 		overlap_clear++;
@@ -2137,7 +2138,8 @@
 			if (test_and_clear_bit(R5_Overlap, &dev->flags))
 				wake_up(&sh->raid_conf->wait_for_overlap);
 		}
-	put_cpu();
+	spin_unlock(&percpu->lock);
+	put_cpu_light();
 }
 
 static void free_stripe(struct kmem_cache *sc, struct stripe_head *sh)
@@ -6902,6 +6904,7 @@
 			__func__, cpu);
 		return -ENOMEM;
 	}
+	spin_lock_init(&per_cpu_ptr(conf->percpu, cpu)->lock);
 	return 0;
 }
 
diff -Naur a/drivers/md/raid5.h b/drivers/md/raid5.h
--- a/drivers/md/raid5.h	2020-11-23 13:48:14.401533794 +0200
+++ b/drivers/md/raid5.h	2021-07-14 15:38:59.682221544 +0300
@@ -627,6 +627,7 @@
 	int			recovery_disabled;
 	/* per cpu variables */
 	struct raid5_percpu {
+		spinlock_t	lock;		/* Protection for -RT */
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
 		void		*scribble;  /* space for constructing buffer
 					     * lists and performing address
diff -Naur a/drivers/scsi/fcoe/fcoe.c b/drivers/scsi/fcoe/fcoe.c
--- a/drivers/scsi/fcoe/fcoe.c	2020-11-23 13:48:25.217749979 +0200
+++ b/drivers/scsi/fcoe/fcoe.c	2021-07-14 15:39:04.562186720 +0300
@@ -1452,11 +1452,11 @@
 static int fcoe_alloc_paged_crc_eof(struct sk_buff *skb, int tlen)
 {
 	struct fcoe_percpu_s *fps;
-	int rc;
+	int rc, cpu = get_cpu_light();
 
-	fps = &get_cpu_var(fcoe_percpu);
+	fps = &per_cpu(fcoe_percpu, cpu);
 	rc = fcoe_get_paged_crc_eof(skb, tlen, fps);
-	put_cpu_var(fcoe_percpu);
+	put_cpu_light();
 
 	return rc;
 }
@@ -1641,11 +1641,11 @@
 		return 0;
 	}
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	stats->InvalidCRCCount++;
 	if (stats->InvalidCRCCount < 5)
 		printk(KERN_WARNING "fcoe: dropping frame with CRC error\n");
-	put_cpu();
+	put_cpu_light();
 	return -EINVAL;
 }
 
@@ -1686,7 +1686,7 @@
 	 */
 	hp = (struct fcoe_hdr *) skb_network_header(skb);
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	if (unlikely(FC_FCOE_DECAPS_VER(hp) != FC_FCOE_VER)) {
 		if (stats->ErrorFrames < 5)
 			printk(KERN_WARNING "fcoe: FCoE version "
@@ -1718,13 +1718,13 @@
 		goto drop;
 
 	if (!fcoe_filter_frames(lport, fp)) {
-		put_cpu();
+		put_cpu_light();
 		fc_exch_recv(lport, fp);
 		return;
 	}
 drop:
 	stats->ErrorFrames++;
-	put_cpu();
+	put_cpu_light();
 	kfree_skb(skb);
 }
 
diff -Naur a/drivers/scsi/fcoe/fcoe_ctlr.c b/drivers/scsi/fcoe/fcoe_ctlr.c
--- a/drivers/scsi/fcoe/fcoe_ctlr.c	2020-11-23 13:48:25.221750059 +0200
+++ b/drivers/scsi/fcoe/fcoe_ctlr.c	2021-07-14 15:39:04.562186720 +0300
@@ -828,7 +828,7 @@
 
 	INIT_LIST_HEAD(&del_list);
 
-	stats = per_cpu_ptr(fip->lp->stats, get_cpu());
+	stats = per_cpu_ptr(fip->lp->stats, get_cpu_light());
 
 	list_for_each_entry_safe(fcf, next, &fip->fcfs, list) {
 		deadline = fcf->time + fcf->fka_period + fcf->fka_period / 2;
@@ -864,7 +864,7 @@
 				sel_time = fcf->time;
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 
 	list_for_each_entry_safe(fcf, next, &del_list, list) {
 		/* Removes fcf from current list */
diff -Naur a/drivers/scsi/libfc/fc_exch.c b/drivers/scsi/libfc/fc_exch.c
--- a/drivers/scsi/libfc/fc_exch.c	2020-11-23 13:48:25.365752942 +0200
+++ b/drivers/scsi/libfc/fc_exch.c	2021-07-14 15:39:04.606186408 +0300
@@ -826,10 +826,10 @@
 	}
 	memset(ep, 0, sizeof(*ep));
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = per_cpu_ptr(mp->pool, cpu);
 	spin_lock_bh(&pool->lock);
-	put_cpu();
+	put_cpu_light();
 
 	/* peek cache of free slot */
 	if (pool->left != FC_XID_UNKNOWN) {
diff -Naur a/drivers/tty/serial/8250/8250_core.c b/drivers/tty/serial/8250/8250_core.c
--- a/drivers/tty/serial/8250/8250_core.c	2020-11-23 13:48:28.293811577 +0200
+++ b/drivers/tty/serial/8250/8250_core.c	2021-07-14 15:39:07.466166126 +0300
@@ -274,10 +274,8 @@
 	 * Must disable interrupts or else we risk racing with the interrupt
 	 * based handler.
 	 */
-	if (up->port.irq) {
-		ier = serial_in(up, UART_IER);
-		serial_out(up, UART_IER, 0);
-	}
+	if (up->port.irq)
+		ier = serial8250_clear_IER(up);
 
 	iir = serial_in(up, UART_IIR);
 
@@ -300,7 +298,7 @@
 		serial8250_tx_chars(up);
 
 	if (up->port.irq)
-		serial_out(up, UART_IER, ier);
+		serial8250_set_IER(up, ier);
 
 	spin_unlock_irqrestore(&up->port.lock, flags);
 
@@ -578,6 +576,14 @@
 
 #ifdef CONFIG_SERIAL_8250_CONSOLE
 
+static void univ8250_console_write_atomic(struct console *co, const char *s,
+					  unsigned int count)
+{
+	struct uart_8250_port *up = &serial8250_ports[co->index];
+
+	serial8250_console_write_atomic(up, s, count);
+}
+
 static void univ8250_console_write(struct console *co, const char *s,
 				   unsigned int count)
 {
@@ -671,6 +677,7 @@
 
 static struct console univ8250_console = {
 	.name		= "ttyS",
+	.write_atomic	= univ8250_console_write_atomic,
 	.write		= univ8250_console_write,
 	.device		= uart_console_device,
 	.setup		= univ8250_console_setup,
diff -Naur a/drivers/tty/serial/8250/8250_fsl.c b/drivers/tty/serial/8250/8250_fsl.c
--- a/drivers/tty/serial/8250/8250_fsl.c	2020-11-23 13:48:28.297811657 +0200
+++ b/drivers/tty/serial/8250/8250_fsl.c	2021-07-14 15:39:07.466166126 +0300
@@ -53,9 +53,18 @@
 
 	/* Stop processing interrupts on input overrun */
 	if ((orig_lsr & UART_LSR_OE) && (up->overrun_backoff_time_ms > 0)) {
+		unsigned int ca_flags;
 		unsigned long delay;
+		bool is_console;
 
+		is_console = uart_console(port);
+
+		if (is_console)
+			console_atomic_lock(&ca_flags);
 		up->ier = port->serial_in(port, UART_IER);
+		if (is_console)
+			console_atomic_unlock(ca_flags);
+
 		if (up->ier & (UART_IER_RLSI | UART_IER_RDI)) {
 			port->ops->stop_rx(port);
 		} else {
diff -Naur a/drivers/tty/serial/8250/8250.h b/drivers/tty/serial/8250/8250.h
--- a/drivers/tty/serial/8250/8250.h	2020-11-23 13:48:28.289811497 +0200
+++ b/drivers/tty/serial/8250/8250.h	2021-07-14 15:39:07.462166155 +0300
@@ -130,12 +130,55 @@
 	up->dl_write(up, value);
 }
 
+static inline void serial8250_set_IER(struct uart_8250_port *up,
+				      unsigned char ier)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	bool is_console;
+
+	is_console = uart_console(port);
+
+	if (is_console)
+		console_atomic_lock(&flags);
+
+	serial_out(up, UART_IER, ier);
+
+	if (is_console)
+		console_atomic_unlock(flags);
+}
+
+static inline unsigned char serial8250_clear_IER(struct uart_8250_port *up)
+{
+	struct uart_port *port = &up->port;
+	unsigned int clearval = 0;
+	unsigned int prior;
+	unsigned int flags;
+	bool is_console;
+
+	is_console = uart_console(port);
+
+	if (up->capabilities & UART_CAP_UUE)
+		clearval = UART_IER_UUE;
+
+	if (is_console)
+		console_atomic_lock(&flags);
+
+	prior = serial_port_in(port, UART_IER);
+	serial_port_out(port, UART_IER, clearval);
+
+	if (is_console)
+		console_atomic_unlock(flags);
+
+	return prior;
+}
+
 static inline bool serial8250_set_THRI(struct uart_8250_port *up)
 {
 	if (up->ier & UART_IER_THRI)
 		return false;
 	up->ier |= UART_IER_THRI;
-	serial_out(up, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 	return true;
 }
 
@@ -144,7 +187,7 @@
 	if (!(up->ier & UART_IER_THRI))
 		return false;
 	up->ier &= ~UART_IER_THRI;
-	serial_out(up, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 	return true;
 }
 
diff -Naur a/drivers/tty/serial/8250/8250_ingenic.c b/drivers/tty/serial/8250/8250_ingenic.c
--- a/drivers/tty/serial/8250/8250_ingenic.c	2020-11-23 13:48:28.301811738 +0200
+++ b/drivers/tty/serial/8250/8250_ingenic.c	2021-07-14 15:39:07.466166126 +0300
@@ -146,6 +146,8 @@
 
 static void ingenic_uart_serial_out(struct uart_port *p, int offset, int value)
 {
+	unsigned int flags;
+	bool is_console;
 	int ier;
 
 	switch (offset) {
@@ -167,7 +169,12 @@
 		 * If we have enabled modem status IRQs we should enable
 		 * modem mode.
 		 */
+		is_console = uart_console(p);
+		if (is_console)
+			console_atomic_lock(&flags);
 		ier = p->serial_in(p, UART_IER);
+		if (is_console)
+			console_atomic_unlock(flags);
 
 		if (ier & UART_IER_MSI)
 			value |= UART_MCR_MDCE | UART_MCR_FCM;
diff -Naur a/drivers/tty/serial/8250/8250_mtk.c b/drivers/tty/serial/8250/8250_mtk.c
--- a/drivers/tty/serial/8250/8250_mtk.c	2020-11-23 13:48:28.301811738 +0200
+++ b/drivers/tty/serial/8250/8250_mtk.c	2021-07-14 15:39:07.466166126 +0300
@@ -213,12 +213,37 @@
 
 static void mtk8250_disable_intrs(struct uart_8250_port *up, int mask)
 {
-	serial_out(up, UART_IER, serial_in(up, UART_IER) & (~mask));
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	unsigned int ier;
+	bool is_console;
+
+	is_console = uart_console(port);
+
+	if (is_console)
+		console_atomic_lock(&flags);
+
+	ier = serial_in(up, UART_IER);
+	serial_out(up, UART_IER, ier & (~mask));
+
+	if (is_console)
+		console_atomic_unlock(flags);
 }
 
 static void mtk8250_enable_intrs(struct uart_8250_port *up, int mask)
 {
-	serial_out(up, UART_IER, serial_in(up, UART_IER) | mask);
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	unsigned int ier;
+
+	if (uart_console(port))
+		console_atomic_lock(&flags);
+
+	ier = serial_in(up, UART_IER);
+	serial_out(up, UART_IER, ier | mask);
+
+	if (uart_console(port))
+		console_atomic_unlock(flags);
 }
 
 static void mtk8250_set_flow_ctrl(struct uart_8250_port *up, int mode)
diff -Naur a/drivers/tty/serial/8250/8250_port.c b/drivers/tty/serial/8250/8250_port.c
--- a/drivers/tty/serial/8250/8250_port.c	2020-11-23 13:48:28.313811978 +0200
+++ b/drivers/tty/serial/8250/8250_port.c	2021-07-14 15:39:07.466166126 +0300
@@ -757,7 +757,7 @@
 			serial_out(p, UART_EFR, UART_EFR_ECB);
 			serial_out(p, UART_LCR, 0);
 		}
-		serial_out(p, UART_IER, sleep ? UART_IERX_SLEEP : 0);
+		serial8250_set_IER(p, sleep ? UART_IERX_SLEEP : 0);
 		if (p->capabilities & UART_CAP_EFR) {
 			serial_out(p, UART_LCR, UART_LCR_CONF_MODE_B);
 			serial_out(p, UART_EFR, efr);
@@ -1429,7 +1429,7 @@
 
 	up->ier &= ~(UART_IER_RLSI | UART_IER_RDI);
 	up->port.read_status_mask &= ~UART_LSR_DR;
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 
 	serial8250_rpm_put(up);
 }
@@ -1459,7 +1459,7 @@
 		serial8250_clear_and_reinit_fifos(p);
 
 		p->ier |= UART_IER_RLSI | UART_IER_RDI;
-		serial_port_out(&p->port, UART_IER, p->ier);
+		serial8250_set_IER(p, p->ier);
 	}
 }
 EXPORT_SYMBOL_GPL(serial8250_em485_stop_tx);
@@ -1687,7 +1687,7 @@
 	mctrl_gpio_disable_ms(up->gpios);
 
 	up->ier &= ~UART_IER_MSI;
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 }
 
 static void serial8250_enable_ms(struct uart_port *port)
@@ -1703,7 +1703,7 @@
 	up->ier |= UART_IER_MSI;
 
 	serial8250_rpm_get(up);
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 	serial8250_rpm_put(up);
 }
 
@@ -2118,14 +2118,7 @@
 	struct uart_8250_port *up = up_to_u8250p(port);
 
 	serial8250_rpm_get(up);
-	/*
-	 *	First save the IER then disable the interrupts
-	 */
-	ier = serial_port_in(port, UART_IER);
-	if (up->capabilities & UART_CAP_UUE)
-		serial_port_out(port, UART_IER, UART_IER_UUE);
-	else
-		serial_port_out(port, UART_IER, 0);
+	ier = serial8250_clear_IER(up);
 
 	wait_for_xmitr(up, BOTH_EMPTY);
 	/*
@@ -2138,7 +2131,7 @@
 	 *	and restore the IER
 	 */
 	wait_for_xmitr(up, BOTH_EMPTY);
-	serial_port_out(port, UART_IER, ier);
+	serial8250_set_IER(up, ier);
 	serial8250_rpm_put(up);
 }
 
@@ -2441,7 +2434,7 @@
 	 */
 	spin_lock_irqsave(&port->lock, flags);
 	up->ier = 0;
-	serial_port_out(port, UART_IER, 0);
+	serial8250_set_IER(up, 0);
 	spin_unlock_irqrestore(&port->lock, flags);
 
 	synchronize_irq(port->irq);
@@ -2768,7 +2761,7 @@
 	if (up->capabilities & UART_CAP_RTOIE)
 		up->ier |= UART_IER_RTOIE;
 
-	serial_port_out(port, UART_IER, up->ier);
+	serial8250_set_IER(up, up->ier);
 
 	if (up->capabilities & UART_CAP_EFR) {
 		unsigned char efr = 0;
@@ -3234,7 +3227,7 @@
 
 #ifdef CONFIG_SERIAL_8250_CONSOLE
 
-static void serial8250_console_putchar(struct uart_port *port, int ch)
+static void serial8250_console_putchar_locked(struct uart_port *port, int ch)
 {
 	struct uart_8250_port *up = up_to_u8250p(port);
 
@@ -3242,6 +3235,18 @@
 	serial_port_out(port, UART_TX, ch);
 }
 
+static void serial8250_console_putchar(struct uart_port *port, int ch)
+{
+	struct uart_8250_port *up = up_to_u8250p(port);
+	unsigned int flags;
+
+	wait_for_xmitr(up, UART_LSR_THRE);
+
+	console_atomic_lock(&flags);
+	serial8250_console_putchar_locked(port, ch);
+	console_atomic_unlock(flags);
+}
+
 /*
  *	Restore serial console when h/w power-off detected
  */
@@ -3263,6 +3268,32 @@
 	serial8250_out_MCR(up, UART_MCR_DTR | UART_MCR_RTS);
 }
 
+void serial8250_console_write_atomic(struct uart_8250_port *up,
+				     const char *s, unsigned int count)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+	unsigned int ier;
+
+	console_atomic_lock(&flags);
+
+	touch_nmi_watchdog();
+
+	ier = serial8250_clear_IER(up);
+
+	if (atomic_fetch_inc(&up->console_printing)) {
+		uart_console_write(port, "\n", 1,
+				   serial8250_console_putchar_locked);
+	}
+	uart_console_write(port, s, count, serial8250_console_putchar_locked);
+	atomic_dec(&up->console_printing);
+
+	wait_for_xmitr(up, BOTH_EMPTY);
+	serial8250_set_IER(up, ier);
+
+	console_atomic_unlock(flags);
+}
+
 /*
  *	Print a string to the serial port trying not to disturb
  *	any possible real use of the port...
@@ -3279,24 +3310,12 @@
 	struct uart_port *port = &up->port;
 	unsigned long flags;
 	unsigned int ier;
-	int locked = 1;
 
 	touch_nmi_watchdog();
 
-	if (oops_in_progress)
-		locked = spin_trylock_irqsave(&port->lock, flags);
-	else
-		spin_lock_irqsave(&port->lock, flags);
-
-	/*
-	 *	First save the IER then disable the interrupts
-	 */
-	ier = serial_port_in(port, UART_IER);
+	spin_lock_irqsave(&port->lock, flags);
 
-	if (up->capabilities & UART_CAP_UUE)
-		serial_port_out(port, UART_IER, UART_IER_UUE);
-	else
-		serial_port_out(port, UART_IER, 0);
+	ier = serial8250_clear_IER(up);
 
 	/* check scratch reg to see if port powered off during system sleep */
 	if (up->canary && (up->canary != serial_port_in(port, UART_SCR))) {
@@ -3310,7 +3329,9 @@
 		mdelay(port->rs485.delay_rts_before_send);
 	}
 
+	atomic_inc(&up->console_printing);
 	uart_console_write(port, s, count, serial8250_console_putchar);
+	atomic_dec(&up->console_printing);
 
 	/*
 	 *	Finally, wait for transmitter to become empty
@@ -3323,8 +3344,7 @@
 		if (em485->tx_stopped)
 			up->rs485_stop_tx(up);
 	}
-
-	serial_port_out(port, UART_IER, ier);
+	serial8250_set_IER(up, ier);
 
 	/*
 	 *	The receive handling will happen properly because the
@@ -3336,8 +3356,7 @@
 	if (up->msr_saved_flags)
 		serial8250_modem_status(up);
 
-	if (locked)
-		spin_unlock_irqrestore(&port->lock, flags);
+	spin_unlock_irqrestore(&port->lock, flags);
 }
 
 static unsigned int probe_baud(struct uart_port *port)
@@ -3357,6 +3376,7 @@
 
 int serial8250_console_setup(struct uart_port *port, char *options, bool probe)
 {
+	struct uart_8250_port *up = up_to_u8250p(port);
 	int baud = 9600;
 	int bits = 8;
 	int parity = 'n';
@@ -3366,6 +3386,8 @@
 	if (!port->iobase && !port->membase)
 		return -ENODEV;
 
+	atomic_set(&up->console_printing, 0);
+
 	if (options)
 		uart_parse_options(options, &baud, &parity, &bits, &flow);
 	else if (probe)
diff -Naur a/drivers/tty/serial/amba-pl011.c b/drivers/tty/serial/amba-pl011.c
--- a/drivers/tty/serial/amba-pl011.c	2020-11-23 13:48:28.321812138 +0200
+++ b/drivers/tty/serial/amba-pl011.c	2021-07-14 15:39:07.502165872 +0300
@@ -2198,18 +2198,24 @@
 {
 	struct uart_amba_port *uap = amba_ports[co->index];
 	unsigned int old_cr = 0, new_cr;
-	unsigned long flags;
+	unsigned long flags = 0;
 	int locked = 1;
 
 	clk_enable(uap->clk);
 
-	local_irq_save(flags);
+	/*
+	 * local_irq_save(flags);
+	 *
+	 * This local_irq_save() is nonsense. If we come in via sysrq
+	 * handling then interrupts are already disabled. Aside of
+	 * that the port.sysrq check is racy on SMP regardless.
+	*/
 	if (uap->port.sysrq)
 		locked = 0;
 	else if (oops_in_progress)
-		locked = spin_trylock(&uap->port.lock);
+		locked = spin_trylock_irqsave(&uap->port.lock, flags);
 	else
-		spin_lock(&uap->port.lock);
+		spin_lock_irqsave(&uap->port.lock, flags);
 
 	/*
 	 *	First save the CR then disable the interrupts
@@ -2235,8 +2241,7 @@
 		pl011_write(old_cr, uap, REG_CR);
 
 	if (locked)
-		spin_unlock(&uap->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&uap->port.lock, flags);
 
 	clk_disable(uap->clk);
 }
diff -Naur a/drivers/tty/serial/omap-serial.c b/drivers/tty/serial/omap-serial.c
--- a/drivers/tty/serial/omap-serial.c	2020-11-23 13:48:28.365813020 +0200
+++ b/drivers/tty/serial/omap-serial.c	2021-07-14 15:39:07.502165872 +0300
@@ -1301,13 +1301,10 @@
 
 	pm_runtime_get_sync(up->dev);
 
-	local_irq_save(flags);
-	if (up->port.sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock(&up->port.lock);
+	if (up->port.sysrq || oops_in_progress)
+		locked = spin_trylock_irqsave(&up->port.lock, flags);
 	else
-		spin_lock(&up->port.lock);
+		spin_lock_irqsave(&up->port.lock, flags);
 
 	/*
 	 * First save the IER then disable the interrupts
@@ -1336,8 +1333,7 @@
 	pm_runtime_mark_last_busy(up->dev);
 	pm_runtime_put_autosuspend(up->dev);
 	if (locked)
-		spin_unlock(&up->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&up->port.lock, flags);
 }
 
 static int __init
diff -Naur a/fs/afs/dir_silly.c b/fs/afs/dir_silly.c
--- a/fs/afs/dir_silly.c	2020-11-23 13:48:29.941844599 +0200
+++ b/fs/afs/dir_silly.c	2021-07-14 15:39:08.342159932 +0300
@@ -236,7 +236,7 @@
 	struct dentry *alias;
 	int ret;
 
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	_enter("%p{%pd},%llx", dentry, dentry, vnode->fid.vnode);
 
diff -Naur a/fs/cifs/readdir.c b/fs/cifs/readdir.c
--- a/fs/cifs/readdir.c	2020-11-23 13:48:30.257850934 +0200
+++ b/fs/cifs/readdir.c	2021-07-14 15:39:08.442159226 +0300
@@ -81,7 +81,7 @@
 	struct inode *inode;
 	struct super_block *sb = parent->d_sb;
 	struct cifs_sb_info *cifs_sb = CIFS_SB(sb);
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	cifs_dbg(FYI, "%s: for %s\n", __func__, name->name);
 
diff -Naur a/fs/dcache.c b/fs/dcache.c
--- a/fs/dcache.c	2020-11-23 13:48:29.817842114 +0200
+++ b/fs/dcache.c	2021-07-14 15:39:08.242160639 +0300
@@ -2503,9 +2503,10 @@
 static inline unsigned start_dir_add(struct inode *dir)
 {
 
+	preempt_disable_rt();
 	for (;;) {
-		unsigned n = dir->i_dir_seq;
-		if (!(n & 1) && cmpxchg(&dir->i_dir_seq, n, n + 1) == n)
+		unsigned n = dir->__i_dir_seq;
+		if (!(n & 1) && cmpxchg(&dir->__i_dir_seq, n, n + 1) == n)
 			return n;
 		cpu_relax();
 	}
@@ -2513,26 +2514,30 @@
 
 static inline void end_dir_add(struct inode *dir, unsigned n)
 {
-	smp_store_release(&dir->i_dir_seq, n + 2);
+	smp_store_release(&dir->__i_dir_seq, n + 2);
+	preempt_enable_rt();
 }
 
 static void d_wait_lookup(struct dentry *dentry)
 {
-	if (d_in_lookup(dentry)) {
-		DECLARE_WAITQUEUE(wait, current);
-		add_wait_queue(dentry->d_wait, &wait);
-		do {
-			set_current_state(TASK_UNINTERRUPTIBLE);
-			spin_unlock(&dentry->d_lock);
-			schedule();
-			spin_lock(&dentry->d_lock);
-		} while (d_in_lookup(dentry));
-	}
+	struct swait_queue __wait;
+
+	if (!d_in_lookup(dentry))
+		return;
+
+	INIT_LIST_HEAD(&__wait.task_list);
+	do {
+		prepare_to_swait_exclusive(dentry->d_wait, &__wait, TASK_UNINTERRUPTIBLE);
+		spin_unlock(&dentry->d_lock);
+		schedule();
+		spin_lock(&dentry->d_lock);
+	} while (d_in_lookup(dentry));
+	finish_swait(dentry->d_wait, &__wait);
 }
 
 struct dentry *d_alloc_parallel(struct dentry *parent,
 				const struct qstr *name,
-				wait_queue_head_t *wq)
+				struct swait_queue_head *wq)
 {
 	unsigned int hash = name->hash;
 	struct hlist_bl_head *b = in_lookup_hash(parent, hash);
@@ -2546,7 +2551,7 @@
 
 retry:
 	rcu_read_lock();
-	seq = smp_load_acquire(&parent->d_inode->i_dir_seq);
+	seq = smp_load_acquire(&parent->d_inode->__i_dir_seq);
 	r_seq = read_seqbegin(&rename_lock);
 	dentry = __d_lookup_rcu(parent, name, &d_seq);
 	if (unlikely(dentry)) {
@@ -2574,7 +2579,7 @@
 	}
 
 	hlist_bl_lock(b);
-	if (unlikely(READ_ONCE(parent->d_inode->i_dir_seq) != seq)) {
+	if (unlikely(READ_ONCE(parent->d_inode->__i_dir_seq) != seq)) {
 		hlist_bl_unlock(b);
 		rcu_read_unlock();
 		goto retry;
@@ -2647,7 +2652,7 @@
 	hlist_bl_lock(b);
 	dentry->d_flags &= ~DCACHE_PAR_LOOKUP;
 	__hlist_bl_del(&dentry->d_u.d_in_lookup_hash);
-	wake_up_all(dentry->d_wait);
+	swake_up_all(dentry->d_wait);
 	dentry->d_wait = NULL;
 	hlist_bl_unlock(b);
 	INIT_HLIST_NODE(&dentry->d_u.d_alias);
diff -Naur a/fs/exec.c b/fs/exec.c
--- a/fs/exec.c	2020-11-23 13:48:29.825842275 +0200
+++ b/fs/exec.c	2021-07-14 15:39:08.242160639 +0300
@@ -1130,11 +1130,24 @@
 	}
 
 	task_lock(tsk);
-	active_mm = tsk->active_mm;
 	membarrier_exec_mmap(mm);
-	tsk->mm = mm;
+
+	local_irq_disable();
+	active_mm = tsk->active_mm;
 	tsk->active_mm = mm;
+	tsk->mm = mm;
+	/*
+	 * This prevents preemption while active_mm is being loaded and
+	 * it and mm are being updated, which could cause problems for
+	 * lazy tlb mm refcounting when these are updated by context
+	 * switches. Not all architectures can handle irqs off over
+	 * activate_mm yet.
+	 */
+	if (!IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
+		local_irq_enable();
 	activate_mm(active_mm, mm);
+	if (IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
+		local_irq_enable();
 	tsk->mm->vmacache_seqnum = 0;
 	vmacache_flush(tsk);
 	task_unlock(tsk);
diff -Naur a/fs/fuse/readdir.c b/fs/fuse/readdir.c
--- a/fs/fuse/readdir.c	2020-11-23 13:48:30.833862479 +0200
+++ b/fs/fuse/readdir.c	2021-07-14 15:39:08.618157983 +0300
@@ -158,7 +158,7 @@
 	struct inode *dir = d_inode(parent);
 	struct fuse_conn *fc;
 	struct inode *inode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	if (!o->nodeid) {
 		/*
diff -Naur a/fs/inode.c b/fs/inode.c
--- a/fs/inode.c	2020-11-23 13:48:29.837842515 +0200
+++ b/fs/inode.c	2021-07-14 15:39:08.242160639 +0300
@@ -158,7 +158,7 @@
 	inode->i_bdev = NULL;
 	inode->i_cdev = NULL;
 	inode->i_link = NULL;
-	inode->i_dir_seq = 0;
+	inode->__i_dir_seq = 0;
 	inode->i_rdev = 0;
 	inode->dirtied_when = 0;
 
diff -Naur a/fs/io-wq.c b/fs/io-wq.c
--- a/fs/io-wq.c	2020-11-23 13:48:29.837842515 +0200
+++ b/fs/io-wq.c	2021-07-14 15:39:08.242160639 +0300
@@ -87,7 +87,7 @@
  */
 struct io_wqe {
 	struct {
-		spinlock_t lock;
+		raw_spinlock_t lock;
 		struct io_wq_work_list work_list;
 		unsigned long hash_map;
 		unsigned flags;
@@ -148,7 +148,7 @@
 
 	if (current->files != worker->restore_files) {
 		__acquire(&wqe->lock);
-		spin_unlock_irq(&wqe->lock);
+		raw_spin_unlock_irq(&wqe->lock);
 		dropped_lock = true;
 
 		task_lock(current);
@@ -166,7 +166,7 @@
 	if (worker->mm) {
 		if (!dropped_lock) {
 			__acquire(&wqe->lock);
-			spin_unlock_irq(&wqe->lock);
+			raw_spin_unlock_irq(&wqe->lock);
 			dropped_lock = true;
 		}
 		__set_current_state(TASK_RUNNING);
@@ -220,17 +220,17 @@
 	worker->flags = 0;
 	preempt_enable();
 
-	spin_lock_irq(&wqe->lock);
+	raw_spin_lock_irq(&wqe->lock);
 	hlist_nulls_del_rcu(&worker->nulls_node);
 	list_del_rcu(&worker->all_list);
 	if (__io_worker_unuse(wqe, worker)) {
 		__release(&wqe->lock);
-		spin_lock_irq(&wqe->lock);
+		raw_spin_lock_irq(&wqe->lock);
 	}
 	acct->nr_workers--;
 	nr_workers = wqe->acct[IO_WQ_ACCT_BOUND].nr_workers +
 			wqe->acct[IO_WQ_ACCT_UNBOUND].nr_workers;
-	spin_unlock_irq(&wqe->lock);
+	raw_spin_unlock_irq(&wqe->lock);
 
 	/* all workers gone, wq exit can proceed */
 	if (!nr_workers && refcount_dec_and_test(&wqe->wq->refs))
@@ -504,7 +504,7 @@
 		else if (!wq_list_empty(&wqe->work_list))
 			wqe->flags |= IO_WQE_FLAG_STALLED;
 
-		spin_unlock_irq(&wqe->lock);
+		raw_spin_unlock_irq(&wqe->lock);
 		if (!work)
 			break;
 		io_assign_current_work(worker, work);
@@ -538,17 +538,17 @@
 				io_wqe_enqueue(wqe, linked);
 
 			if (hash != -1U && !next_hashed) {
-				spin_lock_irq(&wqe->lock);
+				raw_spin_lock_irq(&wqe->lock);
 				wqe->hash_map &= ~BIT_ULL(hash);
 				wqe->flags &= ~IO_WQE_FLAG_STALLED;
 				/* skip unnecessary unlock-lock wqe->lock */
 				if (!work)
 					goto get_next;
-				spin_unlock_irq(&wqe->lock);
+				raw_spin_unlock_irq(&wqe->lock);
 			}
 		} while (work);
 
-		spin_lock_irq(&wqe->lock);
+		raw_spin_lock_irq(&wqe->lock);
 	} while (1);
 }
 
@@ -563,7 +563,7 @@
 	while (!test_bit(IO_WQ_BIT_EXIT, &wq->state)) {
 		set_current_state(TASK_INTERRUPTIBLE);
 loop:
-		spin_lock_irq(&wqe->lock);
+		raw_spin_lock_irq(&wqe->lock);
 		if (io_wqe_run_queue(wqe)) {
 			__set_current_state(TASK_RUNNING);
 			io_worker_handle_work(worker);
@@ -574,7 +574,7 @@
 			__release(&wqe->lock);
 			goto loop;
 		}
-		spin_unlock_irq(&wqe->lock);
+		raw_spin_unlock_irq(&wqe->lock);
 		if (signal_pending(current))
 			flush_signals(current);
 		if (schedule_timeout(WORKER_IDLE_TIMEOUT))
@@ -586,11 +586,11 @@
 	}
 
 	if (test_bit(IO_WQ_BIT_EXIT, &wq->state)) {
-		spin_lock_irq(&wqe->lock);
+		raw_spin_lock_irq(&wqe->lock);
 		if (!wq_list_empty(&wqe->work_list))
 			io_worker_handle_work(worker);
 		else
-			spin_unlock_irq(&wqe->lock);
+			raw_spin_unlock_irq(&wqe->lock);
 	}
 
 	io_worker_exit(worker);
@@ -630,9 +630,9 @@
 
 	worker->flags &= ~IO_WORKER_F_RUNNING;
 
-	spin_lock_irq(&wqe->lock);
+	raw_spin_lock_irq(&wqe->lock);
 	io_wqe_dec_running(wqe, worker);
-	spin_unlock_irq(&wqe->lock);
+	raw_spin_unlock_irq(&wqe->lock);
 }
 
 static bool create_io_worker(struct io_wq *wq, struct io_wqe *wqe, int index)
@@ -656,7 +656,7 @@
 		return false;
 	}
 
-	spin_lock_irq(&wqe->lock);
+	raw_spin_lock_irq(&wqe->lock);
 	hlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);
 	list_add_tail_rcu(&worker->all_list, &wqe->all_list);
 	worker->flags |= IO_WORKER_F_FREE;
@@ -665,7 +665,7 @@
 	if (!acct->nr_workers && (worker->flags & IO_WORKER_F_BOUND))
 		worker->flags |= IO_WORKER_F_FIXED;
 	acct->nr_workers++;
-	spin_unlock_irq(&wqe->lock);
+	raw_spin_unlock_irq(&wqe->lock);
 
 	if (index == IO_WQ_ACCT_UNBOUND)
 		atomic_inc(&wq->user->processes);
@@ -720,12 +720,12 @@
 			if (!node_online(node))
 				continue;
 
-			spin_lock_irq(&wqe->lock);
+			raw_spin_lock_irq(&wqe->lock);
 			if (io_wqe_need_worker(wqe, IO_WQ_ACCT_BOUND))
 				fork_worker[IO_WQ_ACCT_BOUND] = true;
 			if (io_wqe_need_worker(wqe, IO_WQ_ACCT_UNBOUND))
 				fork_worker[IO_WQ_ACCT_UNBOUND] = true;
-			spin_unlock_irq(&wqe->lock);
+			raw_spin_unlock_irq(&wqe->lock);
 			if (fork_worker[IO_WQ_ACCT_BOUND])
 				create_io_worker(wq, wqe, IO_WQ_ACCT_BOUND);
 			if (fork_worker[IO_WQ_ACCT_UNBOUND])
@@ -821,10 +821,10 @@
 	}
 
 	work_flags = work->flags;
-	spin_lock_irqsave(&wqe->lock, flags);
+	raw_spin_lock_irqsave(&wqe->lock, flags);
 	io_wqe_insert_work(wqe, work);
 	wqe->flags &= ~IO_WQE_FLAG_STALLED;
-	spin_unlock_irqrestore(&wqe->lock, flags);
+	raw_spin_unlock_irqrestore(&wqe->lock, flags);
 
 	if ((work_flags & IO_WQ_WORK_CONCURRENT) ||
 	    !atomic_read(&acct->nr_running))
@@ -951,13 +951,13 @@
 	unsigned long flags;
 
 retry:
-	spin_lock_irqsave(&wqe->lock, flags);
+	raw_spin_lock_irqsave(&wqe->lock, flags);
 	wq_list_for_each(node, prev, &wqe->work_list) {
 		work = container_of(node, struct io_wq_work, list);
 		if (!match->fn(work, match->data))
 			continue;
 		io_wqe_remove_pending(wqe, work, prev);
-		spin_unlock_irqrestore(&wqe->lock, flags);
+		raw_spin_unlock_irqrestore(&wqe->lock, flags);
 		io_run_cancel(work, wqe);
 		match->nr_pending++;
 		if (!match->cancel_all)
@@ -966,7 +966,7 @@
 		/* not safe to continue after unlock */
 		goto retry;
 	}
-	spin_unlock_irqrestore(&wqe->lock, flags);
+	raw_spin_unlock_irqrestore(&wqe->lock, flags);
 }
 
 static void io_wqe_cancel_running_work(struct io_wqe *wqe,
@@ -1074,7 +1074,7 @@
 		}
 		atomic_set(&wqe->acct[IO_WQ_ACCT_UNBOUND].nr_running, 0);
 		wqe->wq = wq;
-		spin_lock_init(&wqe->lock);
+		raw_spin_lock_init(&wqe->lock);
 		INIT_WQ_LIST(&wqe->work_list);
 		INIT_HLIST_NULLS_HEAD(&wqe->free_list, 0);
 		INIT_LIST_HEAD(&wqe->all_list);
diff -Naur a/fs/namei.c b/fs/namei.c
--- a/fs/namei.c	2020-11-23 13:48:29.857842916 +0200
+++ b/fs/namei.c	2021-07-14 15:39:08.242160639 +0300
@@ -1520,7 +1520,7 @@
 {
 	struct dentry *dentry, *old;
 	struct inode *inode = dir->d_inode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	/* Don't go there if it's already dead */
 	if (unlikely(IS_DEADDIR(inode)))
@@ -3018,7 +3018,7 @@
 	struct dentry *dentry;
 	int error, create_error = 0;
 	umode_t mode = op->mode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	if (unlikely(IS_DEADDIR(dir_inode)))
 		return ERR_PTR(-ENOENT);
diff -Naur a/fs/namespace.c b/fs/namespace.c
--- a/fs/namespace.c	2020-11-23 13:48:29.857842916 +0200
+++ b/fs/namespace.c	2021-07-14 15:39:08.242160639 +0300
@@ -14,6 +14,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/user_namespace.h>
 #include <linux/namei.h>
+#include <linux/delay.h>
 #include <linux/security.h>
 #include <linux/cred.h>
 #include <linux/idr.h>
@@ -321,8 +322,11 @@
 	 * incremented count after it has set MNT_WRITE_HOLD.
 	 */
 	smp_mb();
-	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
-		cpu_relax();
+	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD) {
+		preempt_enable();
+		cpu_chill();
+		preempt_disable();
+	}
 	/*
 	 * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
 	 * be set to match its requirements. So we must not load that until
diff -Naur a/fs/nfs/dir.c b/fs/nfs/dir.c
--- a/fs/nfs/dir.c	2020-11-23 13:48:31.049866809 +0200
+++ b/fs/nfs/dir.c	2021-07-14 15:39:08.938155723 +0300
@@ -484,7 +484,7 @@
 		unsigned long dir_verifier)
 {
 	struct qstr filename = QSTR_INIT(entry->name, entry->len);
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 	struct dentry *dentry;
 	struct dentry *alias;
 	struct inode *inode;
@@ -1665,7 +1665,7 @@
 		    struct file *file, unsigned open_flags,
 		    umode_t mode)
 {
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 	struct nfs_open_context *ctx;
 	struct dentry *res;
 	struct iattr attr = { .ia_valid = ATTR_OPEN };
diff -Naur a/fs/nfs/unlink.c b/fs/nfs/unlink.c
--- a/fs/nfs/unlink.c	2020-11-23 13:48:31.129868413 +0200
+++ b/fs/nfs/unlink.c	2021-07-14 15:39:08.938155723 +0300
@@ -13,7 +13,7 @@
 #include <linux/sunrpc/clnt.h>
 #include <linux/nfs_fs.h>
 #include <linux/sched.h>
-#include <linux/wait.h>
+#include <linux/swait.h>
 #include <linux/namei.h>
 #include <linux/fsnotify.h>
 
@@ -180,7 +180,7 @@
 
 	data->cred = get_current_cred();
 	data->res.dir_attr = &data->dir_attr;
-	init_waitqueue_head(&data->wq);
+	init_swait_queue_head(&data->wq);
 
 	status = -EBUSY;
 	spin_lock(&dentry->d_lock);
diff -Naur a/fs/proc/array.c b/fs/proc/array.c
--- a/fs/proc/array.c	2020-11-23 13:48:31.581877476 +0200
+++ b/fs/proc/array.c	2021-07-14 15:39:09.098154593 +0300
@@ -382,9 +382,9 @@
 static void task_cpus_allowed(struct seq_file *m, struct task_struct *task)
 {
 	seq_printf(m, "Cpus_allowed:\t%*pb\n",
-		   cpumask_pr_args(task->cpus_ptr));
+		   cpumask_pr_args(&task->cpus_mask));
 	seq_printf(m, "Cpus_allowed_list:\t%*pbl\n",
-		   cpumask_pr_args(task->cpus_ptr));
+		   cpumask_pr_args(&task->cpus_mask));
 }
 
 static inline void task_core_dumping(struct seq_file *m, struct mm_struct *mm)
diff -Naur a/fs/proc/base.c b/fs/proc/base.c
--- a/fs/proc/base.c	2020-11-23 13:48:31.585877557 +0200
+++ b/fs/proc/base.c	2021-07-14 15:39:09.098154593 +0300
@@ -96,6 +96,7 @@
 #include <linux/posix-timers.h>
 #include <linux/time_namespace.h>
 #include <linux/resctrl.h>
+#include <linux/swait.h>
 #include <trace/events/oom.h>
 #include "internal.h"
 #include "fd.h"
@@ -2033,7 +2034,7 @@
 
 	child = d_hash_and_lookup(dir, &qname);
 	if (!child) {
-		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+		DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 		child = d_alloc_parallel(dir, &qname, &wq);
 		if (IS_ERR(child))
 			goto end_instantiate;
diff -Naur a/fs/proc/kmsg.c b/fs/proc/kmsg.c
--- a/fs/proc/kmsg.c	2020-11-23 13:48:31.593877717 +0200
+++ b/fs/proc/kmsg.c	2021-07-14 15:39:09.098154593 +0300
@@ -18,8 +18,6 @@
 #include <linux/uaccess.h>
 #include <asm/io.h>
 
-extern wait_queue_head_t log_wait;
-
 static int kmsg_open(struct inode * inode, struct file * file)
 {
 	return do_syslog(SYSLOG_ACTION_OPEN, NULL, 0, SYSLOG_FROM_PROC);
@@ -42,7 +40,7 @@
 
 static __poll_t kmsg_poll(struct file *file, poll_table *wait)
 {
-	poll_wait(file, &log_wait, wait);
+	poll_wait(file, printk_wait_queue(), wait);
 	if (do_syslog(SYSLOG_ACTION_SIZE_UNREAD, NULL, 0, SYSLOG_FROM_PROC))
 		return EPOLLIN | EPOLLRDNORM;
 	return 0;
diff -Naur a/fs/proc/proc_sysctl.c b/fs/proc/proc_sysctl.c
--- a/fs/proc/proc_sysctl.c	2020-11-23 13:48:31.597877797 +0200
+++ b/fs/proc/proc_sysctl.c	2021-07-14 15:39:09.098154593 +0300
@@ -685,7 +685,7 @@
 
 	child = d_lookup(dir, &qname);
 	if (!child) {
-		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+		DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 		child = d_alloc_parallel(dir, &qname, &wq);
 		if (IS_ERR(child))
 			return false;
diff -Naur a/fs/select.c b/fs/select.c
--- a/fs/select.c	2020-11-23 13:48:29.913844038 +0200
+++ b/fs/select.c	2020-11-23 13:50:10.399439709 +0200
@@ -13,6 +13,9 @@
  *  24 January 2000
  *     Changed sys_poll()/do_poll() to use PAGE_SIZE chunk-based allocation 
  *     of fds to overcome nfds < 16390 descriptors limit (Tigran Aivazian).
+ *	2020/10/16
+ *		Laurentiu-Cristian Duca (laurentiu [dot] duca [at] gmail [dot] com)
+ *		Added RTnet select() and poll() system calls.
  */
 
 #include <linux/kernel.h>
@@ -92,7 +95,11 @@
 	return ret;
 }
 
-
+struct poll_table_page_rtnet {
+	struct poll_table_page_rtnet * next;
+	struct poll_table_entry_rtnet * entry;
+	struct poll_table_entry_rtnet entries[0];
+};
 
 struct poll_table_page {
 	struct poll_table_page * next;
@@ -115,9 +122,23 @@
  * as all select/poll functions have to call it to add an entry to the
  * poll table.
  */
+static void __pollwait_rtnet(struct file *filp, wait_queue_head_rtnet_t *wait_address,
+		       poll_table *p);
 static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
 		       poll_table *p);
 
+void poll_initwait_rtnet(struct poll_wqueues_rtnet *pwq)
+{
+	/* dirty type cast to poll_queue_proc, in reality is not true */
+	init_poll_funcptr(&pwq->pt, (poll_queue_proc)__pollwait_rtnet);
+	pwq->polling_task = current;
+	pwq->triggered = 0;
+	pwq->error = 0;
+	pwq->table = NULL;
+	pwq->inline_index = 0;
+}
+EXPORT_SYMBOL(poll_initwait_rtnet);
+
 void poll_initwait(struct poll_wqueues *pwq)
 {
 	init_poll_funcptr(&pwq->pt, __pollwait);
@@ -129,12 +150,40 @@
 }
 EXPORT_SYMBOL(poll_initwait);
 
+static void free_poll_entry_rtnet(struct poll_table_entry_rtnet *entry)
+{
+	remove_wait_queue_rtnet(entry->wait_address, &entry->wait);
+	fput(entry->filp);
+}
+
 static void free_poll_entry(struct poll_table_entry *entry)
 {
 	remove_wait_queue(entry->wait_address, &entry->wait);
 	fput(entry->filp);
 }
 
+void poll_freewait_rtnet(struct poll_wqueues_rtnet *pwq)
+{
+	struct poll_table_page_rtnet * p = pwq->table;
+	int i;
+	for (i = 0; i < pwq->inline_index; i++)
+		free_poll_entry_rtnet(pwq->inline_entries + i);
+	while (p) {
+		struct poll_table_entry_rtnet * entry;
+		struct poll_table_page_rtnet *old;
+
+		entry = p->entry;
+		do {
+			entry--;
+			free_poll_entry_rtnet(entry);
+		} while (entry > p->entries);
+		old = p;
+		p = p->next;
+		free_page((unsigned long) old);
+	}
+}
+EXPORT_SYMBOL(poll_freewait_rtnet);
+
 void poll_freewait(struct poll_wqueues *pwq)
 {
 	struct poll_table_page * p = pwq->table;
@@ -157,6 +206,30 @@
 }
 EXPORT_SYMBOL(poll_freewait);
 
+static struct poll_table_entry_rtnet *poll_get_entry_rtnet(struct poll_wqueues_rtnet *p)
+{
+	struct poll_table_page_rtnet *table = p->table;
+
+	if (p->inline_index < N_INLINE_POLL_ENTRIES)
+		return p->inline_entries + p->inline_index++;
+
+	if (!table || POLL_TABLE_FULL(table)) {
+		struct poll_table_page_rtnet *new_table;
+
+		new_table = (struct poll_table_page_rtnet *) __get_free_page(GFP_KERNEL);
+		if (!new_table) {
+			p->error = -ENOMEM;
+			return NULL;
+		}
+		new_table->entry = new_table->entries;
+		new_table->next = table;
+		p->table = new_table;
+		table = new_table;
+	}
+
+	return table->entry++;
+}
+
 static struct poll_table_entry *poll_get_entry(struct poll_wqueues *p)
 {
 	struct poll_table_page *table = p->table;
@@ -181,6 +254,32 @@
 	return table->entry++;
 }
 
+static int __pollwake_rtnet(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
+{
+	struct poll_wqueues_rtnet *pwq = wait->private;
+	DECLARE_WAITQUEUE(dummy_wait, pwq->polling_task);
+
+	/*
+	 * Although this function is called under waitqueue lock, LOCK
+	 * doesn't imply write barrier and the users expect write
+	 * barrier semantics on wakeup functions.  The following
+	 * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
+	 * and is paired with smp_store_mb() in poll_schedule_timeout_rtnet.
+	 */
+	smp_wmb();
+	pwq->triggered = 1;
+
+	/*
+	 * Perform the default wake up operation using a dummy
+	 * waitqueue.
+	 *
+	 * TODO: This is hacky but there currently is no interface to
+	 * pass in @sync.  @sync is scheduled to be removed and once
+	 * that happens, wake_up_process() can be used directly.
+	 */
+	return default_wake_function(&dummy_wait, mode, sync, key);
+}
+
 static int __pollwake(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
 	struct poll_wqueues *pwq = wait->private;
@@ -207,6 +306,16 @@
 	return default_wake_function(&dummy_wait, mode, sync, key);
 }
 
+static int pollwake_rtnet(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
+{
+	struct poll_table_entry_rtnet *entry;
+
+	entry = container_of(wait, struct poll_table_entry_rtnet, wait);
+	if (key && !(key_to_poll(key) & entry->key))
+		return 0;
+	return __pollwake_rtnet(wait, mode, sync, key);
+}
+
 static int pollwake(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
 	struct poll_table_entry *entry;
@@ -218,6 +327,22 @@
 }
 
 /* Add a new entry */
+static void __pollwait_rtnet(struct file *filp, wait_queue_head_rtnet_t *wait_address,
+				poll_table *p)
+{
+	struct poll_wqueues_rtnet *pwq = container_of(p, struct poll_wqueues_rtnet, pt);
+	struct poll_table_entry_rtnet *entry = poll_get_entry_rtnet(pwq);
+	if (!entry)
+		return;
+	entry->filp = get_file(filp);
+	entry->wait_address = wait_address;
+	entry->key = p->_key;
+	init_waitqueue_func_entry(&entry->wait, pollwake_rtnet);
+	entry->wait.private = pwq;
+	add_wait_queue_rtnet(wait_address, &entry->wait);
+}
+
+/* Add a new entry */
 static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
 				poll_table *p)
 {
@@ -233,6 +358,32 @@
 	add_wait_queue(wait_address, &entry->wait);
 }
 
+static int poll_schedule_timeout_rtnet(struct poll_wqueues_rtnet *pwq, int state,
+			  ktime_t *expires, unsigned long slack)
+{
+	int rc = -EINTR;
+
+	set_current_state(state);
+	if (!pwq->triggered)
+		rc = schedule_hrtimeout_range(expires, slack, HRTIMER_MODE_ABS);
+	__set_current_state(TASK_RUNNING);
+
+	/*
+	 * Prepare for the next iteration.
+	 *
+	 * The following smp_store_mb() serves two purposes.  First, it's
+	 * the counterpart rmb of the wmb in pollwake() such that data
+	 * written before wake up is always visible after wake up.
+	 * Second, the full barrier guarantees that triggered clearing
+	 * doesn't pass event check of the next iteration.  Note that
+	 * this problem doesn't exist for the first iteration as
+	 * add_wait_queue() has full barrier semantics.
+	 */
+	smp_store_mb(pwq->triggered, 0);
+
+	return rc;
+}
+
 static int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
 			  ktime_t *expires, unsigned long slack)
 {
@@ -473,6 +624,144 @@
 		wait->_key |= POLLOUT_SET;
 }
 
+
+static int do_select_rtnet(int n, fd_set_bits *fds, struct timespec64 *end_time)
+{
+	ktime_t expire, *to = NULL;
+	struct poll_wqueues_rtnet table;
+	poll_table *wait;
+	int retval, i, timed_out = 0;
+	u64 slack = 0;
+	__poll_t busy_flag = net_busy_loop_on() ? POLL_BUSY_LOOP : 0;
+	unsigned long busy_start = 0;
+
+	rcu_read_lock();
+	retval = max_select_fd(n, fds);
+	rcu_read_unlock();
+
+	if (retval < 0)
+		return retval;
+	n = retval;
+
+	poll_initwait_rtnet(&table);
+	wait = &table.pt;
+	if (end_time && !end_time->tv_sec && !end_time->tv_nsec) {
+		wait->_qproc = NULL;
+		timed_out = 1;
+	}
+
+	if (end_time && !timed_out)
+		slack = select_estimate_accuracy(end_time);
+
+	retval = 0;
+	for (;;) {
+		unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp;
+		bool can_busy_loop = false;
+
+		inp = fds->in; outp = fds->out; exp = fds->ex;
+		rinp = fds->res_in; routp = fds->res_out; rexp = fds->res_ex;
+
+		for (i = 0; i < n; ++rinp, ++routp, ++rexp) {
+			unsigned long in, out, ex, all_bits, bit = 1, j;
+			unsigned long res_in = 0, res_out = 0, res_ex = 0;
+			__poll_t mask;
+
+			in = *inp++; out = *outp++; ex = *exp++;
+			all_bits = in | out | ex;
+			if (all_bits == 0) {
+				i += BITS_PER_LONG;
+				continue;
+			}
+
+			for (j = 0; j < BITS_PER_LONG; ++j, ++i, bit <<= 1) {
+				struct fd f;
+				if (i >= n)
+					break;
+				if (!(bit & all_bits))
+					continue;
+				f = fdget(i);
+				if (f.file) {
+					wait_key_set(wait, in, out, bit,
+						     busy_flag);
+					mask = vfs_poll(f.file, wait);
+
+					fdput(f);
+					if ((mask & POLLIN_SET) && (in & bit)) {
+						res_in |= bit;
+						retval++;
+						wait->_qproc = NULL;
+					}
+					if ((mask & POLLOUT_SET) && (out & bit)) {
+						res_out |= bit;
+						retval++;
+						wait->_qproc = NULL;
+					}
+					if ((mask & POLLEX_SET) && (ex & bit)) {
+						res_ex |= bit;
+						retval++;
+						wait->_qproc = NULL;
+					}
+					/* got something, stop busy polling */
+					if (retval) {
+						can_busy_loop = false;
+						busy_flag = 0;
+
+					/*
+					 * only remember a returned
+					 * POLL_BUSY_LOOP if we asked for it
+					 */
+					} else if (busy_flag & mask)
+						can_busy_loop = true;
+
+				}
+			}
+			if (res_in)
+				*rinp = res_in;
+			if (res_out)
+				*routp = res_out;
+			if (res_ex)
+				*rexp = res_ex;
+			cond_resched();
+		}
+		wait->_qproc = NULL;
+		if (retval || timed_out || signal_pending(current))
+			break;
+		if (table.error) {
+			retval = table.error;
+			break;
+		}
+
+		/* only if found POLL_BUSY_LOOP sockets && not out of time */
+		if (can_busy_loop && !need_resched()) {
+			if (!busy_start) {
+				busy_start = busy_loop_current_time();
+				continue;
+			}
+			if (!busy_loop_timeout(busy_start))
+				continue;
+		}
+		busy_flag = 0;
+
+		/*
+		 * If this is the first loop and we have a timeout
+		 * given, then we convert to ktime_t and set the to
+		 * pointer to the expiry value.
+		 */
+		if (end_time && !to) {
+			expire = timespec64_to_ktime(*end_time);
+			to = &expire;
+		}
+
+		if (!poll_schedule_timeout_rtnet(&table, TASK_INTERRUPTIBLE,
+					   to, slack))
+			timed_out = 1;
+	}
+
+	poll_freewait_rtnet(&table);
+
+	return retval;
+}
+
 static int do_select(int n, fd_set_bits *fds, struct timespec64 *end_time)
 {
 	ktime_t expire, *to = NULL;
@@ -618,6 +907,93 @@
  * Update: ERESTARTSYS breaks at least the xview clock binary, so
  * I'm trying ERESTARTNOHAND which restart only when you want to.
  */
+int core_sys_select_rtnet(int n, fd_set __user *inp, fd_set __user *outp,
+			   fd_set __user *exp, struct timespec64 *end_time)
+{
+	fd_set_bits fds;
+	void *bits;
+	int ret, max_fds;
+	size_t size, alloc_size;
+	struct fdtable *fdt;
+	/* Allocate small arguments on the stack to save memory and be faster */
+	long stack_fds[SELECT_STACK_ALLOC/sizeof(long)];
+
+	ret = -EINVAL;
+	if (n < 0)
+		goto out_nofds;
+
+	/* max_fds can increase, so grab it once to avoid race */
+	rcu_read_lock();
+	fdt = files_fdtable(current->files);
+	max_fds = fdt->max_fds;
+	rcu_read_unlock();
+	if (n > max_fds)
+		n = max_fds;
+
+	/*
+	 * We need 6 bitmaps (in/out/ex for both incoming and outgoing),
+	 * since we used fdset we need to allocate memory in units of
+	 * long-words. 
+	 */
+	size = FDS_BYTES(n);
+	bits = stack_fds;
+	if (size > sizeof(stack_fds) / 6) {
+		/* Not enough space in on-stack array; must use kmalloc */
+		ret = -ENOMEM;
+		if (size > (SIZE_MAX / 6))
+			goto out_nofds;
+
+		alloc_size = 6 * size;
+		bits = kvmalloc(alloc_size, GFP_KERNEL);
+		if (!bits)
+			goto out_nofds;
+	}
+	fds.in      = bits;
+	fds.out     = bits +   size;
+	fds.ex      = bits + 2*size;
+	fds.res_in  = bits + 3*size;
+	fds.res_out = bits + 4*size;
+	fds.res_ex  = bits + 5*size;
+
+	if ((ret = get_fd_set(n, inp, fds.in)) ||
+	    (ret = get_fd_set(n, outp, fds.out)) ||
+	    (ret = get_fd_set(n, exp, fds.ex)))
+		goto out;
+	zero_fd_set(n, fds.res_in);
+	zero_fd_set(n, fds.res_out);
+	zero_fd_set(n, fds.res_ex);
+
+	ret = do_select_rtnet(n, &fds, end_time);
+
+	if (ret < 0)
+		goto out;
+	if (!ret) {
+		ret = -ERESTARTNOHAND;
+		if (signal_pending(current))
+			goto out;
+		ret = 0;
+	}
+
+	if (set_fd_set(n, inp, fds.res_in) ||
+	    set_fd_set(n, outp, fds.res_out) ||
+	    set_fd_set(n, exp, fds.res_ex))
+		ret = -EFAULT;
+
+out:
+	if (bits != stack_fds)
+		kvfree(bits);
+out_nofds:
+	return ret;
+}
+
+/*
+ * We can actually return ERESTARTSYS instead of EINTR, but I'd
+ * like to be certain this leads to no problems. So I return
+ * EINTR just for safety.
+ *
+ * Update: ERESTARTSYS breaks at least the xview clock binary, so
+ * I'm trying ERESTARTNOHAND which restart only when you want to.
+ */
 int core_sys_select(int n, fd_set __user *inp, fd_set __user *outp,
 			   fd_set __user *exp, struct timespec64 *end_time)
 {
@@ -697,6 +1073,28 @@
 	return ret;
 }
 
+static int kern_select_rtnet(int n, fd_set __user *inp, fd_set __user *outp,
+		       fd_set __user *exp, struct __kernel_old_timeval __user *tvp)
+{
+	struct timespec64 end_time, *to = NULL;
+	struct __kernel_old_timeval tv;
+	int ret;
+
+	if (tvp) {
+		if (copy_from_user(&tv, tvp, sizeof(tv)))
+			return -EFAULT;
+
+		to = &end_time;
+		if (poll_select_set_timeout(to,
+				tv.tv_sec + (tv.tv_usec / USEC_PER_SEC),
+				(tv.tv_usec % USEC_PER_SEC) * NSEC_PER_USEC))
+			return -EINVAL;
+	}
+
+	ret = core_sys_select_rtnet(n, inp, outp, exp, to);
+	return poll_select_finish(&end_time, tvp, PT_TIMEVAL, ret);
+}
+
 static int kern_select(int n, fd_set __user *inp, fd_set __user *outp,
 		       fd_set __user *exp, struct __kernel_old_timeval __user *tvp)
 {
@@ -719,6 +1117,12 @@
 	return poll_select_finish(&end_time, tvp, PT_TIMEVAL, ret);
 }
 
+SYSCALL_DEFINE5(select_rtnet, int, n, fd_set __user *, inp, fd_set __user *, outp,
+		fd_set __user *, exp, struct __kernel_old_timeval __user *, tvp)
+{
+	return kern_select_rtnet(n, inp, outp, exp, tvp);
+}
+
 SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp,
 		fd_set __user *, exp, struct __kernel_old_timeval __user *, tvp)
 {
@@ -878,6 +1282,92 @@
 	return mask;
 }
 
+static int do_poll_rtnet(struct poll_list *list, struct poll_wqueues_rtnet *wait,
+		   struct timespec64 *end_time)
+{
+	poll_table* pt = &wait->pt;
+	ktime_t expire, *to = NULL;
+	int timed_out = 0, count = 0;
+	u64 slack = 0;
+	__poll_t busy_flag = net_busy_loop_on() ? POLL_BUSY_LOOP : 0;
+	unsigned long busy_start = 0;
+
+	/* Optimise the no-wait case */
+	if (end_time && !end_time->tv_sec && !end_time->tv_nsec) {
+		pt->_qproc = NULL;
+		timed_out = 1;
+	}
+
+	if (end_time && !timed_out)
+		slack = select_estimate_accuracy(end_time);
+
+	for (;;) {
+		struct poll_list *walk;
+		bool can_busy_loop = false;
+
+		for (walk = list; walk != NULL; walk = walk->next) {
+			struct pollfd * pfd, * pfd_end;
+
+			pfd = walk->entries;
+			pfd_end = pfd + walk->len;
+			for (; pfd != pfd_end; pfd++) {
+				/*
+				 * Fish for events. If we found one, record it
+				 * and kill poll_table->_qproc, so we don't
+				 * needlessly register any other waiters after
+				 * this. They'll get immediately deregistered
+				 * when we break out and return.
+				 */
+				if (do_pollfd(pfd, pt, &can_busy_loop,
+					      busy_flag)) {
+					count++;
+					pt->_qproc = NULL;
+					/* found something, stop busy polling */
+					busy_flag = 0;
+					can_busy_loop = false;
+				}
+			}
+		}
+		/*
+		 * All waiters have already been registered, so don't provide
+		 * a poll_table->_qproc to them on the next loop iteration.
+		 */
+		pt->_qproc = NULL;
+		if (!count) {
+			count = wait->error;
+			if (signal_pending(current))
+				count = -ERESTARTNOHAND;
+		}
+		if (count || timed_out)
+			break;
+
+		/* only if found POLL_BUSY_LOOP sockets && not out of time */
+		if (can_busy_loop && !need_resched()) {
+			if (!busy_start) {
+				busy_start = busy_loop_current_time();
+				continue;
+			}
+			if (!busy_loop_timeout(busy_start))
+				continue;
+		}
+		busy_flag = 0;
+
+		/*
+		 * If this is the first loop and we have a timeout
+		 * given, then we convert to ktime_t and set the to
+		 * pointer to the expiry value.
+		 */
+		if (end_time && !to) {
+			expire = timespec64_to_ktime(*end_time);
+			to = &expire;
+		}
+
+		if (!poll_schedule_timeout_rtnet(wait, TASK_INTERRUPTIBLE, to, slack))
+			timed_out = 1;
+	}
+	return count;
+}
+
 static int do_poll(struct poll_list *list, struct poll_wqueues *wait,
 		   struct timespec64 *end_time)
 {
@@ -967,6 +1457,71 @@
 #define N_STACK_PPS ((sizeof(stack_pps) - sizeof(struct poll_list))  / \
 			sizeof(struct pollfd))
 
+static int do_sys_poll_rtnet(struct pollfd __user *ufds, unsigned int nfds,
+		struct timespec64 *end_time)
+{
+	struct poll_wqueues_rtnet table;
+	int err = -EFAULT, fdcount, len;
+	/* Allocate small arguments on the stack to save memory and be
+	   faster - use long to make sure the buffer is aligned properly
+	   on 64 bit archs to avoid unaligned access */
+	long stack_pps[POLL_STACK_ALLOC/sizeof(long)];
+	struct poll_list *const head = (struct poll_list *)stack_pps;
+ 	struct poll_list *walk = head;
+ 	unsigned long todo = nfds;
+
+	if (nfds > rlimit(RLIMIT_NOFILE))
+		return -EINVAL;
+
+	len = min_t(unsigned int, nfds, N_STACK_PPS);
+	for (;;) {
+		walk->next = NULL;
+		walk->len = len;
+		if (!len)
+			break;
+
+		if (copy_from_user(walk->entries, ufds + nfds-todo,
+					sizeof(struct pollfd) * walk->len))
+			goto out_fds;
+
+		todo -= walk->len;
+		if (!todo)
+			break;
+
+		len = min(todo, POLLFD_PER_PAGE);
+		walk = walk->next = kmalloc(struct_size(walk, entries, len),
+					    GFP_KERNEL);
+		if (!walk) {
+			err = -ENOMEM;
+			goto out_fds;
+		}
+	}
+
+	poll_initwait_rtnet(&table);
+	fdcount = do_poll_rtnet(head, &table, end_time);
+	poll_freewait_rtnet(&table);
+
+	for (walk = head; walk; walk = walk->next) {
+		struct pollfd *fds = walk->entries;
+		int j;
+
+		for (j = 0; j < walk->len; j++, ufds++)
+			if (__put_user(fds[j].revents, &ufds->revents))
+				goto out_fds;
+  	}
+
+	err = fdcount;
+out_fds:
+	walk = head->next;
+	while (walk) {
+		struct poll_list *pos = walk;
+		walk = walk->next;
+		kfree(pos);
+	}
+
+	return err;
+}
+
 static int do_sys_poll(struct pollfd __user *ufds, unsigned int nfds,
 		struct timespec64 *end_time)
 {
@@ -1032,6 +1587,28 @@
 	return err;
 }
 
+static long do_restart_poll_rtnet(struct restart_block *restart_block)
+{
+	struct pollfd __user *ufds = restart_block->poll.ufds;
+	int nfds = restart_block->poll.nfds;
+	struct timespec64 *to = NULL, end_time;
+	int ret;
+
+	if (restart_block->poll.has_timeout) {
+		end_time.tv_sec = restart_block->poll.tv_sec;
+		end_time.tv_nsec = restart_block->poll.tv_nsec;
+		to = &end_time;
+	}
+
+	ret = do_sys_poll_rtnet(ufds, nfds, to);
+
+	if (ret == -ERESTARTNOHAND) {
+		restart_block->fn = do_restart_poll_rtnet;
+		ret = -ERESTART_RESTARTBLOCK;
+	}
+	return ret;
+}
+
 static long do_restart_poll(struct restart_block *restart_block)
 {
 	struct pollfd __user *ufds = restart_block->poll.ufds;
@@ -1052,6 +1629,40 @@
 		ret = -ERESTART_RESTARTBLOCK;
 	}
 	return ret;
+}
+
+SYSCALL_DEFINE3(poll_rtnet, struct pollfd __user *, ufds, unsigned int, nfds,
+		int, timeout_msecs)
+{
+	struct timespec64 end_time, *to = NULL;
+	int ret;
+
+	if (timeout_msecs >= 0) {
+		to = &end_time;
+		poll_select_set_timeout(to, timeout_msecs / MSEC_PER_SEC,
+			NSEC_PER_MSEC * (timeout_msecs % MSEC_PER_SEC));
+	}
+
+	ret = do_sys_poll_rtnet(ufds, nfds, to);
+
+	if (ret == -ERESTARTNOHAND) {
+		struct restart_block *restart_block;
+
+		restart_block = &current->restart_block;
+		restart_block->fn = do_restart_poll_rtnet;
+		restart_block->poll.ufds = ufds;
+		restart_block->poll.nfds = nfds;
+
+		if (timeout_msecs >= 0) {
+			restart_block->poll.tv_sec = end_time.tv_sec;
+			restart_block->poll.tv_nsec = end_time.tv_nsec;
+			restart_block->poll.has_timeout = 1;
+		} else
+			restart_block->poll.has_timeout = 0;
+
+		ret = -ERESTART_RESTARTBLOCK;
+	}
+	return ret;
 }
 
 SYSCALL_DEFINE3(poll, struct pollfd __user *, ufds, unsigned int, nfds,
diff -Naur a/include/asm-generic/preempt.h b/include/asm-generic/preempt.h
--- a/include/asm-generic/preempt.h	2020-11-23 13:48:32.089887662 +0200
+++ b/include/asm-generic/preempt.h	2021-07-14 15:39:09.278153324 +0300
@@ -79,6 +79,9 @@
 }
 
 #ifdef CONFIG_PREEMPTION
+#ifdef CONFIG_PREEMPT_RT
+extern void preempt_schedule_lock(void);
+#endif
 extern asmlinkage void preempt_schedule(void);
 #define __preempt_schedule() preempt_schedule()
 extern asmlinkage void preempt_schedule_notrace(void);
diff -Naur a/include/linux/bottom_half.h b/include/linux/bottom_half.h
--- a/include/linux/bottom_half.h	2020-11-23 13:48:32.501895925 +0200
+++ b/include/linux/bottom_half.h	2021-07-14 15:39:09.826149459 +0300
@@ -4,7 +4,7 @@
 
 #include <linux/preempt.h>
 
-#ifdef CONFIG_TRACE_IRQFLAGS
+#if defined(CONFIG_PREEMPT_RT) || defined(CONFIG_TRACE_IRQFLAGS)
 extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
 #else
 static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
@@ -32,4 +32,10 @@
 	__local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern bool local_bh_blocked(void);
+#else
+static inline bool local_bh_blocked(void) { return false; }
+#endif
+
 #endif /* _LINUX_BH_H */
diff -Naur a/include/linux/console.h b/include/linux/console.h
--- a/include/linux/console.h	2020-11-23 13:48:32.533896567 +0200
+++ b/include/linux/console.h	2021-07-14 15:39:09.826149459 +0300
@@ -141,6 +141,7 @@
 struct console {
 	char	name[16];
 	void	(*write)(struct console *, const char *, unsigned);
+	void	(*write_atomic)(struct console *, const char *, unsigned);
 	int	(*read)(struct console *, char *, unsigned);
 	struct tty_driver *(*device)(struct console *, int *);
 	void	(*unblank)(void);
@@ -150,6 +151,8 @@
 	short	flags;
 	short	index;
 	int	cflag;
+	unsigned long printk_seq;
+	int	wrote_history;
 	void	*data;
 	struct	 console *next;
 };
@@ -230,4 +233,7 @@
 void dummycon_register_output_notifier(struct notifier_block *nb);
 void dummycon_unregister_output_notifier(struct notifier_block *nb);
 
+extern void console_atomic_lock(unsigned int *flags);
+extern void console_atomic_unlock(unsigned int flags);
+
 #endif /* _LINUX_CONSOLE_H */
diff -Naur a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h
--- a/include/linux/cpuhotplug.h	2020-11-23 13:48:32.541896728 +0200
+++ b/include/linux/cpuhotplug.h	2021-07-14 15:39:09.830149431 +0300
@@ -151,6 +151,7 @@
 	CPUHP_AP_ONLINE,
 	CPUHP_TEARDOWN_CPU,
 	CPUHP_AP_ONLINE_IDLE,
+	CPUHP_AP_SCHED_WAIT_EMPTY,
 	CPUHP_AP_SMPBOOT_THREADS,
 	CPUHP_AP_X86_VDSO_VMA_ONLINE,
 	CPUHP_AP_IRQ_AFFINITY_ONLINE,
diff -Naur a/include/linux/cpumask.h b/include/linux/cpumask.h
--- a/include/linux/cpumask.h	2020-11-23 13:48:32.541896728 +0200
+++ b/include/linux/cpumask.h	2021-07-14 15:39:09.830149431 +0300
@@ -199,6 +199,11 @@
 	return cpumask_next_and(-1, src1p, src2p);
 }
 
+static inline int cpumask_any_distribute(const struct cpumask *srcp)
+{
+	return cpumask_first(srcp);
+}
+
 #define for_each_cpu(cpu, mask)			\
 	for ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask)
 #define for_each_cpu_not(cpu, mask)		\
@@ -252,6 +257,7 @@
 unsigned int cpumask_local_spread(unsigned int i, int node);
 int cpumask_any_and_distribute(const struct cpumask *src1p,
 			       const struct cpumask *src2p);
+int cpumask_any_distribute(const struct cpumask *srcp);
 
 /**
  * for_each_cpu - iterate over every cpu in a mask
diff -Naur a/include/linux/dcache.h b/include/linux/dcache.h
--- a/include/linux/dcache.h	2020-11-23 13:48:32.553896968 +0200
+++ b/include/linux/dcache.h	2021-07-14 15:39:09.830149431 +0300
@@ -106,7 +106,7 @@
 
 	union {
 		struct list_head d_lru;		/* LRU list */
-		wait_queue_head_t *d_wait;	/* in-lookup ones only */
+		struct swait_queue_head *d_wait;	/* in-lookup ones only */
 	};
 	struct list_head d_child;	/* child of parent list */
 	struct list_head d_subdirs;	/* our children */
@@ -238,7 +238,7 @@
 extern struct dentry * d_alloc(struct dentry *, const struct qstr *);
 extern struct dentry * d_alloc_anon(struct super_block *);
 extern struct dentry * d_alloc_parallel(struct dentry *, const struct qstr *,
-					wait_queue_head_t *);
+					struct swait_queue_head *);
 extern struct dentry * d_splice_alias(struct inode *, struct dentry *);
 extern struct dentry * d_add_ci(struct dentry *, struct inode *, struct qstr *);
 extern struct dentry * d_exact_alias(struct dentry *, struct inode *);
diff -Naur a/include/linux/debug_locks.h b/include/linux/debug_locks.h
--- a/include/linux/debug_locks.h	2020-11-23 13:48:32.553896968 +0200
+++ b/include/linux/debug_locks.h	2021-07-14 15:39:09.830149431 +0300
@@ -2,9 +2,8 @@
 #ifndef __LINUX_DEBUG_LOCKING_H
 #define __LINUX_DEBUG_LOCKING_H
 
-#include <linux/kernel.h>
 #include <linux/atomic.h>
-#include <linux/bug.h>
+#include <linux/cache.h>
 
 struct task_struct;
 
diff -Naur a/include/linux/delay.h b/include/linux/delay.h
--- a/include/linux/delay.h	2020-11-23 13:48:32.553896968 +0200
+++ b/include/linux/delay.h	2021-07-14 15:39:09.830149431 +0300
@@ -76,4 +76,10 @@
 		msleep(DIV_ROUND_UP(usecs, 1000));
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern void cpu_chill(void);
+#else
+# define cpu_chill()	cpu_relax()
+#endif
+
 #endif /* defined(_LINUX_DELAY_H) */
diff -Naur a/include/linux/entry-common.h b/include/linux/entry-common.h
--- a/include/linux/entry-common.h	2020-11-23 13:48:32.585897610 +0200
+++ b/include/linux/entry-common.h	2021-07-14 15:39:09.830149431 +0300
@@ -69,7 +69,7 @@
 
 #define EXIT_TO_USER_MODE_WORK						\
 	(_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_UPROBE |		\
-	 _TIF_NEED_RESCHED | _TIF_PATCH_PENDING |			\
+	 _TIF_NEED_RESCHED_MASK | _TIF_PATCH_PENDING |			\
 	 ARCH_EXIT_TO_USER_MODE_WORK)
 
 /**
diff -Naur a/include/linux/fs.h b/include/linux/fs.h
--- a/include/linux/fs.h	2020-11-23 13:48:32.609898091 +0200
+++ b/include/linux/fs.h	2021-07-14 15:39:09.830149431 +0300
@@ -704,7 +704,7 @@
 		struct block_device	*i_bdev;
 		struct cdev		*i_cdev;
 		char			*i_link;
-		unsigned		i_dir_seq;
+		unsigned		__i_dir_seq;
 	};
 
 	__u32			i_generation;
diff -Naur a/include/linux/hardirq.h b/include/linux/hardirq.h
--- a/include/linux/hardirq.h	2020-11-23 13:48:32.633898573 +0200
+++ b/include/linux/hardirq.h	2021-07-14 15:39:09.830149431 +0300
@@ -8,6 +8,7 @@
 #include <linux/ftrace_irq.h>
 #include <linux/vtime.h>
 #include <asm/hardirq.h>
+#include <linux/sched.h>
 
 extern void synchronize_irq(unsigned int irq);
 extern bool synchronize_hardirq(unsigned int irq);
@@ -115,7 +116,6 @@
 	do {							\
 		lockdep_off();					\
 		arch_nmi_enter();				\
-		printk_nmi_enter();				\
 		BUG_ON(in_nmi() == NMI_MASK);			\
 		__preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 	} while (0)
@@ -134,7 +134,6 @@
 	do {							\
 		BUG_ON(!in_nmi());				\
 		__preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
-		printk_nmi_exit();				\
 		arch_nmi_exit();				\
 		lockdep_on();					\
 	} while (0)
diff -Naur a/include/linux/highmem.h b/include/linux/highmem.h
--- a/include/linux/highmem.h	2020-11-23 13:48:32.637898653 +0200
+++ b/include/linux/highmem.h	2021-07-14 15:39:09.830149431 +0300
@@ -8,6 +8,7 @@
 #include <linux/mm.h>
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
+#include <linux/sched.h>
 
 #include <asm/cacheflush.h>
 
@@ -83,7 +84,7 @@
  */
 static inline void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 {
-	preempt_disable();
+	migrate_disable();
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
@@ -153,7 +154,7 @@
 
 static inline void *kmap_atomic(struct page *page)
 {
-	preempt_disable();
+	migrate_disable();
 	pagefault_disable();
 	return page_address(page);
 }
@@ -178,32 +179,51 @@
 
 #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
 
+#ifndef CONFIG_PREEMPT_RT
 DECLARE_PER_CPU(int, __kmap_atomic_idx);
+#endif
 
 static inline int kmap_atomic_idx_push(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	int idx = __this_cpu_inc_return(__kmap_atomic_idx) - 1;
 
-#ifdef CONFIG_DEBUG_HIGHMEM
+# ifdef CONFIG_DEBUG_HIGHMEM
 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
 	BUG_ON(idx >= KM_TYPE_NR);
-#endif
+# endif
 	return idx;
+#else
+	current->kmap_idx++;
+	BUG_ON(current->kmap_idx > KM_TYPE_NR);
+	return current->kmap_idx - 1;
+#endif
 }
 
 static inline int kmap_atomic_idx(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	return __this_cpu_read(__kmap_atomic_idx) - 1;
+#else
+	return current->kmap_idx - 1;
+#endif
 }
 
 static inline void kmap_atomic_idx_pop(void)
 {
-#ifdef CONFIG_DEBUG_HIGHMEM
+#ifndef CONFIG_PREEMPT_RT
+# ifdef CONFIG_DEBUG_HIGHMEM
 	int idx = __this_cpu_dec_return(__kmap_atomic_idx);
 
 	BUG_ON(idx < 0);
-#else
+# else
 	__this_cpu_dec(__kmap_atomic_idx);
+# endif
+#else
+	current->kmap_idx--;
+# ifdef CONFIG_DEBUG_HIGHMEM
+	BUG_ON(current->kmap_idx < 0);
+# endif
 #endif
 }
 
@@ -218,7 +238,7 @@
 	BUILD_BUG_ON(__same_type((addr), struct page *));       \
 	kunmap_atomic_high(addr);                                  \
 	pagefault_enable();                                     \
-	preempt_enable();                                       \
+	migrate_enable();					\
 } while (0)
 
 
diff -Naur a/include/linux/interrupt.h b/include/linux/interrupt.h
--- a/include/linux/interrupt.h	2020-11-23 13:48:32.677899455 +0200
+++ b/include/linux/interrupt.h	2021-07-14 15:39:09.830149431 +0300
@@ -560,7 +560,7 @@
 asmlinkage void do_softirq(void);
 asmlinkage void __do_softirq(void);
 
-#ifdef __ARCH_HAS_DO_SOFTIRQ
+#if defined(__ARCH_HAS_DO_SOFTIRQ) && !defined(CONFIG_PREEMPT_RT)
 void do_softirq_own_stack(void);
 #else
 static inline void do_softirq_own_stack(void)
@@ -654,7 +654,7 @@
 	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
 };
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)
 static inline int tasklet_trylock(struct tasklet_struct *t)
 {
 	return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
@@ -665,15 +665,11 @@
 	smp_mb__before_atomic();
 	clear_bit(TASKLET_STATE_RUN, &(t)->state);
 }
-
-static inline void tasklet_unlock_wait(struct tasklet_struct *t)
-{
-	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) { barrier(); }
-}
+void tasklet_unlock_wait(struct tasklet_struct *t);
 #else
-#define tasklet_trylock(t) 1
-#define tasklet_unlock_wait(t) do { } while (0)
-#define tasklet_unlock(t) do { } while (0)
+static inline int tasklet_trylock(struct tasklet_struct *t) { return 1; }
+static inline void tasklet_unlock(struct tasklet_struct *t) { }
+static inline void tasklet_unlock_wait(struct tasklet_struct *t) { }
 #endif
 
 extern void __tasklet_schedule(struct tasklet_struct *t);
diff -Naur a/include/linux/irqdesc.h b/include/linux/irqdesc.h
--- a/include/linux/irqdesc.h	2020-11-23 13:48:32.693899776 +0200
+++ b/include/linux/irqdesc.h	2021-07-14 15:39:09.834149403 +0300
@@ -68,6 +68,7 @@
 	unsigned int		irqs_unhandled;
 	atomic_t		threads_handled;
 	int			threads_handled_last;
+	u64			random_ip;
 	raw_spinlock_t		lock;
 	struct cpumask		*percpu_enabled;
 	const struct cpumask	*percpu_affinity;
diff -Naur a/include/linux/irqflags.h b/include/linux/irqflags.h
--- a/include/linux/irqflags.h	2020-11-23 13:48:32.693899776 +0200
+++ b/include/linux/irqflags.h	2021-07-14 15:39:09.834149403 +0300
@@ -71,14 +71,6 @@
 do {						\
 	__this_cpu_dec(hardirq_context);	\
 } while (0)
-# define lockdep_softirq_enter()		\
-do {						\
-	current->softirq_context++;		\
-} while (0)
-# define lockdep_softirq_exit()			\
-do {						\
-	current->softirq_context--;		\
-} while (0)
 
 # define lockdep_hrtimer_enter(__hrtimer)		\
 ({							\
@@ -140,6 +132,21 @@
 # define lockdep_irq_work_exit(__work)		do { } while (0)
 #endif
 
+#if defined(CONFIG_TRACE_IRQFLAGS) && !defined(CONFIG_PREEMPT_RT)
+# define lockdep_softirq_enter()		\
+do {						\
+	current->softirq_context++;		\
+} while (0)
+# define lockdep_softirq_exit()			\
+do {						\
+	current->softirq_context--;		\
+} while (0)
+
+#else
+# define lockdep_softirq_enter()		do { } while (0)
+# define lockdep_softirq_exit()			do { } while (0)
+#endif
+
 #if defined(CONFIG_IRQSOFF_TRACER) || \
 	defined(CONFIG_PREEMPT_TRACER)
  extern void stop_critical_timings(void);
diff -Naur a/include/linux/irq_work.h b/include/linux/irq_work.h
--- a/include/linux/irq_work.h	2020-11-23 13:48:32.693899776 +0200
+++ b/include/linux/irq_work.h	2021-07-14 15:39:09.834149403 +0300
@@ -55,4 +55,10 @@
 static inline void irq_work_single(void *arg) { }
 #endif
 
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT)
+void irq_work_tick_soft(void);
+#else
+static inline void irq_work_tick_soft(void) { }
+#endif
+
 #endif /* _LINUX_IRQ_WORK_H */
diff -Naur a/include/linux/kernel.h b/include/linux/kernel.h
--- a/include/linux/kernel.h	2020-11-23 13:48:32.709900097 +0200
+++ b/include/linux/kernel.h	2021-07-14 15:39:09.834149403 +0300
@@ -218,6 +218,10 @@
  */
 # define might_sleep() \
 	do { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)
+
+# define might_sleep_no_state_check() \
+	do { ___might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)
+
 /**
  * cant_sleep - annotation for functions that cannot sleep
  *
@@ -249,6 +253,7 @@
   static inline void __might_sleep(const char *file, int line,
 				   int preempt_offset) { }
 # define might_sleep() do { might_resched(); } while (0)
+# define might_sleep_no_state_check() do { might_resched(); } while (0)
 # define cant_sleep() do { } while (0)
 # define sched_annotate_sleep() do { } while (0)
 # define non_block_start() do { } while (0)
diff -Naur a/include/linux/kmsg_dump.h b/include/linux/kmsg_dump.h
--- a/include/linux/kmsg_dump.h	2020-11-23 13:48:32.717900258 +0200
+++ b/include/linux/kmsg_dump.h	2021-07-14 15:39:09.834149403 +0300
@@ -45,10 +45,8 @@
 	bool registered;
 
 	/* private state of the kmsg iterator */
-	u32 cur_idx;
-	u32 next_idx;
-	u64 cur_seq;
-	u64 next_seq;
+	u64 line_seq;
+	u64 buffer_end_seq;
 };
 
 #ifdef CONFIG_PRINTK
diff -Naur a/include/linux/local_lock_internal.h b/include/linux/local_lock_internal.h
--- a/include/linux/local_lock_internal.h	2020-11-23 13:48:32.737900659 +0200
+++ b/include/linux/local_lock_internal.h	2021-07-14 15:39:09.834149403 +0300
@@ -7,33 +7,90 @@
 #include <linux/lockdep.h>
 
 typedef struct {
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+#ifdef CONFIG_PREEMPT_RT
+	spinlock_t              lock;
+	struct task_struct      *owner;
+	int                     nestcnt;
+
+#elif defined(CONFIG_DEBUG_LOCK_ALLOC)
 	struct lockdep_map	dep_map;
 	struct task_struct	*owner;
 #endif
 } local_lock_t;
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define LL_DEP_MAP_INIT(lockname)			\
+#ifdef CONFIG_PREEMPT_RT
+
+#define INIT_LOCAL_LOCK(lockname)	{	\
+	__SPIN_LOCK_UNLOCKED((lockname).lock),	\
+	.owner		= NULL,			\
+	.nestcnt	= 0,			\
+	}
+#else
+
+# ifdef CONFIG_DEBUG_LOCK_ALLOC
+#  define LL_DEP_MAP_INIT(lockname)			\
 	.dep_map = {					\
 		.name = #lockname,			\
 		.wait_type_inner = LD_WAIT_CONFIG,	\
 	}
-#else
-# define LL_DEP_MAP_INIT(lockname)
-#endif
+# else
+#  define LL_DEP_MAP_INIT(lockname)
+# endif
 
 #define INIT_LOCAL_LOCK(lockname)	{ LL_DEP_MAP_INIT(lockname) }
 
-#define __local_lock_init(lock)					\
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+
+static inline void ___local_lock_init(local_lock_t *l)
+{
+	l->owner = NULL;
+	l->nestcnt = 0;
+}
+
+#define __local_lock_init(l)					\
+do {								\
+	spin_lock_init(&(l)->lock);				\
+	___local_lock_init(l);					\
+} while (0)
+
+#else
+
+#define __local_lock_init(l)					\
 do {								\
 	static struct lock_class_key __key;			\
 								\
-	debug_check_no_locks_freed((void *)lock, sizeof(*lock));\
-	lockdep_init_map_wait(&(lock)->dep_map, #lock, &__key, 0, LD_WAIT_CONFIG);\
+	debug_check_no_locks_freed((void *)l, sizeof(*l));	\
+	lockdep_init_map_wait(&(l)->dep_map, #l, &__key, 0, LD_WAIT_CONFIG);\
 } while (0)
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+
+static inline void local_lock_acquire(local_lock_t *l)
+{
+	if (l->owner != current) {
+		spin_lock(&l->lock);
+		DEBUG_LOCKS_WARN_ON(l->owner);
+		DEBUG_LOCKS_WARN_ON(l->nestcnt);
+		l->owner = current;
+	}
+	l->nestcnt++;
+}
+
+static inline void local_lock_release(local_lock_t *l)
+{
+	DEBUG_LOCKS_WARN_ON(l->nestcnt == 0);
+	DEBUG_LOCKS_WARN_ON(l->owner != current);
+	if (--l->nestcnt)
+		return;
+
+	l->owner = NULL;
+	spin_unlock(&l->lock);
+}
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+#elif defined(CONFIG_DEBUG_LOCK_ALLOC)
 static inline void local_lock_acquire(local_lock_t *l)
 {
 	lock_map_acquire(&l->dep_map);
@@ -55,26 +112,55 @@
 
 #define __local_lock(lock)					\
 	do {							\
-		preempt_disable();				\
+		migrate_disable();				\
 		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
+#define __local_unlock(lock)					\
+	do {							\
+		local_lock_release(this_cpu_ptr(lock));		\
+		migrate_enable();				\
+	} while (0)
+
+#ifdef CONFIG_PREEMPT_RT
+
 #define __local_lock_irq(lock)					\
 	do {							\
-		local_irq_disable();				\
+		migrate_disable();				\
 		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
 #define __local_lock_irqsave(lock, flags)			\
 	do {							\
-		local_irq_save(flags);				\
+		migrate_disable();				\
+		flags = 0;					\
 		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
-#define __local_unlock(lock)					\
+#define __local_unlock_irq(lock)				\
+	do {							\
+		local_lock_release(this_cpu_ptr(lock));		\
+		migrate_enable();				\
+	} while (0)
+
+#define __local_unlock_irqrestore(lock, flags)			\
 	do {							\
 		local_lock_release(this_cpu_ptr(lock));		\
-		preempt_enable();				\
+		migrate_enable();				\
+	} while (0)
+
+#else
+
+#define __local_lock_irq(lock)					\
+	do {							\
+		local_irq_disable();				\
+		local_lock_acquire(this_cpu_ptr(lock));		\
+	} while (0)
+
+#define __local_lock_irqsave(lock, flags)			\
+	do {							\
+		local_irq_save(flags);				\
+		local_lock_acquire(this_cpu_ptr(lock));		\
 	} while (0)
 
 #define __local_unlock_irq(lock)				\
@@ -88,3 +174,5 @@
 		local_lock_release(this_cpu_ptr(lock));		\
 		local_irq_restore(flags);			\
 	} while (0)
+
+#endif
diff -Naur a/include/linux/mhi.h b/include/linux/mhi.h
--- a/include/linux/mhi.h	2020-11-23 13:48:32.761901140 +0200
+++ b/include/linux/mhi.h	2021-07-14 15:39:09.834149403 +0300
@@ -9,10 +9,9 @@
 #include <linux/device.h>
 #include <linux/dma-direction.h>
 #include <linux/mutex.h>
-#include <linux/rwlock_types.h>
 #include <linux/skbuff.h>
 #include <linux/slab.h>
-#include <linux/spinlock_types.h>
+#include <linux/spinlock.h>
 #include <linux/wait.h>
 #include <linux/workqueue.h>
 
diff -Naur a/include/linux/mm_types.h b/include/linux/mm_types.h
--- a/include/linux/mm_types.h	2020-11-23 13:48:32.765901220 +0200
+++ b/include/linux/mm_types.h	2021-07-14 15:39:09.834149403 +0300
@@ -12,6 +12,7 @@
 #include <linux/completion.h>
 #include <linux/cpumask.h>
 #include <linux/uprobes.h>
+#include <linux/rcupdate.h>
 #include <linux/page-flags-layout.h>
 #include <linux/workqueue.h>
 
@@ -548,6 +549,9 @@
 		bool tlb_flush_batched;
 #endif
 		struct uprobes_state uprobes_state;
+#ifdef CONFIG_PREEMPT_RT
+		struct rcu_head delayed_drop;
+#endif
 #ifdef CONFIG_HUGETLB_PAGE
 		atomic_long_t hugetlb_usage;
 #endif
diff -Naur a/include/linux/mutex.h b/include/linux/mutex.h
--- a/include/linux/mutex.h	2020-11-23 13:48:32.781901541 +0200
+++ b/include/linux/mutex.h	2021-07-14 15:39:09.834149403 +0300
@@ -22,6 +22,20 @@
 
 struct ww_acquire_ctx;
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname)			\
+		, .dep_map = {					\
+			.name = #lockname,			\
+			.wait_type_inner = LD_WAIT_SLEEP,	\
+		}
+#else
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/mutex_rt.h>
+#else
+
 /*
  * Simple, straightforward mutexes with strict semantics:
  *
@@ -68,14 +82,6 @@
 struct ww_class;
 struct ww_acquire_ctx;
 
-struct ww_mutex {
-	struct mutex base;
-	struct ww_acquire_ctx *ctx;
-#ifdef CONFIG_DEBUG_MUTEXES
-	struct ww_class *ww_class;
-#endif
-};
-
 /*
  * This is the control structure for tasks blocked on mutex,
  * which resides on the blocked task's kernel stack:
@@ -119,16 +125,6 @@
 	__mutex_init((mutex), #mutex, &__key);				\
 } while (0)
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)			\
-		, .dep_map = {					\
-			.name = #lockname,			\
-			.wait_type_inner = LD_WAIT_SLEEP,	\
-		}
-#else
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
-#endif
-
 #define __MUTEX_INITIALIZER(lockname) \
 		{ .owner = ATOMIC_LONG_INIT(0) \
 		, .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
@@ -224,4 +220,6 @@
 extern /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
 mutex_trylock_recursive(struct mutex *lock);
 
+#endif /* !PREEMPT_RT */
+
 #endif /* __LINUX_MUTEX_H */
diff -Naur a/include/linux/mutex_rt.h b/include/linux/mutex_rt.h
--- a/include/linux/mutex_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/mutex_rt.h	2021-07-14 15:39:09.834149403 +0300
@@ -0,0 +1,131 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_MUTEX_RT_H
+#define __LINUX_MUTEX_RT_H
+
+#ifndef __LINUX_MUTEX_H
+#error "Please include mutex.h"
+#endif
+
+#include <linux/rtmutex.h>
+
+/* FIXME: Just for __lockfunc */
+#include <linux/spinlock.h>
+
+struct mutex {
+	struct rt_mutex		lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __MUTEX_INITIALIZER(mutexname)					\
+	{								\
+		.lock = __RT_MUTEX_INITIALIZER(mutexname.lock)		\
+		__DEP_MAP_MUTEX_INITIALIZER(mutexname)			\
+	}
+
+#define DEFINE_MUTEX(mutexname)						\
+	struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)
+
+extern void __mutex_do_init(struct mutex *lock, const char *name, struct lock_class_key *key);
+extern void __lockfunc _mutex_lock(struct mutex *lock);
+extern void __lockfunc _mutex_lock_io(struct mutex *lock);
+extern void __lockfunc _mutex_lock_io_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_interruptible(struct mutex *lock);
+extern int __lockfunc _mutex_lock_killable(struct mutex *lock);
+extern void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass);
+extern void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest_lock);
+extern int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_trylock(struct mutex *lock);
+extern void __lockfunc _mutex_unlock(struct mutex *lock);
+
+#define mutex_is_locked(l)		rt_mutex_is_locked(&(l)->lock)
+#define mutex_lock(l)			_mutex_lock(l)
+#define mutex_lock_interruptible(l)	_mutex_lock_interruptible(l)
+#define mutex_lock_killable(l)		_mutex_lock_killable(l)
+#define mutex_trylock(l)		_mutex_trylock(l)
+#define mutex_unlock(l)			_mutex_unlock(l)
+#define mutex_lock_io(l)		_mutex_lock_io(l);
+
+#define __mutex_owner(l)		((l)->lock.owner)
+
+#ifdef CONFIG_DEBUG_MUTEXES
+#define mutex_destroy(l)		rt_mutex_destroy(&(l)->lock)
+#else
+static inline void mutex_destroy(struct mutex *lock) {}
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define mutex_lock_nested(l, s)	_mutex_lock_nested(l, s)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible_nested(l, s)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable_nested(l, s)
+# define mutex_lock_io_nested(l, s)	_mutex_lock_io_nested(l, s)
+
+# define mutex_lock_nest_lock(lock, nest_lock)				\
+do {									\
+	typecheck(struct lockdep_map *, &(nest_lock)->dep_map);		\
+	_mutex_lock_nest_lock(lock, &(nest_lock)->dep_map);		\
+} while (0)
+
+#else
+# define mutex_lock_nested(l, s)	_mutex_lock(l)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible(l)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable(l)
+# define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
+# define mutex_lock_io_nested(l, s)	_mutex_lock_io(l)
+#endif
+
+# define mutex_init(mutex)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), #mutex, &__key);	\
+} while (0)
+
+# define __mutex_init(mutex, name, key)			\
+do {							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), name, key);		\
+} while (0)
+
+/**
+ * These values are chosen such that FAIL and SUCCESS match the
+ * values of the regular mutex_trylock().
+ */
+enum mutex_trylock_recursive_enum {
+	MUTEX_TRYLOCK_FAILED    = 0,
+	MUTEX_TRYLOCK_SUCCESS   = 1,
+	MUTEX_TRYLOCK_RECURSIVE,
+};
+/**
+ * mutex_trylock_recursive - trylock variant that allows recursive locking
+ * @lock: mutex to be locked
+ *
+ * This function should not be used, _ever_. It is purely for hysterical GEM
+ * raisins, and once those are gone this will be removed.
+ *
+ * Returns:
+ *  MUTEX_TRYLOCK_FAILED    - trylock failed,
+ *  MUTEX_TRYLOCK_SUCCESS   - lock acquired,
+ *  MUTEX_TRYLOCK_RECURSIVE - we already owned the lock.
+ */
+int __rt_mutex_owner_current(struct rt_mutex *lock);
+
+static inline /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
+mutex_trylock_recursive(struct mutex *lock)
+{
+	if (unlikely(__rt_mutex_owner_current(&lock->lock)))
+		return MUTEX_TRYLOCK_RECURSIVE;
+
+	return mutex_trylock(lock);
+}
+
+extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
+
+#endif
diff -Naur a/include/linux/nfs_xdr.h b/include/linux/nfs_xdr.h
--- a/include/linux/nfs_xdr.h	2020-11-23 13:48:32.797901862 +0200
+++ b/include/linux/nfs_xdr.h	2021-07-14 15:39:09.834149403 +0300
@@ -1670,7 +1670,7 @@
 	struct nfs_removeargs args;
 	struct nfs_removeres res;
 	struct dentry *dentry;
-	wait_queue_head_t wq;
+	struct swait_queue_head wq;
 	const struct cred *cred;
 	struct nfs_fattr dir_attr;
 	long timeout;
diff -Naur a/include/linux/pid.h b/include/linux/pid.h
--- a/include/linux/pid.h	2020-11-23 13:48:32.905904028 +0200
+++ b/include/linux/pid.h	2021-07-14 15:39:09.834149403 +0300
@@ -3,6 +3,7 @@
 #define _LINUX_PID_H
 
 #include <linux/rculist.h>
+#include <linux/atomic.h>
 #include <linux/wait.h>
 #include <linux/refcount.h>
 
diff -Naur a/include/linux/poll.h b/include/linux/poll.h
--- a/include/linux/poll.h	2020-11-23 13:48:32.917904269 +0200
+++ b/include/linux/poll.h	2020-11-23 13:50:10.403439585 +0200
@@ -1,4 +1,9 @@
-/* SPDX-License-Identifier: GPL-2.0 */
+/* SPDX-License-Identifier: GPL-2.0
+ *	2020/10/16
+ *		Laurentiu-Cristian Duca (laurentiu [dot] duca [at] gmail [dot] com)
+ *		Added RTnet select() and poll() system calls helpers.
+ */
+
 #ifndef _LINUX_POLL_H
 #define _LINUX_POLL_H
 
@@ -45,6 +50,13 @@
 	__poll_t _key;
 } poll_table;
 
+static inline void poll_wait_rtnet(struct file * filp, wait_queue_head_rtnet_t * wait_address, poll_table *p)
+{
+	if (p && p->_qproc && wait_address)
+		/* dirty type convertor, just to compile */
+		p->_qproc(filp, (wait_queue_head_t*) wait_address, p);
+}
+
 static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
 {
 	if (p && p->_qproc && wait_address)
@@ -90,6 +102,13 @@
 	return file->f_op->poll(file, pt);
 }
 
+struct poll_table_entry_rtnet {
+	struct file *filp;
+	__poll_t key;
+	wait_queue_entry_t wait;
+	wait_queue_head_rtnet_t *wait_address;
+};
+
 struct poll_table_entry {
 	struct file *filp;
 	__poll_t key;
@@ -100,6 +119,16 @@
 /*
  * Structures and helpers for select/poll syscall
  */
+struct poll_wqueues_rtnet {
+	poll_table pt;
+	struct poll_table_page_rtnet *table;
+	struct task_struct *polling_task;
+	int triggered;
+	int error;
+	int inline_index;
+	struct poll_table_entry_rtnet inline_entries[N_INLINE_POLL_ENTRIES];
+};
+
 struct poll_wqueues {
 	poll_table pt;
 	struct poll_table_page *table;
@@ -111,7 +140,9 @@
 };
 
 extern void poll_initwait(struct poll_wqueues *pwq);
+extern void poll_initwait_rtnet(struct poll_wqueues_rtnet *pwq);
 extern void poll_freewait(struct poll_wqueues *pwq);
+extern void poll_freewait_rtnet(struct poll_wqueues_rtnet *pwq);
 extern u64 select_estimate_accuracy(struct timespec64 *tv);
 
 #define MAX_INT64_SECONDS (((s64)(~((u64)0)>>1)/HZ)-1)
diff -Naur a/include/linux/preempt.h b/include/linux/preempt.h
--- a/include/linux/preempt.h	2020-11-23 13:48:32.921904349 +0200
+++ b/include/linux/preempt.h	2021-07-14 15:39:09.838149374 +0300
@@ -77,10 +77,14 @@
 /* preempt_count() and related functions, depends on PREEMPT_NEED_RESCHED */
 #include <asm/preempt.h>
 
-#define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
-#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
-#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
-				 | NMI_MASK))
+#define pc_nmi_count()		(preempt_count() & NMI_MASK)
+#define hardirq_count()		(preempt_count() & HARDIRQ_MASK)
+#ifdef CONFIG_PREEMPT_RT
+# define softirq_count()	(current->softirq_disable_cnt & SOFTIRQ_MASK)
+#else
+# define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
+#endif
+#define irq_count()		(pc_nmi_count() | hardirq_count() | softirq_count())
 
 /*
  * Are we doing bottom half or hardware interrupt processing?
@@ -95,13 +99,12 @@
  * Note: due to the BH disabled confusion: in_softirq(),in_interrupt() really
  *       should not be used in new code.
  */
+#define in_nmi()		(pc_nmi_count())
 #define in_irq()		(hardirq_count())
-#define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
+#define in_softirq()		(softirq_count())
 #define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
-#define in_nmi()		(preempt_count() & NMI_MASK)
-#define in_task()		(!(preempt_count() & \
-				   (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))
+#define in_task()		(!(irq_count() & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))
 
 /*
  * The preempt_count offset after preempt_disable();
@@ -115,7 +118,11 @@
 /*
  * The preempt_count offset after spin_lock()
  */
+#if !defined(CONFIG_PREEMPT_RT)
 #define PREEMPT_LOCK_OFFSET	PREEMPT_DISABLE_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
 
 /*
  * The preempt_count offset needed for things like:
@@ -164,6 +171,20 @@
 #define preempt_count_inc() preempt_count_add(1)
 #define preempt_count_dec() preempt_count_sub(1)
 
+#ifdef CONFIG_PREEMPT_LAZY
+#define add_preempt_lazy_count(val)	do { preempt_lazy_count() += (val); } while (0)
+#define sub_preempt_lazy_count(val)	do { preempt_lazy_count() -= (val); } while (0)
+#define inc_preempt_lazy_count()	add_preempt_lazy_count(1)
+#define dec_preempt_lazy_count()	sub_preempt_lazy_count(1)
+#define preempt_lazy_count()		(current_thread_info()->preempt_lazy_count)
+#else
+#define add_preempt_lazy_count(val)	do { } while (0)
+#define sub_preempt_lazy_count(val)	do { } while (0)
+#define inc_preempt_lazy_count()	do { } while (0)
+#define dec_preempt_lazy_count()	do { } while (0)
+#define preempt_lazy_count()		(0)
+#endif
+
 #ifdef CONFIG_PREEMPT_COUNT
 
 #define preempt_disable() \
@@ -172,13 +193,25 @@
 	barrier(); \
 } while (0)
 
+#define preempt_lazy_disable() \
+do { \
+	inc_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define sched_preempt_enable_no_resched() \
 do { \
 	barrier(); \
 	preempt_count_dec(); \
 } while (0)
 
-#define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+#ifdef CONFIG_PREEMPT_RT
+# define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+# define preempt_check_resched_rt() preempt_check_resched()
+#else
+# define preempt_enable_no_resched() preempt_enable()
+# define preempt_check_resched_rt() barrier();
+#endif
 
 #define preemptible()	(preempt_count() == 0 && !irqs_disabled())
 
@@ -203,6 +236,13 @@
 		__preempt_schedule(); \
 } while (0)
 
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+	preempt_check_resched(); \
+} while (0)
+
 #else /* !CONFIG_PREEMPTION */
 #define preempt_enable() \
 do { \
@@ -210,6 +250,12 @@
 	preempt_count_dec(); \
 } while (0)
 
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define preempt_enable_notrace() \
 do { \
 	barrier(); \
@@ -248,6 +294,7 @@
 #define preempt_disable_notrace()		barrier()
 #define preempt_enable_no_resched_notrace()	barrier()
 #define preempt_enable_notrace()		barrier()
+#define preempt_check_resched_rt()		barrier()
 #define preemptible()				0
 
 #endif /* CONFIG_PREEMPT_COUNT */
@@ -268,10 +315,22 @@
 } while (0)
 #define preempt_fold_need_resched() \
 do { \
-	if (tif_need_resched()) \
+	if (tif_need_resched_now()) \
 		set_preempt_need_resched(); \
 } while (0)
 
+#ifdef CONFIG_PREEMPT_RT
+# define preempt_disable_rt()		preempt_disable()
+# define preempt_enable_rt()		preempt_enable()
+# define preempt_disable_nort()		barrier()
+# define preempt_enable_nort()		barrier()
+#else
+# define preempt_disable_rt()		barrier()
+# define preempt_enable_rt()		barrier()
+# define preempt_disable_nort()		preempt_disable()
+# define preempt_enable_nort()		preempt_enable()
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 
 struct preempt_notifier;
@@ -322,6 +381,68 @@
 
 #endif
 
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+
+/*
+ * Migrate-Disable and why it is undesired.
+ *
+ * When a preempted task becomes elegible to run under the ideal model (IOW it
+ * becomes one of the M highest priority tasks), it might still have to wait
+ * for the preemptee's migrate_disable() section to complete. Thereby suffering
+ * a reduction in bandwidth in the exact duration of the migrate_disable()
+ * section.
+ *
+ * Per this argument, the change from preempt_disable() to migrate_disable()
+ * gets us:
+ *
+ * - a higher priority tasks gains reduced wake-up latency; with preempt_disable()
+ *   it would have had to wait for the lower priority task.
+ *
+ * - a lower priority tasks; which under preempt_disable() could've instantly
+ *   migrated away when another CPU becomes available, is now constrained
+ *   by the ability to push the higher priority task away, which might itself be
+ *   in a migrate_disable() section, reducing it's available bandwidth.
+ *
+ * IOW it trades latency / moves the interference term, but it stays in the
+ * system, and as long as it remains unbounded, the system is not fully
+ * deterministic.
+ *
+ *
+ * The reason we have it anyway.
+ *
+ * PREEMPT_RT breaks a number of assumptions traditionally held. By forcing a
+ * number of primitives into becoming preemptible, they would also allow
+ * migration. This turns out to break a bunch of per-cpu usage. To this end,
+ * all these primitives employ migirate_disable() to restore this implicit
+ * assumption.
+ *
+ * This is a 'temporary' work-around at best. The correct solution is getting
+ * rid of the above assumptions and reworking the code to employ explicit
+ * per-cpu locking or short preempt-disable regions.
+ *
+ * The end goal must be to get rid of migrate_disable(), alternatively we need
+ * a schedulability theory that does not depend on abritrary migration.
+ *
+ *
+ * Notes on the implementation.
+ *
+ * The implementation is particularly tricky since existing code patterns
+ * dictate neither migrate_disable() nor migrate_enable() is allowed to block.
+ * This means that it cannot use cpus_read_lock() to serialize against hotplug,
+ * nor can it easily migrate itself into a pending affinity mask change on
+ * migrate_enable().
+ *
+ *
+ * Note: even non-work-conserving schedulers like semi-partitioned depends on
+ *       migration, so migrate_disable() is not only a problem for
+ *       work-conserving schedulers.
+ *
+ */
+extern void migrate_disable(void);
+extern void migrate_enable(void);
+
+#else /* !(CONFIG_SMP && CONFIG_PREEMPT_RT) */
+
 /**
  * migrate_disable - Prevent migration of the current task
  *
@@ -352,4 +473,6 @@
 	preempt_enable();
 }
 
+#endif /* CONFIG_SMP && CONFIG_PREEMPT_RT */
+
 #endif /* __LINUX_PREEMPT_H */
diff -Naur a/include/linux/printk.h b/include/linux/printk.h
--- a/include/linux/printk.h	2020-11-23 13:48:32.921904349 +0200
+++ b/include/linux/printk.h	2021-07-14 15:39:09.838149374 +0300
@@ -59,6 +59,7 @@
  */
 #define CONSOLE_LOGLEVEL_DEFAULT CONFIG_CONSOLE_LOGLEVEL_DEFAULT
 #define CONSOLE_LOGLEVEL_QUIET	 CONFIG_CONSOLE_LOGLEVEL_QUIET
+#define CONSOLE_LOGLEVEL_EMERGENCY CONFIG_CONSOLE_LOGLEVEL_EMERGENCY
 
 extern int console_printk[];
 
@@ -66,6 +67,7 @@
 #define default_message_loglevel (console_printk[1])
 #define minimum_console_loglevel (console_printk[2])
 #define default_console_loglevel (console_printk[3])
+#define emergency_console_loglevel (console_printk[4])
 
 static inline void console_silent(void)
 {
@@ -147,18 +149,6 @@
 void early_printk(const char *s, ...) { }
 #endif
 
-#ifdef CONFIG_PRINTK_NMI
-extern void printk_nmi_enter(void);
-extern void printk_nmi_exit(void);
-extern void printk_nmi_direct_enter(void);
-extern void printk_nmi_direct_exit(void);
-#else
-static inline void printk_nmi_enter(void) { }
-static inline void printk_nmi_exit(void) { }
-static inline void printk_nmi_direct_enter(void) { }
-static inline void printk_nmi_direct_exit(void) { }
-#endif /* PRINTK_NMI */
-
 #ifdef CONFIG_PRINTK
 asmlinkage __printf(5, 0)
 int vprintk_emit(int facility, int level,
@@ -203,8 +193,7 @@
 void dump_stack_print_info(const char *log_lvl);
 void show_regs_print_info(const char *log_lvl);
 extern asmlinkage void dump_stack(void) __cold;
-extern void printk_safe_flush(void);
-extern void printk_safe_flush_on_panic(void);
+struct wait_queue_head *printk_wait_queue(void);
 #else
 static inline __printf(1, 0)
 int vprintk(const char *s, va_list args)
@@ -269,13 +258,6 @@
 {
 }
 
-static inline void printk_safe_flush(void)
-{
-}
-
-static inline void printk_safe_flush_on_panic(void)
-{
-}
 #endif
 
 extern int kptr_restrict;
diff -Naur a/include/linux/printk_ringbuffer.h b/include/linux/printk_ringbuffer.h
--- a/include/linux/printk_ringbuffer.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/printk_ringbuffer.h	2021-07-14 15:39:09.838149374 +0300
@@ -0,0 +1,114 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_PRINTK_RINGBUFFER_H
+#define _LINUX_PRINTK_RINGBUFFER_H
+
+#include <linux/irq_work.h>
+#include <linux/atomic.h>
+#include <linux/percpu.h>
+#include <linux/wait.h>
+
+struct prb_cpulock {
+	atomic_t owner;
+	unsigned long __percpu *irqflags;
+};
+
+struct printk_ringbuffer {
+	void *buffer;
+	unsigned int size_bits;
+
+	u64 seq;
+	atomic_long_t lost;
+
+	atomic_long_t tail;
+	atomic_long_t head;
+	atomic_long_t reserve;
+
+	struct prb_cpulock *cpulock;
+	atomic_t ctx;
+
+	struct wait_queue_head *wq;
+	atomic_long_t wq_counter;
+	struct irq_work *wq_work;
+};
+
+struct prb_entry {
+	unsigned int size;
+	u64 seq;
+	char data[0];
+};
+
+struct prb_handle {
+	struct printk_ringbuffer *rb;
+	unsigned int cpu;
+	struct prb_entry *entry;
+};
+
+#define DECLARE_STATIC_PRINTKRB_CPULOCK(name)				\
+static DEFINE_PER_CPU(unsigned long, _##name##_percpu_irqflags);	\
+static struct prb_cpulock name = {					\
+	.owner = ATOMIC_INIT(-1),					\
+	.irqflags = &_##name##_percpu_irqflags,				\
+}
+
+#define PRB_INIT ((unsigned long)-1)
+
+#define DECLARE_STATIC_PRINTKRB_ITER(name, rbaddr)			\
+static struct prb_iterator name = {					\
+	.rb = rbaddr,							\
+	.lpos = PRB_INIT,						\
+}
+
+struct prb_iterator {
+	struct printk_ringbuffer *rb;
+	unsigned long lpos;
+};
+
+#define DECLARE_STATIC_PRINTKRB(name, szbits, cpulockptr)		\
+static char _##name##_buffer[1 << (szbits)]				\
+	__aligned(__alignof__(long));					\
+static DECLARE_WAIT_QUEUE_HEAD(_##name##_wait);				\
+static void _##name##_wake_work_func(struct irq_work *irq_work)		\
+{									\
+	wake_up_interruptible_all(&_##name##_wait);			\
+}									\
+static struct irq_work _##name##_wake_work = {				\
+	.func = _##name##_wake_work_func,				\
+	.flags = ATOMIC_INIT(IRQ_WORK_LAZY),				\
+};									\
+static struct printk_ringbuffer name = {				\
+	.buffer = &_##name##_buffer[0],					\
+	.size_bits = szbits,						\
+	.seq = 0,							\
+	.lost = ATOMIC_LONG_INIT(0),					\
+	.tail = ATOMIC_LONG_INIT(-111 * sizeof(long)),			\
+	.head = ATOMIC_LONG_INIT(-111 * sizeof(long)),			\
+	.reserve = ATOMIC_LONG_INIT(-111 * sizeof(long)),		\
+	.cpulock = cpulockptr,						\
+	.ctx = ATOMIC_INIT(0),						\
+	.wq = &_##name##_wait,						\
+	.wq_counter = ATOMIC_LONG_INIT(0),				\
+	.wq_work = &_##name##_wake_work,				\
+}
+
+/* writer interface */
+char *prb_reserve(struct prb_handle *h, struct printk_ringbuffer *rb,
+		  unsigned int size);
+void prb_commit(struct prb_handle *h);
+
+/* reader interface */
+void prb_iter_init(struct prb_iterator *iter, struct printk_ringbuffer *rb,
+		   u64 *seq);
+void prb_iter_copy(struct prb_iterator *dest, struct prb_iterator *src);
+int prb_iter_next(struct prb_iterator *iter, char *buf, int size, u64 *seq);
+int prb_iter_wait_next(struct prb_iterator *iter, char *buf, int size,
+		       u64 *seq);
+int prb_iter_seek(struct prb_iterator *iter, u64 seq);
+int prb_iter_data(struct prb_iterator *iter, char *buf, int size, u64 *seq);
+
+/* utility functions */
+int prb_buffer_size(struct printk_ringbuffer *rb);
+void prb_inc_lost(struct printk_ringbuffer *rb);
+void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store);
+void prb_unlock(struct prb_cpulock *cpu_lock, unsigned int cpu_store);
+
+#endif /*_LINUX_PRINTK_RINGBUFFER_H */
diff -Naur a/include/linux/random.h b/include/linux/random.h
--- a/include/linux/random.h	2020-11-23 13:48:32.937904670 +0200
+++ b/include/linux/random.h	2021-07-14 15:39:09.838149374 +0300
@@ -35,7 +35,7 @@
 
 extern void add_input_randomness(unsigned int type, unsigned int code,
 				 unsigned int value) __latent_entropy;
-extern void add_interrupt_randomness(int irq, int irq_flags) __latent_entropy;
+extern void add_interrupt_randomness(int irq, int irq_flags, __u64 ip) __latent_entropy;
 
 extern void get_random_bytes(void *buf, int nbytes);
 extern int wait_for_random_bytes(void);
diff -Naur a/include/linux/ratelimit.h b/include/linux/ratelimit.h
--- a/include/linux/ratelimit.h	2020-11-23 13:48:32.941904750 +0200
+++ b/include/linux/ratelimit.h	2021-07-14 15:39:09.838149374 +0300
@@ -28,7 +28,7 @@
 		return;
 
 	if (rs->missed) {
-		pr_warn("%s: %d output lines suppressed due to ratelimiting\n",
+		pr_info("%s: %d output lines suppressed due to ratelimiting\n",
 			current->comm, rs->missed);
 		rs->missed = 0;
 	}
diff -Naur a/include/linux/rbtree.h b/include/linux/rbtree.h
--- a/include/linux/rbtree.h	2020-11-23 13:48:32.941904750 +0200
+++ b/include/linux/rbtree.h	2021-07-14 15:39:09.838149374 +0300
@@ -19,19 +19,9 @@
 
 #include <linux/kernel.h>
 #include <linux/stddef.h>
+#include <linux/rbtree_type.h>
 #include <linux/rcupdate.h>
 
-struct rb_node {
-	unsigned long  __rb_parent_color;
-	struct rb_node *rb_right;
-	struct rb_node *rb_left;
-} __attribute__((aligned(sizeof(long))));
-    /* The alignment might seem pointless, but allegedly CRIS needs it */
-
-struct rb_root {
-	struct rb_node *rb_node;
-};
-
 #define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))
 
 #define RB_ROOT	(struct rb_root) { NULL, }
@@ -112,21 +102,6 @@
 			typeof(*pos), field); 1; }); \
 	     pos = n)
 
-/*
- * Leftmost-cached rbtrees.
- *
- * We do not cache the rightmost node based on footprint
- * size vs number of potential users that could benefit
- * from O(1) rb_last(). Just not worth it, users that want
- * this feature can always implement the logic explicitly.
- * Furthermore, users that want to cache both pointers may
- * find it a bit asymmetric, but that's ok.
- */
-struct rb_root_cached {
-	struct rb_root rb_root;
-	struct rb_node *rb_leftmost;
-};
-
 #define RB_ROOT_CACHED (struct rb_root_cached) { {NULL, }, NULL }
 
 /* Same as rb_first(), but O(1) */
diff -Naur a/include/linux/rbtree_latch.h b/include/linux/rbtree_latch.h
--- a/include/linux/rbtree_latch.h	2020-11-23 13:48:32.941904750 +0200
+++ b/include/linux/rbtree_latch.h	2021-07-14 15:39:09.838149374 +0300
@@ -42,8 +42,8 @@
 };
 
 struct latch_tree_root {
-	seqcount_t	seq;
-	struct rb_root	tree[2];
+	seqcount_latch_t	seq;
+	struct rb_root		tree[2];
 };
 
 /**
@@ -206,7 +206,7 @@
 	do {
 		seq = raw_read_seqcount_latch(&root->seq);
 		node = __lt_find(key, root, seq & 1, ops->comp);
-	} while (read_seqcount_retry(&root->seq, seq));
+	} while (read_seqcount_latch_retry(&root->seq, seq));
 
 	return node;
 }
diff -Naur a/include/linux/rbtree_type.h b/include/linux/rbtree_type.h
--- a/include/linux/rbtree_type.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/rbtree_type.h	2021-07-14 15:39:09.838149374 +0300
@@ -0,0 +1,31 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+#ifndef _LINUX_RBTREE_TYPE_H
+#define _LINUX_RBTREE_TYPE_H
+
+struct rb_node {
+	unsigned long  __rb_parent_color;
+	struct rb_node *rb_right;
+	struct rb_node *rb_left;
+} __attribute__((aligned(sizeof(long))));
+/* The alignment might seem pointless, but allegedly CRIS needs it */
+
+struct rb_root {
+	struct rb_node *rb_node;
+};
+
+/*
+ * Leftmost-cached rbtrees.
+ *
+ * We do not cache the rightmost node based on footprint
+ * size vs number of potential users that could benefit
+ * from O(1) rb_last(). Just not worth it, users that want
+ * this feature can always implement the logic explicitly.
+ * Furthermore, users that want to cache both pointers may
+ * find it a bit asymmetric, but that's ok.
+ */
+struct rb_root_cached {
+	struct rb_root rb_root;
+	struct rb_node *rb_leftmost;
+};
+
+#endif
diff -Naur a/include/linux/rcupdate.h b/include/linux/rcupdate.h
--- a/include/linux/rcupdate.h	2020-11-23 13:48:32.945904830 +0200
+++ b/include/linux/rcupdate.h	2021-07-14 15:39:09.838149374 +0300
@@ -52,6 +52,11 @@
  * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.
  */
 #define rcu_preempt_depth() (current->rcu_read_lock_nesting)
+#ifndef CONFIG_PREEMPT_RT
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+#else
+static inline int sched_rcu_preempt_depth(void) { return 0; }
+#endif
 
 #else /* #ifdef CONFIG_PREEMPT_RCU */
 
@@ -70,6 +75,8 @@
 	return 0;
 }
 
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+
 #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
 
 /* Internal to kernel */
@@ -312,7 +319,8 @@
 #define rcu_sleep_check()						\
 	do {								\
 		rcu_preempt_sleep_check();				\
-		RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),	\
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))			\
+		    RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),	\
 				 "Illegal context switch in RCU-bh read-side critical section"); \
 		RCU_LOCKDEP_WARN(lock_is_held(&rcu_sched_lock_map),	\
 				 "Illegal context switch in RCU-sched read-side critical section"); \
diff -Naur a/include/linux/rtmutex.h b/include/linux/rtmutex.h
--- a/include/linux/rtmutex.h	2020-11-23 13:48:32.961905151 +0200
+++ b/include/linux/rtmutex.h	2021-07-14 15:39:09.838149374 +0300
@@ -14,11 +14,15 @@
 #define __LINUX_RT_MUTEX_H
 
 #include <linux/linkage.h>
-#include <linux/rbtree.h>
-#include <linux/spinlock_types.h>
+#include <linux/rbtree_type.h>
+#include <linux/spinlock_types_raw.h>
 
 extern int max_lock_depth; /* for sysctl */
 
+#ifdef CONFIG_DEBUG_MUTEXES
+#include <linux/debug_locks.h>
+#endif
+
 /**
  * The rt_mutex structure
  *
@@ -31,12 +35,7 @@
 	raw_spinlock_t		wait_lock;
 	struct rb_root_cached   waiters;
 	struct task_struct	*owner;
-#ifdef CONFIG_DEBUG_RT_MUTEXES
 	int			save_state;
-	const char		*name, *file;
-	int			line;
-	void			*magic;
-#endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
 #endif
@@ -49,6 +48,7 @@
  extern int rt_mutex_debug_check_no_locks_freed(const void *from,
 						unsigned long len);
  extern void rt_mutex_debug_check_no_locks_held(struct task_struct *task);
+ extern void rt_mutex_debug_task_free(struct task_struct *tsk);
 #else
  static inline int rt_mutex_debug_check_no_locks_freed(const void *from,
 						       unsigned long len)
@@ -56,25 +56,15 @@
 	return 0;
  }
 # define rt_mutex_debug_check_no_locks_held(task)	do { } while (0)
+# define rt_mutex_debug_task_free(t)			do { } while (0)
 #endif
 
-#ifdef CONFIG_DEBUG_RT_MUTEXES
-# define __DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
-	, .name = #mutexname, .file = __FILE__, .line = __LINE__
-
-# define rt_mutex_init(mutex) \
+#define rt_mutex_init(mutex) \
 do { \
 	static struct lock_class_key __key; \
 	__rt_mutex_init(mutex, __func__, &__key); \
 } while (0)
 
- extern void rt_mutex_debug_task_free(struct task_struct *tsk);
-#else
-# define __DEBUG_RT_MUTEX_INITIALIZER(mutexname)
-# define rt_mutex_init(mutex)			__rt_mutex_init(mutex, NULL, NULL)
-# define rt_mutex_debug_task_free(t)			do { } while (0)
-#endif
-
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 #define __DEP_MAP_RT_MUTEX_INITIALIZER(mutexname) \
 	, .dep_map = { .name = #mutexname }
@@ -82,12 +72,19 @@
 #define __DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)
 #endif
 
-#define __RT_MUTEX_INITIALIZER(mutexname) \
-	{ .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
+#define __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
+	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
 	, .waiters = RB_ROOT_CACHED \
 	, .owner = NULL \
-	__DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
-	__DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)}
+	__DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)
+
+#define __RT_MUTEX_INITIALIZER(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
+	, .save_state = 0 }
+
+#define __RT_MUTEX_INITIALIZER_SAVE_STATE(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname)    \
+	, .save_state = 1 }
 
 #define DEFINE_RT_MUTEX(mutexname) \
 	struct rt_mutex mutexname = __RT_MUTEX_INITIALIZER(mutexname)
@@ -115,9 +112,7 @@
 #endif
 
 extern int rt_mutex_lock_interruptible(struct rt_mutex *lock);
-extern int rt_mutex_timed_lock(struct rt_mutex *lock,
-			       struct hrtimer_sleeper *timeout);
-
+extern int rt_mutex_lock_killable(struct rt_mutex *lock);
 extern int rt_mutex_trylock(struct rt_mutex *lock);
 
 extern void rt_mutex_unlock(struct rt_mutex *lock);
diff -Naur a/include/linux/rwlock_rt.h b/include/linux/rwlock_rt.h
--- a/include/linux/rwlock_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/rwlock_rt.h	2021-07-14 15:39:09.838149374 +0300
@@ -0,0 +1,109 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_RWLOCK_RT_H
+#define __LINUX_RWLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+extern void __lockfunc rt_write_lock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_trylock(rwlock_t *rwlock);
+extern int __lockfunc rt_read_trylock(rwlock_t *rwlock);
+extern void __lockfunc rt_write_unlock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_unlock(rwlock_t *rwlock);
+extern int __lockfunc rt_read_can_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_can_lock(rwlock_t *rwlock);
+extern void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key);
+
+#define read_can_lock(rwlock)		rt_read_can_lock(rwlock)
+#define write_can_lock(rwlock)		rt_write_can_lock(rwlock)
+
+#define read_trylock(lock)	__cond_lock(lock, rt_read_trylock(lock))
+#define write_trylock(lock)	__cond_lock(lock, rt_write_trylock(lock))
+
+static inline int __write_trylock_rt_irqsave(rwlock_t *lock, unsigned long *flags)
+{
+	*flags = 0;
+	return rt_write_trylock(lock);
+}
+
+#define write_trylock_irqsave(lock, flags)		\
+	__cond_lock(lock, __write_trylock_rt_irqsave(lock, &(flags)))
+
+#define read_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		rt_read_lock(lock);			\
+		flags = 0;				\
+	} while (0)
+
+#define write_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		rt_write_lock(lock);			\
+		flags = 0;				\
+	} while (0)
+
+#define read_lock(lock)		rt_read_lock(lock)
+
+#define read_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_read_lock(lock);			\
+	} while (0)
+
+#define read_lock_irq(lock)	read_lock(lock)
+
+#define write_lock(lock)	rt_write_lock(lock)
+
+#define write_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_write_lock(lock);			\
+	} while (0)
+
+#define write_lock_irq(lock)	write_lock(lock)
+
+#define read_unlock(lock)	rt_read_unlock(lock)
+
+#define read_unlock_bh(lock)				\
+	do {						\
+		rt_read_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define read_unlock_irq(lock)	read_unlock(lock)
+
+#define write_unlock(lock)	rt_write_unlock(lock)
+
+#define write_unlock_bh(lock)				\
+	do {						\
+		rt_write_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define write_unlock_irq(lock)	write_unlock(lock)
+
+#define read_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_read_unlock(lock);			\
+	} while (0)
+
+#define write_unlock_irqrestore(lock, flags) \
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_write_unlock(lock);			\
+	} while (0)
+
+#define rwlock_init(rwl)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__rt_rwlock_init(rwl, #rwl, &__key);		\
+} while (0)
+
+#endif
diff -Naur a/include/linux/rwlock_types.h b/include/linux/rwlock_types.h
--- a/include/linux/rwlock_types.h	2020-11-23 13:48:32.965905231 +0200
+++ b/include/linux/rwlock_types.h	2021-07-14 15:39:09.838149374 +0300
@@ -1,6 +1,10 @@
 #ifndef __LINUX_RWLOCK_TYPES_H
 #define __LINUX_RWLOCK_TYPES_H
 
+#if !defined(__LINUX_SPINLOCK_TYPES_H)
+# error "Do not include directly, include spinlock_types.h"
+#endif
+
 /*
  * include/linux/rwlock_types.h - generic rwlock type definitions
  *				  and initializers
diff -Naur a/include/linux/rwlock_types_rt.h b/include/linux/rwlock_types_rt.h
--- a/include/linux/rwlock_types_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/rwlock_types_rt.h	2021-07-14 15:39:09.838149374 +0300
@@ -0,0 +1,56 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_RWLOCK_TYPES_RT_H
+#define __LINUX_RWLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define RW_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+#else
+# define RW_DEP_MAP_INIT(lockname)
+#endif
+
+typedef struct rt_rw_lock rwlock_t;
+
+#define __RW_LOCK_UNLOCKED(name) __RWLOCK_RT_INITIALIZER(name)
+
+#define DEFINE_RWLOCK(name) \
+	rwlock_t name = __RW_LOCK_UNLOCKED(name)
+
+/*
+ * A reader biased implementation primarily for CPU pinning.
+ *
+ * Can be selected as general replacement for the single reader RT rwlock
+ * variant
+ */
+struct rt_rw_lock {
+	struct rt_mutex		rtmutex;
+	atomic_t		readers;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define READER_BIAS	(1U << 31)
+#define WRITER_BIAS	(1U << 30)
+
+#define __RWLOCK_RT_INITIALIZER(name)					\
+{									\
+	.readers = ATOMIC_INIT(READER_BIAS),				\
+	.rtmutex = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.rtmutex),	\
+	RW_DEP_MAP_INIT(name)						\
+}
+
+void __rwlock_biased_rt_init(struct rt_rw_lock *lock, const char *name,
+			     struct lock_class_key *key);
+
+#define rwlock_biased_rt_init(rwlock)					\
+	do {								\
+		static struct lock_class_key __key;			\
+									\
+		__rwlock_biased_rt_init((rwlock), #rwlock, &__key);	\
+	} while (0)
+
+#endif
diff -Naur a/include/linux/rwsem.h b/include/linux/rwsem.h
--- a/include/linux/rwsem.h	2020-11-23 13:48:32.965905231 +0200
+++ b/include/linux/rwsem.h	2021-07-14 15:39:09.842149346 +0300
@@ -16,6 +16,11 @@
 #include <linux/spinlock.h>
 #include <linux/atomic.h>
 #include <linux/err.h>
+
+#ifdef CONFIG_PREEMPT_RT
+#include <linux/rwsem-rt.h>
+#else /* PREEMPT_RT */
+
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #include <linux/osq_lock.h>
 #endif
@@ -119,6 +124,13 @@
 	return !list_empty(&sem->wait_list);
 }
 
+#endif /* !PREEMPT_RT */
+
+/*
+ * The functions below are the same for all rwsem implementations including
+ * the RT specific variant.
+ */
+
 /*
  * lock for reading
  */
diff -Naur a/include/linux/rwsem-rt.h b/include/linux/rwsem-rt.h
--- a/include/linux/rwsem-rt.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/rwsem-rt.h	2021-07-14 15:39:09.838149374 +0300
@@ -0,0 +1,69 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef _LINUX_RWSEM_RT_H
+#define _LINUX_RWSEM_RT_H
+
+#ifndef _LINUX_RWSEM_H
+#error "Include rwsem.h"
+#endif
+
+#include <linux/rtmutex.h>
+#include <linux/swait.h>
+
+#define READER_BIAS		(1U << 31)
+#define WRITER_BIAS		(1U << 30)
+
+struct rw_semaphore {
+	atomic_t		readers;
+	struct rt_mutex		rtmutex;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __RWSEM_INITIALIZER(name)				\
+{								\
+	.readers = ATOMIC_INIT(READER_BIAS),			\
+	.rtmutex = __RT_MUTEX_INITIALIZER(name.rtmutex),	\
+	RW_DEP_MAP_INIT(name)					\
+}
+
+#define DECLARE_RWSEM(lockname) \
+	struct rw_semaphore lockname = __RWSEM_INITIALIZER(lockname)
+
+extern void  __rwsem_init(struct rw_semaphore *rwsem, const char *name,
+			  struct lock_class_key *key);
+
+#define __init_rwsem(sem, name, key)			\
+do {							\
+		rt_mutex_init(&(sem)->rtmutex);		\
+		__rwsem_init((sem), (name), (key));	\
+} while (0)
+
+#define init_rwsem(sem)					\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__init_rwsem((sem), #sem, &__key);		\
+} while (0)
+
+static inline int rwsem_is_locked(struct rw_semaphore *sem)
+{
+	return atomic_read(&sem->readers) != READER_BIAS;
+}
+
+static inline int rwsem_is_contended(struct rw_semaphore *sem)
+{
+	return atomic_read(&sem->readers) > 0;
+}
+
+extern void __down_read(struct rw_semaphore *sem);
+extern int __down_read_killable(struct rw_semaphore *sem);
+extern int __down_read_trylock(struct rw_semaphore *sem);
+extern void __down_write(struct rw_semaphore *sem);
+extern int __must_check __down_write_killable(struct rw_semaphore *sem);
+extern int __down_write_trylock(struct rw_semaphore *sem);
+extern void __up_read(struct rw_semaphore *sem);
+extern void __up_write(struct rw_semaphore *sem);
+extern void __downgrade_write(struct rw_semaphore *sem);
+
+#endif
diff -Naur a/include/linux/sched/hotplug.h b/include/linux/sched/hotplug.h
--- a/include/linux/sched/hotplug.h	2020-11-23 13:48:33.605918070 +0200
+++ b/include/linux/sched/hotplug.h	2021-07-14 15:39:10.094147570 +0300
@@ -11,8 +11,10 @@
 extern int sched_cpu_deactivate(unsigned int cpu);
 
 #ifdef CONFIG_HOTPLUG_CPU
+extern int sched_cpu_wait_empty(unsigned int cpu);
 extern int sched_cpu_dying(unsigned int cpu);
 #else
+# define sched_cpu_wait_empty	NULL
 # define sched_cpu_dying	NULL
 #endif
 
diff -Naur a/include/linux/sched/mm.h b/include/linux/sched/mm.h
--- a/include/linux/sched/mm.h	2020-11-23 13:48:33.609918150 +0200
+++ b/include/linux/sched/mm.h	2021-07-14 15:39:10.094147570 +0300
@@ -49,6 +49,17 @@
 		__mmdrop(mm);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern void __mmdrop_delayed(struct rcu_head *rhp);
+static inline void mmdrop_delayed(struct mm_struct *mm)
+{
+	if (atomic_dec_and_test(&mm->mm_count))
+		call_rcu(&mm->delayed_drop, __mmdrop_delayed);
+}
+#else
+# define mmdrop_delayed(mm)	mmdrop(mm)
+#endif
+
 /*
  * This has to be called after a get_task_mm()/mmget_not_zero()
  * followed by taking the mmap_lock for writing before modifying the
diff -Naur a/include/linux/sched/rt.h b/include/linux/sched/rt.h
--- a/include/linux/sched/rt.h	2020-11-23 13:48:33.609918150 +0200
+++ b/include/linux/sched/rt.h	2021-07-14 15:39:10.094147570 +0300
@@ -39,20 +39,12 @@
 }
 extern void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task);
 extern void rt_mutex_adjust_pi(struct task_struct *p);
-static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
-{
-	return tsk->pi_blocked_on != NULL;
-}
 #else
 static inline struct task_struct *rt_mutex_get_top_task(struct task_struct *task)
 {
 	return NULL;
 }
 # define rt_mutex_adjust_pi(p)		do { } while (0)
-static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
-{
-	return false;
-}
 #endif
 
 extern void normalize_rt_tasks(void);
diff -Naur a/include/linux/sched/wake_q.h b/include/linux/sched/wake_q.h
--- a/include/linux/sched/wake_q.h	2020-11-23 13:48:33.613918231 +0200
+++ b/include/linux/sched/wake_q.h	2021-07-14 15:39:10.094147570 +0300
@@ -58,6 +58,17 @@
 
 extern void wake_q_add(struct wake_q_head *head, struct task_struct *task);
 extern void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task);
-extern void wake_up_q(struct wake_q_head *head);
+extern void wake_q_add_sleeper(struct wake_q_head *head, struct task_struct *task);
+extern void __wake_up_q(struct wake_q_head *head, bool sleeper);
+
+static inline void wake_up_q(struct wake_q_head *head)
+{
+	__wake_up_q(head, false);
+}
+
+static inline void wake_up_q_sleeper(struct wake_q_head *head)
+{
+	__wake_up_q(head, true);
+}
 
 #endif /* _LINUX_SCHED_WAKE_Q_H */
diff -Naur a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h	2020-11-23 13:48:32.969905311 +0200
+++ b/include/linux/sched.h	2021-07-14 15:39:09.842149346 +0300
@@ -34,6 +34,7 @@
 #include <linux/rseq.h>
 #include <linux/seqlock.h>
 #include <linux/kcsan.h>
+#include <asm/kmap_types.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -110,12 +111,8 @@
 					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
 					 TASK_PARKED)
 
-#define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
-
 #define task_is_stopped(task)		((task->state & __TASK_STOPPED) != 0)
 
-#define task_is_stopped_or_traced(task)	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
-
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
 /*
@@ -139,6 +136,9 @@
 		smp_store_mb(current->state, (state_value));	\
 	} while (0)
 
+#define __set_current_state_no_track(state_value)		\
+	current->state = (state_value);
+
 #define set_special_state(state_value)					\
 	do {								\
 		unsigned long flags; /* may shadow */			\
@@ -192,6 +192,9 @@
 #define set_current_state(state_value)					\
 	smp_store_mb(current->state, (state_value))
 
+#define __set_current_state_no_track(state_value)	\
+	__set_current_state(state_value)
+
 /*
  * set_special_state() should be used for those states when the blocking task
  * can not use the regular condition based wait-loop. In that case we must
@@ -638,6 +641,8 @@
 #endif
 	/* -1 unrunnable, 0 runnable, >0 stopped: */
 	volatile long			state;
+	/* saved state for "spinlock sleepers" */
+	volatile long			saved_state;
 
 	/*
 	 * This begins the randomizable portion of task_struct. Only
@@ -713,6 +718,11 @@
 	int				nr_cpus_allowed;
 	const cpumask_t			*cpus_ptr;
 	cpumask_t			cpus_mask;
+	void				*migration_pending;
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	unsigned short			migration_disabled;
+#endif
+	unsigned short			migration_flags;
 
 #ifdef CONFIG_PREEMPT_RCU
 	int				rcu_read_lock_nesting;
@@ -941,11 +951,16 @@
 	/* Signal handlers: */
 	struct signal_struct		*signal;
 	struct sighand_struct __rcu		*sighand;
+	struct sigqueue			*sigqueue_cache;
 	sigset_t			blocked;
 	sigset_t			real_blocked;
 	/* Restored if set_restore_sigmask() was used: */
 	sigset_t			saved_sigmask;
 	struct sigpending		pending;
+#ifdef CONFIG_PREEMPT_RT
+	/* TODO: move me into ->restart_block ? */
+	struct				kernel_siginfo forced_info;
+#endif
 	unsigned long			sas_ss_sp;
 	size_t				sas_ss_size;
 	unsigned int			sas_ss_flags;
@@ -972,6 +987,7 @@
 	raw_spinlock_t			pi_lock;
 
 	struct wake_q_node		wake_q;
+	struct wake_q_node		wake_q_sleeper;
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task: */
@@ -999,6 +1015,9 @@
 	int				softirq_context;
 	int				irq_config;
 #endif
+#ifdef CONFIG_PREEMPT_RT
+	int				softirq_disable_cnt;
+#endif
 
 #ifdef CONFIG_LOCKDEP
 # define MAX_LOCK_DEPTH			48UL
@@ -1280,6 +1299,12 @@
 	unsigned int			sequential_io;
 	unsigned int			sequential_io_avg;
 #endif
+#ifdef CONFIG_PREEMPT_RT
+# if defined CONFIG_HIGHMEM || defined CONFIG_X86_32
+	int				kmap_idx;
+	pte_t				kmap_pte[KM_TYPE_NR];
+# endif
+#endif
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	unsigned long			task_state_change;
 #endif
@@ -1722,6 +1747,7 @@
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
+extern int wake_up_lock_sleeper(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
@@ -1812,6 +1838,89 @@
 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+static inline void set_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int test_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY));
+}
+
+static inline int need_resched_lazy(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#else
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk) { }
+static inline int need_resched_lazy(void) { return 0; }
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#endif
+
+
+static inline bool __task_is_stopped_or_traced(struct task_struct *task)
+{
+	if (task->state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#ifdef CONFIG_PREEMPT_RT
+	if (task->saved_state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#endif
+	return false;
+}
+
+static inline bool task_is_stopped_or_traced(struct task_struct *task)
+{
+	bool traced_stopped;
+
+#ifdef CONFIG_PREEMPT_RT
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&task->pi_lock, flags);
+	traced_stopped = __task_is_stopped_or_traced(task);
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+#else
+	traced_stopped = __task_is_stopped_or_traced(task);
+#endif
+	return traced_stopped;
+}
+
+static inline bool task_is_traced(struct task_struct *task)
+{
+	bool traced = false;
+
+	if (task->state & __TASK_TRACED)
+		return true;
+#ifdef CONFIG_PREEMPT_RT
+	/* in case the task is sleeping on tasklist_lock */
+	raw_spin_lock_irq(&task->pi_lock);
+	if (task->state & __TASK_TRACED)
+		traced = true;
+	else if (task->saved_state & __TASK_TRACED)
+		traced = true;
+	raw_spin_unlock_irq(&task->pi_lock);
+#endif
+	return traced;
+}
+
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return
diff -Naur a/include/linux/semaphore.h b/include/linux/semaphore.h
--- a/include/linux/semaphore.h	2020-11-23 13:48:32.981905552 +0200
+++ b/include/linux/semaphore.h	2020-11-23 13:50:10.383440207 +0200
@@ -1,5 +1,10 @@
 /* SPDX-License-Identifier: GPL-2.0-only */
 /*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *
  * Copyright (c) 2008 Intel Corporation
  * Author: Matthew Wilcox <willy@linux.intel.com>
  *
@@ -10,6 +15,7 @@
 
 #include <linux/list.h>
 #include <linux/spinlock.h>
+#include <linux/hrtimer.h>
 
 /* Please don't access any members of this structure directly */
 struct semaphore {
@@ -40,6 +46,7 @@
 extern int __must_check down_killable(struct semaphore *sem);
 extern int __must_check down_trylock(struct semaphore *sem);
 extern int __must_check down_timeout(struct semaphore *sem, long jiffies);
+extern int __must_check down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout);
 extern void up(struct semaphore *sem);
 
 #endif /* __LINUX_SEMAPHORE_H */
diff -Naur a/include/linux/seqlock.h b/include/linux/seqlock.h
--- a/include/linux/seqlock.h	2020-11-23 13:48:32.985905632 +0200
+++ b/include/linux/seqlock.h	2021-07-14 15:39:09.842149346 +0300
@@ -17,6 +17,7 @@
 #include <linux/kcsan-checks.h>
 #include <linux/lockdep.h>
 #include <linux/mutex.h>
+#include <linux/ww_mutex.h>
 #include <linux/preempt.h>
 #include <linux/spinlock.h>
 
@@ -53,7 +54,7 @@
  *
  * If the write serialization mechanism is one of the common kernel
  * locking primitives, use a sequence counter with associated lock
- * (seqcount_LOCKTYPE_t) instead.
+ * (seqcount_LOCKNAME_t) instead.
  *
  * If it's desired to automatically handle the sequence counter writer
  * serialization and non-preemptibility requirements, use a sequential
@@ -117,7 +118,7 @@
 #define SEQCNT_ZERO(name) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(name) }
 
 /*
- * Sequence counters with associated locks (seqcount_LOCKTYPE_t)
+ * Sequence counters with associated locks (seqcount_LOCKNAME_t)
  *
  * A sequence counter which associates the lock used for writer
  * serialization at initialization time. This enables lockdep to validate
@@ -131,37 +132,59 @@
  * See Documentation/locking/seqlock.rst
  */
 
-#ifdef CONFIG_LOCKDEP
+/*
+ * For PREEMPT_RT, seqcount_LOCKNAME_t write side critical sections cannot
+ * disable preemption. It can lead to higher latencies, and the write side
+ * sections will not be able to acquire locks which become sleeping locks
+ * (e.g. spinlock_t).
+ *
+ * To remain preemptible while avoiding a possible livelock caused by the
+ * reader preempting the writer, use a different technique: let the reader
+ * detect if a seqcount_LOCKNAME_t writer is in progress. If that is the
+ * case, acquire then release the associated LOCKNAME writer serialization
+ * lock. This will allow any possibly-preempted writer to make progress
+ * until the end of its writer serialization lock critical section.
+ *
+ * This lock-unlock technique must be implemented for all of PREEMPT_RT
+ * sleeping locks.  See Documentation/locking/locktypes.rst
+ */
+#if defined(CONFIG_LOCKDEP) || defined(CONFIG_PREEMPT_RT)
 #define __SEQ_LOCK(expr)	expr
 #else
 #define __SEQ_LOCK(expr)
 #endif
 
 /**
- * typedef seqcount_LOCKNAME_t - sequence counter with LOCKTYPR associated
+ * typedef seqcount_LOCKNAME_t - sequence counter with LOCKNAME associated
  * @seqcount:	The real sequence counter
- * @lock:	Pointer to the associated spinlock
+ * @lock:	Pointer to the associated lock
  *
- * A plain sequence counter with external writer synchronization by a
- * spinlock. The spinlock is associated to the sequence count in the
+ * A plain sequence counter with external writer synchronization by
+ * LOCKNAME @lock. The lock is associated to the sequence counter in the
  * static initializer or init function. This enables lockdep to validate
  * that the write side critical section is properly serialized.
+ *
+ * LOCKNAME:	raw_spinlock, spinlock, rwlock, mutex, or ww_mutex.
  */
 
-/**
+/*
  * seqcount_LOCKNAME_init() - runtime initializer for seqcount_LOCKNAME_t
  * @s:		Pointer to the seqcount_LOCKNAME_t instance
- * @lock:	Pointer to the associated LOCKTYPE
+ * @lock:	Pointer to the associated lock
  */
 
 /*
- * SEQCOUNT_LOCKTYPE() - Instantiate seqcount_LOCKNAME_t and helpers
- * @locktype:		actual typename
- * @lockname:		name
+ * SEQCOUNT_LOCKNAME()	- Instantiate seqcount_LOCKNAME_t and helpers
+ * seqprop_LOCKNAME_*()	- Property accessors for seqcount_LOCKNAME_t
+ *
+ * @lockname:		"LOCKNAME" part of seqcount_LOCKNAME_t
+ * @locktype:		LOCKNAME canonical C data type
  * @preemptible:	preemptibility of above locktype
  * @lockmember:		argument for lockdep_assert_held()
+ * @lockbase:		associated lock release function (prefix only)
+ * @lock_acquire:	associated lock acquisition function (full call)
  */
-#define SEQCOUNT_LOCKTYPE(locktype, lockname, preemptible, lockmember)	\
+#define SEQCOUNT_LOCKNAME(lockname, locktype, preemptible, lockmember, lockbase, lock_acquire) \
 typedef struct seqcount_##lockname {					\
 	seqcount_t		seqcount;				\
 	__SEQ_LOCK(locktype	*lock);					\
@@ -175,19 +198,45 @@
 }									\
 									\
 static __always_inline seqcount_t *					\
-__seqcount_##lockname##_ptr(seqcount_##lockname##_t *s)			\
+__seqprop_##lockname##_ptr(seqcount_##lockname##_t *s)			\
 {									\
 	return &s->seqcount;						\
 }									\
 									\
+static __always_inline unsigned						\
+__seqprop_##lockname##_sequence(const seqcount_##lockname##_t *s)	\
+{									\
+	unsigned seq = READ_ONCE(s->seqcount.sequence);			\
+									\
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))				\
+		return seq;						\
+									\
+	if (preemptible && unlikely(seq & 1)) {				\
+		__SEQ_LOCK(lock_acquire);				\
+		__SEQ_LOCK(lockbase##_unlock(s->lock));			\
+									\
+		/*							\
+		 * Re-read the sequence counter since the (possibly	\
+		 * preempted) writer made progress.			\
+		 */							\
+		seq = READ_ONCE(s->seqcount.sequence);			\
+	}								\
+									\
+	return seq;							\
+}									\
+									\
 static __always_inline bool						\
-__seqcount_##lockname##_preemptible(seqcount_##lockname##_t *s)		\
+__seqprop_##lockname##_preemptible(const seqcount_##lockname##_t *s)	\
 {									\
-	return preemptible;						\
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))				\
+		return preemptible;					\
+									\
+	/* PREEMPT_RT relies on the above LOCK+UNLOCK */		\
+	return false;							\
 }									\
 									\
 static __always_inline void						\
-__seqcount_##lockname##_assert(seqcount_##lockname##_t *s)		\
+__seqprop_##lockname##_assert(const seqcount_##lockname##_t *s)		\
 {									\
 	__SEQ_LOCK(lockdep_assert_held(lockmember));			\
 }
@@ -196,50 +245,56 @@
  * __seqprop() for seqcount_t
  */
 
-static inline seqcount_t *__seqcount_ptr(seqcount_t *s)
+static inline seqcount_t *__seqprop_ptr(seqcount_t *s)
 {
 	return s;
 }
 
-static inline bool __seqcount_preemptible(seqcount_t *s)
+static inline unsigned __seqprop_sequence(const seqcount_t *s)
+{
+	return READ_ONCE(s->sequence);
+}
+
+static inline bool __seqprop_preemptible(const seqcount_t *s)
 {
 	return false;
 }
 
-static inline void __seqcount_assert(seqcount_t *s)
+static inline void __seqprop_assert(const seqcount_t *s)
 {
 	lockdep_assert_preemption_disabled();
 }
 
-SEQCOUNT_LOCKTYPE(raw_spinlock_t,	raw_spinlock,	false,	s->lock)
-SEQCOUNT_LOCKTYPE(spinlock_t,		spinlock,	false,	s->lock)
-SEQCOUNT_LOCKTYPE(rwlock_t,		rwlock,		false,	s->lock)
-SEQCOUNT_LOCKTYPE(struct mutex,		mutex,		true,	s->lock)
-SEQCOUNT_LOCKTYPE(struct ww_mutex,	ww_mutex,	true,	&s->lock->base)
+#define __SEQ_RT	IS_ENABLED(CONFIG_PREEMPT_RT)
 
-/**
+SEQCOUNT_LOCKNAME(raw_spinlock, raw_spinlock_t,  false,    s->lock,        raw_spin, raw_spin_lock(s->lock))
+SEQCOUNT_LOCKNAME(spinlock,     spinlock_t,      __SEQ_RT, s->lock,        spin,     spin_lock(s->lock))
+SEQCOUNT_LOCKNAME(rwlock,       rwlock_t,        __SEQ_RT, s->lock,        read,     read_lock(s->lock))
+SEQCOUNT_LOCKNAME(mutex,        struct mutex,    true,     s->lock,        mutex,    mutex_lock(s->lock))
+SEQCOUNT_LOCKNAME(ww_mutex,     struct ww_mutex, true,     &s->lock->base, ww_mutex, ww_mutex_lock(s->lock, NULL))
+
+/*
  * SEQCNT_LOCKNAME_ZERO - static initializer for seqcount_LOCKNAME_t
  * @name:	Name of the seqcount_LOCKNAME_t instance
- * @lock:	Pointer to the associated LOCKTYPE
+ * @lock:	Pointer to the associated LOCKNAME
  */
 
-#define SEQCOUNT_LOCKTYPE_ZERO(seq_name, assoc_lock) {			\
+#define SEQCOUNT_LOCKNAME_ZERO(seq_name, assoc_lock) {			\
 	.seqcount		= SEQCNT_ZERO(seq_name.seqcount),	\
 	__SEQ_LOCK(.lock	= (assoc_lock))				\
 }
 
-#define SEQCNT_SPINLOCK_ZERO(name, lock)	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
-#define SEQCNT_RAW_SPINLOCK_ZERO(name, lock)	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
-#define SEQCNT_RWLOCK_ZERO(name, lock)		SEQCOUNT_LOCKTYPE_ZERO(name, lock)
-#define SEQCNT_MUTEX_ZERO(name, lock)		SEQCOUNT_LOCKTYPE_ZERO(name, lock)
-#define SEQCNT_WW_MUTEX_ZERO(name, lock) 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
-
+#define SEQCNT_SPINLOCK_ZERO(name, lock)	SEQCOUNT_LOCKNAME_ZERO(name, lock)
+#define SEQCNT_RAW_SPINLOCK_ZERO(name, lock)	SEQCOUNT_LOCKNAME_ZERO(name, lock)
+#define SEQCNT_RWLOCK_ZERO(name, lock)		SEQCOUNT_LOCKNAME_ZERO(name, lock)
+#define SEQCNT_MUTEX_ZERO(name, lock)		SEQCOUNT_LOCKNAME_ZERO(name, lock)
+#define SEQCNT_WW_MUTEX_ZERO(name, lock) 	SEQCOUNT_LOCKNAME_ZERO(name, lock)
 
 #define __seqprop_case(s, lockname, prop)				\
-	seqcount_##lockname##_t: __seqcount_##lockname##_##prop((void *)(s))
+	seqcount_##lockname##_t: __seqprop_##lockname##_##prop((void *)(s))
 
 #define __seqprop(s, prop) _Generic(*(s),				\
-	seqcount_t:		__seqcount_##prop((void *)(s)),		\
+	seqcount_t:		__seqprop_##prop((void *)(s)),		\
 	__seqprop_case((s),	raw_spinlock,	prop),			\
 	__seqprop_case((s),	spinlock,	prop),			\
 	__seqprop_case((s),	rwlock,		prop),			\
@@ -247,12 +302,13 @@
 	__seqprop_case((s),	ww_mutex,	prop))
 
 #define __seqcount_ptr(s)		__seqprop(s, ptr)
+#define __seqcount_sequence(s)		__seqprop(s, sequence)
 #define __seqcount_lock_preemptible(s)	__seqprop(s, preemptible)
 #define __seqcount_assert_lock_held(s)	__seqprop(s, assert)
 
 /**
  * __read_seqcount_begin() - begin a seqcount_t read section w/o barrier
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()
  * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
@@ -265,56 +321,45 @@
  * Return: count to be passed to read_seqcount_retry()
  */
 #define __read_seqcount_begin(s)					\
-	__read_seqcount_t_begin(__seqcount_ptr(s))
-
-static inline unsigned __read_seqcount_t_begin(const seqcount_t *s)
-{
-	unsigned ret;
-
-repeat:
-	ret = READ_ONCE(s->sequence);
-	if (unlikely(ret & 1)) {
-		cpu_relax();
-		goto repeat;
-	}
-	kcsan_atomic_next(KCSAN_SEQLOCK_REGION_MAX);
-	return ret;
-}
+({									\
+	unsigned seq;							\
+									\
+	while ((seq = __seqcount_sequence(s)) & 1)			\
+		cpu_relax();						\
+									\
+	kcsan_atomic_next(KCSAN_SEQLOCK_REGION_MAX);			\
+	seq;								\
+})
 
 /**
  * raw_read_seqcount_begin() - begin a seqcount_t read section w/o lockdep
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * Return: count to be passed to read_seqcount_retry()
  */
 #define raw_read_seqcount_begin(s)					\
-	raw_read_seqcount_t_begin(__seqcount_ptr(s))
-
-static inline unsigned raw_read_seqcount_t_begin(const seqcount_t *s)
-{
-	unsigned ret = __read_seqcount_t_begin(s);
-	smp_rmb();
-	return ret;
-}
+({									\
+	unsigned seq = __read_seqcount_begin(s);			\
+									\
+	smp_rmb();							\
+	seq;								\
+})
 
 /**
  * read_seqcount_begin() - begin a seqcount_t read critical section
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * Return: count to be passed to read_seqcount_retry()
  */
 #define read_seqcount_begin(s)						\
-	read_seqcount_t_begin(__seqcount_ptr(s))
-
-static inline unsigned read_seqcount_t_begin(const seqcount_t *s)
-{
-	seqcount_lockdep_reader_access(s);
-	return raw_read_seqcount_t_begin(s);
-}
+({									\
+	seqcount_lockdep_reader_access(__seqcount_ptr(s));		\
+	raw_read_seqcount_begin(s);					\
+})
 
 /**
  * raw_read_seqcount() - read the raw seqcount_t counter value
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * raw_read_seqcount opens a read critical section of the given
  * seqcount_t, without any lockdep checking, and without checking or
@@ -324,20 +369,18 @@
  * Return: count to be passed to read_seqcount_retry()
  */
 #define raw_read_seqcount(s)						\
-	raw_read_seqcount_t(__seqcount_ptr(s))
-
-static inline unsigned raw_read_seqcount_t(const seqcount_t *s)
-{
-	unsigned ret = READ_ONCE(s->sequence);
-	smp_rmb();
-	kcsan_atomic_next(KCSAN_SEQLOCK_REGION_MAX);
-	return ret;
-}
+({									\
+	unsigned seq = __seqcount_sequence(s);				\
+									\
+	smp_rmb();							\
+	kcsan_atomic_next(KCSAN_SEQLOCK_REGION_MAX);			\
+	seq;								\
+})
 
 /**
  * raw_seqcount_begin() - begin a seqcount_t read critical section w/o
  *                        lockdep and w/o counter stabilization
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * raw_seqcount_begin opens a read critical section of the given
  * seqcount_t. Unlike read_seqcount_begin(), this function will not wait
@@ -352,20 +395,17 @@
  * Return: count to be passed to read_seqcount_retry()
  */
 #define raw_seqcount_begin(s)						\
-	raw_seqcount_t_begin(__seqcount_ptr(s))
-
-static inline unsigned raw_seqcount_t_begin(const seqcount_t *s)
-{
-	/*
-	 * If the counter is odd, let read_seqcount_retry() fail
-	 * by decrementing the counter.
-	 */
-	return raw_read_seqcount_t(s) & ~1;
-}
+({									\
+	/*								\
+	 * If the counter is odd, let read_seqcount_retry() fail	\
+	 * by decrementing the counter.					\
+	 */								\
+	raw_read_seqcount(s) & ~1;					\
+})
 
 /**
  * __read_seqcount_retry() - end a seqcount_t read section w/o barrier
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  * @start: count, from read_seqcount_begin()
  *
  * __read_seqcount_retry is like read_seqcount_retry, but has no smp_rmb()
@@ -389,7 +429,7 @@
 
 /**
  * read_seqcount_retry() - end a seqcount_t read critical section
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  * @start: count, from read_seqcount_begin()
  *
  * read_seqcount_retry closes the read critical section of given
@@ -409,7 +449,7 @@
 
 /**
  * raw_write_seqcount_begin() - start a seqcount_t write section w/o lockdep
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  */
 #define raw_write_seqcount_begin(s)					\
 do {									\
@@ -428,7 +468,7 @@
 
 /**
  * raw_write_seqcount_end() - end a seqcount_t write section w/o lockdep
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  */
 #define raw_write_seqcount_end(s)					\
 do {									\
@@ -448,7 +488,7 @@
 /**
  * write_seqcount_begin_nested() - start a seqcount_t write section with
  *                                 custom lockdep nesting level
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  * @subclass: lockdep nesting level
  *
  * See Documentation/locking/lockdep-design.rst
@@ -471,7 +511,7 @@
 
 /**
  * write_seqcount_begin() - start a seqcount_t write side critical section
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * write_seqcount_begin opens a write side critical section of the given
  * seqcount_t.
@@ -497,7 +537,7 @@
 
 /**
  * write_seqcount_end() - end a seqcount_t write side critical section
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * The write section must've been opened with write_seqcount_begin().
  */
@@ -517,7 +557,7 @@
 
 /**
  * raw_write_seqcount_barrier() - do a seqcount_t write barrier
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * This can be used to provide an ordering guarantee instead of the usual
  * consistency guarantee. It is one wmb cheaper, because it can collapse
@@ -571,7 +611,7 @@
 /**
  * write_seqcount_invalidate() - invalidate in-progress seqcount_t read
  *                               side operations
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
  *
  * After write_seqcount_invalidate, no seqcount_t read side operations
  * will complete successfully and see data older than this.
@@ -587,34 +627,73 @@
 	kcsan_nestable_atomic_end();
 }
 
-/**
- * raw_read_seqcount_latch() - pick even/odd seqcount_t latch data copy
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+/*
+ * Latch sequence counters (seqcount_latch_t)
  *
- * Use seqcount_t latching to switch between two storage places protected
- * by a sequence counter. Doing so allows having interruptible, preemptible,
- * seqcount_t write side critical sections.
+ * A sequence counter variant where the counter even/odd value is used to
+ * switch between two copies of protected data. This allows the read path,
+ * typically NMIs, to safely interrupt the write side critical section.
+ *
+ * As the write sections are fully preemptible, no special handling for
+ * PREEMPT_RT is needed.
+ */
+typedef struct {
+	seqcount_t seqcount;
+} seqcount_latch_t;
+
+/**
+ * SEQCNT_LATCH_ZERO() - static initializer for seqcount_latch_t
+ * @seq_name: Name of the seqcount_latch_t instance
+ */
+#define SEQCNT_LATCH_ZERO(seq_name) {					\
+	.seqcount		= SEQCNT_ZERO(seq_name.seqcount),	\
+}
+
+/**
+ * seqcount_latch_init() - runtime initializer for seqcount_latch_t
+ * @s: Pointer to the seqcount_latch_t instance
+ */
+static inline void seqcount_latch_init(seqcount_latch_t *s)
+{
+	seqcount_init(&s->seqcount);
+}
+
+/**
+ * raw_read_seqcount_latch() - pick even/odd latch data copy
+ * @s: Pointer to seqcount_latch_t
  *
- * Check raw_write_seqcount_latch() for more details and a full reader and
- * writer usage example.
+ * See raw_write_seqcount_latch() for details and a full reader/writer
+ * usage example.
  *
  * Return: sequence counter raw value. Use the lowest bit as an index for
- * picking which data copy to read. The full counter value must then be
- * checked with read_seqcount_retry().
+ * picking which data copy to read. The full counter must then be checked
+ * with read_seqcount_latch_retry().
  */
-#define raw_read_seqcount_latch(s)					\
-	raw_read_seqcount_t_latch(__seqcount_ptr(s))
+static inline unsigned raw_read_seqcount_latch(const seqcount_latch_t *s)
+{
+	/*
+	 * Pairs with the first smp_wmb() in raw_write_seqcount_latch().
+	 * Due to the dependent load, a full smp_rmb() is not needed.
+	 */
+	return READ_ONCE(s->seqcount.sequence);
+}
 
-static inline int raw_read_seqcount_t_latch(seqcount_t *s)
+/**
+ * read_seqcount_latch_retry() - end a seqcount_latch_t read section
+ * @s:		Pointer to seqcount_latch_t
+ * @start:	count, from raw_read_seqcount_latch()
+ *
+ * Return: true if a read section retry is required, else false
+ */
+static inline int
+read_seqcount_latch_retry(const seqcount_latch_t *s, unsigned start)
 {
-	/* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
-	int seq = READ_ONCE(s->sequence); /* ^^^ */
-	return seq;
+	return read_seqcount_retry(&s->seqcount, start);
 }
 
 /**
- * raw_write_seqcount_latch() - redirect readers to even/odd copy
- * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+ * raw_write_seqcount_latch() - redirect latch readers to even/odd copy
+ * @s: Pointer to seqcount_latch_t
  *
  * The latch technique is a multiversion concurrency control method that allows
  * queries during non-atomic modifications. If you can guarantee queries never
@@ -633,7 +712,7 @@
  * The basic form is a data structure like::
  *
  *	struct latch_struct {
- *		seqcount_t		seq;
+ *		seqcount_latch_t	seq;
  *		struct data_struct	data[2];
  *	};
  *
@@ -643,13 +722,13 @@
  *	void latch_modify(struct latch_struct *latch, ...)
  *	{
  *		smp_wmb();	// Ensure that the last data[1] update is visible
- *		latch->seq++;
+ *		latch->seq.sequence++;
  *		smp_wmb();	// Ensure that the seqcount update is visible
  *
  *		modify(latch->data[0], ...);
  *
  *		smp_wmb();	// Ensure that the data[0] update is visible
- *		latch->seq++;
+ *		latch->seq.sequence++;
  *		smp_wmb();	// Ensure that the seqcount update is visible
  *
  *		modify(latch->data[1], ...);
@@ -668,8 +747,8 @@
  *			idx = seq & 0x01;
  *			entry = data_query(latch->data[idx], ...);
  *
- *		// read_seqcount_retry() includes needed smp_rmb()
- *		} while (read_seqcount_retry(&latch->seq, seq));
+ *		// This includes needed smp_rmb()
+ *		} while (read_seqcount_latch_retry(&latch->seq, seq));
  *
  *		return entry;
  *	}
@@ -688,19 +767,16 @@
  *	to miss an entire modification sequence, once it resumes it might
  *	observe the new entry.
  *
- * NOTE:
+ * NOTE2:
  *
  *	When data is a dynamic data structure; one should use regular RCU
  *	patterns to manage the lifetimes of the objects within.
  */
-#define raw_write_seqcount_latch(s)					\
-	raw_write_seqcount_t_latch(__seqcount_ptr(s))
-
-static inline void raw_write_seqcount_t_latch(seqcount_t *s)
+static inline void raw_write_seqcount_latch(seqcount_latch_t *s)
 {
-       smp_wmb();      /* prior stores before incrementing "sequence" */
-       s->sequence++;
-       smp_wmb();      /* increment "sequence" before following stores */
+	smp_wmb();	/* prior stores before incrementing "sequence" */
+	s->seqcount.sequence++;
+	smp_wmb();      /* increment "sequence" before following stores */
 }
 
 /*
@@ -714,13 +790,17 @@
  *    - Documentation/locking/seqlock.rst
  */
 typedef struct {
-	struct seqcount seqcount;
+	/*
+	 * Make sure that readers don't starve writers on PREEMPT_RT: use
+	 * seqcount_spinlock_t instead of seqcount_t. Check __SEQ_LOCK().
+	 */
+	seqcount_spinlock_t seqcount;
 	spinlock_t lock;
 } seqlock_t;
 
 #define __SEQLOCK_UNLOCKED(lockname)					\
 	{								\
-		.seqcount = SEQCNT_ZERO(lockname),			\
+		.seqcount = SEQCNT_SPINLOCK_ZERO(lockname, &(lockname).lock), \
 		.lock =	__SPIN_LOCK_UNLOCKED(lockname)			\
 	}
 
@@ -730,8 +810,8 @@
  */
 #define seqlock_init(sl)						\
 	do {								\
-		seqcount_init(&(sl)->seqcount);				\
 		spin_lock_init(&(sl)->lock);				\
+		seqcount_spinlock_init(&(sl)->seqcount, &(sl)->lock);	\
 	} while (0)
 
 /**
@@ -778,6 +858,12 @@
 	return read_seqcount_retry(&sl->seqcount, start);
 }
 
+/*
+ * For all seqlock_t write side functions, use write_seqcount_*t*_begin()
+ * instead of the generic write_seqcount_begin(). This way, no redundant
+ * lockdep_assert_held() checks are added.
+ */
+
 /**
  * write_seqlock() - start a seqlock_t write side critical section
  * @sl: Pointer to seqlock_t
@@ -794,7 +880,7 @@
 static inline void write_seqlock(seqlock_t *sl)
 {
 	spin_lock(&sl->lock);
-	write_seqcount_t_begin(&sl->seqcount);
+	write_seqcount_t_begin(&sl->seqcount.seqcount);
 }
 
 /**
@@ -806,7 +892,7 @@
  */
 static inline void write_sequnlock(seqlock_t *sl)
 {
-	write_seqcount_t_end(&sl->seqcount);
+	write_seqcount_t_end(&sl->seqcount.seqcount);
 	spin_unlock(&sl->lock);
 }
 
@@ -820,7 +906,7 @@
 static inline void write_seqlock_bh(seqlock_t *sl)
 {
 	spin_lock_bh(&sl->lock);
-	write_seqcount_t_begin(&sl->seqcount);
+	write_seqcount_t_begin(&sl->seqcount.seqcount);
 }
 
 /**
@@ -833,7 +919,7 @@
  */
 static inline void write_sequnlock_bh(seqlock_t *sl)
 {
-	write_seqcount_t_end(&sl->seqcount);
+	write_seqcount_t_end(&sl->seqcount.seqcount);
 	spin_unlock_bh(&sl->lock);
 }
 
@@ -847,7 +933,7 @@
 static inline void write_seqlock_irq(seqlock_t *sl)
 {
 	spin_lock_irq(&sl->lock);
-	write_seqcount_t_begin(&sl->seqcount);
+	write_seqcount_t_begin(&sl->seqcount.seqcount);
 }
 
 /**
@@ -859,7 +945,7 @@
  */
 static inline void write_sequnlock_irq(seqlock_t *sl)
 {
-	write_seqcount_t_end(&sl->seqcount);
+	write_seqcount_t_end(&sl->seqcount.seqcount);
 	spin_unlock_irq(&sl->lock);
 }
 
@@ -868,7 +954,7 @@
 	unsigned long flags;
 
 	spin_lock_irqsave(&sl->lock, flags);
-	write_seqcount_t_begin(&sl->seqcount);
+	write_seqcount_t_begin(&sl->seqcount.seqcount);
 	return flags;
 }
 
@@ -897,7 +983,7 @@
 static inline void
 write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
 {
-	write_seqcount_t_end(&sl->seqcount);
+	write_seqcount_t_end(&sl->seqcount.seqcount);
 	spin_unlock_irqrestore(&sl->lock, flags);
 }
 
diff -Naur a/include/linux/serial_8250.h b/include/linux/serial_8250.h
--- a/include/linux/serial_8250.h	2020-11-23 13:48:32.985905632 +0200
+++ b/include/linux/serial_8250.h	2021-07-14 15:39:09.842149346 +0300
@@ -7,6 +7,7 @@
 #ifndef _LINUX_SERIAL_8250_H
 #define _LINUX_SERIAL_8250_H
 
+#include <linux/atomic.h>
 #include <linux/serial_core.h>
 #include <linux/serial_reg.h>
 #include <linux/platform_device.h>
@@ -125,6 +126,8 @@
 #define MSR_SAVE_FLAGS UART_MSR_ANY_DELTA
 	unsigned char		msr_saved_flags;
 
+	atomic_t		console_printing;
+
 	struct uart_8250_dma	*dma;
 	const struct uart_8250_ops *ops;
 
@@ -180,6 +183,8 @@
 void serial8250_set_defaults(struct uart_8250_port *up);
 void serial8250_console_write(struct uart_8250_port *up, const char *s,
 			      unsigned int count);
+void serial8250_console_write_atomic(struct uart_8250_port *up, const char *s,
+				     unsigned int count);
 int serial8250_console_setup(struct uart_port *port, char *options, bool probe);
 int serial8250_console_exit(struct uart_port *port);
 
diff -Naur a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h
--- a/include/linux/shmem_fs.h	2020-11-23 13:48:32.993905793 +0200
+++ b/include/linux/shmem_fs.h	2021-07-14 15:39:09.842149346 +0300
@@ -31,7 +31,7 @@
 	struct percpu_counter used_blocks;  /* How many are allocated */
 	unsigned long max_inodes;   /* How many inodes are allowed */
 	unsigned long free_inodes;  /* How many are left for allocation */
-	spinlock_t stat_lock;	    /* Serialize shmem_sb_info changes */
+	raw_spinlock_t stat_lock;   /* Serialize shmem_sb_info changes */
 	umode_t mode;		    /* Mount mode for root directory */
 	unsigned char huge;	    /* Whether to try for hugepages */
 	kuid_t uid;		    /* Mount uid for root directory */
diff -Naur a/include/linux/signal.h b/include/linux/signal.h
--- a/include/linux/signal.h	2020-11-23 13:48:32.993905793 +0200
+++ b/include/linux/signal.h	2021-07-14 15:39:09.842149346 +0300
@@ -263,6 +263,7 @@
 }
 
 extern void flush_sigqueue(struct sigpending *queue);
+extern void flush_task_sigqueue(struct task_struct *tsk);
 
 /* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
 static inline int valid_signal(unsigned long sig)
diff -Naur a/include/linux/skbuff.h b/include/linux/skbuff.h
--- a/include/linux/skbuff.h	2020-11-23 13:48:33.001905954 +0200
+++ b/include/linux/skbuff.h	2021-07-14 15:39:09.842149346 +0300
@@ -295,6 +295,7 @@
 
 	__u32		qlen;
 	spinlock_t	lock;
+	raw_spinlock_t	raw_lock;
 };
 
 struct sk_buff;
@@ -1884,6 +1885,12 @@
 	__skb_queue_head_init(list);
 }
 
+static inline void skb_queue_head_init_raw(struct sk_buff_head *list)
+{
+	raw_spin_lock_init(&list->raw_lock);
+	__skb_queue_head_init(list);
+}
+
 static inline void skb_queue_head_init_class(struct sk_buff_head *list,
 		struct lock_class_key *class)
 {
diff -Naur a/include/linux/smp.h b/include/linux/smp.h
--- a/include/linux/smp.h	2020-11-23 13:48:33.005906033 +0200
+++ b/include/linux/smp.h	2021-07-14 15:39:09.842149346 +0300
@@ -236,6 +236,9 @@
 #define get_cpu()		({ preempt_disable(); __smp_processor_id(); })
 #define put_cpu()		preempt_enable()
 
+#define get_cpu_light()		({ migrate_disable(); __smp_processor_id(); })
+#define put_cpu_light()		migrate_enable()
+
 /*
  * Callback to arch code if there's nosmp or maxcpus=0 on the
  * boot command line:
diff -Naur a/include/linux/spinlock_api_smp.h b/include/linux/spinlock_api_smp.h
--- a/include/linux/spinlock_api_smp.h	2020-11-23 13:48:33.009906114 +0200
+++ b/include/linux/spinlock_api_smp.h	2021-07-14 15:39:09.846149318 +0300
@@ -187,6 +187,8 @@
 	return 0;
 }
 
-#include <linux/rwlock_api_smp.h>
+#ifndef CONFIG_PREEMPT_RT
+# include <linux/rwlock_api_smp.h>
+#endif
 
 #endif /* __LINUX_SPINLOCK_API_SMP_H */
diff -Naur a/include/linux/spinlock.h b/include/linux/spinlock.h
--- a/include/linux/spinlock.h	2020-11-23 13:48:33.009906114 +0200
+++ b/include/linux/spinlock.h	2021-07-14 15:39:09.842149346 +0300
@@ -309,7 +309,11 @@
 })
 
 /* Include rwlock functions */
-#include <linux/rwlock.h>
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/rwlock_rt.h>
+#else
+# include <linux/rwlock.h>
+#endif
 
 /*
  * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
@@ -320,6 +324,10 @@
 # include <linux/spinlock_api_up.h>
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/spinlock_rt.h>
+#else /* PREEMPT_RT */
+
 /*
  * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
  */
@@ -454,6 +462,8 @@
 
 #define assert_spin_locked(lock)	assert_raw_spin_locked(&(lock)->rlock)
 
+#endif /* !PREEMPT_RT */
+
 /*
  * Pull the atomic_t declaration:
  * (asm-mips/atomic.h needs above definitions)
diff -Naur a/include/linux/spinlock_rt.h b/include/linux/spinlock_rt.h
--- a/include/linux/spinlock_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/spinlock_rt.h	2021-07-14 15:39:09.846149318 +0300
@@ -0,0 +1,155 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_SPINLOCK_RT_H
+#define __LINUX_SPINLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+#include <linux/bug.h>
+
+extern void
+__rt_spin_lock_init(spinlock_t *lock, const char *name, struct lock_class_key *key);
+
+#define spin_lock_init(slock)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(slock)->lock);			\
+	__rt_spin_lock_init(slock, #slock, &__key);	\
+} while (0)
+
+extern void __lockfunc rt_spin_lock(spinlock_t *lock);
+extern void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass);
+extern void __lockfunc rt_spin_lock_nest_lock(spinlock_t *lock, struct lockdep_map *nest_lock);
+extern void __lockfunc rt_spin_unlock(spinlock_t *lock);
+extern void __lockfunc rt_spin_lock_unlock(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags);
+extern int __lockfunc rt_spin_trylock_bh(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock(spinlock_t *lock);
+extern int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock);
+
+/*
+ * lockdep-less calls, for derived types like rwlock:
+ * (for trylock they can use rt_mutex_trylock() directly.
+ * Migrate disable handling must be done at the call site.
+ */
+extern void __lockfunc __rt_spin_lock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_trylock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_unlock(struct rt_mutex *lock);
+
+#define spin_lock(lock)			rt_spin_lock(lock)
+
+#define spin_lock_bh(lock)			\
+	do {					\
+		local_bh_disable();		\
+		rt_spin_lock(lock);		\
+	} while (0)
+
+#define spin_lock_irq(lock)		spin_lock(lock)
+
+#define spin_do_trylock(lock)		__cond_lock(lock, rt_spin_trylock(lock))
+
+#define spin_trylock(lock)			\
+({						\
+	int __locked;				\
+	__locked = spin_do_trylock(lock);	\
+	__locked;				\
+})
+
+#ifdef CONFIG_LOCKDEP
+# define spin_lock_nested(lock, subclass)		\
+	do {						\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+#define spin_lock_bh_nested(lock, subclass)		\
+	do {						\
+		local_bh_disable();			\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+# define spin_lock_nest_lock(lock, subclass)		\
+	do {                                                           \
+		typecheck(struct lockdep_map *, &(subclass)->dep_map);	\
+		rt_spin_lock_nest_lock(lock, &(subclass)->dep_map);	\
+	} while (0)
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		rt_spin_lock_nested(lock, subclass);	 \
+	} while (0)
+#else
+# define spin_lock_nested(lock, subclass)	spin_lock(((void)(subclass), (lock)))
+# define spin_lock_nest_lock(lock, subclass)	spin_lock(((void)(subclass), (lock)))
+# define spin_lock_bh_nested(lock, subclass)	spin_lock_bh(((void)(subclass), (lock)))
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(((void)(subclass), (lock)));	 \
+	} while (0)
+#endif
+
+#define spin_lock_irqsave(lock, flags)			 \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(lock);			 \
+	} while (0)
+
+#define spin_unlock(lock)			rt_spin_unlock(lock)
+
+#define spin_unlock_bh(lock)				\
+	do {						\
+		rt_spin_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define spin_unlock_irq(lock)		spin_unlock(lock)
+
+#define spin_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		spin_unlock(lock);			\
+	} while (0)
+
+#define spin_trylock_bh(lock)	__cond_lock(lock, rt_spin_trylock_bh(lock))
+#define spin_trylock_irq(lock)	spin_trylock(lock)
+
+#define spin_trylock_irqsave(lock, flags)		\
+({							\
+	int __locked;					\
+							\
+	typecheck(unsigned long, flags);		\
+	flags = 0;					\
+	__locked = spin_trylock(lock);			\
+	__locked;					\
+})
+
+#ifdef CONFIG_GENERIC_LOCKBREAK
+# define spin_is_contended(lock)	((lock)->break_lock)
+#else
+# define spin_is_contended(lock)	(((void)(lock), 0))
+#endif
+
+static inline int spin_can_lock(spinlock_t *lock)
+{
+	return !rt_mutex_is_locked(&lock->lock);
+}
+
+static inline int spin_is_locked(spinlock_t *lock)
+{
+	return rt_mutex_is_locked(&lock->lock);
+}
+
+static inline void assert_spin_locked(spinlock_t *lock)
+{
+	BUG_ON(!spin_is_locked(lock));
+}
+
+#endif
diff -Naur a/include/linux/spinlock_types.h b/include/linux/spinlock_types.h
--- a/include/linux/spinlock_types.h	2020-11-23 13:48:33.009906114 +0200
+++ b/include/linux/spinlock_types.h	2021-07-14 15:39:09.846149318 +0300
@@ -9,93 +9,15 @@
  * Released under the General Public License (GPL).
  */
 
-#if defined(CONFIG_SMP)
-# include <asm/spinlock_types.h>
-#else
-# include <linux/spinlock_types_up.h>
-#endif
-
-#include <linux/lockdep_types.h>
-
-typedef struct raw_spinlock {
-	arch_spinlock_t raw_lock;
-#ifdef CONFIG_DEBUG_SPINLOCK
-	unsigned int magic, owner_cpu;
-	void *owner;
-#endif
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	struct lockdep_map dep_map;
-#endif
-} raw_spinlock_t;
-
-#define SPINLOCK_MAGIC		0xdead4ead
-
-#define SPINLOCK_OWNER_INIT	((void *)-1L)
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define RAW_SPIN_DEP_MAP_INIT(lockname)		\
-	.dep_map = {					\
-		.name = #lockname,			\
-		.wait_type_inner = LD_WAIT_SPIN,	\
-	}
-# define SPIN_DEP_MAP_INIT(lockname)			\
-	.dep_map = {					\
-		.name = #lockname,			\
-		.wait_type_inner = LD_WAIT_CONFIG,	\
-	}
-#else
-# define RAW_SPIN_DEP_MAP_INIT(lockname)
-# define SPIN_DEP_MAP_INIT(lockname)
-#endif
+#include <linux/spinlock_types_raw.h>
 
-#ifdef CONFIG_DEBUG_SPINLOCK
-# define SPIN_DEBUG_INIT(lockname)		\
-	.magic = SPINLOCK_MAGIC,		\
-	.owner_cpu = -1,			\
-	.owner = SPINLOCK_OWNER_INIT,
+#ifndef CONFIG_PREEMPT_RT
+# include <linux/spinlock_types_nort.h>
+# include <linux/rwlock_types.h>
 #else
-# define SPIN_DEBUG_INIT(lockname)
+# include <linux/rtmutex.h>
+# include <linux/spinlock_types_rt.h>
+# include <linux/rwlock_types_rt.h>
 #endif
 
-#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
-	{					\
-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
-	SPIN_DEBUG_INIT(lockname)		\
-	RAW_SPIN_DEP_MAP_INIT(lockname) }
-
-#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
-	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
-
-typedef struct spinlock {
-	union {
-		struct raw_spinlock rlock;
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
-		struct {
-			u8 __padding[LOCK_PADSIZE];
-			struct lockdep_map dep_map;
-		};
-#endif
-	};
-} spinlock_t;
-
-#define ___SPIN_LOCK_INITIALIZER(lockname)	\
-	{					\
-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
-	SPIN_DEBUG_INIT(lockname)		\
-	SPIN_DEP_MAP_INIT(lockname) }
-
-#define __SPIN_LOCK_INITIALIZER(lockname) \
-	{ { .rlock = ___SPIN_LOCK_INITIALIZER(lockname) } }
-
-#define __SPIN_LOCK_UNLOCKED(lockname) \
-	(spinlock_t) __SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
-
-#include <linux/rwlock_types.h>
-
 #endif /* __LINUX_SPINLOCK_TYPES_H */
diff -Naur a/include/linux/spinlock_types_nort.h b/include/linux/spinlock_types_nort.h
--- a/include/linux/spinlock_types_nort.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/spinlock_types_nort.h	2021-07-14 15:39:09.846149318 +0300
@@ -0,0 +1,39 @@
+#ifndef __LINUX_SPINLOCK_TYPES_NORT_H
+#define __LINUX_SPINLOCK_TYPES_NORT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+/*
+ * The non RT version maps spinlocks to raw_spinlocks
+ */
+typedef struct spinlock {
+	union {
+		struct raw_spinlock rlock;
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
+		struct {
+			u8 __padding[LOCK_PADSIZE];
+			struct lockdep_map dep_map;
+		};
+#endif
+	};
+} spinlock_t;
+
+#define ___SPIN_LOCK_INITIALIZER(lockname)	\
+{						\
+	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+	SPIN_DEBUG_INIT(lockname)		\
+	SPIN_DEP_MAP_INIT(lockname) }
+
+#define __SPIN_LOCK_INITIALIZER(lockname) \
+	{ { .rlock = ___SPIN_LOCK_INITIALIZER(lockname) } }
+
+#define __SPIN_LOCK_UNLOCKED(lockname) \
+	(spinlock_t) __SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff -Naur a/include/linux/spinlock_types_raw.h b/include/linux/spinlock_types_raw.h
--- a/include/linux/spinlock_types_raw.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/spinlock_types_raw.h	2021-07-14 15:39:09.846149318 +0300
@@ -0,0 +1,65 @@
+#ifndef __LINUX_SPINLOCK_TYPES_RAW_H
+#define __LINUX_SPINLOCK_TYPES_RAW_H
+
+#include <linux/types.h>
+
+#if defined(CONFIG_SMP)
+# include <asm/spinlock_types.h>
+#else
+# include <linux/spinlock_types_up.h>
+#endif
+
+#include <linux/lockdep_types.h>
+
+typedef struct raw_spinlock {
+	arch_spinlock_t raw_lock;
+#ifdef CONFIG_DEBUG_SPINLOCK
+	unsigned int magic, owner_cpu;
+	void *owner;
+#endif
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
+} raw_spinlock_t;
+
+#define SPINLOCK_MAGIC		0xdead4ead
+
+#define SPINLOCK_OWNER_INIT	((void *)-1L)
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define RAW_SPIN_DEP_MAP_INIT(lockname)		\
+	.dep_map = {					\
+		.name = #lockname,			\
+		.wait_type_inner = LD_WAIT_SPIN,	\
+	}
+# define SPIN_DEP_MAP_INIT(lockname)			\
+	.dep_map = {					\
+		.name = #lockname,			\
+		.wait_type_inner = LD_WAIT_CONFIG,	\
+	}
+#else
+# define RAW_SPIN_DEP_MAP_INIT(lockname)
+# define SPIN_DEP_MAP_INIT(lockname)
+#endif
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+# define SPIN_DEBUG_INIT(lockname)		\
+	.magic = SPINLOCK_MAGIC,		\
+	.owner_cpu = -1,			\
+	.owner = SPINLOCK_OWNER_INIT,
+#else
+# define SPIN_DEBUG_INIT(lockname)
+#endif
+
+#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
+{						\
+	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+	SPIN_DEBUG_INIT(lockname)		\
+	RAW_SPIN_DEP_MAP_INIT(lockname) }
+
+#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
+	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_RAW_SPINLOCK(x)  raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff -Naur a/include/linux/spinlock_types_rt.h b/include/linux/spinlock_types_rt.h
--- a/include/linux/spinlock_types_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/linux/spinlock_types_rt.h	2021-07-14 15:39:09.846149318 +0300
@@ -0,0 +1,38 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#ifndef __LINUX_SPINLOCK_TYPES_RT_H
+#define __LINUX_SPINLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#include <linux/cache.h>
+
+/*
+ * PREEMPT_RT: spinlocks - an RT mutex plus lock-break field:
+ */
+typedef struct spinlock {
+	struct rt_mutex		lock;
+	unsigned int		break_lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+} spinlock_t;
+
+#define __RT_SPIN_INITIALIZER(name) \
+	{ \
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock), \
+	.save_state = 1, \
+	}
+/*
+.wait_list = PLIST_HEAD_INIT_RAW((name).lock.wait_list, (name).lock.wait_lock)
+*/
+
+#define __SPIN_LOCK_UNLOCKED(name)			\
+	{ .lock = __RT_SPIN_INITIALIZER(name.lock),		\
+	  SPIN_DEP_MAP_INIT(name) }
+
+#define DEFINE_SPINLOCK(name) \
+	spinlock_t name = __SPIN_LOCK_UNLOCKED(name)
+
+#endif
diff -Naur a/include/linux/spinlock_types_up.h b/include/linux/spinlock_types_up.h
--- a/include/linux/spinlock_types_up.h	2020-11-23 13:48:33.009906114 +0200
+++ b/include/linux/spinlock_types_up.h	2021-07-14 15:39:09.846149318 +0300
@@ -1,10 +1,6 @@
 #ifndef __LINUX_SPINLOCK_TYPES_UP_H
 #define __LINUX_SPINLOCK_TYPES_UP_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 /*
  * include/linux/spinlock_types_up.h - spinlock type definitions for UP
  *
diff -Naur a/include/linux/stop_machine.h b/include/linux/stop_machine.h
--- a/include/linux/stop_machine.h	2020-11-23 13:48:33.017906275 +0200
+++ b/include/linux/stop_machine.h	2021-07-14 15:39:09.846149318 +0300
@@ -24,6 +24,7 @@
 struct cpu_stop_work {
 	struct list_head	list;		/* cpu_stopper->works */
 	cpu_stop_fn_t		fn;
+	unsigned long		caller;
 	void			*arg;
 	struct cpu_stop_done	*done;
 };
@@ -36,6 +37,8 @@
 void stop_machine_unpark(int cpu);
 void stop_machine_yield(const struct cpumask *cpumask);
 
+extern void print_stop_info(const char *log_lvl, struct task_struct *task);
+
 #else	/* CONFIG_SMP */
 
 #include <linux/workqueue.h>
@@ -80,6 +83,8 @@
 	return false;
 }
 
+static inline void print_stop_info(const char *log_lvl, struct task_struct *task) { }
+
 #endif	/* CONFIG_SMP */
 
 /*
diff -Naur a/include/linux/thread_info.h b/include/linux/thread_info.h
--- a/include/linux/thread_info.h	2020-11-23 13:48:33.049906917 +0200
+++ b/include/linux/thread_info.h	2021-07-14 15:39:09.846149318 +0300
@@ -97,7 +97,17 @@
 #define test_thread_flag(flag) \
 	test_ti_thread_flag(current_thread_info(), flag)
 
-#define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED)
+#ifdef CONFIG_PREEMPT_LAZY
+#define tif_need_resched()	(test_thread_flag(TIF_NEED_RESCHED) || \
+				 test_thread_flag(TIF_NEED_RESCHED_LAZY))
+#define tif_need_resched_now()	(test_thread_flag(TIF_NEED_RESCHED))
+#define tif_need_resched_lazy()	test_thread_flag(TIF_NEED_RESCHED_LAZY))
+
+#else
+#define tif_need_resched()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_now()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_lazy()	0
+#endif
 
 #ifndef CONFIG_HAVE_ARCH_WITHIN_STACK_FRAMES
 static inline int arch_within_stack_frames(const void * const stack,
diff -Naur a/include/linux/trace_events.h b/include/linux/trace_events.h
--- a/include/linux/trace_events.h	2020-11-23 13:48:33.065907237 +0200
+++ b/include/linux/trace_events.h	2021-07-14 15:39:09.846149318 +0300
@@ -67,6 +67,8 @@
 	unsigned char		flags;
 	unsigned char		preempt_count;
 	int			pid;
+	unsigned char		migrate_disable;
+	unsigned char		preempt_lazy_count;
 };
 
 #define TRACE_EVENT_TYPE_MAX						\
diff -Naur a/include/linux/u64_stats_sync.h b/include/linux/u64_stats_sync.h
--- a/include/linux/u64_stats_sync.h	2020-11-23 13:48:33.073907398 +0200
+++ b/include/linux/u64_stats_sync.h	2021-07-14 15:39:09.846149318 +0300
@@ -66,7 +66,7 @@
 #include <linux/seqlock.h>
 
 struct u64_stats_sync {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG==32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	seqcount_t	seq;
 #endif
 };
@@ -117,22 +117,26 @@
 
 static inline void u64_stats_init(struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	seqcount_init(&syncp->seq);
 #endif
 }
 
 static inline void u64_stats_update_begin(struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_disable();
 	write_seqcount_begin(&syncp->seq);
 #endif
 }
 
 static inline void u64_stats_update_end(struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	write_seqcount_end(&syncp->seq);
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_enable();
 #endif
 }
 
@@ -141,8 +145,11 @@
 {
 	unsigned long flags = 0;
 
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
-	local_irq_save(flags);
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_disable();
+	else
+		local_irq_save(flags);
 	write_seqcount_begin(&syncp->seq);
 #endif
 	return flags;
@@ -152,15 +159,18 @@
 u64_stats_update_end_irqrestore(struct u64_stats_sync *syncp,
 				unsigned long flags)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	write_seqcount_end(&syncp->seq);
-	local_irq_restore(flags);
+	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+		preempt_enable();
+	else
+		local_irq_restore(flags);
 #endif
 }
 
 static inline unsigned int __u64_stats_fetch_begin(const struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	return read_seqcount_begin(&syncp->seq);
 #else
 	return 0;
@@ -169,7 +179,7 @@
 
 static inline unsigned int u64_stats_fetch_begin(const struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (!defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT_RT))
 	preempt_disable();
 #endif
 	return __u64_stats_fetch_begin(syncp);
@@ -178,7 +188,7 @@
 static inline bool __u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
 					 unsigned int start)
 {
-#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT))
 	return read_seqcount_retry(&syncp->seq, start);
 #else
 	return false;
@@ -188,7 +198,7 @@
 static inline bool u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
 					 unsigned int start)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && (!defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT_RT))
 	preempt_enable();
 #endif
 	return __u64_stats_fetch_retry(syncp, start);
@@ -202,7 +212,9 @@
  */
 static inline unsigned int u64_stats_fetch_begin_irq(const struct u64_stats_sync *syncp)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && defined(CONFIG_PREEMPT_RT)
+	preempt_disable();
+#elif BITS_PER_LONG == 32 && !defined(CONFIG_SMP)
 	local_irq_disable();
 #endif
 	return __u64_stats_fetch_begin(syncp);
@@ -211,7 +223,9 @@
 static inline bool u64_stats_fetch_retry_irq(const struct u64_stats_sync *syncp,
 					     unsigned int start)
 {
-#if BITS_PER_LONG==32 && !defined(CONFIG_SMP)
+#if BITS_PER_LONG == 32 && defined(CONFIG_PREEMPT_RT)
+	preempt_enable();
+#elif BITS_PER_LONG == 32 && !defined(CONFIG_SMP)
 	local_irq_enable();
 #endif
 	return __u64_stats_fetch_retry(syncp, start);
diff -Naur a/include/linux/vmstat.h b/include/linux/vmstat.h
--- a/include/linux/vmstat.h	2020-11-23 13:48:33.097907879 +0200
+++ b/include/linux/vmstat.h	2021-07-14 15:39:09.846149318 +0300
@@ -63,7 +63,9 @@
  */
 static inline void __count_vm_event(enum vm_event_item item)
 {
+	preempt_disable_rt();
 	raw_cpu_inc(vm_event_states.event[item]);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_event(enum vm_event_item item)
@@ -73,7 +75,9 @@
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
+	preempt_disable_rt();
 	raw_cpu_add(vm_event_states.event[item], delta);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
diff -Naur a/include/linux/wait.h b/include/linux/wait.h
--- a/include/linux/wait.h	2020-11-23 13:48:33.101907960 +0200
+++ b/include/linux/wait.h	2020-11-23 13:50:10.403439585 +0200
@@ -1,4 +1,9 @@
-/* SPDX-License-Identifier: GPL-2.0 */
+/* SPDX-License-Identifier: GPL-2.0
+ *	2020/10/16
+ *		Laurentiu-Cristian Duca (laurentiu [dot] duca [at] gmail [dot] com)
+ *		Added RTnet select() and poll() system calls helpers.
+ */
+
 #ifndef _LINUX_WAIT_H
 #define _LINUX_WAIT_H
 /*
@@ -10,6 +15,7 @@
 
 #include <asm/current.h>
 #include <uapi/linux/wait.h>
+#include <linux/atomic.h>
 
 typedef struct wait_queue_entry wait_queue_entry_t;
 
@@ -33,6 +39,12 @@
 	struct list_head	entry;
 };
 
+struct wait_queue_head_rtnet {
+	raw_spinlock_t lock;
+	struct list_head	head;
+};
+typedef struct wait_queue_head_rtnet wait_queue_head_rtnet_t;
+
 struct wait_queue_head {
 	spinlock_t		lock;
 	struct list_head	head;
@@ -60,8 +72,20 @@
 #define DECLARE_WAIT_QUEUE_HEAD(name) \
 	struct wait_queue_head name = __WAIT_QUEUE_HEAD_INITIALIZER(name)
 
+#define __WAIT_QUEUE_HEAD_RTNET_INITIALIZER(name) {					\
+	.raw_lock	= __RAW_SPIN_LOCK_UNLOCKED(name.raw_lock)	\
+	.head		= { &(name).head, &(name).head } }
+
+extern void __init_waitqueue_head_rtnet(struct wait_queue_head_rtnet *wq_head, const char *name, struct lock_class_key *);
 extern void __init_waitqueue_head(struct wait_queue_head *wq_head, const char *name, struct lock_class_key *);
 
+#define init_waitqueue_head_rtnet(wq_head)						\
+	do {									\
+		static struct lock_class_key __key;				\
+										\
+		__init_waitqueue_head_rtnet((wq_head), #wq_head, &__key);		\
+	} while (0)
+
 #define init_waitqueue_head(wq_head)						\
 	do {									\
 		static struct lock_class_key __key;				\
@@ -123,6 +147,11 @@
  * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
  * which (when the lock is uncontended) are of roughly equal cost.
  */
+static inline int waitqueue_active_rtnet(struct wait_queue_head_rtnet *wq_head)
+{
+	return !list_empty(&wq_head->head);
+}
+
 static inline int waitqueue_active(struct wait_queue_head *wq_head)
 {
 	return !list_empty(&wq_head->head);
@@ -149,6 +178,19 @@
  *
  * Please refer to the comment for waitqueue_active.
  */
+static inline bool wq_has_sleeper_rtnet(struct wait_queue_head_rtnet *wq_head)
+{
+	/*
+	 * We need to be sure we are in sync with the
+	 * add_wait_queue modifications to the wait queue.
+	 *
+	 * This memory barrier should be paired with one on the
+	 * waiting side.
+	 */
+	smp_mb();
+	return waitqueue_active_rtnet(wq_head);
+}
+
 static inline bool wq_has_sleeper(struct wait_queue_head *wq_head)
 {
 	/*
@@ -162,9 +204,17 @@
 	return waitqueue_active(wq_head);
 }
 
+extern void add_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry);
 extern void add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
 extern void add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
 extern void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
+extern void remove_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry);
+
+static inline void __add_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry)
+{
+	list_add(&wq_entry->entry, &wq_head->head);
+}
+
 
 static inline void __add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
 {
@@ -194,6 +244,12 @@
 }
 
 static inline void
+__remove_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry)
+{
+	list_del(&wq_entry->entry);
+}
+
+static inline void
 __remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
 {
 	list_del(&wq_entry->entry);
@@ -203,6 +259,7 @@
 void __wake_up_locked_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
 void __wake_up_locked_key_bookmark(struct wait_queue_head *wq_head,
 		unsigned int mode, void *key, wait_queue_entry_t *bookmark);
+void __wake_up_sync_key_rtnet(struct wait_queue_head_rtnet *wq_head, unsigned int mode, void *key);
 void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
 void __wake_up_locked_sync_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
 void __wake_up_locked(struct wait_queue_head *wq_head, unsigned int mode, int nr);
@@ -232,6 +289,8 @@
 	__wake_up(x, TASK_INTERRUPTIBLE, 1, poll_to_key(m))
 #define wake_up_interruptible_sync_poll(x, m)					\
 	__wake_up_sync_key((x), TASK_INTERRUPTIBLE, poll_to_key(m))
+#define wake_up_interruptible_sync_poll_rtnet(x, m)					\
+	__wake_up_sync_key_rtnet((x), TASK_INTERRUPTIBLE, poll_to_key(m))
 #define wake_up_interruptible_sync_poll_locked(x, m)				\
 	__wake_up_locked_sync_key((x), TASK_INTERRUPTIBLE, poll_to_key(m))
 
diff -Naur a/include/linux/ww_mutex.h b/include/linux/ww_mutex.h
--- a/include/linux/ww_mutex.h	2020-11-23 13:48:33.109908120 +0200
+++ b/include/linux/ww_mutex.h	2021-07-14 15:39:09.846149318 +0300
@@ -28,6 +28,14 @@
 	unsigned int is_wait_die;
 };
 
+struct ww_mutex {
+	struct mutex base;
+	struct ww_acquire_ctx *ctx;
+#ifdef CONFIG_DEBUG_MUTEXES
+	struct ww_class *ww_class;
+#endif
+};
+
 struct ww_acquire_ctx {
 	struct task_struct *task;
 	unsigned long stamp;
diff -Naur a/include/net/gen_stats.h b/include/net/gen_stats.h
--- a/include/net/gen_stats.h	2020-11-23 13:48:34.029926576 +0200
+++ b/include/net/gen_stats.h	2021-07-14 15:39:10.286146218 +0300
@@ -6,6 +6,7 @@
 #include <linux/socket.h>
 #include <linux/rtnetlink.h>
 #include <linux/pkt_sched.h>
+#include <net/net_seq_lock.h>
 
 /* Note: this used to be in include/uapi/linux/gen_stats.h */
 struct gnet_stats_basic_packed {
@@ -42,15 +43,15 @@
 				 spinlock_t *lock, struct gnet_dump *d,
 				 int padattr);
 
-int gnet_stats_copy_basic(const seqcount_t *running,
+int gnet_stats_copy_basic(net_seqlock_t *running,
 			  struct gnet_dump *d,
 			  struct gnet_stats_basic_cpu __percpu *cpu,
 			  struct gnet_stats_basic_packed *b);
-void __gnet_stats_copy_basic(const seqcount_t *running,
+void __gnet_stats_copy_basic(net_seqlock_t *running,
 			     struct gnet_stats_basic_packed *bstats,
 			     struct gnet_stats_basic_cpu __percpu *cpu,
 			     struct gnet_stats_basic_packed *b);
-int gnet_stats_copy_basic_hw(const seqcount_t *running,
+int gnet_stats_copy_basic_hw(net_seqlock_t *running,
 			     struct gnet_dump *d,
 			     struct gnet_stats_basic_cpu __percpu *cpu,
 			     struct gnet_stats_basic_packed *b);
@@ -70,13 +71,13 @@
 		      struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 		      struct net_rate_estimator __rcu **rate_est,
 		      spinlock_t *lock,
-		      seqcount_t *running, struct nlattr *opt);
+		      net_seqlock_t *running, struct nlattr *opt);
 void gen_kill_estimator(struct net_rate_estimator __rcu **ptr);
 int gen_replace_estimator(struct gnet_stats_basic_packed *bstats,
 			  struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 			  struct net_rate_estimator __rcu **ptr,
 			  spinlock_t *lock,
-			  seqcount_t *running, struct nlattr *opt);
+			  net_seqlock_t *running, struct nlattr *opt);
 bool gen_estimator_active(struct net_rate_estimator __rcu **ptr);
 bool gen_estimator_read(struct net_rate_estimator __rcu **ptr,
 			struct gnet_stats_rate_est64 *sample);
diff -Naur a/include/net/net_seq_lock.h b/include/net/net_seq_lock.h
--- a/include/net/net_seq_lock.h	1970-01-01 02:00:00.000000000 +0200
+++ b/include/net/net_seq_lock.h	2021-07-14 15:39:10.290146190 +0300
@@ -0,0 +1,15 @@
+#ifndef __NET_NET_SEQ_LOCK_H__
+#define __NET_NET_SEQ_LOCK_H__
+
+#ifdef CONFIG_PREEMPT_RT
+# define net_seqlock_t			seqlock_t
+# define net_seq_begin(__r)		read_seqbegin(__r)
+# define net_seq_retry(__r, __s)	read_seqretry(__r, __s)
+
+#else
+# define net_seqlock_t			seqcount_t
+# define net_seq_begin(__r)		read_seqcount_begin(__r)
+# define net_seq_retry(__r, __s)	read_seqcount_retry(__r, __s)
+#endif
+
+#endif
diff -Naur a/include/net/sch_generic.h b/include/net/sch_generic.h
--- a/include/net/sch_generic.h	2020-11-23 13:48:34.117928342 +0200
+++ b/include/net/sch_generic.h	2021-07-14 15:39:10.290146190 +0300
@@ -10,6 +10,7 @@
 #include <linux/percpu.h>
 #include <linux/dynamic_queue_limits.h>
 #include <linux/list.h>
+#include <net/net_seq_lock.h>
 #include <linux/refcount.h>
 #include <linux/workqueue.h>
 #include <linux/mutex.h>
@@ -100,7 +101,7 @@
 	struct sk_buff_head	gso_skb ____cacheline_aligned_in_smp;
 	struct qdisc_skb_head	q;
 	struct gnet_stats_basic_packed bstats;
-	seqcount_t		running;
+	net_seqlock_t		running;
 	struct gnet_stats_queue	qstats;
 	unsigned long		state;
 	struct Qdisc            *next_sched;
@@ -138,7 +139,11 @@
 {
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		return spin_is_locked(&qdisc->seqlock);
+#ifdef CONFIG_PREEMPT_RT
+	return spin_is_locked(&qdisc->running.lock) ? true : false;
+#else
 	return (raw_read_seqcount(&qdisc->running) & 1) ? true : false;
+#endif
 }
 
 static inline bool qdisc_is_percpu_stats(const struct Qdisc *q)
@@ -162,17 +167,35 @@
 	} else if (qdisc_is_running(qdisc)) {
 		return false;
 	}
+#ifdef CONFIG_PREEMPT_RT
+	if (spin_trylock(&qdisc->running.lock)) {
+		seqcount_t *s = &qdisc->running.seqcount.seqcount;
+		/*
+		 * Variant of write_seqcount_t_begin() telling lockdep that a
+		 * trylock was attempted.
+		 */
+		raw_write_seqcount_t_begin(s);
+		seqcount_acquire(&s->dep_map, 0, 1, _RET_IP_);
+		return true;
+	}
+	return false;
+#else
 	/* Variant of write_seqcount_begin() telling lockdep a trylock
 	 * was attempted.
 	 */
 	raw_write_seqcount_begin(&qdisc->running);
 	seqcount_acquire(&qdisc->running.dep_map, 0, 1, _RET_IP_);
 	return true;
+#endif
 }
 
 static inline void qdisc_run_end(struct Qdisc *qdisc)
 {
+#ifdef CONFIG_PREEMPT_RT
+	write_sequnlock(&qdisc->running);
+#else
 	write_seqcount_end(&qdisc->running);
+#endif
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		spin_unlock(&qdisc->seqlock);
 }
@@ -547,7 +570,7 @@
 	return qdisc_lock(root);
 }
 
-static inline seqcount_t *qdisc_root_sleeping_running(const struct Qdisc *qdisc)
+static inline net_seqlock_t *qdisc_root_sleeping_running(const struct Qdisc *qdisc)
 {
 	struct Qdisc *root = qdisc_root_sleeping(qdisc);
 
diff -Naur a/include/trace/events/sched.h b/include/trace/events/sched.h
--- a/include/trace/events/sched.h	2020-11-23 13:48:34.361933238 +0200
+++ b/include/trace/events/sched.h	2021-07-14 15:39:10.426145232 +0300
@@ -646,6 +646,18 @@
 	TP_PROTO(struct rq *rq, int change),
 	TP_ARGS(rq, change));
 
+DECLARE_TRACE(sched_migrate_disable_tp,
+	      TP_PROTO(struct task_struct *p),
+	      TP_ARGS(p));
+
+DECLARE_TRACE(sched_migrate_enable_tp,
+	      TP_PROTO(struct task_struct *p),
+	      TP_ARGS(p));
+
+DECLARE_TRACE(sched_migrate_pull_tp,
+	      TP_PROTO(struct task_struct *p),
+	      TP_ARGS(p));
+
 #endif /* _TRACE_SCHED_H */
 
 /* This part must be outside protection */
diff -Naur a/init/Kconfig b/init/Kconfig
--- a/init/Kconfig	2020-11-23 13:48:34.813942307 +0200
+++ b/init/Kconfig	2021-07-14 15:39:10.830142388 +0300
@@ -964,6 +964,7 @@
 config RT_GROUP_SCHED
 	bool "Group scheduling for SCHED_RR/FIFO"
 	depends on CGROUP_SCHED
+	depends on !PREEMPT_RT
 	default n
 	help
 	  This feature lets you explicitly allocate real CPU bandwidth
@@ -1871,6 +1872,7 @@
 
 config SLAB
 	bool "SLAB"
+	depends on !PREEMPT_RT
 	select HAVE_HARDENED_USERCOPY_ALLOCATOR
 	help
 	  The regular slab allocator that is established and known to work
@@ -1891,6 +1893,7 @@
 config SLOB
 	depends on EXPERT
 	bool "SLOB (Simple Allocator)"
+	depends on !PREEMPT_RT
 	help
 	   SLOB replaces the stock allocator with a drastically simpler
 	   allocator. SLOB is generally more space efficient but
@@ -1957,7 +1960,7 @@
 
 config SLUB_CPU_PARTIAL
 	default y
-	depends on SLUB && SMP
+	depends on SLUB && SMP && !PREEMPT_RT
 	bool "SLUB per cpu partial cache"
 	help
 	  Per cpu partial caches accelerate objects allocation and freeing
diff -Naur a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c
--- a/kernel/cgroup/cpuset.c	2020-11-23 13:48:35.021946481 +0200
+++ b/kernel/cgroup/cpuset.c	2021-07-14 15:39:11.002141176 +0300
@@ -345,7 +345,7 @@
 	percpu_up_read(&cpuset_rwsem);
 }
 
-static DEFINE_SPINLOCK(callback_lock);
+static DEFINE_RAW_SPINLOCK(callback_lock);
 
 static struct workqueue_struct *cpuset_migrate_mm_wq;
 
@@ -1257,7 +1257,7 @@
 	 * Newly added CPUs will be removed from effective_cpus and
 	 * newly deleted ones will be added back to effective_cpus.
 	 */
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	if (adding) {
 		cpumask_or(parent->subparts_cpus,
 			   parent->subparts_cpus, tmp->addmask);
@@ -1276,7 +1276,7 @@
 	}
 
 	parent->nr_subparts_cpus = cpumask_weight(parent->subparts_cpus);
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	return cmd == partcmd_update;
 }
@@ -1381,7 +1381,7 @@
 			continue;
 		rcu_read_unlock();
 
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 
 		cpumask_copy(cp->effective_cpus, tmp->new_cpus);
 		if (cp->nr_subparts_cpus &&
@@ -1412,7 +1412,7 @@
 					= cpumask_weight(cp->subparts_cpus);
 			}
 		}
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 
 		WARN_ON(!is_in_v2_mode() &&
 			!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));
@@ -1530,7 +1530,7 @@
 			return -EINVAL;
 	}
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);
 
 	/*
@@ -1541,7 +1541,7 @@
 			       cs->cpus_allowed);
 		cs->nr_subparts_cpus = cpumask_weight(cs->subparts_cpus);
 	}
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	update_cpumasks_hier(cs, &tmp);
 
@@ -1735,9 +1735,9 @@
 			continue;
 		rcu_read_unlock();
 
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		cp->effective_mems = *new_mems;
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 
 		WARN_ON(!is_in_v2_mode() &&
 			!nodes_equal(cp->mems_allowed, cp->effective_mems));
@@ -1805,9 +1805,9 @@
 	if (retval < 0)
 		goto done;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->mems_allowed = trialcs->mems_allowed;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	/* use trialcs->mems_allowed as a temp variable */
 	update_nodemasks_hier(cs, &trialcs->mems_allowed);
@@ -1898,9 +1898,9 @@
 	spread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))
 			|| (is_spread_page(cs) != is_spread_page(trialcs)));
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->flags = trialcs->flags;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)
 		rebuild_sched_domains_locked();
@@ -2409,7 +2409,7 @@
 	cpuset_filetype_t type = seq_cft(sf)->private;
 	int ret = 0;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 
 	switch (type) {
 	case FILE_CPULIST:
@@ -2431,7 +2431,7 @@
 		ret = -EINVAL;
 	}
 
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 	return ret;
 }
 
@@ -2744,14 +2744,14 @@
 
 	cpuset_inc();
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	if (is_in_v2_mode()) {
 		cpumask_copy(cs->effective_cpus, parent->effective_cpus);
 		cs->effective_mems = parent->effective_mems;
 		cs->use_parent_ecpus = true;
 		parent->child_ecpus_count++;
 	}
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))
 		goto out_unlock;
@@ -2778,12 +2778,12 @@
 	}
 	rcu_read_unlock();
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->mems_allowed = parent->mems_allowed;
 	cs->effective_mems = parent->mems_allowed;
 	cpumask_copy(cs->cpus_allowed, parent->cpus_allowed);
 	cpumask_copy(cs->effective_cpus, parent->cpus_allowed);
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 out_unlock:
 	percpu_up_write(&cpuset_rwsem);
 	put_online_cpus();
@@ -2839,7 +2839,7 @@
 static void cpuset_bind(struct cgroup_subsys_state *root_css)
 {
 	percpu_down_write(&cpuset_rwsem);
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 
 	if (is_in_v2_mode()) {
 		cpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);
@@ -2850,7 +2850,7 @@
 		top_cpuset.mems_allowed = top_cpuset.effective_mems;
 	}
 
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 	percpu_up_write(&cpuset_rwsem);
 }
 
@@ -2947,12 +2947,12 @@
 {
 	bool is_empty;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->cpus_allowed, new_cpus);
 	cpumask_copy(cs->effective_cpus, new_cpus);
 	cs->mems_allowed = *new_mems;
 	cs->effective_mems = *new_mems;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	/*
 	 * Don't call update_tasks_cpumask() if the cpuset becomes empty,
@@ -2989,10 +2989,10 @@
 	if (nodes_empty(*new_mems))
 		*new_mems = parent_cs(cs)->effective_mems;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->effective_cpus, new_cpus);
 	cs->effective_mems = *new_mems;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (cpus_updated)
 		update_tasks_cpumask(cs);
@@ -3147,7 +3147,7 @@
 
 	/* synchronize cpus_allowed to cpu_active_mask */
 	if (cpus_updated) {
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		if (!on_dfl)
 			cpumask_copy(top_cpuset.cpus_allowed, &new_cpus);
 		/*
@@ -3167,17 +3167,17 @@
 			}
 		}
 		cpumask_copy(top_cpuset.effective_cpus, &new_cpus);
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 		/* we don't mess with cpumasks of tasks in top_cpuset */
 	}
 
 	/* synchronize mems_allowed to N_MEMORY */
 	if (mems_updated) {
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		if (!on_dfl)
 			top_cpuset.mems_allowed = new_mems;
 		top_cpuset.effective_mems = new_mems;
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 		update_tasks_nodemask(&top_cpuset);
 	}
 
@@ -3278,11 +3278,11 @@
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 	rcu_read_lock();
 	guarantee_online_cpus(task_cs(tsk), pmask);
 	rcu_read_unlock();
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 }
 
 /**
@@ -3343,11 +3343,11 @@
 	nodemask_t mask;
 	unsigned long flags;
 
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 	rcu_read_lock();
 	guarantee_online_mems(task_cs(tsk), &mask);
 	rcu_read_unlock();
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 
 	return mask;
 }
@@ -3439,14 +3439,14 @@
 		return true;
 
 	/* Not hardwall and node outside mems_allowed: scan up cpusets */
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 
 	rcu_read_lock();
 	cs = nearest_hardwall_ancestor(task_cs(current));
 	allowed = node_isset(node, cs->mems_allowed);
 	rcu_read_unlock();
 
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 	return allowed;
 }
 
diff -Naur a/kernel/cgroup/rstat.c b/kernel/cgroup/rstat.c
--- a/kernel/cgroup/rstat.c	2020-11-23 13:48:35.025946562 +0200
+++ b/kernel/cgroup/rstat.c	2021-07-14 15:39:11.002141176 +0300
@@ -149,8 +149,9 @@
 		raw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock,
 						       cpu);
 		struct cgroup *pos = NULL;
+		unsigned long flags;
 
-		raw_spin_lock(cpu_lock);
+		raw_spin_lock_irqsave(cpu_lock, flags);
 		while ((pos = cgroup_rstat_cpu_pop_updated(pos, cgrp, cpu))) {
 			struct cgroup_subsys_state *css;
 
@@ -162,7 +163,7 @@
 				css->ss->css_rstat_flush(css, cpu);
 			rcu_read_unlock();
 		}
-		raw_spin_unlock(cpu_lock);
+		raw_spin_unlock_irqrestore(cpu_lock, flags);
 
 		/* if @may_sleep, play nice and yield if necessary */
 		if (may_sleep && (need_resched() ||
diff -Naur a/kernel/cpu.c b/kernel/cpu.c
--- a/kernel/cpu.c	2020-11-23 13:48:34.893943913 +0200
+++ b/kernel/cpu.c	2021-07-14 15:39:10.882142022 +0300
@@ -1602,7 +1602,7 @@
 		.name			= "ap:online",
 	},
 	/*
-	 * Handled on controll processor until the plugged processor manages
+	 * Handled on control processor until the plugged processor manages
 	 * this itself.
 	 */
 	[CPUHP_TEARDOWN_CPU] = {
@@ -1611,6 +1611,13 @@
 		.teardown.single	= takedown_cpu,
 		.cant_stop		= true,
 	},
+
+	[CPUHP_AP_SCHED_WAIT_EMPTY] = {
+		.name			= "sched:waitempty",
+		.startup.single		= NULL,
+		.teardown.single	= sched_cpu_wait_empty,
+	},
+
 	/* Handle smpboot threads park/unpark */
 	[CPUHP_AP_SMPBOOT_THREADS] = {
 		.name			= "smpboot/threads:online",
diff -Naur a/kernel/entry/common.c b/kernel/entry/common.c
--- a/kernel/entry/common.c	2020-11-23 13:48:35.045946963 +0200
+++ b/kernel/entry/common.c	2021-07-14 15:39:11.038140923 +0300
@@ -148,9 +148,17 @@
 
 		local_irq_enable_exit_to_user(ti_work);
 
-		if (ti_work & _TIF_NEED_RESCHED)
+		if (ti_work & _TIF_NEED_RESCHED_MASK)
 			schedule();
 
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+		if (unlikely(current->forced_info.si_signo)) {
+			struct task_struct *t = current;
+			force_sig_info(&t->forced_info);
+			t->forced_info.si_signo = 0;
+		}
+#endif
+
 		if (ti_work & _TIF_UPROBE)
 			uprobe_notify_resume(regs);
 
@@ -354,7 +362,7 @@
 		rcu_irq_exit_check_preempt();
 		if (IS_ENABLED(CONFIG_DEBUG_ENTRY))
 			WARN_ON_ONCE(!on_thread_stack());
-		if (need_resched())
+		if (should_resched(0))
 			preempt_schedule_irq();
 	}
 }
diff -Naur a/kernel/exit.c b/kernel/exit.c
--- a/kernel/exit.c	2020-11-23 13:48:34.897943993 +0200
+++ b/kernel/exit.c	2021-07-14 15:39:10.886141993 +0300
@@ -151,7 +151,7 @@
 	 * Do this under ->siglock, we can race with another thread
 	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
 	 */
-	flush_sigqueue(&tsk->pending);
+	flush_task_sigqueue(tsk);
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 
diff -Naur a/kernel/fork.c b/kernel/fork.c
--- a/kernel/fork.c	2020-11-23 13:48:34.901944073 +0200
+++ b/kernel/fork.c	2021-07-14 15:39:10.886141993 +0300
@@ -42,6 +42,7 @@
 #include <linux/mmu_notifier.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
+#include <linux/kprobes.h>
 #include <linux/vmacache.h>
 #include <linux/nsproxy.h>
 #include <linux/capability.h>
@@ -287,7 +288,7 @@
 			return;
 		}
 
-		vfree_atomic(tsk->stack);
+		vfree(tsk->stack);
 		return;
 	}
 #endif
@@ -687,6 +688,19 @@
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * RCU callback for delayed mm drop. Not strictly rcu, but we don't
+ * want another facility to make this work.
+ */
+void __mmdrop_delayed(struct rcu_head *rhp)
+{
+	struct mm_struct *mm = container_of(rhp, struct mm_struct, delayed_drop);
+
+	__mmdrop(mm);
+}
+#endif
+
 static void mmdrop_async_fn(struct work_struct *work)
 {
 	struct mm_struct *mm;
@@ -728,6 +742,15 @@
 	WARN_ON(refcount_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	/*
+	 * Remove function-return probe instances associated with this
+	 * task and put them back on the free list.
+	 */
+	kprobe_flush_task(tsk);
+
+	/* Task is done with its stack. */
+	put_task_stack(tsk);
+
 	cgroup_free(tsk);
 	task_numa_free(tsk, true);
 	security_task_free(tsk);
@@ -924,6 +947,7 @@
 	tsk->splice_pipe = NULL;
 	tsk->task_frag.page = NULL;
 	tsk->wake_q.next = NULL;
+	tsk->wake_q_sleeper.next = NULL;
 
 	account_kernel_stack(tsk, 1);
 
@@ -1970,6 +1994,7 @@
 	spin_lock_init(&p->alloc_lock);
 
 	init_sigpending(&p->pending);
+	p->sigqueue_cache = NULL;
 
 	p->utime = p->stime = p->gtime = 0;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
diff -Naur a/kernel/futex.c b/kernel/futex.c
--- a/kernel/futex.c	2020-11-23 13:48:34.905944153 +0200
+++ b/kernel/futex.c	2021-07-14 15:39:10.886141993 +0300
@@ -1479,6 +1479,7 @@
 	struct task_struct *new_owner;
 	bool postunlock = false;
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 	int ret = 0;
 
 	new_owner = rt_mutex_next_owner(&pi_state->pi_mutex);
@@ -1538,13 +1539,13 @@
 	pi_state->owner = new_owner;
 	raw_spin_unlock(&new_owner->pi_lock);
 
-	postunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q);
-
+	postunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q,
+					     &wake_sleeper_q);
 out_unlock:
 	raw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);
 
 	if (postunlock)
-		rt_mutex_postunlock(&wake_q);
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 
 	return ret;
 }
@@ -2145,6 +2146,16 @@
 				 */
 				requeue_pi_wake_futex(this, &key2, hb2);
 				continue;
+			} else if (ret == -EAGAIN) {
+				/*
+				 * Waiter was woken by timeout or
+				 * signal and has set pi_blocked_on to
+				 * PI_WAKEUP_INPROGRESS before we
+				 * tried to enqueue it on the rtmutex.
+				 */
+				this->pi_state = NULL;
+				put_pi_state(pi_state);
+				continue;
 			} else if (ret) {
 				/*
 				 * rt_mutex_start_proxy_lock() detected a
@@ -2830,7 +2841,7 @@
 		goto no_block;
 	}
 
-	rt_mutex_init_waiter(&rt_waiter);
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	/*
 	 * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not
@@ -3171,7 +3182,7 @@
 	struct hrtimer_sleeper timeout, *to;
 	struct futex_pi_state *pi_state = NULL;
 	struct rt_mutex_waiter rt_waiter;
-	struct futex_hash_bucket *hb;
+	struct futex_hash_bucket *hb, *hb2;
 	union futex_key key2 = FUTEX_KEY_INIT;
 	struct futex_q q = futex_q_init;
 	int res, ret;
@@ -3192,7 +3203,7 @@
 	 * The waiter is allocated on our stack, manipulated by the requeue
 	 * code while we sleep on uaddr.
 	 */
-	rt_mutex_init_waiter(&rt_waiter);
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);
 	if (unlikely(ret != 0))
@@ -3223,20 +3234,55 @@
 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
 	futex_wait_queue_me(hb, &q, to);
 
-	spin_lock(&hb->lock);
-	ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
-	spin_unlock(&hb->lock);
-	if (ret)
-		goto out;
+	/*
+	 * On RT we must avoid races with requeue and trying to block
+	 * on two mutexes (hb->lock and uaddr2's rtmutex) by
+	 * serializing access to pi_blocked_on with pi_lock.
+	 */
+	raw_spin_lock_irq(&current->pi_lock);
+	if (current->pi_blocked_on) {
+		/*
+		 * We have been requeued or are in the process of
+		 * being requeued.
+		 */
+		raw_spin_unlock_irq(&current->pi_lock);
+	} else {
+		/*
+		 * Setting pi_blocked_on to PI_WAKEUP_INPROGRESS
+		 * prevents a concurrent requeue from moving us to the
+		 * uaddr2 rtmutex. After that we can safely acquire
+		 * (and possibly block on) hb->lock.
+		 */
+		current->pi_blocked_on = PI_WAKEUP_INPROGRESS;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		spin_lock(&hb->lock);
+
+		/*
+		 * Clean up pi_blocked_on. We might leak it otherwise
+		 * when we succeeded with the hb->lock in the fast
+		 * path.
+		 */
+		raw_spin_lock_irq(&current->pi_lock);
+		current->pi_blocked_on = NULL;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
+		spin_unlock(&hb->lock);
+		if (ret)
+			goto out;
+	}
 
 	/*
-	 * In order for us to be here, we know our q.key == key2, and since
-	 * we took the hb->lock above, we also know that futex_requeue() has
-	 * completed and we no longer have to concern ourselves with a wakeup
-	 * race with the atomic proxy lock acquisition by the requeue code. The
-	 * futex_requeue dropped our key1 reference and incremented our key2
-	 * reference count.
+	 * In order to be here, we have either been requeued, are in
+	 * the process of being requeued, or requeue successfully
+	 * acquired uaddr2 on our behalf.  If pi_blocked_on was
+	 * non-null above, we may be racing with a requeue.  Do not
+	 * rely on q->lock_ptr to be hb2->lock until after blocking on
+	 * hb->lock or hb2->lock. The futex_requeue dropped our key1
+	 * reference and incremented our key2 reference count.
 	 */
+	hb2 = hash_futex(&key2);
 
 	/* Check if the requeue code acquired the second futex for us. */
 	if (!q.rt_waiter) {
@@ -3245,7 +3291,8 @@
 		 * did a lock-steal - fix up the PI-state in that case.
 		 */
 		if (q.pi_state && (q.pi_state->owner != current)) {
-			spin_lock(q.lock_ptr);
+			spin_lock(&hb2->lock);
+			BUG_ON(&hb2->lock != q.lock_ptr);
 			ret = fixup_pi_state_owner(uaddr2, &q, current);
 			if (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {
 				pi_state = q.pi_state;
@@ -3256,7 +3303,7 @@
 			 * the requeue_pi() code acquired for us.
 			 */
 			put_pi_state(q.pi_state);
-			spin_unlock(q.lock_ptr);
+			spin_unlock(&hb2->lock);
 		}
 	} else {
 		struct rt_mutex *pi_mutex;
@@ -3270,7 +3317,8 @@
 		pi_mutex = &q.pi_state->pi_mutex;
 		ret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);
 
-		spin_lock(q.lock_ptr);
+		spin_lock(&hb2->lock);
+		BUG_ON(&hb2->lock != q.lock_ptr);
 		if (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))
 			ret = 0;
 
diff -Naur a/kernel/irq/handle.c b/kernel/irq/handle.c
--- a/kernel/irq/handle.c	2020-11-23 13:48:35.069947445 +0200
+++ b/kernel/irq/handle.c	2021-07-14 15:39:11.086140586 +0300
@@ -192,10 +192,16 @@
 {
 	irqreturn_t retval;
 	unsigned int flags = 0;
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 
 	retval = __handle_irq_event_percpu(desc, &flags);
 
-	add_interrupt_randomness(desc->irq_data.irq, flags);
+#ifdef CONFIG_PREEMPT_RT
+	desc->random_ip = ip;
+#else
+	add_interrupt_randomness(desc->irq_data.irq, flags, ip);
+#endif
 
 	if (!noirqdebug)
 		note_interrupt(desc, retval);
diff -Naur a/kernel/irq/manage.c b/kernel/irq/manage.c
--- a/kernel/irq/manage.c	2020-11-23 13:48:35.077947605 +0200
+++ b/kernel/irq/manage.c	2021-07-14 15:39:11.086140586 +0300
@@ -1175,6 +1175,12 @@
 		if (action_ret == IRQ_WAKE_THREAD)
 			irq_wake_secondary(desc, action);
 
+		if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+			migrate_disable();
+			add_interrupt_randomness(action->irq, 0,
+				 desc->random_ip ^ (unsigned long) action);
+			migrate_enable();
+		}
 		wake_threads_waitq(desc);
 	}
 
@@ -2711,7 +2717,7 @@
  *	This call sets the internal irqchip state of an interrupt,
  *	depending on the value of @which.
  *
- *	This function should be called with preemption disabled if the
+ *	This function should be called with migration disabled if the
  *	interrupt controller has per-cpu registers.
  */
 int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
diff -Naur a/kernel/irq/spurious.c b/kernel/irq/spurious.c
--- a/kernel/irq/spurious.c	2020-11-23 13:48:35.081947686 +0200
+++ b/kernel/irq/spurious.c	2021-07-14 15:39:11.086140586 +0300
@@ -443,6 +443,10 @@
 
 static int __init irqfixup_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT
+	pr_warn("irqfixup boot option not supported w/ CONFIG_PREEMPT_RT\n");
+	return 1;
+#endif
 	irqfixup = 1;
 	printk(KERN_WARNING "Misrouted IRQ fixup support enabled.\n");
 	printk(KERN_WARNING "This may impact system performance.\n");
@@ -455,6 +459,10 @@
 
 static int __init irqpoll_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT
+	pr_warn("irqpoll boot option not supported w/ CONFIG_PREEMPT_RT\n");
+	return 1;
+#endif
 	irqfixup = 2;
 	printk(KERN_WARNING "Misrouted IRQ fixup and polling support "
 				"enabled\n");
diff -Naur a/kernel/irq_work.c b/kernel/irq_work.c
--- a/kernel/irq_work.c	2020-11-23 13:48:34.905944153 +0200
+++ b/kernel/irq_work.c	2021-07-14 15:39:10.886141993 +0300
@@ -18,6 +18,7 @@
 #include <linux/cpu.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/interrupt.h>
 #include <asm/processor.h>
 
 
@@ -52,13 +53,19 @@
 /* Enqueue on current CPU, work must already be claimed and preempt disabled */
 static void __irq_work_queue_local(struct irq_work *work)
 {
+	struct llist_head *list;
+	bool lazy_work, realtime = IS_ENABLED(CONFIG_PREEMPT_RT);
+
+	lazy_work = atomic_read(&work->flags) & IRQ_WORK_LAZY;
+
 	/* If the work is "lazy", handle it from next tick if any */
-	if (atomic_read(&work->flags) & IRQ_WORK_LAZY) {
-		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
-		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
-	} else {
-		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
+	if (lazy_work || (realtime && !(atomic_read(&work->flags) & IRQ_WORK_HARD_IRQ)))
+		list = this_cpu_ptr(&lazy_list);
+	else
+		list = this_cpu_ptr(&raised_list);
+
+	if (llist_add(&work->llnode, list)) {
+		if (!lazy_work || tick_nohz_tick_stopped())
 			arch_irq_work_raise();
 	}
 }
@@ -102,7 +109,13 @@
 	if (cpu != smp_processor_id()) {
 		/* Arch remote IPI send/receive backend aren't NMI safe */
 		WARN_ON_ONCE(in_nmi());
-		__smp_call_single_queue(cpu, &work->llnode);
+
+		if (IS_ENABLED(CONFIG_PREEMPT_RT) && !(atomic_read(&work->flags) & IRQ_WORK_HARD_IRQ)) {
+			if (llist_add(&work->llnode, &per_cpu(lazy_list, cpu)))
+				arch_send_call_function_single_ipi(cpu);
+		} else {
+			__smp_call_single_queue(cpu, &work->llnode);
+		}
 	} else {
 		__irq_work_queue_local(work);
 	}
@@ -120,9 +133,8 @@
 	raised = this_cpu_ptr(&raised_list);
 	lazy = this_cpu_ptr(&lazy_list);
 
-	if (llist_empty(raised) || arch_irq_work_has_interrupt())
-		if (llist_empty(lazy))
-			return false;
+	if (llist_empty(raised) && llist_empty(lazy))
+		return false;
 
 	/* All work should have been flushed before going offline */
 	WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
@@ -160,8 +172,12 @@
 	struct irq_work *work, *tmp;
 	struct llist_node *llnode;
 
+#ifndef CONFIG_PREEMPT_RT
+	/*
+	 * nort: On RT IRQ-work may run in SOFTIRQ context.
+	 */
 	BUG_ON(!irqs_disabled());
-
+#endif
 	if (llist_empty(list))
 		return;
 
@@ -177,7 +193,16 @@
 void irq_work_run(void)
 {
 	irq_work_run_list(this_cpu_ptr(&raised_list));
-	irq_work_run_list(this_cpu_ptr(&lazy_list));
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * NOTE: we raise softirq via IPI for safety,
+		 * and execute in irq_work_tick() to move the
+		 * overhead from hard to soft irq context.
+		 */
+		if (!llist_empty(this_cpu_ptr(&lazy_list)))
+			raise_softirq(TIMER_SOFTIRQ);
+	} else
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
 EXPORT_SYMBOL_GPL(irq_work_run);
 
@@ -187,8 +212,17 @@
 
 	if (!llist_empty(raised) && !arch_irq_work_has_interrupt())
 		irq_work_run_list(raised);
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
+}
+
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT)
+void irq_work_tick_soft(void)
+{
 	irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
+#endif
 
 /*
  * Synchronize against the irq_work @entry, ensures the entry is not
diff -Naur a/kernel/Kconfig.locks b/kernel/Kconfig.locks
--- a/kernel/Kconfig.locks	2020-11-23 13:48:34.877943592 +0200
+++ b/kernel/Kconfig.locks	2021-07-14 15:39:10.882142022 +0300
@@ -251,7 +251,7 @@
 
 config QUEUED_RWLOCKS
 	def_bool y if ARCH_USE_QUEUED_RWLOCKS
-	depends on SMP
+	depends on SMP && !PREEMPT_RT
 
 config ARCH_HAS_MMIOWB
 	bool
diff -Naur a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
--- a/kernel/Kconfig.preempt	2020-11-23 13:48:34.877943592 +0200
+++ b/kernel/Kconfig.preempt	2021-07-14 15:39:10.882142022 +0300
@@ -1,5 +1,11 @@
 # SPDX-License-Identifier: GPL-2.0-only
 
+config HAVE_PREEMPT_LAZY
+	bool
+
+config PREEMPT_LAZY
+	def_bool y if HAVE_PREEMPT_LAZY && PREEMPT_RT
+
 choice
 	prompt "Preemption Model"
 	default PREEMPT_NONE
diff -Naur a/kernel/kexec_core.c b/kernel/kexec_core.c
--- a/kernel/kexec_core.c	2020-11-23 13:48:34.909944234 +0200
+++ b/kernel/kexec_core.c	2021-07-14 15:39:10.886141993 +0300
@@ -978,7 +978,6 @@
 	old_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);
 	if (old_cpu == PANIC_CPU_INVALID) {
 		/* This is the 1st CPU which comes here, so go ahead. */
-		printk_safe_flush_on_panic();
 		__crash_kexec(regs);
 
 		/*
diff -Naur a/kernel/ksysfs.c b/kernel/ksysfs.c
--- a/kernel/ksysfs.c	2020-11-23 13:48:34.913944314 +0200
+++ b/kernel/ksysfs.c	2021-07-14 15:39:10.886141993 +0300
@@ -138,6 +138,15 @@
 
 #endif /* CONFIG_CRASH_CORE */
 
+#if defined(CONFIG_PREEMPT_RT)
+static ssize_t realtime_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", 1);
+}
+KERNEL_ATTR_RO(realtime);
+#endif
+
 /* whether file capabilities are enabled */
 static ssize_t fscaps_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
@@ -229,6 +238,9 @@
 	&rcu_expedited_attr.attr,
 	&rcu_normal_attr.attr,
 #endif
+#ifdef CONFIG_PREEMPT_RT
+	&realtime_attr.attr,
+#endif
 	NULL
 };
 
diff -Naur a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c
--- a/kernel/locking/lockdep.c	2020-11-23 13:48:35.097948007 +0200
+++ b/kernel/locking/lockdep.c	2021-07-14 15:39:11.110140417 +0300
@@ -4896,6 +4896,7 @@
 		}
 	}
 
+#ifndef CONFIG_PREEMPT_RT
 	/*
 	 * We dont accurately track softirq state in e.g.
 	 * hardirq contexts (such as on 4KSTACKS), so only
@@ -4910,6 +4911,7 @@
 			DEBUG_LOCKS_WARN_ON(!current->softirqs_enabled);
 		}
 	}
+#endif
 
 	if (!debug_locks)
 		print_irqtrace_events(current);
diff -Naur a/kernel/locking/Makefile b/kernel/locking/Makefile
--- a/kernel/locking/Makefile	2020-11-23 13:48:35.093947926 +0200
+++ b/kernel/locking/Makefile	2021-07-14 15:39:11.110140417 +0300
@@ -3,7 +3,7 @@
 # and is generally not a function of system call inputs.
 KCOV_INSTRUMENT		:= n
 
-obj-y += mutex.o semaphore.o rwsem.o percpu-rwsem.o
+obj-y += semaphore.o rwsem.o percpu-rwsem.o
 
 # Avoid recursion lockdep -> KCSAN -> ... -> lockdep.
 KCSAN_SANITIZE_lockdep.o := n
@@ -15,19 +15,23 @@
 CFLAGS_REMOVE_rtmutex-debug.o = $(CC_FLAGS_FTRACE)
 endif
 
-obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
 obj-$(CONFIG_LOCKDEP) += lockdep.o
 ifeq ($(CONFIG_PROC_FS),y)
 obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
 endif
 obj-$(CONFIG_SMP) += spinlock.o
-obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
 obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
 obj-$(CONFIG_QUEUED_SPINLOCKS) += qspinlock.o
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o
+ifneq ($(CONFIG_PREEMPT_RT),y)
+obj-y += mutex.o
+obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
+obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
+endif
+obj-$(CONFIG_PREEMPT_RT) += mutex-rt.o rwsem-rt.o rwlock-rt.o
 obj-$(CONFIG_QUEUED_RWLOCKS) += qrwlock.o
 obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
 obj-$(CONFIG_WW_MUTEX_SELFTEST) += test-ww_mutex.o
diff -Naur a/kernel/locking/mutex-rt.c b/kernel/locking/mutex-rt.c
--- a/kernel/locking/mutex-rt.c	1970-01-01 02:00:00.000000000 +0200
+++ b/kernel/locking/mutex-rt.c	2021-07-14 15:39:11.110140417 +0300
@@ -0,0 +1,222 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Real-Time Preemption Support
+ *
+ * started by Ingo Molnar:
+ *
+ *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *
+ * historic credit for proving that Linux spinlocks can be implemented via
+ * RT-aware mutexes goes to many people: The Pmutex project (Dirk Grambow
+ * and others) who prototyped it on 2.4 and did lots of comparative
+ * research and analysis; TimeSys, for proving that you can implement a
+ * fully preemptible kernel via the use of IRQ threading and mutexes;
+ * Bill Huey for persuasively arguing on lkml that the mutex model is the
+ * right one; and to MontaVista, who ported pmutexes to 2.6.
+ *
+ * This code is a from-scratch implementation and is not based on pmutexes,
+ * but the idea of converting spinlocks to mutexes is used here too.
+ *
+ * lock debugging, locking tree, deadlock detection:
+ *
+ *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey
+ *  Released under the General Public License (GPL).
+ *
+ * Includes portions of the generic R/W semaphore implementation from:
+ *
+ *  Copyright (c) 2001   David Howells (dhowells@redhat.com).
+ *  - Derived partially from idea by Andrea Arcangeli <andrea@suse.de>
+ *  - Derived also from comments by Linus
+ *
+ * Pending ownership of locks and ownership stealing:
+ *
+ *  Copyright (C) 2005, Kihon Technologies Inc., Steven Rostedt
+ *
+ *   (also by Steven Rostedt)
+ *    - Converted single pi_lock to individual task locks.
+ *
+ * By Esben Nielsen:
+ *    Doing priority inheritance with help of the scheduler.
+ *
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *  - major rework based on Esben Nielsens initial patch
+ *  - replaced thread_info references by task_struct refs
+ *  - removed task->pending_owner dependency
+ *  - BKL drop/reacquire for semaphore style locks to avoid deadlocks
+ *    in the scheduler return path as discussed with Steven Rostedt
+ *
+ *  Copyright (C) 2006, Kihon Technologies Inc.
+ *    Steven Rostedt <rostedt@goodmis.org>
+ *  - debugged and patched Thomas Gleixner's rework.
+ *  - added back the cmpxchg to the rework.
+ *  - turned atomic require back on for SMP.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/rtmutex.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/syscalls.h>
+#include <linux/interrupt.h>
+#include <linux/plist.h>
+#include <linux/fs.h>
+#include <linux/futex.h>
+#include <linux/hrtimer.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * struct mutex functions
+ */
+void __mutex_do_init(struct mutex *mutex, const char *name,
+		     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)mutex, sizeof(*mutex));
+	lockdep_init_map(&mutex->dep_map, name, key, 0);
+#endif
+	mutex->lock.save_state = 0;
+}
+EXPORT_SYMBOL(__mutex_do_init);
+
+void __lockfunc _mutex_lock(struct mutex *lock)
+{
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock);
+
+void __lockfunc _mutex_lock_io(struct mutex *lock)
+{
+	int token;
+
+	token = io_schedule_prepare();
+	_mutex_lock(lock);
+	io_schedule_finish(token);
+}
+EXPORT_SYMBOL_GPL(_mutex_lock_io);
+
+int __lockfunc _mutex_lock_interruptible(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_INTERRUPTIBLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible);
+
+int __lockfunc _mutex_lock_killable(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_KILLABLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass)
+{
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock_nested);
+
+void __lockfunc _mutex_lock_io_nested(struct mutex *lock, int subclass)
+{
+	int token;
+
+	token = io_schedule_prepare();
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+
+	io_schedule_finish(token);
+}
+EXPORT_SYMBOL_GPL(_mutex_lock_io_nested);
+
+void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)
+{
+	mutex_acquire_nest(&lock->dep_map, 0, 0, nest, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock_nest_lock);
+
+int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_INTERRUPTIBLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible_nested);
+
+int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_KILLABLE);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable_nested);
+#endif
+
+int __lockfunc _mutex_trylock(struct mutex *lock)
+{
+	int ret = __rt_mutex_trylock(&lock->lock);
+
+	if (ret)
+		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_trylock);
+
+void __lockfunc _mutex_unlock(struct mutex *lock)
+{
+	mutex_release(&lock->dep_map, _RET_IP_);
+	__rt_mutex_unlock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_unlock);
+
+/**
+ * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0
+ * @cnt: the atomic which we are to dec
+ * @lock: the mutex to return holding if we dec to 0
+ *
+ * return true and hold lock if we dec to 0, return false otherwise
+ */
+int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)
+{
+	/* dec if we can't possibly hit 0 */
+	if (atomic_add_unless(cnt, -1, 1))
+		return 0;
+	/* we might hit 0, so take the lock */
+	mutex_lock(lock);
+	if (!atomic_dec_and_test(cnt)) {
+		/* when we actually did the dec, we didn't hit 0 */
+		mutex_unlock(lock);
+		return 0;
+	}
+	/* we hit 0, and we hold the lock */
+	return 1;
+}
+EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
diff -Naur a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
--- a/kernel/locking/rtmutex.c	2020-11-23 13:48:35.105948167 +0200
+++ b/kernel/locking/rtmutex.c	2021-07-14 15:39:11.110140417 +0300
@@ -8,6 +8,11 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ * Adaptive Spinlocks:
+ *  Copyright (C) 2008 Novell, Inc., Gregory Haskins, Sven Dietrich,
+ *				     and Peter Morreale,
+ * Adaptive Spinlocks simplification:
+ *  Copyright (C) 2008 Red Hat, Inc., Steven Rostedt <srostedt@redhat.com>
  *
  *  See Documentation/locking/rt-mutex-design.rst for details.
  */
@@ -19,6 +24,7 @@
 #include <linux/sched/wake_q.h>
 #include <linux/sched/debug.h>
 #include <linux/timer.h>
+#include <linux/ww_mutex.h>
 
 #include "rtmutex_common.h"
 
@@ -136,6 +142,12 @@
 		WRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);
 }
 
+static int rt_mutex_real_waiter(struct rt_mutex_waiter *waiter)
+{
+	return waiter && waiter != PI_WAKEUP_INPROGRESS &&
+		waiter != PI_REQUEUE_INPROGRESS;
+}
+
 /*
  * We can speed up the acquire/release, if there's no debugging state to be
  * set up.
@@ -227,7 +239,7 @@
  * Only use with rt_mutex_waiter_{less,equal}()
  */
 #define task_to_waiter(p)	\
-	&(struct rt_mutex_waiter){ .prio = (p)->prio, .deadline = (p)->dl.deadline }
+	&(struct rt_mutex_waiter){ .prio = (p)->prio, .deadline = (p)->dl.deadline, .task = (p) }
 
 static inline int
 rt_mutex_waiter_less(struct rt_mutex_waiter *left,
@@ -267,6 +279,27 @@
 	return 1;
 }
 
+#define STEAL_NORMAL  0
+#define STEAL_LATERAL 1
+
+static inline int
+rt_mutex_steal(struct rt_mutex *lock, struct rt_mutex_waiter *waiter, int mode)
+{
+	struct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);
+
+	if (waiter == top_waiter || rt_mutex_waiter_less(waiter, top_waiter))
+		return 1;
+
+	/*
+	 * Note that RT tasks are excluded from lateral-steals
+	 * to prevent the introduction of an unbounded latency.
+	 */
+	if (mode == STEAL_NORMAL || rt_task(waiter->task))
+		return 0;
+
+	return rt_mutex_waiter_equal(waiter, top_waiter);
+}
+
 static void
 rt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)
 {
@@ -371,6 +404,14 @@
 	return debug_rt_mutex_detect_deadlock(waiter, chwalk);
 }
 
+static void rt_mutex_wake_waiter(struct rt_mutex_waiter *waiter)
+{
+	if (waiter->savestate)
+		wake_up_lock_sleeper(waiter->task);
+	else
+		wake_up_process(waiter->task);
+}
+
 /*
  * Max number of times we'll walk the boosting chain:
  */
@@ -378,7 +419,8 @@
 
 static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)
 {
-	return p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;
+	return rt_mutex_real_waiter(p->pi_blocked_on) ?
+		p->pi_blocked_on->lock : NULL;
 }
 
 /*
@@ -514,7 +556,7 @@
 	 * reached or the state of the chain has changed while we
 	 * dropped the locks.
 	 */
-	if (!waiter)
+	if (!rt_mutex_real_waiter(waiter))
 		goto out_unlock_pi;
 
 	/*
@@ -597,7 +639,6 @@
 	 * walk, we detected a deadlock.
 	 */
 	if (lock == orig_lock || rt_mutex_owner(lock) == top_task) {
-		debug_rt_mutex_deadlock(chwalk, orig_waiter, lock);
 		raw_spin_unlock(&lock->wait_lock);
 		ret = -EDEADLK;
 		goto out_unlock_pi;
@@ -694,13 +735,16 @@
 	 * follow here. This is the end of the chain we are walking.
 	 */
 	if (!rt_mutex_owner(lock)) {
+		struct rt_mutex_waiter *lock_top_waiter;
+
 		/*
 		 * If the requeue [7] above changed the top waiter,
 		 * then we need to wake the new top waiter up to try
 		 * to get the lock.
 		 */
-		if (prerequeue_top_waiter != rt_mutex_top_waiter(lock))
-			wake_up_process(rt_mutex_top_waiter(lock)->task);
+		lock_top_waiter = rt_mutex_top_waiter(lock);
+		if (prerequeue_top_waiter != lock_top_waiter)
+			rt_mutex_wake_waiter(lock_top_waiter);
 		raw_spin_unlock_irq(&lock->wait_lock);
 		return 0;
 	}
@@ -801,9 +845,11 @@
  * @task:   The task which wants to acquire the lock
  * @waiter: The waiter that is queued to the lock's wait tree if the
  *	    callsite called task_blocked_on_lock(), otherwise NULL
+ * @mode:   Lock steal mode (STEAL_NORMAL, STEAL_LATERAL)
  */
-static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
-				struct rt_mutex_waiter *waiter)
+static int __try_to_take_rt_mutex(struct rt_mutex *lock,
+				  struct task_struct *task,
+				  struct rt_mutex_waiter *waiter, int mode)
 {
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -839,12 +885,11 @@
 	 */
 	if (waiter) {
 		/*
-		 * If waiter is not the highest priority waiter of
-		 * @lock, give up.
+		 * If waiter is not the highest priority waiter of @lock,
+		 * or its peer when lateral steal is allowed, give up.
 		 */
-		if (waiter != rt_mutex_top_waiter(lock))
+		if (!rt_mutex_steal(lock, waiter, mode))
 			return 0;
-
 		/*
 		 * We can acquire the lock. Remove the waiter from the
 		 * lock waiters tree.
@@ -862,14 +907,12 @@
 		 */
 		if (rt_mutex_has_waiters(lock)) {
 			/*
-			 * If @task->prio is greater than or equal to
-			 * the top waiter priority (kernel view),
-			 * @task lost.
+			 * If @task->prio is greater than the top waiter
+			 * priority (kernel view), or equal to it when a
+			 * lateral steal is forbidden, @task lost.
 			 */
-			if (!rt_mutex_waiter_less(task_to_waiter(task),
-						  rt_mutex_top_waiter(lock)))
+			if (!rt_mutex_steal(lock, task_to_waiter(task), mode))
 				return 0;
-
 			/*
 			 * The current top waiter stays enqueued. We
 			 * don't have to change anything in the lock
@@ -916,6 +959,329 @@
 	return 1;
 }
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * preemptible spin_lock functions:
+ */
+static inline void rt_spin_lock_fastlock(struct rt_mutex *lock,
+					 void  (*slowfn)(struct rt_mutex *lock))
+{
+	might_sleep_no_state_check();
+
+	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
+		return;
+	else
+		slowfn(lock);
+}
+
+static inline void rt_spin_lock_fastunlock(struct rt_mutex *lock,
+					   void  (*slowfn)(struct rt_mutex *lock))
+{
+	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
+		return;
+	else
+		slowfn(lock);
+}
+#ifdef CONFIG_SMP
+/*
+ * Note that owner is a speculative pointer and dereferencing relies
+ * on rcu_read_lock() and the check against the lock owner.
+ */
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *owner)
+{
+	int res = 0;
+
+	rcu_read_lock();
+	for (;;) {
+		if (owner != rt_mutex_owner(lock))
+			break;
+		/*
+		 * Ensure that owner->on_cpu is dereferenced _after_
+		 * checking the above to be valid.
+		 */
+		barrier();
+		if (!owner->on_cpu) {
+			res = 1;
+			break;
+		}
+		cpu_relax();
+	}
+	rcu_read_unlock();
+	return res;
+}
+#else
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *orig_owner)
+{
+	return 1;
+}
+#endif
+
+static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
+				   struct rt_mutex_waiter *waiter,
+				   struct task_struct *task,
+				   enum rtmutex_chainwalk chwalk);
+/*
+ * Slow path lock function spin_lock style: this variant is very
+ * careful not to miss any non-lock wakeups.
+ *
+ * We store the current state under p->pi_lock in p->saved_state and
+ * the try_to_wake_up() code handles this accordingly.
+ */
+void __sched rt_spin_lock_slowlock_locked(struct rt_mutex *lock,
+					  struct rt_mutex_waiter *waiter,
+					  unsigned long flags)
+{
+	struct task_struct *lock_owner, *self = current;
+	struct rt_mutex_waiter *top_waiter;
+	int ret;
+
+	if (__try_to_take_rt_mutex(lock, self, NULL, STEAL_LATERAL))
+		return;
+
+	BUG_ON(rt_mutex_owner(lock) == self);
+
+	/*
+	 * We save whatever state the task is in and we'll restore it
+	 * after acquiring the lock taking real wakeups into account
+	 * as well. We are serialized via pi_lock against wakeups. See
+	 * try_to_wake_up().
+	 */
+	raw_spin_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock(&self->pi_lock);
+
+	ret = task_blocks_on_rt_mutex(lock, waiter, self, RT_MUTEX_MIN_CHAINWALK);
+	BUG_ON(ret);
+
+	for (;;) {
+		/* Try to acquire the lock again. */
+		if (__try_to_take_rt_mutex(lock, self, waiter, STEAL_LATERAL))
+			break;
+
+		top_waiter = rt_mutex_top_waiter(lock);
+		lock_owner = rt_mutex_owner(lock);
+
+		raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+
+		if (top_waiter != waiter || adaptive_wait(lock, lock_owner))
+			preempt_schedule_lock();
+
+		raw_spin_lock_irqsave(&lock->wait_lock, flags);
+
+		raw_spin_lock(&self->pi_lock);
+		__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+		raw_spin_unlock(&self->pi_lock);
+	}
+
+	/*
+	 * Restore the task state to current->saved_state. We set it
+	 * to the original state above and the try_to_wake_up() code
+	 * has possibly updated it when a real (non-rtmutex) wakeup
+	 * happened while we were blocked. Clear saved_state so
+	 * try_to_wakeup() does not get confused.
+	 */
+	raw_spin_lock(&self->pi_lock);
+	__set_current_state_no_track(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	raw_spin_unlock(&self->pi_lock);
+
+	/*
+	 * try_to_take_rt_mutex() sets the waiter bit
+	 * unconditionally. We might have to fix that up:
+	 */
+	fixup_rt_mutex_waiters(lock);
+
+	BUG_ON(rt_mutex_has_waiters(lock) && waiter == rt_mutex_top_waiter(lock));
+	BUG_ON(!RB_EMPTY_NODE(&waiter->tree_entry));
+}
+
+static void noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
+{
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+
+	rt_mutex_init_waiter(&waiter, true);
+
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	rt_spin_lock_slowlock_locked(lock, &waiter, flags);
+	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+static bool __sched __rt_mutex_unlock_common(struct rt_mutex *lock,
+					     struct wake_q_head *wake_q,
+					     struct wake_q_head *wq_sleeper);
+/*
+ * Slow path to release a rt_mutex spin_lock style
+ */
+void __sched rt_spin_lock_slowunlock(struct rt_mutex *lock)
+{
+	unsigned long flags;
+	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
+	bool postunlock;
+
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	postunlock = __rt_mutex_unlock_common(lock, &wake_q, &wake_sleeper_q);
+	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+
+	if (postunlock)
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
+}
+
+void __lockfunc rt_spin_lock(spinlock_t *lock)
+{
+	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_spin_lock);
+
+void __lockfunc __rt_spin_lock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastlock(lock, rt_spin_lock_slowlock);
+}
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass)
+{
+	spin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_spin_lock_nested);
+
+void __lockfunc rt_spin_lock_nest_lock(spinlock_t *lock,
+				       struct lockdep_map *nest_lock)
+{
+	spin_acquire_nest(&lock->dep_map, 0, 0, nest_lock, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_spin_lock_nest_lock);
+#endif
+
+void __lockfunc rt_spin_unlock(spinlock_t *lock)
+{
+	/* NOTE: we always pass in '1' for nested, for simplicity */
+	spin_release(&lock->dep_map, _RET_IP_);
+	migrate_enable();
+	rcu_read_unlock();
+	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(rt_spin_unlock);
+
+void __lockfunc __rt_spin_unlock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastunlock(lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(__rt_spin_unlock);
+
+/*
+ * Wait for the lock to get unlocked: instead of polling for an unlock
+ * (like raw spinlocks do), we lock and unlock, to force the kernel to
+ * schedule if there's contention:
+ */
+void __lockfunc rt_spin_lock_unlock(spinlock_t *lock)
+{
+	spin_lock(lock);
+	spin_unlock(lock);
+}
+EXPORT_SYMBOL(rt_spin_lock_unlock);
+
+int __lockfunc rt_spin_trylock(spinlock_t *lock)
+{
+	int ret;
+
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock);
+
+int __lockfunc rt_spin_trylock_bh(spinlock_t *lock)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	} else {
+		local_bh_enable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock_bh);
+
+void
+__rt_spin_lock_init(spinlock_t *lock, const char *name, struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+}
+EXPORT_SYMBOL(__rt_spin_lock_init);
+
+#endif /* PREEMPT_RT */
+
+#ifdef CONFIG_PREEMPT_RT
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct ww_acquire_ctx *hold_ctx = READ_ONCE(ww->ctx);
+
+	if (!hold_ctx)
+		return 0;
+
+	if (unlikely(ctx == hold_ctx))
+		return -EALREADY;
+
+	if (ctx->stamp - hold_ctx->stamp <= LONG_MAX &&
+	    (ctx->stamp != hold_ctx->stamp || ctx > hold_ctx)) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(ctx->contending_lock);
+		ctx->contending_lock = ww;
+#endif
+		return -EDEADLK;
+	}
+
+	return 0;
+}
+#else
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	BUG();
+	return 0;
+}
+
+#endif
+
+static inline int
+try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+		     struct rt_mutex_waiter *waiter)
+{
+	return __try_to_take_rt_mutex(lock, task, waiter, STEAL_NORMAL);
+}
+
 /*
  * Task blocks on lock.
  *
@@ -948,6 +1314,22 @@
 		return -EDEADLK;
 
 	raw_spin_lock(&task->pi_lock);
+	/*
+	 * In the case of futex requeue PI, this will be a proxy
+	 * lock. The task will wake unaware that it is enqueueed on
+	 * this lock. Avoid blocking on two locks and corrupting
+	 * pi_blocked_on via the PI_WAKEUP_INPROGRESS
+	 * flag. futex_wait_requeue_pi() sets this when it wakes up
+	 * before requeue (due to a signal or timeout). Do not enqueue
+	 * the task if PI_WAKEUP_INPROGRESS is set.
+	 */
+	if (task != current && task->pi_blocked_on == PI_WAKEUP_INPROGRESS) {
+		raw_spin_unlock(&task->pi_lock);
+		return -EAGAIN;
+	}
+
+       BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on));
+
 	waiter->task = task;
 	waiter->lock = lock;
 	waiter->prio = task->prio;
@@ -971,7 +1353,7 @@
 		rt_mutex_enqueue_pi(owner, waiter);
 
 		rt_mutex_adjust_prio(owner);
-		if (owner->pi_blocked_on)
+		if (rt_mutex_real_waiter(owner->pi_blocked_on))
 			chain_walk = 1;
 	} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {
 		chain_walk = 1;
@@ -1013,6 +1395,7 @@
  * Called with lock->wait_lock held and interrupts disabled.
  */
 static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,
+				    struct wake_q_head *wake_sleeper_q,
 				    struct rt_mutex *lock)
 {
 	struct rt_mutex_waiter *waiter;
@@ -1052,7 +1435,10 @@
 	 * Pairs with preempt_enable() in rt_mutex_postunlock();
 	 */
 	preempt_disable();
-	wake_q_add(wake_q, waiter->task);
+	if (waiter->savestate)
+		wake_q_add_sleeper(wake_sleeper_q, waiter->task);
+	else
+		wake_q_add(wake_q, waiter->task);
 	raw_spin_unlock(&current->pi_lock);
 }
 
@@ -1067,7 +1453,7 @@
 {
 	bool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));
 	struct task_struct *owner = rt_mutex_owner(lock);
-	struct rt_mutex *next_lock;
+	struct rt_mutex *next_lock = NULL;
 
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -1093,7 +1479,8 @@
 	rt_mutex_adjust_prio(owner);
 
 	/* Store the lock on which owner is blocked or NULL */
-	next_lock = task_blocked_on_lock(owner);
+	if (rt_mutex_real_waiter(owner->pi_blocked_on))
+		next_lock = task_blocked_on_lock(owner);
 
 	raw_spin_unlock(&owner->pi_lock);
 
@@ -1129,26 +1516,28 @@
 	raw_spin_lock_irqsave(&task->pi_lock, flags);
 
 	waiter = task->pi_blocked_on;
-	if (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {
+	if (!rt_mutex_real_waiter(waiter) ||
+	    rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {
 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		return;
 	}
 	next_lock = waiter->lock;
-	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 
 	/* gets dropped in rt_mutex_adjust_prio_chain()! */
 	get_task_struct(task);
 
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 	rt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,
 				   next_lock, NULL, task);
 }
 
-void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
+void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savestate)
 {
 	debug_rt_mutex_init_waiter(waiter);
 	RB_CLEAR_NODE(&waiter->pi_tree_entry);
 	RB_CLEAR_NODE(&waiter->tree_entry);
 	waiter->task = NULL;
+	waiter->savestate = savestate;
 }
 
 /**
@@ -1164,7 +1553,8 @@
 static int __sched
 __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		    struct hrtimer_sleeper *timeout,
-		    struct rt_mutex_waiter *waiter)
+		    struct rt_mutex_waiter *waiter,
+		    struct ww_acquire_ctx *ww_ctx)
 {
 	int ret = 0;
 
@@ -1173,24 +1563,23 @@
 		if (try_to_take_rt_mutex(lock, current, waiter))
 			break;
 
-		/*
-		 * TASK_INTERRUPTIBLE checks for signals and
-		 * timeout. Ignored otherwise.
-		 */
-		if (likely(state == TASK_INTERRUPTIBLE)) {
-			/* Signal pending? */
-			if (signal_pending(current))
-				ret = -EINTR;
-			if (timeout && !timeout->task)
-				ret = -ETIMEDOUT;
+		if (timeout && !timeout->task) {
+			ret = -ETIMEDOUT;
+			break;
+		}
+		if (signal_pending_state(state, current)) {
+			ret = -EINTR;
+			break;
+		}
+
+		if (ww_ctx && ww_ctx->acquired > 0) {
+			ret = __mutex_lock_check_stamp(lock, ww_ctx);
 			if (ret)
 				break;
 		}
 
 		raw_spin_unlock_irq(&lock->wait_lock);
 
-		debug_rt_mutex_print_deadlock(waiter);
-
 		schedule();
 
 		raw_spin_lock_irq(&lock->wait_lock);
@@ -1211,43 +1600,110 @@
 	if (res != -EDEADLOCK || detect_deadlock)
 		return;
 
-	/*
-	 * Yell lowdly and stop the task right here.
-	 */
-	rt_mutex_print_deadlock(w);
 	while (1) {
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 	}
 }
 
-/*
- * Slow path lock function:
- */
-static int __sched
-rt_mutex_slowlock(struct rt_mutex *lock, int state,
-		  struct hrtimer_sleeper *timeout,
-		  enum rtmutex_chainwalk chwalk)
+static __always_inline void ww_mutex_lock_acquired(struct ww_mutex *ww,
+						   struct ww_acquire_ctx *ww_ctx)
 {
-	struct rt_mutex_waiter waiter;
-	unsigned long flags;
-	int ret = 0;
+#ifdef CONFIG_DEBUG_MUTEXES
+	/*
+	 * If this WARN_ON triggers, you used ww_mutex_lock to acquire,
+	 * but released with a normal mutex_unlock in this call.
+	 *
+	 * This should never happen, always use ww_mutex_unlock.
+	 */
+	DEBUG_LOCKS_WARN_ON(ww->ctx);
+
+	/*
+	 * Not quite done after calling ww_acquire_done() ?
+	 */
+	DEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);
+
+	if (ww_ctx->contending_lock) {
+		/*
+		 * After -EDEADLK you tried to
+		 * acquire a different ww_mutex? Bad!
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);
 
-	rt_mutex_init_waiter(&waiter);
+		/*
+		 * You called ww_mutex_lock after receiving -EDEADLK,
+		 * but 'forgot' to unlock everything else first?
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);
+		ww_ctx->contending_lock = NULL;
+	}
 
 	/*
-	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
-	 * be called in early boot if the cmpxchg() fast path is disabled
-	 * (debug, no architecture support). In this case we will acquire the
-	 * rtmutex with lock->wait_lock held. But we cannot unconditionally
-	 * enable interrupts in that early boot case. So we need to use the
-	 * irqsave/restore variants.
+	 * Naughty, using a different class will lead to undefined behavior!
 	 */
-	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	DEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);
+#endif
+	ww_ctx->acquired++;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct rt_mutex_waiter *waiter, *n;
+
+	/*
+	 * This branch gets optimized out for the common case,
+	 * and is only important for ww_mutex_lock.
+	 */
+	ww_mutex_lock_acquired(ww, ww_ctx);
+	ww->ctx = ww_ctx;
+
+	/*
+	 * Give any possible sleeping processes the chance to wake up,
+	 * so they can recheck if they have to back off.
+	 */
+	rbtree_postorder_for_each_entry_safe(waiter, n, &lock->waiters.rb_root,
+					     tree_entry) {
+		/* XXX debug rt mutex waiter wakeup */
+
+		BUG_ON(waiter->lock != lock);
+		rt_mutex_wake_waiter(waiter);
+	}
+}
+
+#else
+
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	BUG();
+}
+#endif
+
+int __sched rt_mutex_slowlock_locked(struct rt_mutex *lock, int state,
+				     struct hrtimer_sleeper *timeout,
+				     enum rtmutex_chainwalk chwalk,
+				     struct ww_acquire_ctx *ww_ctx,
+				     struct rt_mutex_waiter *waiter)
+{
+	int ret;
+
+#ifdef CONFIG_PREEMPT_RT
+	if (ww_ctx) {
+		struct ww_mutex *ww;
+
+		ww = container_of(lock, struct ww_mutex, base.lock);
+		if (unlikely(ww_ctx == READ_ONCE(ww->ctx)))
+			return -EALREADY;
+	}
+#endif
 
 	/* Try to acquire the lock again: */
 	if (try_to_take_rt_mutex(lock, current, NULL)) {
-		raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+		if (ww_ctx)
+			ww_mutex_account_lock(lock, ww_ctx);
 		return 0;
 	}
 
@@ -1257,16 +1713,26 @@
 	if (unlikely(timeout))
 		hrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);
 
-	ret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);
+	ret = task_blocks_on_rt_mutex(lock, waiter, current, chwalk);
 
-	if (likely(!ret))
+	if (likely(!ret)) {
 		/* sleep on the mutex */
-		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter);
+		ret = __rt_mutex_slowlock(lock, state, timeout, waiter,
+					  ww_ctx);
+	} else if (ww_ctx) {
+		/* ww_mutex received EDEADLK, let it become EALREADY */
+		ret = __mutex_lock_check_stamp(lock, ww_ctx);
+		BUG_ON(!ret);
+	}
 
 	if (unlikely(ret)) {
 		__set_current_state(TASK_RUNNING);
-		remove_waiter(lock, &waiter);
-		rt_mutex_handle_deadlock(ret, chwalk, &waiter);
+		remove_waiter(lock, waiter);
+		/* ww_mutex wants to report EDEADLK/EALREADY, let it */
+		if (!ww_ctx)
+			rt_mutex_handle_deadlock(ret, chwalk, waiter);
+	} else if (ww_ctx) {
+		ww_mutex_account_lock(lock, ww_ctx);
 	}
 
 	/*
@@ -1274,6 +1740,36 @@
 	 * unconditionally. We might have to fix that up.
 	 */
 	fixup_rt_mutex_waiters(lock);
+	return ret;
+}
+
+/*
+ * Slow path lock function:
+ */
+static int __sched
+rt_mutex_slowlock(struct rt_mutex *lock, int state,
+		  struct hrtimer_sleeper *timeout,
+		  enum rtmutex_chainwalk chwalk,
+		  struct ww_acquire_ctx *ww_ctx)
+{
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+	int ret = 0;
+
+	rt_mutex_init_waiter(&waiter, false);
+
+	/*
+	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
+	 * be called in early boot if the cmpxchg() fast path is disabled
+	 * (debug, no architecture support). In this case we will acquire the
+	 * rtmutex with lock->wait_lock held. But we cannot unconditionally
+	 * enable interrupts in that early boot case. So we need to use the
+	 * irqsave/restore variants.
+	 */
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+
+	ret = rt_mutex_slowlock_locked(lock, state, timeout, chwalk, ww_ctx,
+				       &waiter);
 
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
@@ -1334,7 +1830,8 @@
  * Return whether the current task needs to call rt_mutex_postunlock().
  */
 static bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,
-					struct wake_q_head *wake_q)
+					struct wake_q_head *wake_q,
+					struct wake_q_head *wake_sleeper_q)
 {
 	unsigned long flags;
 
@@ -1388,7 +1885,7 @@
 	 *
 	 * Queue the next waiter for wakeup once we release the wait_lock.
 	 */
-	mark_wakeup_next_waiter(wake_q, lock);
+	mark_wakeup_next_waiter(wake_q, wake_sleeper_q, lock);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	return true; /* call rt_mutex_postunlock() */
@@ -1402,29 +1899,16 @@
  */
 static inline int
 rt_mutex_fastlock(struct rt_mutex *lock, int state,
+		  struct ww_acquire_ctx *ww_ctx,
 		  int (*slowfn)(struct rt_mutex *lock, int state,
 				struct hrtimer_sleeper *timeout,
-				enum rtmutex_chainwalk chwalk))
+				enum rtmutex_chainwalk chwalk,
+				struct ww_acquire_ctx *ww_ctx))
 {
 	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
 		return 0;
 
-	return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);
-}
-
-static inline int
-rt_mutex_timed_fastlock(struct rt_mutex *lock, int state,
-			struct hrtimer_sleeper *timeout,
-			enum rtmutex_chainwalk chwalk,
-			int (*slowfn)(struct rt_mutex *lock, int state,
-				      struct hrtimer_sleeper *timeout,
-				      enum rtmutex_chainwalk chwalk))
-{
-	if (chwalk == RT_MUTEX_MIN_CHAINWALK &&
-	    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
-		return 0;
-
-	return slowfn(lock, state, timeout, chwalk);
+	return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK, ww_ctx);
 }
 
 static inline int
@@ -1440,9 +1924,11 @@
 /*
  * Performs the wakeup of the the top-waiter and re-enables preemption.
  */
-void rt_mutex_postunlock(struct wake_q_head *wake_q)
+void rt_mutex_postunlock(struct wake_q_head *wake_q,
+			 struct wake_q_head *wake_sleeper_q)
 {
 	wake_up_q(wake_q);
+	wake_up_q_sleeper(wake_sleeper_q);
 
 	/* Pairs with preempt_disable() in rt_mutex_slowunlock() */
 	preempt_enable();
@@ -1451,23 +1937,46 @@
 static inline void
 rt_mutex_fastunlock(struct rt_mutex *lock,
 		    bool (*slowfn)(struct rt_mutex *lock,
-				   struct wake_q_head *wqh))
+				   struct wake_q_head *wqh,
+				   struct wake_q_head *wq_sleeper))
 {
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 
 	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
 		return;
 
-	if (slowfn(lock, &wake_q))
-		rt_mutex_postunlock(&wake_q);
+	if (slowfn(lock, &wake_q, &wake_sleeper_q))
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 }
 
-static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)
+int __sched __rt_mutex_lock_state(struct rt_mutex *lock, int state)
 {
 	might_sleep();
+	return rt_mutex_fastlock(lock, state, NULL, rt_mutex_slowlock);
+}
+
+/**
+ * rt_mutex_lock_state - lock a rt_mutex with a given state
+ *
+ * @lock:      The rt_mutex to be locked
+ * @state:     The state to set when blocking on the rt_mutex
+ */
+static inline int __sched rt_mutex_lock_state(struct rt_mutex *lock,
+					      unsigned int subclass, int state)
+{
+	int ret;
 
 	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
-	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);
+	ret = __rt_mutex_lock_state(lock, state);
+	if (ret)
+		mutex_release(&lock->dep_map, _RET_IP_);
+	return ret;
+}
+
+static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)
+{
+	rt_mutex_lock_state(lock, subclass, TASK_UNINTERRUPTIBLE);
 }
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
@@ -1508,16 +2017,7 @@
  */
 int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)
 {
-	int ret;
-
-	might_sleep();
-
-	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
-	ret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);
-	if (ret)
-		mutex_release(&lock->dep_map, _RET_IP_);
-
-	return ret;
+	return rt_mutex_lock_state(lock, 0, TASK_INTERRUPTIBLE);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_lock_interruptible);
 
@@ -1535,35 +2035,31 @@
 }
 
 /**
- * rt_mutex_timed_lock - lock a rt_mutex interruptible
- *			the timeout structure is provided
- *			by the caller
+ * rt_mutex_lock_killable - lock a rt_mutex killable
  *
  * @lock:		the rt_mutex to be locked
- * @timeout:		timeout structure or NULL (no timeout)
  *
  * Returns:
  *  0		on success
  * -EINTR	when interrupted by a signal
- * -ETIMEDOUT	when the timeout expired
  */
-int
-rt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)
+int __sched rt_mutex_lock_killable(struct rt_mutex *lock)
 {
-	int ret;
-
-	might_sleep();
+	return rt_mutex_lock_state(lock, 0, TASK_KILLABLE);
+}
+EXPORT_SYMBOL_GPL(rt_mutex_lock_killable);
 
-	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
-	ret = rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
-				       RT_MUTEX_MIN_CHAINWALK,
-				       rt_mutex_slowlock);
-	if (ret)
-		mutex_release(&lock->dep_map, _RET_IP_);
+int __sched __rt_mutex_trylock(struct rt_mutex *lock)
+{
+#ifdef CONFIG_PREEMPT_RT
+	if (WARN_ON_ONCE(in_irq() || in_nmi()))
+#else
+	if (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))
+#endif
+		return 0;
 
-	return ret;
+	return rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
 }
-EXPORT_SYMBOL_GPL(rt_mutex_timed_lock);
 
 /**
  * rt_mutex_trylock - try to lock a rt_mutex
@@ -1580,10 +2076,7 @@
 {
 	int ret;
 
-	if (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))
-		return 0;
-
-	ret = rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
+	ret = __rt_mutex_trylock(lock);
 	if (ret)
 		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
 
@@ -1591,6 +2084,11 @@
 }
 EXPORT_SYMBOL_GPL(rt_mutex_trylock);
 
+void __sched __rt_mutex_unlock(struct rt_mutex *lock)
+{
+	rt_mutex_fastunlock(lock, rt_mutex_slowunlock);
+}
+
 /**
  * rt_mutex_unlock - unlock a rt_mutex
  *
@@ -1599,16 +2097,13 @@
 void __sched rt_mutex_unlock(struct rt_mutex *lock)
 {
 	mutex_release(&lock->dep_map, _RET_IP_);
-	rt_mutex_fastunlock(lock, rt_mutex_slowunlock);
+	__rt_mutex_unlock(lock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_unlock);
 
-/**
- * Futex variant, that since futex variants do not use the fast-path, can be
- * simple and will not need to retry.
- */
-bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
-				    struct wake_q_head *wake_q)
+static bool __sched __rt_mutex_unlock_common(struct rt_mutex *lock,
+					     struct wake_q_head *wake_q,
+					     struct wake_q_head *wq_sleeper)
 {
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -1625,23 +2120,35 @@
 	 * avoid inversion prior to the wakeup.  preempt_disable()
 	 * therein pairs with rt_mutex_postunlock().
 	 */
-	mark_wakeup_next_waiter(wake_q, lock);
+	mark_wakeup_next_waiter(wake_q, wq_sleeper, lock);
 
 	return true; /* call postunlock() */
 }
 
+/**
+ * Futex variant, that since futex variants do not use the fast-path, can be
+ * simple and will not need to retry.
+ */
+bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
+				     struct wake_q_head *wake_q,
+				     struct wake_q_head *wq_sleeper)
+{
+	return __rt_mutex_unlock_common(lock, wake_q, wq_sleeper);
+}
+
 void __sched rt_mutex_futex_unlock(struct rt_mutex *lock)
 {
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 	unsigned long flags;
 	bool postunlock;
 
 	raw_spin_lock_irqsave(&lock->wait_lock, flags);
-	postunlock = __rt_mutex_futex_unlock(lock, &wake_q);
+	postunlock = __rt_mutex_futex_unlock(lock, &wake_q, &wake_sleeper_q);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	if (postunlock)
-		rt_mutex_postunlock(&wake_q);
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 }
 
 /**
@@ -1655,9 +2162,6 @@
 void rt_mutex_destroy(struct rt_mutex *lock)
 {
 	WARN_ON(rt_mutex_is_locked(lock));
-#ifdef CONFIG_DEBUG_RT_MUTEXES
-	lock->magic = NULL;
-#endif
 }
 EXPORT_SYMBOL_GPL(rt_mutex_destroy);
 
@@ -1680,7 +2184,7 @@
 	if (name && key)
 		debug_rt_mutex_init(lock, name, key);
 }
-EXPORT_SYMBOL_GPL(__rt_mutex_init);
+EXPORT_SYMBOL(__rt_mutex_init);
 
 /**
  * rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a
@@ -1700,6 +2204,14 @@
 				struct task_struct *proxy_owner)
 {
 	__rt_mutex_init(lock, NULL, NULL);
+#ifdef CONFIG_DEBUG_SPINLOCK
+	/*
+	 * get another key class for the wait_lock. LOCK_PI and UNLOCK_PI is
+	 * holding the ->wait_lock of the proxy_lock while unlocking a sleeping
+	 * lock.
+	 */
+	raw_spin_lock_init(&lock->wait_lock);
+#endif
 	debug_rt_mutex_proxy_lock(lock, proxy_owner);
 	rt_mutex_set_owner(lock, proxy_owner);
 }
@@ -1723,6 +2235,26 @@
 	rt_mutex_set_owner(lock, NULL);
 }
 
+static void fixup_rt_mutex_blocked(struct rt_mutex *lock)
+{
+	struct task_struct *tsk = current;
+	/*
+	 * RT has a problem here when the wait got interrupted by a timeout
+	 * or a signal. task->pi_blocked_on is still set. The task must
+	 * acquire the hash bucket lock when returning from this function.
+	 *
+	 * If the hash bucket lock is contended then the
+	 * BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on)) in
+	 * task_blocks_on_rt_mutex() will trigger. This can be avoided by
+	 * clearing task->pi_blocked_on which removes the task from the
+	 * boosting chain of the rtmutex. That's correct because the task
+	 * is not longer blocked on it.
+	 */
+	raw_spin_lock(&tsk->pi_lock);
+	tsk->pi_blocked_on = NULL;
+	raw_spin_unlock(&tsk->pi_lock);
+}
+
 /**
  * __rt_mutex_start_proxy_lock() - Start lock acquisition for another task
  * @lock:		the rt_mutex to take
@@ -1753,6 +2285,34 @@
 	if (try_to_take_rt_mutex(lock, task, NULL))
 		return 1;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * In PREEMPT_RT there's an added race.
+	 * If the task, that we are about to requeue, times out,
+	 * it can set the PI_WAKEUP_INPROGRESS. This tells the requeue
+	 * to skip this task. But right after the task sets
+	 * its pi_blocked_on to PI_WAKEUP_INPROGRESS it can then
+	 * block on the spin_lock(&hb->lock), which in RT is an rtmutex.
+	 * This will replace the PI_WAKEUP_INPROGRESS with the actual
+	 * lock that it blocks on. We *must not* place this task
+	 * on this proxy lock in that case.
+	 *
+	 * To prevent this race, we first take the task's pi_lock
+	 * and check if it has updated its pi_blocked_on. If it has,
+	 * we assume that it woke up and we return -EAGAIN.
+	 * Otherwise, we set the task's pi_blocked_on to
+	 * PI_REQUEUE_INPROGRESS, so that if the task is waking up
+	 * it will know that we are in the process of requeuing it.
+	 */
+	raw_spin_lock(&task->pi_lock);
+	if (task->pi_blocked_on) {
+		raw_spin_unlock(&task->pi_lock);
+		return -EAGAIN;
+	}
+	task->pi_blocked_on = PI_REQUEUE_INPROGRESS;
+	raw_spin_unlock(&task->pi_lock);
+#endif
+
 	/* We enforce deadlock detection for futexes */
 	ret = task_blocks_on_rt_mutex(lock, waiter, task,
 				      RT_MUTEX_FULL_CHAINWALK);
@@ -1767,7 +2327,8 @@
 		ret = 0;
 	}
 
-	debug_rt_mutex_print_deadlock(waiter);
+	if (ret)
+		fixup_rt_mutex_blocked(lock);
 
 	return ret;
 }
@@ -1852,12 +2413,15 @@
 	raw_spin_lock_irq(&lock->wait_lock);
 	/* sleep on the mutex */
 	set_current_state(TASK_INTERRUPTIBLE);
-	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);
+	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter, NULL);
 	/*
 	 * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might
 	 * have to fix that up.
 	 */
 	fixup_rt_mutex_waiters(lock);
+	if (ret)
+		fixup_rt_mutex_blocked(lock);
+
 	raw_spin_unlock_irq(&lock->wait_lock);
 
 	return ret;
@@ -1919,3 +2483,97 @@
 
 	return cleanup;
 }
+
+static inline int
+ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH
+	unsigned int tmp;
+
+	if (ctx->deadlock_inject_countdown-- == 0) {
+		tmp = ctx->deadlock_inject_interval;
+		if (tmp > UINT_MAX/4)
+			tmp = UINT_MAX;
+		else
+			tmp = tmp*2 + tmp + tmp/2;
+
+		ctx->deadlock_inject_interval = tmp;
+		ctx->deadlock_inject_countdown = tmp;
+		ctx->contending_lock = lock;
+
+		ww_mutex_unlock(lock);
+
+		return -EDEADLK;
+	}
+#endif
+
+	return 0;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+int __sched
+ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0,
+			   ctx ? &ctx->dep_map : NULL, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_INTERRUPTIBLE, NULL, 0,
+				ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, _RET_IP_);
+	else if (!ret && ctx && ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ww_mutex_lock_interruptible);
+
+int __sched
+ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0,
+			   ctx ? &ctx->dep_map : NULL, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_UNINTERRUPTIBLE, NULL, 0,
+				ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, _RET_IP_);
+	else if (!ret && ctx && ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ww_mutex_lock);
+
+void __sched ww_mutex_unlock(struct ww_mutex *lock)
+{
+	/*
+	 * The unlocking fastpath is the 0->1 transition from 'locked'
+	 * into 'unlocked' state:
+	 */
+	if (lock->ctx) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);
+#endif
+		if (lock->ctx->acquired > 0)
+			lock->ctx->acquired--;
+		lock->ctx = NULL;
+	}
+
+	mutex_release(&lock->base.dep_map, _RET_IP_);
+	__rt_mutex_unlock(&lock->base.lock);
+}
+EXPORT_SYMBOL(ww_mutex_unlock);
+
+int __rt_mutex_owner_current(struct rt_mutex *lock)
+{
+	return rt_mutex_owner(lock) == current;
+}
+EXPORT_SYMBOL(__rt_mutex_owner_current);
+#endif
diff -Naur a/kernel/locking/rtmutex_common.h b/kernel/locking/rtmutex_common.h
--- a/kernel/locking/rtmutex_common.h	2020-11-23 13:48:35.109948248 +0200
+++ b/kernel/locking/rtmutex_common.h	2021-07-14 15:39:11.110140417 +0300
@@ -15,6 +15,7 @@
 
 #include <linux/rtmutex.h>
 #include <linux/sched/wake_q.h>
+#include <linux/sched/debug.h>
 
 /*
  * This is the control structure for tasks blocked on a rt_mutex,
@@ -29,12 +30,8 @@
 	struct rb_node          pi_tree_entry;
 	struct task_struct	*task;
 	struct rt_mutex		*lock;
-#ifdef CONFIG_DEBUG_RT_MUTEXES
-	unsigned long		ip;
-	struct pid		*deadlock_task_pid;
-	struct rt_mutex		*deadlock_lock;
-#endif
 	int prio;
+	bool			savestate;
 	u64 deadline;
 };
 
@@ -130,12 +127,15 @@
 /*
  * PI-futex support (proxy locking functions, etc.):
  */
+#define PI_WAKEUP_INPROGRESS	((struct rt_mutex_waiter *) 1)
+#define PI_REQUEUE_INPROGRESS	((struct rt_mutex_waiter *) 2)
+
 extern struct task_struct *rt_mutex_next_owner(struct rt_mutex *lock);
 extern void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
 				       struct task_struct *proxy_owner);
 extern void rt_mutex_proxy_unlock(struct rt_mutex *lock,
 				  struct task_struct *proxy_owner);
-extern void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter);
+extern void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savetate);
 extern int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 				     struct rt_mutex_waiter *waiter,
 				     struct task_struct *task);
@@ -153,9 +153,27 @@
 
 extern void rt_mutex_futex_unlock(struct rt_mutex *lock);
 extern bool __rt_mutex_futex_unlock(struct rt_mutex *lock,
-				 struct wake_q_head *wqh);
+				 struct wake_q_head *wqh,
+				 struct wake_q_head *wq_sleeper);
+
+extern void rt_mutex_postunlock(struct wake_q_head *wake_q,
+				struct wake_q_head *wake_sleeper_q);
+
+/* RW semaphore special interface */
+struct ww_acquire_ctx;
 
-extern void rt_mutex_postunlock(struct wake_q_head *wake_q);
+extern int __rt_mutex_lock_state(struct rt_mutex *lock, int state);
+extern int __rt_mutex_trylock(struct rt_mutex *lock);
+extern void __rt_mutex_unlock(struct rt_mutex *lock);
+int __sched rt_mutex_slowlock_locked(struct rt_mutex *lock, int state,
+				     struct hrtimer_sleeper *timeout,
+				     enum rtmutex_chainwalk chwalk,
+				     struct ww_acquire_ctx *ww_ctx,
+				     struct rt_mutex_waiter *waiter);
+void __sched rt_spin_lock_slowlock_locked(struct rt_mutex *lock,
+					  struct rt_mutex_waiter *waiter,
+					  unsigned long flags);
+void __sched rt_spin_lock_slowunlock(struct rt_mutex *lock);
 
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 # include "rtmutex-debug.h"
diff -Naur a/kernel/locking/rtmutex-debug.c b/kernel/locking/rtmutex-debug.c
--- a/kernel/locking/rtmutex-debug.c	2020-11-23 13:48:35.105948167 +0200
+++ b/kernel/locking/rtmutex-debug.c	2021-07-14 15:39:11.110140417 +0300
@@ -32,110 +32,12 @@
 
 #include "rtmutex_common.h"
 
-static void printk_task(struct task_struct *p)
-{
-	if (p)
-		printk("%16s:%5d [%p, %3d]", p->comm, task_pid_nr(p), p, p->prio);
-	else
-		printk("<none>");
-}
-
-static void printk_lock(struct rt_mutex *lock, int print_owner)
-{
-	if (lock->name)
-		printk(" [%p] {%s}\n",
-			lock, lock->name);
-	else
-		printk(" [%p] {%s:%d}\n",
-			lock, lock->file, lock->line);
-
-	if (print_owner && rt_mutex_owner(lock)) {
-		printk(".. ->owner: %p\n", lock->owner);
-		printk(".. held by:  ");
-		printk_task(rt_mutex_owner(lock));
-		printk("\n");
-	}
-}
-
 void rt_mutex_debug_task_free(struct task_struct *task)
 {
 	DEBUG_LOCKS_WARN_ON(!RB_EMPTY_ROOT(&task->pi_waiters.rb_root));
 	DEBUG_LOCKS_WARN_ON(task->pi_blocked_on);
 }
 
-/*
- * We fill out the fields in the waiter to store the information about
- * the deadlock. We print when we return. act_waiter can be NULL in
- * case of a remove waiter operation.
- */
-void debug_rt_mutex_deadlock(enum rtmutex_chainwalk chwalk,
-			     struct rt_mutex_waiter *act_waiter,
-			     struct rt_mutex *lock)
-{
-	struct task_struct *task;
-
-	if (!debug_locks || chwalk == RT_MUTEX_FULL_CHAINWALK || !act_waiter)
-		return;
-
-	task = rt_mutex_owner(act_waiter->lock);
-	if (task && task != current) {
-		act_waiter->deadlock_task_pid = get_pid(task_pid(task));
-		act_waiter->deadlock_lock = lock;
-	}
-}
-
-void debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter)
-{
-	struct task_struct *task;
-
-	if (!waiter->deadlock_lock || !debug_locks)
-		return;
-
-	rcu_read_lock();
-	task = pid_task(waiter->deadlock_task_pid, PIDTYPE_PID);
-	if (!task) {
-		rcu_read_unlock();
-		return;
-	}
-
-	if (!debug_locks_off()) {
-		rcu_read_unlock();
-		return;
-	}
-
-	pr_warn("\n");
-	pr_warn("============================================\n");
-	pr_warn("WARNING: circular locking deadlock detected!\n");
-	pr_warn("%s\n", print_tainted());
-	pr_warn("--------------------------------------------\n");
-	printk("%s/%d is deadlocking current task %s/%d\n\n",
-	       task->comm, task_pid_nr(task),
-	       current->comm, task_pid_nr(current));
-
-	printk("\n1) %s/%d is trying to acquire this lock:\n",
-	       current->comm, task_pid_nr(current));
-	printk_lock(waiter->lock, 1);
-
-	printk("\n2) %s/%d is blocked on this lock:\n",
-		task->comm, task_pid_nr(task));
-	printk_lock(waiter->deadlock_lock, 1);
-
-	debug_show_held_locks(current);
-	debug_show_held_locks(task);
-
-	printk("\n%s/%d's [blocked] stackdump:\n\n",
-		task->comm, task_pid_nr(task));
-	show_stack(task, NULL, KERN_DEFAULT);
-	printk("\n%s/%d's [current] stackdump:\n\n",
-		current->comm, task_pid_nr(current));
-	dump_stack();
-	debug_show_all_locks();
-	rcu_read_unlock();
-
-	printk("[ turning off deadlock detection."
-	       "Please report this trace. ]\n\n");
-}
-
 void debug_rt_mutex_lock(struct rt_mutex *lock)
 {
 }
@@ -158,12 +60,10 @@
 void debug_rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
 {
 	memset(waiter, 0x11, sizeof(*waiter));
-	waiter->deadlock_task_pid = NULL;
 }
 
 void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)
 {
-	put_pid(waiter->deadlock_task_pid);
 	memset(waiter, 0x22, sizeof(*waiter));
 }
 
@@ -173,10 +73,8 @@
 	 * Make sure we are not reinitializing a held lock:
 	 */
 	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
-	lock->name = name;
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	lockdep_init_map(&lock->dep_map, name, key, 0);
 #endif
 }
-
diff -Naur a/kernel/locking/rtmutex-debug.h b/kernel/locking/rtmutex-debug.h
--- a/kernel/locking/rtmutex-debug.h	2020-11-23 13:48:35.105948167 +0200
+++ b/kernel/locking/rtmutex-debug.h	2021-07-14 15:39:11.110140417 +0300
@@ -18,20 +18,9 @@
 extern void debug_rt_mutex_proxy_lock(struct rt_mutex *lock,
 				      struct task_struct *powner);
 extern void debug_rt_mutex_proxy_unlock(struct rt_mutex *lock);
-extern void debug_rt_mutex_deadlock(enum rtmutex_chainwalk chwalk,
-				    struct rt_mutex_waiter *waiter,
-				    struct rt_mutex *lock);
-extern void debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter);
-# define debug_rt_mutex_reset_waiter(w)			\
-	do { (w)->deadlock_lock = NULL; } while (0)
 
 static inline bool debug_rt_mutex_detect_deadlock(struct rt_mutex_waiter *waiter,
 						  enum rtmutex_chainwalk walk)
 {
 	return (waiter != NULL);
 }
-
-static inline void rt_mutex_print_deadlock(struct rt_mutex_waiter *w)
-{
-	debug_rt_mutex_print_deadlock(w);
-}
diff -Naur a/kernel/locking/rtmutex.h b/kernel/locking/rtmutex.h
--- a/kernel/locking/rtmutex.h	2020-11-23 13:48:35.105948167 +0200
+++ b/kernel/locking/rtmutex.h	2021-07-14 15:39:11.110140417 +0300
@@ -19,15 +19,8 @@
 #define debug_rt_mutex_proxy_unlock(l)			do { } while (0)
 #define debug_rt_mutex_unlock(l)			do { } while (0)
 #define debug_rt_mutex_init(m, n, k)			do { } while (0)
-#define debug_rt_mutex_deadlock(d, a ,l)		do { } while (0)
-#define debug_rt_mutex_print_deadlock(w)		do { } while (0)
 #define debug_rt_mutex_reset_waiter(w)			do { } while (0)
 
-static inline void rt_mutex_print_deadlock(struct rt_mutex_waiter *w)
-{
-	WARN(1, "rtmutex deadlock detected\n");
-}
-
 static inline bool debug_rt_mutex_detect_deadlock(struct rt_mutex_waiter *w,
 						  enum rtmutex_chainwalk walk)
 {
diff -Naur a/kernel/locking/rwlock-rt.c b/kernel/locking/rwlock-rt.c
--- a/kernel/locking/rwlock-rt.c	1970-01-01 02:00:00.000000000 +0200
+++ b/kernel/locking/rwlock-rt.c	2021-07-14 15:39:11.110140417 +0300
@@ -0,0 +1,334 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#include <linux/sched/debug.h>
+#include <linux/export.h>
+
+#include "rtmutex_common.h"
+#include <linux/rwlock_types_rt.h>
+
+/*
+ * RT-specific reader/writer locks
+ *
+ * write_lock()
+ *  1) Lock lock->rtmutex
+ *  2) Remove the reader BIAS to force readers into the slow path
+ *  3) Wait until all readers have left the critical region
+ *  4) Mark it write locked
+ *
+ * write_unlock()
+ *  1) Remove the write locked marker
+ *  2) Set the reader BIAS so readers can use the fast path again
+ *  3) Unlock lock->rtmutex to release blocked readers
+ *
+ * read_lock()
+ *  1) Try fast path acquisition (reader BIAS is set)
+ *  2) Take lock->rtmutex.wait_lock which protects the writelocked flag
+ *  3) If !writelocked, acquire it for read
+ *  4) If writelocked, block on lock->rtmutex
+ *  5) unlock lock->rtmutex, goto 1)
+ *
+ * read_unlock()
+ *  1) Try fast path release (reader count != 1)
+ *  2) Wake the writer waiting in write_lock()#3
+ *
+ * read_lock()#3 has the consequence, that rw locks on RT are not writer
+ * fair, but writers, which should be avoided in RT tasks (think tasklist
+ * lock), are subject to the rtmutex priority/DL inheritance mechanism.
+ *
+ * It's possible to make the rw locks writer fair by keeping a list of
+ * active readers. A blocked writer would force all newly incoming readers
+ * to block on the rtmutex, but the rtmutex would have to be proxy locked
+ * for one reader after the other. We can't use multi-reader inheritance
+ * because there is no way to support that with
+ * SCHED_DEADLINE. Implementing the one by one reader boosting/handover
+ * mechanism is a major surgery for a very dubious value.
+ *
+ * The risk of writer starvation is there, but the pathological use cases
+ * which trigger it are not necessarily the typical RT workloads.
+ */
+
+void __rwlock_biased_rt_init(struct rt_rw_lock *lock, const char *name,
+			     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held semaphore:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+	atomic_set(&lock->readers, READER_BIAS);
+	rt_mutex_init(&lock->rtmutex);
+	lock->rtmutex.save_state = 1;
+}
+
+int __read_rt_trylock(struct rt_rw_lock *lock)
+{
+	int r, old;
+
+	/*
+	 * Increment reader count, if lock->readers < 0, i.e. READER_BIAS is
+	 * set.
+	 */
+	for (r = atomic_read(&lock->readers); r < 0;) {
+		old = atomic_cmpxchg(&lock->readers, r, r + 1);
+		if (likely(old == r))
+			return 1;
+		r = old;
+	}
+	return 0;
+}
+
+static void __read_rt_lock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+
+	if (__read_rt_trylock(lock))
+		return;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	/*
+	 * Allow readers as long as the writer has not completely
+	 * acquired the semaphore for write.
+	 */
+	if (atomic_read(&lock->readers) != WRITER_BIAS) {
+		atomic_inc(&lock->readers);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return;
+	}
+
+	/*
+	 * Call into the slow lock path with the rtmutex->wait_lock
+	 * held, so this can't result in the following race:
+	 *
+	 * Reader1		Reader2		Writer
+	 *			read_lock()
+	 *					write_lock()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * read_lock()
+	 * unlock(m->wait_lock)
+	 *			read_unlock()
+	 *			swake()
+	 *					lock(m->wait_lock)
+	 *					lock->writelocked=true
+	 *					unlock(m->wait_lock)
+	 *
+	 *					write_unlock()
+	 *					lock->writelocked=false
+	 *					rtmutex_unlock(m)
+	 *			read_lock()
+	 *					write_lock()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * rtmutex_lock(m)
+	 *
+	 * That would put Reader1 behind the writer waiting on
+	 * Reader2 to call read_unlock() which might be unbound.
+	 */
+	rt_mutex_init_waiter(&waiter, true);
+	rt_spin_lock_slowlock_locked(m, &waiter, flags);
+	/*
+	 * The slowlock() above is guaranteed to return with the rtmutex is
+	 * now held, so there can't be a writer active. Increment the reader
+	 * count and immediately drop the rtmutex again.
+	 */
+	atomic_inc(&lock->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	rt_spin_lock_slowunlock(m);
+
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+static void __read_rt_unlock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct task_struct *tsk;
+
+	/*
+	 * sem->readers can only hit 0 when a writer is waiting for the
+	 * active readers to leave the critical region.
+	 */
+	if (!atomic_dec_and_test(&lock->readers))
+		return;
+
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Wake the writer, i.e. the rtmutex owner. It might release the
+	 * rtmutex concurrently in the fast path, but to clean up the rw
+	 * lock it needs to acquire m->wait_lock. The worst case which can
+	 * happen is a spurious wakeup.
+	 */
+	tsk = rt_mutex_owner(m);
+	if (tsk)
+		wake_up_process(tsk);
+
+	raw_spin_unlock_irq(&m->wait_lock);
+}
+
+static void __write_unlock_common(struct rt_rw_lock *lock, int bias,
+				  unsigned long flags)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+
+	atomic_add(READER_BIAS - bias, &lock->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	rt_spin_lock_slowunlock(m);
+}
+
+static void __write_rt_lock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct task_struct *self = current;
+	unsigned long flags;
+
+	/* Take the rtmutex as a first step */
+	__rt_spin_lock(m);
+
+	/* Force readers into slow path */
+	atomic_sub(READER_BIAS, &lock->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+
+	raw_spin_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock(&self->pi_lock);
+
+	for (;;) {
+		/* Have all readers left the critical region? */
+		if (!atomic_read(&lock->readers)) {
+			atomic_set(&lock->readers, WRITER_BIAS);
+			raw_spin_lock(&self->pi_lock);
+			__set_current_state_no_track(self->saved_state);
+			self->saved_state = TASK_RUNNING;
+			raw_spin_unlock(&self->pi_lock);
+			raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+			return;
+		}
+
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+
+		if (atomic_read(&lock->readers) != 0)
+			preempt_schedule_lock();
+
+		raw_spin_lock_irqsave(&m->wait_lock, flags);
+
+		raw_spin_lock(&self->pi_lock);
+		__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+		raw_spin_unlock(&self->pi_lock);
+	}
+}
+
+static int __write_rt_trylock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	unsigned long flags;
+
+	if (!__rt_mutex_trylock(m))
+		return 0;
+
+	atomic_sub(READER_BIAS, &lock->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	if (!atomic_read(&lock->readers)) {
+		atomic_set(&lock->readers, WRITER_BIAS);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return 1;
+	}
+	__write_unlock_common(lock, 0, flags);
+	return 0;
+}
+
+static void __write_rt_unlock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	__write_unlock_common(lock, WRITER_BIAS, flags);
+}
+
+int __lockfunc rt_read_can_lock(rwlock_t *rwlock)
+{
+	return  atomic_read(&rwlock->readers) < 0;
+}
+
+int __lockfunc rt_write_can_lock(rwlock_t *rwlock)
+{
+	return atomic_read(&rwlock->readers) == READER_BIAS;
+}
+
+/*
+ * The common functions which get wrapped into the rwlock API.
+ */
+int __lockfunc rt_read_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	ret = __read_rt_trylock(rwlock);
+	if (ret) {
+		rwlock_acquire_read(&rwlock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_read_trylock);
+
+int __lockfunc rt_write_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	ret = __write_rt_trylock(rwlock);
+	if (ret) {
+		rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+		migrate_disable();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_write_trylock);
+
+void __lockfunc rt_read_lock(rwlock_t *rwlock)
+{
+	rwlock_acquire_read(&rwlock->dep_map, 0, 0, _RET_IP_);
+	__read_rt_lock(rwlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_read_lock);
+
+void __lockfunc rt_write_lock(rwlock_t *rwlock)
+{
+	rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
+	__write_rt_lock(rwlock);
+	rcu_read_lock();
+	migrate_disable();
+}
+EXPORT_SYMBOL(rt_write_lock);
+
+void __lockfunc rt_read_unlock(rwlock_t *rwlock)
+{
+	rwlock_release(&rwlock->dep_map, _RET_IP_);
+	migrate_enable();
+	rcu_read_unlock();
+	__read_rt_unlock(rwlock);
+}
+EXPORT_SYMBOL(rt_read_unlock);
+
+void __lockfunc rt_write_unlock(rwlock_t *rwlock)
+{
+	rwlock_release(&rwlock->dep_map, _RET_IP_);
+	migrate_enable();
+	rcu_read_unlock();
+	__write_rt_unlock(rwlock);
+}
+EXPORT_SYMBOL(rt_write_unlock);
+
+void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key)
+{
+	__rwlock_biased_rt_init(rwlock, name, key);
+}
+EXPORT_SYMBOL(__rt_rwlock_init);
diff -Naur a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
--- a/kernel/locking/rwsem.c	2020-11-23 13:48:35.109948248 +0200
+++ b/kernel/locking/rwsem.c	2021-07-14 15:39:11.114140389 +0300
@@ -28,6 +28,7 @@
 #include <linux/rwsem.h>
 #include <linux/atomic.h>
 
+#ifndef CONFIG_PREEMPT_RT
 #include "lock_events.h"
 
 /*
@@ -1482,6 +1483,7 @@
 	if (tmp & RWSEM_FLAG_WAITERS)
 		rwsem_downgrade_wake(sem);
 }
+#endif
 
 /*
  * lock for reading
@@ -1617,7 +1619,9 @@
 {
 	might_sleep();
 	__down_read(sem);
+#ifndef CONFIG_PREEMPT_RT
 	__rwsem_set_reader_owned(sem, NULL);
+#endif
 }
 EXPORT_SYMBOL(down_read_non_owner);
 
@@ -1646,7 +1650,9 @@
 
 void up_read_non_owner(struct rw_semaphore *sem)
 {
+#ifndef CONFIG_PREEMPT_RT
 	DEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);
+#endif
 	__up_read(sem);
 }
 EXPORT_SYMBOL(up_read_non_owner);
diff -Naur a/kernel/locking/rwsem-rt.c b/kernel/locking/rwsem-rt.c
--- a/kernel/locking/rwsem-rt.c	1970-01-01 02:00:00.000000000 +0200
+++ b/kernel/locking/rwsem-rt.c	2021-07-14 15:39:11.110140417 +0300
@@ -0,0 +1,292 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#include <linux/rwsem.h>
+#include <linux/sched/debug.h>
+#include <linux/sched/signal.h>
+#include <linux/export.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * RT-specific reader/writer semaphores
+ *
+ * down_write()
+ *  1) Lock sem->rtmutex
+ *  2) Remove the reader BIAS to force readers into the slow path
+ *  3) Wait until all readers have left the critical region
+ *  4) Mark it write locked
+ *
+ * up_write()
+ *  1) Remove the write locked marker
+ *  2) Set the reader BIAS so readers can use the fast path again
+ *  3) Unlock sem->rtmutex to release blocked readers
+ *
+ * down_read()
+ *  1) Try fast path acquisition (reader BIAS is set)
+ *  2) Take sem->rtmutex.wait_lock which protects the writelocked flag
+ *  3) If !writelocked, acquire it for read
+ *  4) If writelocked, block on sem->rtmutex
+ *  5) unlock sem->rtmutex, goto 1)
+ *
+ * up_read()
+ *  1) Try fast path release (reader count != 1)
+ *  2) Wake the writer waiting in down_write()#3
+ *
+ * down_read()#3 has the consequence, that rw semaphores on RT are not writer
+ * fair, but writers, which should be avoided in RT tasks (think mmap_sem),
+ * are subject to the rtmutex priority/DL inheritance mechanism.
+ *
+ * It's possible to make the rw semaphores writer fair by keeping a list of
+ * active readers. A blocked writer would force all newly incoming readers to
+ * block on the rtmutex, but the rtmutex would have to be proxy locked for one
+ * reader after the other. We can't use multi-reader inheritance because there
+ * is no way to support that with SCHED_DEADLINE. Implementing the one by one
+ * reader boosting/handover mechanism is a major surgery for a very dubious
+ * value.
+ *
+ * The risk of writer starvation is there, but the pathological use cases
+ * which trigger it are not necessarily the typical RT workloads.
+ */
+
+void __rwsem_init(struct rw_semaphore *sem, const char *name,
+		  struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held semaphore:
+	 */
+	debug_check_no_locks_freed((void *)sem, sizeof(*sem));
+	lockdep_init_map(&sem->dep_map, name, key, 0);
+#endif
+	atomic_set(&sem->readers, READER_BIAS);
+}
+EXPORT_SYMBOL(__rwsem_init);
+
+int __down_read_trylock(struct rw_semaphore *sem)
+{
+	int r, old;
+
+	/*
+	 * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is
+	 * set.
+	 */
+	for (r = atomic_read(&sem->readers); r < 0;) {
+		old = atomic_cmpxchg(&sem->readers, r, r + 1);
+		if (likely(old == r))
+			return 1;
+		r = old;
+	}
+	return 0;
+}
+
+static int __sched __down_read_common(struct rw_semaphore *sem, int state)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	struct rt_mutex_waiter waiter;
+	int ret;
+
+	if (__down_read_trylock(sem))
+		return 0;
+
+	might_sleep();
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Allow readers as long as the writer has not completely
+	 * acquired the semaphore for write.
+	 */
+	if (atomic_read(&sem->readers) != WRITER_BIAS) {
+		atomic_inc(&sem->readers);
+		raw_spin_unlock_irq(&m->wait_lock);
+		return 0;
+	}
+
+	/*
+	 * Call into the slow lock path with the rtmutex->wait_lock
+	 * held, so this can't result in the following race:
+	 *
+	 * Reader1		Reader2		Writer
+	 *			down_read()
+	 *					down_write()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * down_read()
+	 * unlock(m->wait_lock)
+	 *			up_read()
+	 *			swake()
+	 *					lock(m->wait_lock)
+	 *					sem->writelocked=true
+	 *					unlock(m->wait_lock)
+	 *
+	 *					up_write()
+	 *					sem->writelocked=false
+	 *					rtmutex_unlock(m)
+	 *			down_read()
+	 *					down_write()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * rtmutex_lock(m)
+	 *
+	 * That would put Reader1 behind the writer waiting on
+	 * Reader2 to call up_read() which might be unbound.
+	 */
+	rt_mutex_init_waiter(&waiter, false);
+	ret = rt_mutex_slowlock_locked(m, state, NULL, RT_MUTEX_MIN_CHAINWALK,
+				       NULL, &waiter);
+	/*
+	 * The slowlock() above is guaranteed to return with the rtmutex (for
+	 * ret = 0) is now held, so there can't be a writer active. Increment
+	 * the reader count and immediately drop the rtmutex again.
+	 * For ret != 0 we don't hold the rtmutex and need unlock the wait_lock.
+	 * We don't own the lock then.
+	 */
+	if (!ret)
+		atomic_inc(&sem->readers);
+	raw_spin_unlock_irq(&m->wait_lock);
+	if (!ret)
+		__rt_mutex_unlock(m);
+
+	debug_rt_mutex_free_waiter(&waiter);
+	return ret;
+}
+
+void __down_read(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_UNINTERRUPTIBLE);
+	WARN_ON_ONCE(ret);
+}
+
+int __down_read_killable(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_KILLABLE);
+	if (likely(!ret))
+		return ret;
+	WARN_ONCE(ret != -EINTR, "Unexpected state: %d\n", ret);
+	return -EINTR;
+}
+
+void __up_read(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	struct task_struct *tsk;
+
+	/*
+	 * sem->readers can only hit 0 when a writer is waiting for the
+	 * active readers to leave the critical region.
+	 */
+	if (!atomic_dec_and_test(&sem->readers))
+		return;
+
+	might_sleep();
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Wake the writer, i.e. the rtmutex owner. It might release the
+	 * rtmutex concurrently in the fast path (due to a signal), but to
+	 * clean up the rwsem it needs to acquire m->wait_lock. The worst
+	 * case which can happen is a spurious wakeup.
+	 */
+	tsk = rt_mutex_owner(m);
+	if (tsk)
+		wake_up_process(tsk);
+
+	raw_spin_unlock_irq(&m->wait_lock);
+}
+
+static void __up_write_unlock(struct rw_semaphore *sem, int bias,
+			      unsigned long flags)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+
+	atomic_add(READER_BIAS - bias, &sem->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	__rt_mutex_unlock(m);
+}
+
+static int __sched __down_write_common(struct rw_semaphore *sem, int state)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	/* Take the rtmutex as a first step */
+	if (__rt_mutex_lock_state(m, state))
+		return -EINTR;
+
+	/* Force readers into slow path */
+	atomic_sub(READER_BIAS, &sem->readers);
+	might_sleep();
+
+	set_current_state(state);
+	for (;;) {
+		raw_spin_lock_irqsave(&m->wait_lock, flags);
+		/* Have all readers left the critical region? */
+		if (!atomic_read(&sem->readers)) {
+			atomic_set(&sem->readers, WRITER_BIAS);
+			__set_current_state(TASK_RUNNING);
+			raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+			return 0;
+		}
+
+		if (signal_pending_state(state, current)) {
+			__set_current_state(TASK_RUNNING);
+			__up_write_unlock(sem, 0, flags);
+			return -EINTR;
+		}
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+
+		if (atomic_read(&sem->readers) != 0) {
+			schedule();
+			set_current_state(state);
+		}
+	}
+}
+
+void __sched __down_write(struct rw_semaphore *sem)
+{
+	__down_write_common(sem, TASK_UNINTERRUPTIBLE);
+}
+
+int __sched __down_write_killable(struct rw_semaphore *sem)
+{
+	return __down_write_common(sem, TASK_KILLABLE);
+}
+
+int __down_write_trylock(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	if (!__rt_mutex_trylock(m))
+		return 0;
+
+	atomic_sub(READER_BIAS, &sem->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	if (!atomic_read(&sem->readers)) {
+		atomic_set(&sem->readers, WRITER_BIAS);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return 1;
+	}
+	__up_write_unlock(sem, 0, flags);
+	return 0;
+}
+
+void __up_write(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	__up_write_unlock(sem, WRITER_BIAS, flags);
+}
+
+void __downgrade_write(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	/* Release it and account current as reader */
+	__up_write_unlock(sem, WRITER_BIAS - 1, flags);
+}
diff -Naur a/kernel/locking/semaphore.c b/kernel/locking/semaphore.c
--- a/kernel/locking/semaphore.c	2020-11-23 13:48:35.109948248 +0200
+++ b/kernel/locking/semaphore.c	2020-11-23 13:50:10.383440207 +0200
@@ -1,5 +1,10 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *
  * Copyright (c) 2008 Intel Corporation
  * Author: Matthew Wilcox <willy@linux.intel.com>
  *
@@ -37,6 +42,7 @@
 static noinline int __down_interruptible(struct semaphore *sem);
 static noinline int __down_killable(struct semaphore *sem);
 static noinline int __down_timeout(struct semaphore *sem, long timeout);
+static noinline int __down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout);
 static noinline void __up(struct semaphore *sem);
 
 /**
@@ -168,6 +174,22 @@
 }
 EXPORT_SYMBOL(down_timeout);
 
+int down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout)
+{
+	unsigned long flags;
+	int result = 0;
+
+	raw_spin_lock_irqsave(&sem->lock, flags);
+	if (likely(sem->count > 0))
+		sem->count--;
+	else
+		result = __down_hrtimeout(sem, timeout);
+	raw_spin_unlock_irqrestore(&sem->lock, flags);
+
+	return result;
+}
+EXPORT_SYMBOL(down_hrtimeout);
+
 /**
  * up - release the semaphore
  * @sem: the semaphore to release
@@ -232,6 +254,42 @@
 	return -EINTR;
 }
 
+/*
+ * Because this function is inlined, the 'state' parameter will be
+ * constant, and thus optimised away by the compiler.  Likewise the
+ * 'timeout' parameter for the cases without timeouts.
+ */
+static inline int __sched __down_common_hrtimeout(struct semaphore *sem, long state,
+								struct hrtimer_sleeper *timeout)
+{
+	struct semaphore_waiter waiter;
+
+	list_add_tail(&waiter.list, &sem->wait_list);
+	waiter.task = current;
+	waiter.up = false;
+
+	for (;;) {
+		if (signal_pending_state(state, current))
+			goto interrupted;
+		if (timeout && !timeout->task)
+			goto timed_out;
+		__set_current_state(state);
+		raw_spin_unlock_irq(&sem->lock);
+		schedule();
+		raw_spin_lock_irq(&sem->lock);
+		if (waiter.up)
+			return 0;
+	}
+
+ timed_out:
+	list_del(&waiter.list);
+	return -ETIME;
+
+ interrupted:
+	list_del(&waiter.list);
+	return -EINTR;
+}
+
 static noinline void __sched __down(struct semaphore *sem)
 {
 	__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
@@ -252,6 +310,11 @@
 	return __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);
 }
 
+static noinline int __sched __down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout)
+{
+	return __down_common_hrtimeout(sem, TASK_INTERRUPTIBLE, timeout);
+}
+
 static noinline void __sched __up(struct semaphore *sem)
 {
 	struct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,
diff -Naur a/kernel/locking/spinlock.c b/kernel/locking/spinlock.c
--- a/kernel/locking/spinlock.c	2020-11-23 13:48:35.109948248 +0200
+++ b/kernel/locking/spinlock.c	2021-07-14 15:39:11.114140389 +0300
@@ -124,8 +124,11 @@
  *         __[spin|read|write]_lock_bh()
  */
 BUILD_LOCK_OPS(spin, raw_spinlock);
+
+#ifndef CONFIG_PREEMPT_RT
 BUILD_LOCK_OPS(read, rwlock);
 BUILD_LOCK_OPS(write, rwlock);
+#endif
 
 #endif
 
@@ -209,6 +212,8 @@
 EXPORT_SYMBOL(_raw_spin_unlock_bh);
 #endif
 
+#ifndef CONFIG_PREEMPT_RT
+
 #ifndef CONFIG_INLINE_READ_TRYLOCK
 int __lockfunc _raw_read_trylock(rwlock_t *lock)
 {
@@ -353,6 +358,8 @@
 EXPORT_SYMBOL(_raw_write_unlock_bh);
 #endif
 
+#endif /* !PREEMPT_RT */
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
diff -Naur a/kernel/locking/spinlock_debug.c b/kernel/locking/spinlock_debug.c
--- a/kernel/locking/spinlock_debug.c	2020-11-23 13:48:35.109948248 +0200
+++ b/kernel/locking/spinlock_debug.c	2021-07-14 15:39:11.114140389 +0300
@@ -31,6 +31,7 @@
 
 EXPORT_SYMBOL(__raw_spin_lock_init);
 
+#ifndef CONFIG_PREEMPT_RT
 void __rwlock_init(rwlock_t *lock, const char *name,
 		   struct lock_class_key *key)
 {
@@ -48,6 +49,7 @@
 }
 
 EXPORT_SYMBOL(__rwlock_init);
+#endif
 
 static void spin_dump(raw_spinlock_t *lock, const char *msg)
 {
@@ -139,6 +141,7 @@
 	arch_spin_unlock(&lock->raw_lock);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 static void rwlock_bug(rwlock_t *lock, const char *msg)
 {
 	if (!debug_locks_off())
@@ -228,3 +231,5 @@
 	debug_write_unlock(lock);
 	arch_write_unlock(&lock->raw_lock);
 }
+
+#endif
diff -Naur a/kernel/panic.c b/kernel/panic.c
--- a/kernel/panic.c	2020-11-23 13:48:34.937944796 +0200
+++ b/kernel/panic.c	2021-07-14 15:39:10.886141993 +0300
@@ -247,7 +247,6 @@
 	 * Bypass the panic_cpu check and call __crash_kexec directly.
 	 */
 	if (!_crash_kexec_post_notifiers) {
-		printk_safe_flush_on_panic();
 		__crash_kexec(NULL);
 
 		/*
@@ -271,8 +270,6 @@
 	 */
 	atomic_notifier_call_chain(&panic_notifier_list, 0, buf);
 
-	/* Call flush even twice. It tries harder with a single online CPU */
-	printk_safe_flush_on_panic();
 	kmsg_dump(KMSG_DUMP_PANIC);
 
 	/*
@@ -542,9 +539,11 @@
 
 static int init_oops_id(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	if (!oops_id)
 		get_random_bytes(&oops_id, sizeof(oops_id));
 	else
+#endif
 		oops_id++;
 
 	return 0;
diff -Naur a/kernel/printk/internal.h b/kernel/printk/internal.h
--- a/kernel/printk/internal.h	2020-11-23 13:48:35.125948569 +0200
+++ b/kernel/printk/internal.h	1970-01-01 02:00:00.000000000 +0200
@@ -1,74 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-/*
- * internal.h - printk internal definitions
- */
-#include <linux/percpu.h>
-
-#ifdef CONFIG_PRINTK
-
-#define PRINTK_SAFE_CONTEXT_MASK	0x007ffffff
-#define PRINTK_NMI_DIRECT_CONTEXT_MASK	0x008000000
-#define PRINTK_NMI_CONTEXT_MASK		0xff0000000
-
-#define PRINTK_NMI_CONTEXT_OFFSET	0x010000000
-
-extern raw_spinlock_t logbuf_lock;
-
-__printf(5, 0)
-int vprintk_store(int facility, int level,
-		  const char *dict, size_t dictlen,
-		  const char *fmt, va_list args);
-
-__printf(1, 0) int vprintk_default(const char *fmt, va_list args);
-__printf(1, 0) int vprintk_deferred(const char *fmt, va_list args);
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args);
-void __printk_safe_enter(void);
-void __printk_safe_exit(void);
-
-void printk_safe_init(void);
-bool printk_percpu_data_ready(void);
-
-#define printk_safe_enter_irqsave(flags)	\
-	do {					\
-		local_irq_save(flags);		\
-		__printk_safe_enter();		\
-	} while (0)
-
-#define printk_safe_exit_irqrestore(flags)	\
-	do {					\
-		__printk_safe_exit();		\
-		local_irq_restore(flags);	\
-	} while (0)
-
-#define printk_safe_enter_irq()		\
-	do {					\
-		local_irq_disable();		\
-		__printk_safe_enter();		\
-	} while (0)
-
-#define printk_safe_exit_irq()			\
-	do {					\
-		__printk_safe_exit();		\
-		local_irq_enable();		\
-	} while (0)
-
-void defer_console_output(void);
-
-#else
-
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args) { return 0; }
-
-/*
- * In !PRINTK builds we still export logbuf_lock spin_lock, console_sem
- * semaphore and some of console functions (console_unlock()/etc.), so
- * printk-safe must preserve the existing local IRQ guarantees.
- */
-#define printk_safe_enter_irqsave(flags) local_irq_save(flags)
-#define printk_safe_exit_irqrestore(flags) local_irq_restore(flags)
-
-#define printk_safe_enter_irq() local_irq_disable()
-#define printk_safe_exit_irq() local_irq_enable()
-
-static inline void printk_safe_init(void) { }
-static inline bool printk_percpu_data_ready(void) { return false; }
-#endif /* CONFIG_PRINTK */
diff -Naur a/kernel/printk/Makefile b/kernel/printk/Makefile
--- a/kernel/printk/Makefile	2020-11-23 13:48:35.121948488 +0200
+++ b/kernel/printk/Makefile	2021-07-14 15:39:11.130140276 +0300
@@ -1,4 +1,3 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-y	= printk.o
-obj-$(CONFIG_PRINTK)	+= printk_safe.o
 obj-$(CONFIG_A11Y_BRAILLE_CONSOLE)	+= braille.o
diff -Naur a/kernel/printk/printk.c b/kernel/printk/printk.c
--- a/kernel/printk/printk.c	2020-11-23 13:48:35.125948569 +0200
+++ b/kernel/printk/printk.c	2021-07-14 15:39:11.130140276 +0300
@@ -44,6 +44,9 @@
 #include <linux/irq_work.h>
 #include <linux/ctype.h>
 #include <linux/uio.h>
+#include <linux/kthread.h>
+#include <linux/clocksource.h>
+#include <linux/printk_ringbuffer.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
@@ -57,13 +60,13 @@
 
 #include "console_cmdline.h"
 #include "braille.h"
-#include "internal.h"
 
-int console_printk[4] = {
+int console_printk[5] = {
 	CONSOLE_LOGLEVEL_DEFAULT,	/* console_loglevel */
 	MESSAGE_LOGLEVEL_DEFAULT,	/* default_message_loglevel */
 	CONSOLE_LOGLEVEL_MIN,		/* minimum_console_loglevel */
 	CONSOLE_LOGLEVEL_DEFAULT,	/* default_console_loglevel */
+	CONSOLE_LOGLEVEL_EMERGENCY,	/* emergency_console_loglevel */
 };
 EXPORT_SYMBOL_GPL(console_printk);
 
@@ -224,19 +227,7 @@
 
 static int __down_trylock_console_sem(unsigned long ip)
 {
-	int lock_failed;
-	unsigned long flags;
-
-	/*
-	 * Here and in __up_console_sem() we need to be in safe mode,
-	 * because spindump/WARN/etc from under console ->lock will
-	 * deadlock in printk()->down_trylock_console_sem() otherwise.
-	 */
-	printk_safe_enter_irqsave(flags);
-	lock_failed = down_trylock(&console_sem);
-	printk_safe_exit_irqrestore(flags);
-
-	if (lock_failed)
+	if (down_trylock(&console_sem))
 		return 1;
 	mutex_acquire(&console_lock_dep_map, 0, 1, ip);
 	return 0;
@@ -245,13 +236,9 @@
 
 static void __up_console_sem(unsigned long ip)
 {
-	unsigned long flags;
-
 	mutex_release(&console_lock_dep_map, ip);
 
-	printk_safe_enter_irqsave(flags);
 	up(&console_sem);
-	printk_safe_exit_irqrestore(flags);
 }
 #define up_console_sem() __up_console_sem(_RET_IP_)
 
@@ -266,11 +253,6 @@
 static int console_locked, console_suspended;
 
 /*
- * If exclusive_console is non-NULL then only this console is to be printed to.
- */
-static struct console *exclusive_console;
-
-/*
  *	Array of consoles built from command line options (console=)
  */
 
@@ -367,6 +349,7 @@
 
 struct printk_log {
 	u64 ts_nsec;		/* timestamp in nanoseconds */
+	u16 cpu;		/* cpu that generated record */
 	u16 len;		/* length of entire record */
 	u16 text_len;		/* length of text buffer */
 	u16 dict_len;		/* length of dictionary buffer */
@@ -382,65 +365,22 @@
 #endif
 ;
 
-/*
- * The logbuf_lock protects kmsg buffer, indices, counters.  This can be taken
- * within the scheduler's rq lock. It must be released before calling
- * console_unlock() or anything else that might wake up a process.
- */
-DEFINE_RAW_SPINLOCK(logbuf_lock);
-
-/*
- * Helper macros to lock/unlock logbuf_lock and switch between
- * printk-safe/unsafe modes.
- */
-#define logbuf_lock_irq()				\
-	do {						\
-		printk_safe_enter_irq();		\
-		raw_spin_lock(&logbuf_lock);		\
-	} while (0)
-
-#define logbuf_unlock_irq()				\
-	do {						\
-		raw_spin_unlock(&logbuf_lock);		\
-		printk_safe_exit_irq();			\
-	} while (0)
-
-#define logbuf_lock_irqsave(flags)			\
-	do {						\
-		printk_safe_enter_irqsave(flags);	\
-		raw_spin_lock(&logbuf_lock);		\
-	} while (0)
-
-#define logbuf_unlock_irqrestore(flags)		\
-	do {						\
-		raw_spin_unlock(&logbuf_lock);		\
-		printk_safe_exit_irqrestore(flags);	\
-	} while (0)
+DECLARE_STATIC_PRINTKRB_CPULOCK(printk_cpulock);
 
 #ifdef CONFIG_PRINTK
-DECLARE_WAIT_QUEUE_HEAD(log_wait);
-/* the next printk record to read by syslog(READ) or /proc/kmsg */
+/* record buffer */
+DECLARE_STATIC_PRINTKRB(printk_rb, CONFIG_LOG_BUF_SHIFT, &printk_cpulock);
+
+static DEFINE_MUTEX(syslog_lock);
+DECLARE_STATIC_PRINTKRB_ITER(syslog_iter, &printk_rb);
+
+/* the last printk record to read by syslog(READ) or /proc/kmsg */
 static u64 syslog_seq;
-static u32 syslog_idx;
 static size_t syslog_partial;
 static bool syslog_time;
 
-/* index and sequence number of the first record stored in the buffer */
-static u64 log_first_seq;
-static u32 log_first_idx;
-
-/* index and sequence number of the next record to store in the buffer */
-static u64 log_next_seq;
-static u32 log_next_idx;
-
-/* the next printk record to write to the console */
-static u64 console_seq;
-static u32 console_idx;
-static u64 exclusive_console_stop_seq;
-
 /* the next printk record to read after the last 'clear' command */
 static u64 clear_seq;
-static u32 clear_idx;
 
 #ifdef CONFIG_PRINTK_CALLER
 #define PREFIX_MAX		48
@@ -452,36 +392,16 @@
 #define LOG_LEVEL(v)		((v) & 0x07)
 #define LOG_FACILITY(v)		((v) >> 3 & 0xff)
 
-/* record buffer */
-#define LOG_ALIGN __alignof__(struct printk_log)
-#define __LOG_BUF_LEN (1 << CONFIG_LOG_BUF_SHIFT)
-#define LOG_BUF_LEN_MAX (u32)(1 << 31)
-static char __log_buf[__LOG_BUF_LEN] __aligned(LOG_ALIGN);
-static char *log_buf = __log_buf;
-static u32 log_buf_len = __LOG_BUF_LEN;
-
-/*
- * We cannot access per-CPU data (e.g. per-CPU flush irq_work) before
- * per_cpu_areas are initialised. This variable is set to true when
- * it's safe to access per-CPU data.
- */
-static bool __printk_percpu_data_ready __read_mostly;
-
-bool printk_percpu_data_ready(void)
-{
-	return __printk_percpu_data_ready;
-}
-
 /* Return log buffer address */
 char *log_buf_addr_get(void)
 {
-	return log_buf;
+	return printk_rb.buffer;
 }
 
 /* Return log buffer size */
 u32 log_buf_len_get(void)
 {
-	return log_buf_len;
+	return (1 << printk_rb.size_bits);
 }
 
 /* human readable text of the record */
@@ -496,180 +416,50 @@
 	return (char *)msg + sizeof(struct printk_log) + msg->text_len;
 }
 
-/* get record by index; idx must point to valid msg */
-static struct printk_log *log_from_idx(u32 idx)
-{
-	struct printk_log *msg = (struct printk_log *)(log_buf + idx);
-
-	/*
-	 * A length == 0 record is the end of buffer marker. Wrap around and
-	 * read the message at the start of the buffer.
-	 */
-	if (!msg->len)
-		return (struct printk_log *)log_buf;
-	return msg;
-}
-
-/* get next record; idx must point to valid msg */
-static u32 log_next(u32 idx)
-{
-	struct printk_log *msg = (struct printk_log *)(log_buf + idx);
-
-	/* length == 0 indicates the end of the buffer; wrap */
-	/*
-	 * A length == 0 record is the end of buffer marker. Wrap around and
-	 * read the message at the start of the buffer as *this* one, and
-	 * return the one after that.
-	 */
-	if (!msg->len) {
-		msg = (struct printk_log *)log_buf;
-		return msg->len;
-	}
-	return idx + msg->len;
-}
-
-/*
- * Check whether there is enough free space for the given message.
- *
- * The same values of first_idx and next_idx mean that the buffer
- * is either empty or full.
- *
- * If the buffer is empty, we must respect the position of the indexes.
- * They cannot be reset to the beginning of the buffer.
- */
-static int logbuf_has_space(u32 msg_size, bool empty)
-{
-	u32 free;
-
-	if (log_next_idx > log_first_idx || empty)
-		free = max(log_buf_len - log_next_idx, log_first_idx);
-	else
-		free = log_first_idx - log_next_idx;
-
-	/*
-	 * We need space also for an empty header that signalizes wrapping
-	 * of the buffer.
-	 */
-	return free >= msg_size + sizeof(struct printk_log);
-}
-
-static int log_make_free_space(u32 msg_size)
-{
-	while (log_first_seq < log_next_seq &&
-	       !logbuf_has_space(msg_size, false)) {
-		/* drop old messages until we have enough contiguous space */
-		log_first_idx = log_next(log_first_idx);
-		log_first_seq++;
-	}
-
-	if (clear_seq < log_first_seq) {
-		clear_seq = log_first_seq;
-		clear_idx = log_first_idx;
-	}
-
-	/* sequence numbers are equal, so the log buffer is empty */
-	if (logbuf_has_space(msg_size, log_first_seq == log_next_seq))
-		return 0;
-
-	return -ENOMEM;
-}
-
-/* compute the message size including the padding bytes */
-static u32 msg_used_size(u16 text_len, u16 dict_len, u32 *pad_len)
-{
-	u32 size;
-
-	size = sizeof(struct printk_log) + text_len + dict_len;
-	*pad_len = (-size) & (LOG_ALIGN - 1);
-	size += *pad_len;
-
-	return size;
-}
-
-/*
- * Define how much of the log buffer we could take at maximum. The value
- * must be greater than two. Note that only half of the buffer is available
- * when the index points to the middle.
- */
-#define MAX_LOG_TAKE_PART 4
-static const char trunc_msg[] = "<truncated>";
-
-static u32 truncate_msg(u16 *text_len, u16 *trunc_msg_len,
-			u16 *dict_len, u32 *pad_len)
-{
-	/*
-	 * The message should not take the whole buffer. Otherwise, it might
-	 * get removed too soon.
-	 */
-	u32 max_text_len = log_buf_len / MAX_LOG_TAKE_PART;
-	if (*text_len > max_text_len)
-		*text_len = max_text_len;
-	/* enable the warning message */
-	*trunc_msg_len = strlen(trunc_msg);
-	/* disable the "dict" completely */
-	*dict_len = 0;
-	/* compute the size again, count also the warning message */
-	return msg_used_size(*text_len + *trunc_msg_len, 0, pad_len);
-}
+static void printk_emergency(char *buffer, int level, u64 ts_nsec, u16 cpu,
+			     char *text, u16 text_len);
 
 /* insert record into the buffer, discard old ones, update heads */
 static int log_store(u32 caller_id, int facility, int level,
-		     enum log_flags flags, u64 ts_nsec,
+		     enum log_flags flags, u64 ts_nsec, u16 cpu,
 		     const char *dict, u16 dict_len,
 		     const char *text, u16 text_len)
 {
 	struct printk_log *msg;
-	u32 size, pad_len;
-	u16 trunc_msg_len = 0;
+	struct prb_handle h;
+	char *rbuf;
+	u32 size;
 
-	/* number of '\0' padding bytes to next message */
-	size = msg_used_size(text_len, dict_len, &pad_len);
+	size = sizeof(*msg) + text_len + dict_len;
 
-	if (log_make_free_space(size)) {
-		/* truncate the message if it is too long for empty buffer */
-		size = truncate_msg(&text_len, &trunc_msg_len,
-				    &dict_len, &pad_len);
-		/* survive when the log buffer is too small for trunc_msg */
-		if (log_make_free_space(size))
-			return 0;
-	}
-
-	if (log_next_idx + size + sizeof(struct printk_log) > log_buf_len) {
+	rbuf = prb_reserve(&h, &printk_rb, size);
+	if (!rbuf) {
 		/*
-		 * This message + an additional empty header does not fit
-		 * at the end of the buffer. Add an empty header with len == 0
-		 * to signify a wrap around.
+		 * An emergency message would have been printed, but
+		 * it cannot be stored in the log.
 		 */
-		memset(log_buf + log_next_idx, 0, sizeof(struct printk_log));
-		log_next_idx = 0;
+		prb_inc_lost(&printk_rb);
+		return 0;
 	}
 
 	/* fill message */
-	msg = (struct printk_log *)(log_buf + log_next_idx);
+	msg = (struct printk_log *)rbuf;
 	memcpy(log_text(msg), text, text_len);
 	msg->text_len = text_len;
-	if (trunc_msg_len) {
-		memcpy(log_text(msg) + text_len, trunc_msg, trunc_msg_len);
-		msg->text_len += trunc_msg_len;
-	}
 	memcpy(log_dict(msg), dict, dict_len);
 	msg->dict_len = dict_len;
 	msg->facility = facility;
 	msg->level = level & 7;
 	msg->flags = flags & 0x1f;
-	if (ts_nsec > 0)
-		msg->ts_nsec = ts_nsec;
-	else
-		msg->ts_nsec = local_clock();
+	msg->ts_nsec = ts_nsec;
 #ifdef CONFIG_PRINTK_CALLER
 	msg->caller_id = caller_id;
 #endif
-	memset(log_dict(msg) + dict_len, 0, pad_len);
+	msg->cpu = cpu;
 	msg->len = size;
 
 	/* insert message */
-	log_next_idx += msg->len;
-	log_next_seq++;
+	prb_commit(&h);
 
 	return msg->text_len;
 }
@@ -739,9 +529,9 @@
 
 	do_div(ts_usec, 1000);
 
-	return scnprintf(buf, size, "%u,%llu,%llu,%c%s;",
+	return scnprintf(buf, size, "%u,%llu,%llu,%c%s,%hu;",
 			 (msg->facility << 3) | msg->level, seq, ts_usec,
-			 msg->flags & LOG_CONT ? 'c' : '-', caller);
+			 msg->flags & LOG_CONT ? 'c' : '-', caller, msg->cpu);
 }
 
 static ssize_t msg_print_ext_body(char *buf, size_t size,
@@ -792,13 +582,18 @@
 	return p - buf;
 }
 
+#define PRINTK_SPRINT_MAX (LOG_LINE_MAX + PREFIX_MAX)
+#define PRINTK_RECORD_MAX (sizeof(struct printk_log) + \
+				CONSOLE_EXT_LOG_MAX + PRINTK_SPRINT_MAX)
+
 /* /dev/kmsg - userspace message inject/listen interface */
 struct devkmsg_user {
 	u64 seq;
-	u32 idx;
+	struct prb_iterator iter;
 	struct ratelimit_state rs;
 	struct mutex lock;
 	char buf[CONSOLE_EXT_LOG_MAX];
+	char msgbuf[PRINTK_RECORD_MAX];
 };
 
 static __printf(3, 4) __cold
@@ -881,9 +676,11 @@
 			    size_t count, loff_t *ppos)
 {
 	struct devkmsg_user *user = file->private_data;
+	struct prb_iterator backup_iter;
 	struct printk_log *msg;
-	size_t len;
 	ssize_t ret;
+	size_t len;
+	u64 seq;
 
 	if (!user)
 		return -EBADF;
@@ -892,52 +689,63 @@
 	if (ret)
 		return ret;
 
-	logbuf_lock_irq();
-	while (user->seq == log_next_seq) {
-		if (file->f_flags & O_NONBLOCK) {
-			ret = -EAGAIN;
-			logbuf_unlock_irq();
-			goto out;
-		}
+	/* make a backup copy in case there is a problem */
+	prb_iter_copy(&backup_iter, &user->iter);
 
-		logbuf_unlock_irq();
-		ret = wait_event_interruptible(log_wait,
-					       user->seq != log_next_seq);
-		if (ret)
-			goto out;
-		logbuf_lock_irq();
+	if (file->f_flags & O_NONBLOCK) {
+		ret = prb_iter_next(&user->iter, &user->msgbuf[0],
+				      sizeof(user->msgbuf), &seq);
+	} else {
+		ret = prb_iter_wait_next(&user->iter, &user->msgbuf[0],
+					   sizeof(user->msgbuf), &seq);
 	}
-
-	if (user->seq < log_first_seq) {
-		/* our last seen message is gone, return error and reset */
-		user->idx = log_first_idx;
-		user->seq = log_first_seq;
+	if (ret == 0) {
+		/* end of list */
+		ret = -EAGAIN;
+		goto out;
+	} else if (ret == -EINVAL) {
+		/* iterator invalid, return error and reset */
 		ret = -EPIPE;
-		logbuf_unlock_irq();
+		prb_iter_init(&user->iter, &printk_rb, &user->seq);
+		goto out;
+	} else if (ret < 0) {
+		/* interrupted by signal */
 		goto out;
 	}
 
-	msg = log_from_idx(user->idx);
+	user->seq++;
+	if (user->seq < seq) {
+		ret = -EPIPE;
+		goto restore_out;
+	}
+
+	msg = (struct printk_log *)&user->msgbuf[0];
 	len = msg_print_ext_header(user->buf, sizeof(user->buf),
 				   msg, user->seq);
 	len += msg_print_ext_body(user->buf + len, sizeof(user->buf) - len,
 				  log_dict(msg), msg->dict_len,
 				  log_text(msg), msg->text_len);
 
-	user->idx = log_next(user->idx);
-	user->seq++;
-	logbuf_unlock_irq();
-
 	if (len > count) {
 		ret = -EINVAL;
-		goto out;
+		goto restore_out;
 	}
 
 	if (copy_to_user(buf, user->buf, len)) {
 		ret = -EFAULT;
-		goto out;
+		goto restore_out;
 	}
+
 	ret = len;
+	goto out;
+restore_out:
+	/*
+	 * There was an error, but this message should not be
+	 * lost because of it. Restore the backup and setup
+	 * seq so that it will work with the next read.
+	 */
+	prb_iter_copy(&user->iter, &backup_iter);
+	user->seq = seq - 1;
 out:
 	mutex_unlock(&user->lock);
 	return ret;
@@ -954,19 +762,22 @@
 static loff_t devkmsg_llseek(struct file *file, loff_t offset, int whence)
 {
 	struct devkmsg_user *user = file->private_data;
-	loff_t ret = 0;
+	loff_t ret;
+	u64 seq;
 
 	if (!user)
 		return -EBADF;
 	if (offset)
 		return -ESPIPE;
 
-	logbuf_lock_irq();
+	ret = mutex_lock_interruptible(&user->lock);
+	if (ret)
+		return ret;
+
 	switch (whence) {
 	case SEEK_SET:
 		/* the first record */
-		user->idx = log_first_idx;
-		user->seq = log_first_seq;
+		prb_iter_init(&user->iter, &printk_rb, &user->seq);
 		break;
 	case SEEK_DATA:
 		/*
@@ -974,40 +785,87 @@
 		 * like issued by 'dmesg -c'. Reading /dev/kmsg itself
 		 * changes no global state, and does not clear anything.
 		 */
-		user->idx = clear_idx;
-		user->seq = clear_seq;
+		for (;;) {
+			prb_iter_init(&user->iter, &printk_rb, &seq);
+			ret = prb_iter_seek(&user->iter, clear_seq);
+			if (ret > 0) {
+				/* seeked to clear seq */
+				user->seq = clear_seq;
+				break;
+			} else if (ret == 0) {
+				/*
+				 * The end of the list was hit without
+				 * ever seeing the clear seq. Just
+				 * seek to the beginning of the list.
+				 */
+				prb_iter_init(&user->iter, &printk_rb,
+					      &user->seq);
+				break;
+			}
+			/* iterator invalid, start over */
+
+			/* reset clear_seq if it is no longer available */
+			if (seq > clear_seq)
+				clear_seq = 0;
+		}
+		ret = 0;
 		break;
 	case SEEK_END:
 		/* after the last record */
-		user->idx = log_next_idx;
-		user->seq = log_next_seq;
+		for (;;) {
+			ret = prb_iter_next(&user->iter, NULL, 0, &user->seq);
+			if (ret == 0)
+				break;
+			else if (ret > 0)
+				continue;
+			/* iterator invalid, start over */
+			prb_iter_init(&user->iter, &printk_rb, &user->seq);
+		}
+		ret = 0;
 		break;
 	default:
 		ret = -EINVAL;
 	}
-	logbuf_unlock_irq();
+
+	mutex_unlock(&user->lock);
 	return ret;
 }
 
+struct wait_queue_head *printk_wait_queue(void)
+{
+	/* FIXME: using prb internals! */
+	return printk_rb.wq;
+}
+
 static __poll_t devkmsg_poll(struct file *file, poll_table *wait)
 {
 	struct devkmsg_user *user = file->private_data;
+	struct prb_iterator iter;
 	__poll_t ret = 0;
+	int rbret;
+	u64 seq;
 
 	if (!user)
 		return EPOLLERR|EPOLLNVAL;
 
-	poll_wait(file, &log_wait, wait);
+	poll_wait(file, printk_wait_queue(), wait);
 
-	logbuf_lock_irq();
-	if (user->seq < log_next_seq) {
-		/* return error when data has vanished underneath us */
-		if (user->seq < log_first_seq)
-			ret = EPOLLIN|EPOLLRDNORM|EPOLLERR|EPOLLPRI;
-		else
-			ret = EPOLLIN|EPOLLRDNORM;
-	}
-	logbuf_unlock_irq();
+	mutex_lock(&user->lock);
+
+	/* use copy so no actual iteration takes place */
+	prb_iter_copy(&iter, &user->iter);
+
+	rbret = prb_iter_next(&iter, &user->msgbuf[0],
+			      sizeof(user->msgbuf), &seq);
+	if (rbret == 0)
+		goto out;
+
+	ret = EPOLLIN|EPOLLRDNORM;
+
+	if (rbret < 0 || (seq - user->seq) != 1)
+		ret |= EPOLLERR|EPOLLPRI;
+out:
+	mutex_unlock(&user->lock);
 
 	return ret;
 }
@@ -1037,10 +895,7 @@
 
 	mutex_init(&user->lock);
 
-	logbuf_lock_irq();
-	user->idx = log_first_idx;
-	user->seq = log_first_seq;
-	logbuf_unlock_irq();
+	prb_iter_init(&user->iter, &printk_rb, &user->seq);
 
 	file->private_data = user;
 	return 0;
@@ -1080,11 +935,6 @@
  */
 void log_buf_vmcoreinfo_setup(void)
 {
-	VMCOREINFO_SYMBOL(log_buf);
-	VMCOREINFO_SYMBOL(log_buf_len);
-	VMCOREINFO_SYMBOL(log_first_idx);
-	VMCOREINFO_SYMBOL(clear_idx);
-	VMCOREINFO_SYMBOL(log_next_idx);
 	/*
 	 * Export struct printk_log size and field offsets. User space tools can
 	 * parse it and detect any changes to structure down the line.
@@ -1100,6 +950,8 @@
 }
 #endif
 
+/* FIXME: no support for buffer resizing */
+#if 0
 /* requested log_buf_len from kernel cmdline */
 static unsigned long __initdata new_log_buf_len;
 
@@ -1165,29 +1017,16 @@
 #else /* !CONFIG_SMP */
 static inline void log_buf_add_cpu(void) {}
 #endif /* CONFIG_SMP */
-
-static void __init set_percpu_data_ready(void)
-{
-	printk_safe_init();
-	/* Make sure we set this flag only after printk_safe() init is done */
-	barrier();
-	__printk_percpu_data_ready = true;
-}
+#endif /* 0 */
 
 void __init setup_log_buf(int early)
 {
+/* FIXME: no support for buffer resizing */
+#if 0
 	unsigned long flags;
 	char *new_log_buf;
 	unsigned int free;
 
-	/*
-	 * Some archs call setup_log_buf() multiple times - first is very
-	 * early, e.g. from setup_arch(), and second - when percpu_areas
-	 * are initialised.
-	 */
-	if (!early)
-		set_percpu_data_ready();
-
 	if (log_buf != __log_buf)
 		return;
 
@@ -1215,6 +1054,7 @@
 	pr_info("log_buf_len: %u bytes\n", log_buf_len);
 	pr_info("early log buf free: %u(%u%%)\n",
 		free, (free * 100) / __LOG_BUF_LEN);
+#endif
 }
 
 static bool __read_mostly ignore_loglevel;
@@ -1295,6 +1135,11 @@
 static bool printk_time = IS_ENABLED(CONFIG_PRINTK_TIME);
 module_param_named(time, printk_time, bool, S_IRUGO | S_IWUSR);
 
+static size_t print_cpu(u16 cpu, char *buf)
+{
+	return sprintf(buf, "%03hu: ", cpu);
+}
+
 static size_t print_syslog(unsigned int level, char *buf)
 {
 	return sprintf(buf, "<%u>", level);
@@ -1338,6 +1183,7 @@
 		buf[len++] = ' ';
 		buf[len] = '\0';
 	}
+	len += print_cpu(msg->cpu, buf + len);
 
 	return len;
 }
@@ -1383,30 +1229,42 @@
 	return len;
 }
 
-static int syslog_print(char __user *buf, int size)
+static int syslog_print(char __user *buf, int size, char *text,
+			char *msgbuf, int *locked)
 {
-	char *text;
+	struct prb_iterator iter;
 	struct printk_log *msg;
 	int len = 0;
-
-	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
-	if (!text)
-		return -ENOMEM;
+	u64 seq;
+	int ret;
 
 	while (size > 0) {
 		size_t n;
 		size_t skip;
 
-		logbuf_lock_irq();
-		if (syslog_seq < log_first_seq) {
-			/* messages are gone, move to first one */
-			syslog_seq = log_first_seq;
-			syslog_idx = log_first_idx;
-			syslog_partial = 0;
+		for (;;) {
+			prb_iter_copy(&iter, &syslog_iter);
+			ret = prb_iter_next(&iter, msgbuf,
+					    PRINTK_RECORD_MAX, &seq);
+			if (ret < 0) {
+				/* messages are gone, move to first one */
+				prb_iter_init(&syslog_iter, &printk_rb,
+					      &syslog_seq);
+				syslog_partial = 0;
+				continue;
+			}
+			break;
 		}
-		if (syslog_seq == log_next_seq) {
-			logbuf_unlock_irq();
+		if (ret == 0)
 			break;
+
+		/*
+		 * If messages have been missed, the partial tracker
+		 * is no longer valid and must be reset.
+		 */
+		if (syslog_seq > 0 && seq - 1 != syslog_seq) {
+			syslog_seq = seq - 1;
+			syslog_partial = 0;
 		}
 
 		/*
@@ -1416,131 +1274,213 @@
 		if (!syslog_partial)
 			syslog_time = printk_time;
 
+		msg = (struct printk_log *)msgbuf;
+
 		skip = syslog_partial;
-		msg = log_from_idx(syslog_idx);
 		n = msg_print_text(msg, true, syslog_time, text,
-				   LOG_LINE_MAX + PREFIX_MAX);
+				   PRINTK_SPRINT_MAX);
 		if (n - syslog_partial <= size) {
 			/* message fits into buffer, move forward */
-			syslog_idx = log_next(syslog_idx);
-			syslog_seq++;
+			prb_iter_next(&syslog_iter, NULL, 0, &syslog_seq);
 			n -= syslog_partial;
 			syslog_partial = 0;
-		} else if (!len){
+		} else if (!len) {
 			/* partial read(), remember position */
 			n = size;
 			syslog_partial += n;
 		} else
 			n = 0;
-		logbuf_unlock_irq();
 
 		if (!n)
 			break;
 
+		mutex_unlock(&syslog_lock);
 		if (copy_to_user(buf, text + skip, n)) {
 			if (!len)
 				len = -EFAULT;
+			*locked = 0;
 			break;
 		}
+		ret = mutex_lock_interruptible(&syslog_lock);
 
 		len += n;
 		size -= n;
 		buf += n;
+
+		if (ret) {
+			if (!len)
+				len = ret;
+			*locked = 0;
+			break;
+		}
 	}
 
-	kfree(text);
 	return len;
 }
 
-static int syslog_print_all(char __user *buf, int size, bool clear)
+static int count_remaining(struct prb_iterator *iter, u64 until_seq,
+			   char *msgbuf, int size, bool records, bool time)
 {
-	char *text;
+	struct prb_iterator local_iter;
+	struct printk_log *msg;
 	int len = 0;
-	u64 next_seq;
 	u64 seq;
-	u32 idx;
+	int ret;
+
+	prb_iter_copy(&local_iter, iter);
+	for (;;) {
+		ret = prb_iter_next(&local_iter, msgbuf, size, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/* the iter is invalid, restart from head */
+			prb_iter_init(&local_iter, &printk_rb, NULL);
+			len = 0;
+			continue;
+		}
+
+		if (until_seq && seq >= until_seq)
+			break;
+
+		if (records) {
+			len++;
+		} else {
+			msg = (struct printk_log *)msgbuf;
+			len += msg_print_text(msg, true, time, NULL, 0);
+		}
+	}
+
+	return len;
+}
+
+static void syslog_clear(void)
+{
+	struct prb_iterator iter;
+	int ret;
+
+	prb_iter_init(&iter, &printk_rb, &clear_seq);
+	for (;;) {
+		ret = prb_iter_next(&iter, NULL, 0, &clear_seq);
+		if (ret == 0)
+			break;
+		else if (ret < 0)
+			prb_iter_init(&iter, &printk_rb, &clear_seq);
+	}
+}
+
+static int syslog_print_all(char __user *buf, int size, bool clear)
+{
+	struct prb_iterator iter;
+	struct printk_log *msg;
+	char *msgbuf = NULL;
+	char *text = NULL;
+	int textlen;
+	u64 seq = 0;
+	int len = 0;
 	bool time;
+	int ret;
 
-	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
+	text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
 	if (!text)
 		return -ENOMEM;
+	msgbuf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+	if (!msgbuf) {
+		kfree(text);
+		return -ENOMEM;
+	}
 
 	time = printk_time;
-	logbuf_lock_irq();
+
 	/*
-	 * Find first record that fits, including all following records,
-	 * into the user-provided buffer for this dump.
+	 * Setup iter to last event before clear. Clear may
+	 * be lost, but keep going with a best effort.
 	 */
-	seq = clear_seq;
-	idx = clear_idx;
-	while (seq < log_next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
-
-		len += msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
-	}
-
-	/* move first record forward until length fits into the buffer */
-	seq = clear_seq;
-	idx = clear_idx;
-	while (len > size && seq < log_next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	prb_iter_init(&iter, &printk_rb, NULL);
+	prb_iter_seek(&iter, clear_seq);
+
+	/* count the total bytes after clear */
+	len = count_remaining(&iter, 0, msgbuf, PRINTK_RECORD_MAX,
+			      false, time);
+
+	/* move iter forward until length fits into the buffer */
+	while (len > size) {
+		ret = prb_iter_next(&iter, msgbuf,
+				    PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/*
+			 * The iter is now invalid so clear will
+			 * also be invalid. Restart from the head.
+			 */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			len = count_remaining(&iter, 0, msgbuf,
+					      PRINTK_RECORD_MAX, false, time);
+			continue;
+		}
 
+		msg = (struct printk_log *)msgbuf;
 		len -= msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
-	}
 
-	/* last message fitting into this dump */
-	next_seq = log_next_seq;
+		if (clear)
+			clear_seq = seq;
+	}
 
+	/* copy messages to buffer */
 	len = 0;
-	while (len >= 0 && seq < next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
-		int textlen = msg_print_text(msg, true, time, text,
-					     LOG_LINE_MAX + PREFIX_MAX);
+	while (len >= 0 && len < size) {
+		if (clear)
+			clear_seq = seq;
+
+		ret = prb_iter_next(&iter, msgbuf,
+				    PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/*
+			 * The iter is now invalid. Make a best
+			 * effort to grab the rest of the log
+			 * from the new head.
+			 */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
 
-		idx = log_next(idx);
-		seq++;
+		msg = (struct printk_log *)msgbuf;
+		textlen = msg_print_text(msg, true, time, text,
+					 PRINTK_SPRINT_MAX);
+		if (textlen < 0) {
+			len = textlen;
+			break;
+		}
+
+		if (len + textlen > size)
+			break;
 
-		logbuf_unlock_irq();
 		if (copy_to_user(buf + len, text, textlen))
 			len = -EFAULT;
 		else
 			len += textlen;
-		logbuf_lock_irq();
-
-		if (seq < log_first_seq) {
-			/* messages are gone, move to next one */
-			seq = log_first_seq;
-			idx = log_first_idx;
-		}
 	}
 
-	if (clear) {
-		clear_seq = log_next_seq;
-		clear_idx = log_next_idx;
-	}
-	logbuf_unlock_irq();
+	if (clear && !seq)
+		syslog_clear();
 
 	kfree(text);
+	kfree(msgbuf);
 	return len;
 }
 
-static void syslog_clear(void)
-{
-	logbuf_lock_irq();
-	clear_seq = log_next_seq;
-	clear_idx = log_next_idx;
-	logbuf_unlock_irq();
-}
-
 int do_syslog(int type, char __user *buf, int len, int source)
 {
 	bool clear = false;
 	static int saved_console_loglevel = LOGLEVEL_DEFAULT;
+	struct prb_iterator iter;
+	char *msgbuf = NULL;
+	char *text = NULL;
+	int locked;
 	int error;
+	int ret;
 
 	error = check_syslog_permissions(type, source);
 	if (error)
@@ -1558,11 +1498,49 @@
 			return 0;
 		if (!access_ok(buf, len))
 			return -EFAULT;
-		error = wait_event_interruptible(log_wait,
-						 syslog_seq != log_next_seq);
+
+		text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
+		msgbuf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+		if (!text || !msgbuf) {
+			error = -ENOMEM;
+			goto out;
+		}
+
+		error = mutex_lock_interruptible(&syslog_lock);
 		if (error)
-			return error;
-		error = syslog_print(buf, len);
+			goto out;
+
+		/*
+		 * Wait until a first message is available. Use a copy
+		 * because no iteration should occur for syslog now.
+		 */
+		for (;;) {
+			prb_iter_copy(&iter, &syslog_iter);
+
+			mutex_unlock(&syslog_lock);
+			ret = prb_iter_wait_next(&iter, NULL, 0, NULL);
+			if (ret == -ERESTARTSYS) {
+				error = ret;
+				goto out;
+			}
+			error = mutex_lock_interruptible(&syslog_lock);
+			if (error)
+				goto out;
+
+			if (ret == -EINVAL) {
+				prb_iter_init(&syslog_iter, &printk_rb,
+					      &syslog_seq);
+				syslog_partial = 0;
+				continue;
+			}
+			break;
+		}
+
+		/* print as much as will fit in the user buffer */
+		locked = 1;
+		error = syslog_print(buf, len, text, msgbuf, &locked);
+		if (locked)
+			mutex_unlock(&syslog_lock);
 		break;
 	/* Read/clear last kernel messages */
 	case SYSLOG_ACTION_READ_CLEAR:
@@ -1607,47 +1585,43 @@
 		break;
 	/* Number of chars in the log buffer */
 	case SYSLOG_ACTION_SIZE_UNREAD:
-		logbuf_lock_irq();
-		if (syslog_seq < log_first_seq) {
-			/* messages are gone, move to first one */
-			syslog_seq = log_first_seq;
-			syslog_idx = log_first_idx;
-			syslog_partial = 0;
-		}
+		msgbuf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+		if (!msgbuf)
+			return -ENOMEM;
+
+		error = mutex_lock_interruptible(&syslog_lock);
+		if (error)
+			goto out;
+
 		if (source == SYSLOG_FROM_PROC) {
 			/*
 			 * Short-cut for poll(/"proc/kmsg") which simply checks
 			 * for pending data, not the size; return the count of
 			 * records, not the length.
 			 */
-			error = log_next_seq - syslog_seq;
+			error = count_remaining(&syslog_iter, 0, msgbuf,
+						PRINTK_RECORD_MAX, true,
+						printk_time);
 		} else {
-			u64 seq = syslog_seq;
-			u32 idx = syslog_idx;
-			bool time = syslog_partial ? syslog_time : printk_time;
-
-			while (seq < log_next_seq) {
-				struct printk_log *msg = log_from_idx(idx);
-
-				error += msg_print_text(msg, true, time, NULL,
-							0);
-				time = printk_time;
-				idx = log_next(idx);
-				seq++;
-			}
+			error = count_remaining(&syslog_iter, 0, msgbuf,
+						PRINTK_RECORD_MAX, false,
+						printk_time);
 			error -= syslog_partial;
 		}
-		logbuf_unlock_irq();
+
+		mutex_unlock(&syslog_lock);
 		break;
 	/* Size of the log buffer */
 	case SYSLOG_ACTION_SIZE_BUFFER:
-		error = log_buf_len;
+		error = prb_buffer_size(&printk_rb);
 		break;
 	default:
 		error = -EINVAL;
 		break;
 	}
-
+out:
+	kfree(msgbuf);
+	kfree(text);
 	return error;
 }
 
@@ -1656,144 +1630,128 @@
 	return do_syslog(type, buf, len, SYSLOG_FROM_READER);
 }
 
-/*
- * Special console_lock variants that help to reduce the risk of soft-lockups.
- * They allow to pass console_lock to another printk() call using a busy wait.
- */
+int printk_delay_msec __read_mostly;
 
-#ifdef CONFIG_LOCKDEP
-static struct lockdep_map console_owner_dep_map = {
-	.name = "console_owner"
-};
-#endif
+static inline void printk_delay(int level)
+{
+	boot_delay_msec(level);
+	if (unlikely(printk_delay_msec)) {
+		int m = printk_delay_msec;
 
-static DEFINE_RAW_SPINLOCK(console_owner_lock);
-static struct task_struct *console_owner;
-static bool console_waiter;
+		while (m--) {
+			mdelay(1);
+			touch_nmi_watchdog();
+		}
+	}
+}
 
-/**
- * console_lock_spinning_enable - mark beginning of code where another
- *	thread might safely busy wait
- *
- * This basically converts console_lock into a spinlock. This marks
- * the section where the console_lock owner can not sleep, because
- * there may be a waiter spinning (like a spinlock). Also it must be
- * ready to hand over the lock at the end of the section.
- */
-static void console_lock_spinning_enable(void)
-{
-	raw_spin_lock(&console_owner_lock);
-	console_owner = current;
-	raw_spin_unlock(&console_owner_lock);
+static void print_console_dropped(struct console *con, u64 count)
+{
+	char text[64];
+	int len;
 
-	/* The waiter may spin on us after setting console_owner */
-	spin_acquire(&console_owner_dep_map, 0, 0, _THIS_IP_);
+	len = sprintf(text, "** %llu printk message%s dropped **\n",
+		      count, count > 1 ? "s" : "");
+	con->write(con, text, len);
 }
 
-/**
- * console_lock_spinning_disable_and_check - mark end of code where another
- *	thread was able to busy wait and check if there is a waiter
- *
- * This is called at the end of the section where spinning is allowed.
- * It has two functions. First, it is a signal that it is no longer
- * safe to start busy waiting for the lock. Second, it checks if
- * there is a busy waiter and passes the lock rights to her.
- *
- * Important: Callers lose the lock if there was a busy waiter.
- *	They must not touch items synchronized by console_lock
- *	in this case.
- *
- * Return: 1 if the lock rights were passed, 0 otherwise.
- */
-static int console_lock_spinning_disable_and_check(void)
+static void format_text(struct printk_log *msg, u64 seq,
+			char *ext_text, size_t *ext_len,
+			char *text, size_t *len, bool time)
 {
-	int waiter;
-
-	raw_spin_lock(&console_owner_lock);
-	waiter = READ_ONCE(console_waiter);
-	console_owner = NULL;
-	raw_spin_unlock(&console_owner_lock);
+	if (suppress_message_printing(msg->level)) {
+		/*
+		 * Skip record that has level above the console
+		 * loglevel and update each console's local seq.
+		 */
+		*len = 0;
+		*ext_len = 0;
+		return;
+	}
 
-	if (!waiter) {
-		spin_release(&console_owner_dep_map, _THIS_IP_);
-		return 0;
+	*len = msg_print_text(msg, console_msg_format & MSG_FORMAT_SYSLOG,
+			      time, text, PRINTK_SPRINT_MAX);
+	if (nr_ext_console_drivers) {
+		*ext_len = msg_print_ext_header(ext_text, CONSOLE_EXT_LOG_MAX,
+						msg, seq);
+		*ext_len += msg_print_ext_body(ext_text + *ext_len,
+					       CONSOLE_EXT_LOG_MAX - *ext_len,
+					       log_dict(msg), msg->dict_len,
+					       log_text(msg), msg->text_len);
+	} else {
+		*ext_len = 0;
 	}
+}
 
-	/* The waiter is now free to continue */
-	WRITE_ONCE(console_waiter, false);
+static void printk_write_history(struct console *con, u64 master_seq)
+{
+	struct prb_iterator iter;
+	bool time = printk_time;
+	static char *ext_text;
+	static char *text;
+	static char *buf;
+	u64 seq;
 
-	spin_release(&console_owner_dep_map, _THIS_IP_);
+	ext_text = kmalloc(CONSOLE_EXT_LOG_MAX, GFP_KERNEL);
+	text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
+	buf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+	if (!ext_text || !text || !buf)
+		return;
 
-	/*
-	 * Hand off console_lock to waiter. The waiter will perform
-	 * the up(). After this, the waiter is the console_lock owner.
-	 */
-	mutex_release(&console_lock_dep_map, _THIS_IP_);
-	return 1;
-}
+	if (!(con->flags & CON_ENABLED))
+		goto out;
 
-/**
- * console_trylock_spinning - try to get console_lock by busy waiting
- *
- * This allows to busy wait for the console_lock when the current
- * owner is running in specially marked sections. It means that
- * the current owner is running and cannot reschedule until it
- * is ready to lose the lock.
- *
- * Return: 1 if we got the lock, 0 othrewise
- */
-static int console_trylock_spinning(void)
-{
-	struct task_struct *owner = NULL;
-	bool waiter;
-	bool spin = false;
-	unsigned long flags;
+	if (!con->write)
+		goto out;
 
-	if (console_trylock())
-		return 1;
+	if (!cpu_online(raw_smp_processor_id()) &&
+	    !(con->flags & CON_ANYTIME))
+		goto out;
 
-	printk_safe_enter_irqsave(flags);
+	prb_iter_init(&iter, &printk_rb, NULL);
 
-	raw_spin_lock(&console_owner_lock);
-	owner = READ_ONCE(console_owner);
-	waiter = READ_ONCE(console_waiter);
-	if (!waiter && owner && owner != current) {
-		WRITE_ONCE(console_waiter, true);
-		spin = true;
-	}
-	raw_spin_unlock(&console_owner_lock);
-
-	/*
-	 * If there is an active printk() writing to the
-	 * consoles, instead of having it write our data too,
-	 * see if we can offload that load from the active
-	 * printer, and do some printing ourselves.
-	 * Go into a spin only if there isn't already a waiter
-	 * spinning, and there is an active printer, and
-	 * that active printer isn't us (recursive printk?).
-	 */
-	if (!spin) {
-		printk_safe_exit_irqrestore(flags);
-		return 0;
-	}
+	for (;;) {
+		struct printk_log *msg;
+		size_t ext_len;
+		size_t len;
+		int ret;
 
-	/* We spin waiting for the owner to release us */
-	spin_acquire(&console_owner_dep_map, 0, 0, _THIS_IP_);
-	/* Owner will clear console_waiter on hand off */
-	while (READ_ONCE(console_waiter))
-		cpu_relax();
-	spin_release(&console_owner_dep_map, _THIS_IP_);
+		ret = prb_iter_next(&iter, buf, PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
 
-	printk_safe_exit_irqrestore(flags);
-	/*
-	 * The owner passed the console lock to us.
-	 * Since we did not spin on console lock, annotate
-	 * this as a trylock. Otherwise lockdep will
-	 * complain.
-	 */
-	mutex_acquire(&console_lock_dep_map, 0, 1, _THIS_IP_);
+		if (seq > master_seq)
+			break;
 
-	return 1;
+		con->printk_seq++;
+		if (con->printk_seq < seq) {
+			print_console_dropped(con, seq - con->printk_seq);
+			con->printk_seq = seq;
+		}
+
+		msg = (struct printk_log *)buf;
+		format_text(msg, master_seq, ext_text, &ext_len, text,
+			    &len, time);
+
+		if (len == 0 && ext_len == 0)
+			continue;
+
+		if (con->flags & CON_EXTENDED)
+			con->write(con, ext_text, ext_len);
+		else
+			con->write(con, text, len);
+
+		printk_delay(msg->level);
+	}
+out:
+	con->wrote_history = 1;
+	kfree(ext_text);
+	kfree(text);
+	kfree(buf);
 }
 
 /*
@@ -1801,23 +1759,49 @@
  * log_buf[start] to log_buf[end - 1].
  * The console_lock must be held.
  */
-static void call_console_drivers(const char *ext_text, size_t ext_len,
-				 const char *text, size_t len)
+static void call_console_drivers(u64 seq, const char *ext_text, size_t ext_len,
+				 const char *text, size_t len, int level,
+				 int facility)
 {
 	struct console *con;
 
 	trace_console_rcuidle(text, len);
 
 	for_each_console(con) {
-		if (exclusive_console && con != exclusive_console)
-			continue;
 		if (!(con->flags & CON_ENABLED))
 			continue;
+		if (!con->wrote_history) {
+			if (con->flags & CON_PRINTBUFFER) {
+				printk_write_history(con, seq);
+				continue;
+			}
+			con->wrote_history = 1;
+			con->printk_seq = seq - 1;
+		}
+		if (con->flags & CON_BOOT && facility == 0) {
+			/* skip boot messages, already printed */
+			if (con->printk_seq < seq)
+				con->printk_seq = seq;
+			continue;
+		}
 		if (!con->write)
 			continue;
-		if (!cpu_online(smp_processor_id()) &&
+		if (!cpu_online(raw_smp_processor_id()) &&
 		    !(con->flags & CON_ANYTIME))
 			continue;
+		if (con->printk_seq >= seq)
+			continue;
+
+		con->printk_seq++;
+		if (con->printk_seq < seq) {
+			print_console_dropped(con, seq - con->printk_seq);
+			con->printk_seq = seq;
+		}
+
+		/* for supressed messages, only seq is updated */
+		if (len == 0 && ext_len == 0)
+			continue;
+
 		if (con->flags & CON_EXTENDED)
 			con->write(con, ext_text, ext_len);
 		else
@@ -1825,20 +1809,6 @@
 	}
 }
 
-int printk_delay_msec __read_mostly;
-
-static inline void printk_delay(void)
-{
-	if (unlikely(printk_delay_msec)) {
-		int m = printk_delay_msec;
-
-		while (m--) {
-			mdelay(1);
-			touch_nmi_watchdog();
-		}
-	}
-}
-
 static inline u32 printk_caller_id(void)
 {
 	return in_task() ? task_pid_nr(current) :
@@ -1855,101 +1825,95 @@
 	char buf[LOG_LINE_MAX];
 	size_t len;			/* length == 0 means unused buffer */
 	u32 caller_id;			/* printk_caller_id() of first print */
+	int cpu_owner;			/* cpu of first print */
 	u64 ts_nsec;			/* time of first print */
 	u8 level;			/* log level of first message */
 	u8 facility;			/* log facility of first message */
 	enum log_flags flags;		/* prefix, newline flags */
-} cont;
+} cont[2];
 
-static void cont_flush(void)
+static void cont_flush(int ctx)
 {
-	if (cont.len == 0)
+	struct cont *c = &cont[ctx];
+
+	if (c->len == 0)
 		return;
 
-	log_store(cont.caller_id, cont.facility, cont.level, cont.flags,
-		  cont.ts_nsec, NULL, 0, cont.buf, cont.len);
-	cont.len = 0;
+	log_store(c->caller_id, c->facility, c->level, c->flags,
+		  c->ts_nsec, c->cpu_owner, NULL, 0, c->buf, c->len);
+	c->len = 0;
 }
 
-static bool cont_add(u32 caller_id, int facility, int level,
+static void cont_add(int ctx, int cpu, u32 caller_id, int facility, int level,
 		     enum log_flags flags, const char *text, size_t len)
 {
+	struct cont *c = &cont[ctx];
+
+	if (cpu != c->cpu_owner || !(flags & LOG_CONT))
+		cont_flush(ctx);
+
 	/* If the line gets too long, split it up in separate records. */
-	if (cont.len + len > sizeof(cont.buf)) {
-		cont_flush();
-		return false;
-	}
+	while (c->len + len > sizeof(c->buf))
+		cont_flush(ctx);
 
-	if (!cont.len) {
-		cont.facility = facility;
-		cont.level = level;
-		cont.caller_id = caller_id;
-		cont.ts_nsec = local_clock();
-		cont.flags = flags;
+	if (!c->len) {
+		c->facility = facility;
+		c->level = level;
+		c->caller_id = caller_id;
+		c->ts_nsec = local_clock();
+		c->flags = flags;
+		c->cpu_owner = cpu;
 	}
 
-	memcpy(cont.buf + cont.len, text, len);
-	cont.len += len;
+	memcpy(c->buf + c->len, text, len);
+	c->len += len;
 
 	// The original flags come from the first line,
 	// but later continuations can add a newline.
 	if (flags & LOG_NEWLINE) {
-		cont.flags |= LOG_NEWLINE;
-		cont_flush();
+		c->flags |= LOG_NEWLINE;
+		cont_flush(ctx);
 	}
-
-	return true;
 }
 
-static size_t log_output(int facility, int level, enum log_flags lflags, const char *dict, size_t dictlen, char *text, size_t text_len)
+/* ring buffer used as memory allocator for temporary sprint buffers */
+DECLARE_STATIC_PRINTKRB(sprint_rb,
+			ilog2(PRINTK_RECORD_MAX + sizeof(struct prb_entry) +
+			      sizeof(long)) + 2, &printk_cpulock);
+
+asmlinkage int vprintk_emit(int facility, int level,
+			    const char *dict, size_t dictlen,
+			    const char *fmt, va_list args)
 {
 	const u32 caller_id = printk_caller_id();
+	int ctx = !!in_nmi();
+	enum log_flags lflags = 0;
+	int printed_len = 0;
+	struct prb_handle h;
+	size_t text_len;
+	u64 ts_nsec;
+	char *text;
+	char *rbuf;
+	int cpu;
 
-	/*
-	 * If an earlier line was buffered, and we're a continuation
-	 * write from the same context, try to add it to the buffer.
-	 */
-	if (cont.len) {
-		if (cont.caller_id == caller_id && (lflags & LOG_CONT)) {
-			if (cont_add(caller_id, facility, level, lflags, text, text_len))
-				return text_len;
-		}
-		/* Otherwise, make sure it's flushed */
-		cont_flush();
-	}
-
-	/* Skip empty continuation lines that couldn't be added - they just flush */
-	if (!text_len && (lflags & LOG_CONT))
-		return 0;
+	ts_nsec = local_clock();
 
-	/* If it doesn't end in a newline, try to buffer the current line */
-	if (!(lflags & LOG_NEWLINE)) {
-		if (cont_add(caller_id, facility, level, lflags, text, text_len))
-			return text_len;
+	rbuf = prb_reserve(&h, &sprint_rb, PRINTK_SPRINT_MAX);
+	if (!rbuf) {
+		prb_inc_lost(&printk_rb);
+		return printed_len;
 	}
 
-	/* Store it in the record log */
-	return log_store(caller_id, facility, level, lflags, 0,
-			 dict, dictlen, text, text_len);
-}
-
-/* Must be called under logbuf_lock. */
-int vprintk_store(int facility, int level,
-		  const char *dict, size_t dictlen,
-		  const char *fmt, va_list args)
-{
-	static char textbuf[LOG_LINE_MAX];
-	char *text = textbuf;
-	size_t text_len;
-	enum log_flags lflags = 0;
+	cpu = raw_smp_processor_id();
 
 	/*
-	 * The printf needs to come first; we need the syslog
-	 * prefix which might be passed-in as a parameter.
+	 * If this turns out to be an emergency message, there
+	 * may need to be a prefix added. Leave room for it.
 	 */
-	text_len = vscnprintf(text, sizeof(textbuf), fmt, args);
+	text = rbuf + PREFIX_MAX;
+	text_len = vscnprintf(text, PRINTK_SPRINT_MAX - PREFIX_MAX, fmt, args);
 
-	/* mark and strip a trailing newline */
+	/* strip and flag a trailing newline */
 	if (text_len && text[text_len-1] == '\n') {
 		text_len--;
 		lflags |= LOG_NEWLINE;
@@ -1980,73 +1944,42 @@
 	if (dict)
 		lflags |= LOG_NEWLINE;
 
-	return log_output(facility, level, lflags,
-			  dict, dictlen, text, text_len);
-}
-
-asmlinkage int vprintk_emit(int facility, int level,
-			    const char *dict, size_t dictlen,
-			    const char *fmt, va_list args)
-{
-	int printed_len;
-	bool in_sched = false, pending_output;
-	unsigned long flags;
-	u64 curr_log_seq;
-
-	/* Suppress unimportant messages after panic happens */
-	if (unlikely(suppress_printk))
-		return 0;
-
-	if (level == LOGLEVEL_SCHED) {
-		level = LOGLEVEL_DEFAULT;
-		in_sched = true;
+	/*
+	 * NOTE:
+	 * - rbuf points to beginning of allocated buffer
+	 * - text points to beginning of text
+	 * - there is room before text for prefix
+	 */
+	if (facility == 0) {
+		/* only the kernel can create emergency messages */
+		printk_emergency(rbuf, level & 7, ts_nsec, cpu, text, text_len);
 	}
 
-	boot_delay_msec(level);
-	printk_delay();
-
-	/* This stops the holder of console_sem just where we want him */
-	logbuf_lock_irqsave(flags);
-	curr_log_seq = log_next_seq;
-	printed_len = vprintk_store(facility, level, dict, dictlen, fmt, args);
-	pending_output = (curr_log_seq != log_next_seq);
-	logbuf_unlock_irqrestore(flags);
-
-	/* If called from the scheduler, we can not call up(). */
-	if (!in_sched && pending_output) {
-		/*
-		 * Disable preemption to avoid being preempted while holding
-		 * console_sem which would prevent anyone from printing to
-		 * console
-		 */
-		preempt_disable();
-		/*
-		 * Try to acquire and then immediately release the console
-		 * semaphore.  The release will print out buffers and wake up
-		 * /dev/kmsg and syslog() users.
-		 */
-		if (console_trylock_spinning())
-			console_unlock();
-		preempt_enable();
+	if ((lflags & LOG_CONT) || !(lflags & LOG_NEWLINE)) {
+		 cont_add(ctx, cpu, caller_id, facility, level, lflags, text, text_len);
+		 printed_len = text_len;
+	} else {
+		if (cpu == cont[ctx].cpu_owner)
+			cont_flush(ctx);
+		printed_len = log_store(caller_id, facility, level, lflags, ts_nsec, cpu,
+					dict, dictlen, text, text_len);
 	}
 
-	if (pending_output)
-		wake_up_klogd();
+	prb_commit(&h);
 	return printed_len;
 }
 EXPORT_SYMBOL(vprintk_emit);
 
-asmlinkage int vprintk(const char *fmt, va_list args)
+static __printf(1, 0) int vprintk_func(const char *fmt, va_list args)
 {
-	return vprintk_func(fmt, args);
+	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
 }
-EXPORT_SYMBOL(vprintk);
 
-int vprintk_default(const char *fmt, va_list args)
+asmlinkage int vprintk(const char *fmt, va_list args)
 {
-	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
+	return vprintk_func(fmt, args);
 }
-EXPORT_SYMBOL_GPL(vprintk_default);
+EXPORT_SYMBOL(vprintk);
 
 /**
  * printk - print a kernel message
@@ -2081,39 +2014,6 @@
 	return r;
 }
 EXPORT_SYMBOL(printk);
-
-#else /* CONFIG_PRINTK */
-
-#define LOG_LINE_MAX		0
-#define PREFIX_MAX		0
-#define printk_time		false
-
-static u64 syslog_seq;
-static u32 syslog_idx;
-static u64 console_seq;
-static u32 console_idx;
-static u64 exclusive_console_stop_seq;
-static u64 log_first_seq;
-static u32 log_first_idx;
-static u64 log_next_seq;
-static char *log_text(const struct printk_log *msg) { return NULL; }
-static char *log_dict(const struct printk_log *msg) { return NULL; }
-static struct printk_log *log_from_idx(u32 idx) { return NULL; }
-static u32 log_next(u32 idx) { return 0; }
-static ssize_t msg_print_ext_header(char *buf, size_t size,
-				    struct printk_log *msg,
-				    u64 seq) { return 0; }
-static ssize_t msg_print_ext_body(char *buf, size_t size,
-				  char *dict, size_t dict_len,
-				  char *text, size_t text_len) { return 0; }
-static void console_lock_spinning_enable(void) { }
-static int console_lock_spinning_disable_and_check(void) { return 0; }
-static void call_console_drivers(const char *ext_text, size_t ext_len,
-				 const char *text, size_t len) {}
-static size_t msg_print_text(const struct printk_log *msg, bool syslog,
-			     bool time, char *buf, size_t size) { return 0; }
-static bool suppress_message_printing(int level) { return false; }
-
 #endif /* CONFIG_PRINTK */
 
 #ifdef CONFIG_EARLY_PRINTK
@@ -2350,187 +2250,24 @@
 }
 EXPORT_SYMBOL(is_console_locked);
 
-/*
- * Check if we have any console that is capable of printing while cpu is
- * booting or shutting down. Requires console_sem.
- */
-static int have_callable_console(void)
-{
-	struct console *con;
-
-	for_each_console(con)
-		if ((con->flags & CON_ENABLED) &&
-				(con->flags & CON_ANYTIME))
-			return 1;
-
-	return 0;
-}
-
-/*
- * Can we actually use the console at this time on this cpu?
- *
- * Console drivers may assume that per-cpu resources have been allocated. So
- * unless they're explicitly marked as being able to cope (CON_ANYTIME) don't
- * call them until this CPU is officially up.
- */
-static inline int can_use_console(void)
-{
-	return cpu_online(raw_smp_processor_id()) || have_callable_console();
-}
-
 /**
  * console_unlock - unlock the console system
  *
  * Releases the console_lock which the caller holds on the console system
  * and the console driver list.
  *
- * While the console_lock was held, console output may have been buffered
- * by printk().  If this is the case, console_unlock(); emits
- * the output prior to releasing the lock.
- *
- * If there is output waiting, we wake /dev/kmsg and syslog() users.
- *
  * console_unlock(); may be called from any context.
  */
 void console_unlock(void)
 {
-	static char ext_text[CONSOLE_EXT_LOG_MAX];
-	static char text[LOG_LINE_MAX + PREFIX_MAX];
-	unsigned long flags;
-	bool do_cond_resched, retry;
-
 	if (console_suspended) {
 		up_console_sem();
 		return;
 	}
 
-	/*
-	 * Console drivers are called with interrupts disabled, so
-	 * @console_may_schedule should be cleared before; however, we may
-	 * end up dumping a lot of lines, for example, if called from
-	 * console registration path, and should invoke cond_resched()
-	 * between lines if allowable.  Not doing so can cause a very long
-	 * scheduling stall on a slow console leading to RCU stall and
-	 * softlockup warnings which exacerbate the issue with more
-	 * messages practically incapacitating the system.
-	 *
-	 * console_trylock() is not able to detect the preemptive
-	 * context reliably. Therefore the value must be stored before
-	 * and cleared after the the "again" goto label.
-	 */
-	do_cond_resched = console_may_schedule;
-again:
-	console_may_schedule = 0;
-
-	/*
-	 * We released the console_sem lock, so we need to recheck if
-	 * cpu is online and (if not) is there at least one CON_ANYTIME
-	 * console.
-	 */
-	if (!can_use_console()) {
-		console_locked = 0;
-		up_console_sem();
-		return;
-	}
-
-	for (;;) {
-		struct printk_log *msg;
-		size_t ext_len = 0;
-		size_t len;
-
-		printk_safe_enter_irqsave(flags);
-		raw_spin_lock(&logbuf_lock);
-		if (console_seq < log_first_seq) {
-			len = snprintf(text, sizeof(text),
-				       "** %llu printk messages dropped **\n",
-				       log_first_seq - console_seq);
-
-			/* messages are gone, move to first one */
-			console_seq = log_first_seq;
-			console_idx = log_first_idx;
-		} else {
-			len = 0;
-		}
-skip:
-		if (console_seq == log_next_seq)
-			break;
-
-		msg = log_from_idx(console_idx);
-		if (suppress_message_printing(msg->level)) {
-			/*
-			 * Skip record we have buffered and already printed
-			 * directly to the console when we received it, and
-			 * record that has level above the console loglevel.
-			 */
-			console_idx = log_next(console_idx);
-			console_seq++;
-			goto skip;
-		}
-
-		/* Output to all consoles once old messages replayed. */
-		if (unlikely(exclusive_console &&
-			     console_seq >= exclusive_console_stop_seq)) {
-			exclusive_console = NULL;
-		}
-
-		len += msg_print_text(msg,
-				console_msg_format & MSG_FORMAT_SYSLOG,
-				printk_time, text + len, sizeof(text) - len);
-		if (nr_ext_console_drivers) {
-			ext_len = msg_print_ext_header(ext_text,
-						sizeof(ext_text),
-						msg, console_seq);
-			ext_len += msg_print_ext_body(ext_text + ext_len,
-						sizeof(ext_text) - ext_len,
-						log_dict(msg), msg->dict_len,
-						log_text(msg), msg->text_len);
-		}
-		console_idx = log_next(console_idx);
-		console_seq++;
-		raw_spin_unlock(&logbuf_lock);
-
-		/*
-		 * While actively printing out messages, if another printk()
-		 * were to occur on another CPU, it may wait for this one to
-		 * finish. This task can not be preempted if there is a
-		 * waiter waiting to take over.
-		 */
-		console_lock_spinning_enable();
-
-		stop_critical_timings();	/* don't trace print latency */
-		call_console_drivers(ext_text, ext_len, text, len);
-		start_critical_timings();
-
-		if (console_lock_spinning_disable_and_check()) {
-			printk_safe_exit_irqrestore(flags);
-			return;
-		}
-
-		printk_safe_exit_irqrestore(flags);
-
-		if (do_cond_resched)
-			cond_resched();
-	}
-
 	console_locked = 0;
 
-	raw_spin_unlock(&logbuf_lock);
-
 	up_console_sem();
-
-	/*
-	 * Someone could have filled up the buffer again, so re-check if there's
-	 * something to flush. In case we cannot trylock the console_sem again,
-	 * there's a new owner and the console_unlock() from them will do the
-	 * flush, no worries.
-	 */
-	raw_spin_lock(&logbuf_lock);
-	retry = console_seq != log_next_seq;
-	raw_spin_unlock(&logbuf_lock);
-	printk_safe_exit_irqrestore(flags);
-
-	if (retry && console_trylock())
-		goto again;
 }
 EXPORT_SYMBOL(console_unlock);
 
@@ -2581,24 +2318,10 @@
 void console_flush_on_panic(enum con_flush_mode mode)
 {
 	/*
-	 * If someone else is holding the console lock, trylock will fail
-	 * and may_schedule may be set.  Ignore and proceed to unlock so
-	 * that messages are flushed out.  As this can be called from any
-	 * context and we don't want to get preempted while flushing,
-	 * ensure may_schedule is cleared.
+	 * FIXME: This is currently a NOP. Emergency messages will have been
+	 * printed, but what about if write_atomic is not available on the
+	 * console? What if the printk kthread is still alive?
 	 */
-	console_trylock();
-	console_may_schedule = 0;
-
-	if (mode == CONSOLE_REPLAY_ALL) {
-		unsigned long flags;
-
-		logbuf_lock_irqsave(flags);
-		console_seq = log_first_seq;
-		console_idx = log_first_idx;
-		logbuf_unlock_irqrestore(flags);
-	}
-	console_unlock();
 }
 
 /*
@@ -2732,7 +2455,6 @@
  */
 void register_console(struct console *newcon)
 {
-	unsigned long flags;
 	struct console *bcon = NULL;
 	int err;
 
@@ -2820,27 +2542,6 @@
 	if (newcon->flags & CON_EXTENDED)
 		nr_ext_console_drivers++;
 
-	if (newcon->flags & CON_PRINTBUFFER) {
-		/*
-		 * console_unlock(); will print out the buffered messages
-		 * for us.
-		 */
-		logbuf_lock_irqsave(flags);
-		/*
-		 * We're about to replay the log buffer.  Only do this to the
-		 * just-registered console to avoid excessive message spam to
-		 * the already-registered consoles.
-		 *
-		 * Set exclusive_console with disabled interrupts to reduce
-		 * race window with eventual console_flush_on_panic() that
-		 * ignores console_lock.
-		 */
-		exclusive_console = newcon;
-		exclusive_console_stop_seq = console_seq;
-		console_seq = syslog_seq;
-		console_idx = syslog_idx;
-		logbuf_unlock_irqrestore(flags);
-	}
 	console_unlock();
 	console_sysfs_notify();
 
@@ -2850,6 +2551,10 @@
 	 * boot consoles, real consoles, etc - this is to ensure that end
 	 * users know there might be something in the kernel's log buffer that
 	 * went to the bootconsole (that they do not see on the real console)
+	 *
+	 * This message is also important because it will trigger the
+	 * printk kthread to begin dumping the log buffer to the newly
+	 * registered console.
 	 */
 	pr_info("%sconsole [%s%d] enabled\n",
 		(newcon->flags & CON_BOOT) ? "boot" : "" ,
@@ -3007,65 +2712,75 @@
 late_initcall(printk_late_init);
 
 #if defined CONFIG_PRINTK
-/*
- * Delayed printk version, for scheduler-internal messages:
- */
-#define PRINTK_PENDING_WAKEUP	0x01
-#define PRINTK_PENDING_OUTPUT	0x02
-
-static DEFINE_PER_CPU(int, printk_pending);
-
-static void wake_up_klogd_work_func(struct irq_work *irq_work)
+static int printk_kthread_func(void *data)
 {
-	int pending = __this_cpu_xchg(printk_pending, 0);
+	struct prb_iterator iter;
+	struct printk_log *msg;
+	size_t ext_len;
+	char *ext_text;
+	u64 master_seq;
+	size_t len;
+	char *text;
+	char *buf;
+	int ret;
 
-	if (pending & PRINTK_PENDING_OUTPUT) {
-		/* If trylock fails, someone else is doing the printing */
-		if (console_trylock())
-			console_unlock();
-	}
+	ext_text = kmalloc(CONSOLE_EXT_LOG_MAX, GFP_KERNEL);
+	text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
+	buf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+	if (!ext_text || !text || !buf)
+		return -1;
 
-	if (pending & PRINTK_PENDING_WAKEUP)
-		wake_up_interruptible(&log_wait);
-}
+	prb_iter_init(&iter, &printk_rb, NULL);
 
-static DEFINE_PER_CPU(struct irq_work, wake_up_klogd_work) = {
-	.func = wake_up_klogd_work_func,
-	.flags = ATOMIC_INIT(IRQ_WORK_LAZY),
-};
+	/* the printk kthread never exits */
+	for (;;) {
+		ret = prb_iter_wait_next(&iter, buf,
+					 PRINTK_RECORD_MAX, &master_seq);
+		if (ret == -ERESTARTSYS) {
+			continue;
+		} else if (ret < 0) {
+			/* iterator invalid, start over */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
 
-void wake_up_klogd(void)
-{
-	if (!printk_percpu_data_ready())
-		return;
+		msg = (struct printk_log *)buf;
+		format_text(msg, master_seq, ext_text, &ext_len, text,
+			    &len, printk_time);
 
-	preempt_disable();
-	if (waitqueue_active(&log_wait)) {
-		this_cpu_or(printk_pending, PRINTK_PENDING_WAKEUP);
-		irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
+		console_lock();
+		console_may_schedule = 0;
+		call_console_drivers(master_seq, ext_text, ext_len, text, len,
+				     msg->level, msg->facility);
+		if (len > 0 || ext_len > 0)
+			printk_delay(msg->level);
+		console_unlock();
 	}
-	preempt_enable();
-}
 
-void defer_console_output(void)
-{
-	if (!printk_percpu_data_ready())
-		return;
+	kfree(ext_text);
+	kfree(text);
+	kfree(buf);
 
-	preempt_disable();
-	__this_cpu_or(printk_pending, PRINTK_PENDING_OUTPUT);
-	irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
-	preempt_enable();
+	return 0;
 }
 
-int vprintk_deferred(const char *fmt, va_list args)
+static int __init init_printk_kthread(void)
 {
-	int r;
+	struct task_struct *thread;
 
-	r = vprintk_emit(0, LOGLEVEL_SCHED, NULL, 0, fmt, args);
-	defer_console_output();
+	thread = kthread_run(printk_kthread_func, NULL, "printk");
+	if (IS_ERR(thread)) {
+		pr_err("printk: unable to create printing thread\n");
+		return PTR_ERR(thread);
+	}
 
-	return r;
+	return 0;
+}
+late_initcall(init_printk_kthread);
+
+__printf(1, 0) static int vprintk_deferred(const char *fmt, va_list args)
+{
+	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
 }
 
 int printk_deferred(const char *fmt, ...)
@@ -3204,8 +2919,8 @@
  */
 void kmsg_dump(enum kmsg_dump_reason reason)
 {
+	struct kmsg_dumper dumper_local;
 	struct kmsg_dumper *dumper;
-	unsigned long flags;
 
 	rcu_read_lock();
 	list_for_each_entry_rcu(dumper, &dump_list, list) {
@@ -3222,21 +2937,18 @@
 		if (reason > max_reason)
 			continue;
 
-		/* initialize iterator with data about the stored records */
-		dumper->active = true;
+		/*
+		 * use a local copy to avoid modifying the
+		 * iterator used by any other cpus/contexts
+		 */
+		memcpy(&dumper_local, dumper, sizeof(dumper_local));
 
-		logbuf_lock_irqsave(flags);
-		dumper->cur_seq = clear_seq;
-		dumper->cur_idx = clear_idx;
-		dumper->next_seq = log_next_seq;
-		dumper->next_idx = log_next_idx;
-		logbuf_unlock_irqrestore(flags);
+		/* initialize iterator with data about the stored records */
+		dumper_local.active = true;
+		kmsg_dump_rewind(&dumper_local);
 
 		/* invoke dumper which will iterate over records */
-		dumper->dump(dumper, reason);
-
-		/* reset iterator */
-		dumper->active = false;
+		dumper_local.dump(&dumper_local, reason);
 	}
 	rcu_read_unlock();
 }
@@ -3263,33 +2975,67 @@
 bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
 			       char *line, size_t size, size_t *len)
 {
+	struct prb_iterator iter;
 	struct printk_log *msg;
-	size_t l = 0;
-	bool ret = false;
+	struct prb_handle h;
+	bool cont = false;
+	char *msgbuf;
+	char *rbuf;
+	size_t l;
+	u64 seq;
+	int ret;
 
 	if (!dumper->active)
-		goto out;
+		return cont;
+
+	rbuf = prb_reserve(&h, &sprint_rb, PRINTK_RECORD_MAX);
+	if (!rbuf)
+		return cont;
+	msgbuf = rbuf;
+retry:
+	for (;;) {
+		prb_iter_init(&iter, &printk_rb, &seq);
+
+		if (dumper->line_seq == seq) {
+			/* already where we want to be */
+			break;
+		} else if (dumper->line_seq < seq) {
+			/* messages are gone, move to first available one */
+			dumper->line_seq = seq;
+			break;
+		}
 
-	if (dumper->cur_seq < log_first_seq) {
-		/* messages are gone, move to first available one */
-		dumper->cur_seq = log_first_seq;
-		dumper->cur_idx = log_first_idx;
+		ret = prb_iter_seek(&iter, dumper->line_seq);
+		if (ret > 0) {
+			/* seeked to line_seq */
+			break;
+		} else if (ret == 0) {
+			/*
+			 * The end of the list was hit without ever seeing
+			 * line_seq. Reset it to the beginning of the list.
+			 */
+			prb_iter_init(&iter, &printk_rb, &dumper->line_seq);
+			break;
+		}
+		/* iterator invalid, start over */
 	}
 
-	/* last entry */
-	if (dumper->cur_seq >= log_next_seq)
+	ret = prb_iter_next(&iter, msgbuf, PRINTK_RECORD_MAX,
+			    &dumper->line_seq);
+	if (ret == 0)
 		goto out;
+	else if (ret < 0)
+		goto retry;
 
-	msg = log_from_idx(dumper->cur_idx);
+	msg = (struct printk_log *)msgbuf;
 	l = msg_print_text(msg, syslog, printk_time, line, size);
 
-	dumper->cur_idx = log_next(dumper->cur_idx);
-	dumper->cur_seq++;
-	ret = true;
-out:
 	if (len)
 		*len = l;
-	return ret;
+	cont = true;
+out:
+	prb_commit(&h);
+	return cont;
 }
 
 /**
@@ -3312,12 +3058,9 @@
 bool kmsg_dump_get_line(struct kmsg_dumper *dumper, bool syslog,
 			char *line, size_t size, size_t *len)
 {
-	unsigned long flags;
 	bool ret;
 
-	logbuf_lock_irqsave(flags);
 	ret = kmsg_dump_get_line_nolock(dumper, syslog, line, size, len);
-	logbuf_unlock_irqrestore(flags);
 
 	return ret;
 }
@@ -3345,74 +3088,101 @@
 bool kmsg_dump_get_buffer(struct kmsg_dumper *dumper, bool syslog,
 			  char *buf, size_t size, size_t *len)
 {
-	unsigned long flags;
-	u64 seq;
-	u32 idx;
-	u64 next_seq;
-	u32 next_idx;
-	size_t l = 0;
-	bool ret = false;
+	struct prb_iterator iter;
 	bool time = printk_time;
+	struct printk_log *msg;
+	u64 new_end_seq = 0;
+	struct prb_handle h;
+	bool cont = false;
+	char *msgbuf;
+	u64 end_seq;
+	int textlen;
+	u64 seq = 0;
+	char *rbuf;
+	int l = 0;
+	int ret;
 
 	if (!dumper->active)
-		goto out;
+		return cont;
 
-	logbuf_lock_irqsave(flags);
-	if (dumper->cur_seq < log_first_seq) {
-		/* messages are gone, move to first available one */
-		dumper->cur_seq = log_first_seq;
-		dumper->cur_idx = log_first_idx;
-	}
+	rbuf = prb_reserve(&h, &sprint_rb, PRINTK_RECORD_MAX);
+	if (!rbuf)
+		return cont;
+	msgbuf = rbuf;
 
-	/* last entry */
-	if (dumper->cur_seq >= dumper->next_seq) {
-		logbuf_unlock_irqrestore(flags);
-		goto out;
-	}
+	prb_iter_init(&iter, &printk_rb, NULL);
 
-	/* calculate length of entire buffer */
-	seq = dumper->cur_seq;
-	idx = dumper->cur_idx;
-	while (seq < dumper->next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	/*
+	 * seek to the start record, which is set/modified
+	 * by kmsg_dump_get_line_nolock()
+	 */
+	ret = prb_iter_seek(&iter, dumper->line_seq);
+	if (ret <= 0)
+		prb_iter_init(&iter, &printk_rb, &seq);
 
-		l += msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
+	/* work with a local end seq to have a constant value */
+	end_seq = dumper->buffer_end_seq;
+	if (!end_seq) {
+		/* initialize end seq to "infinity" */
+		end_seq = -1;
+		dumper->buffer_end_seq = end_seq;
 	}
+retry:
+	if (seq >= end_seq)
+		goto out;
 
-	/* move first record forward until length fits into the buffer */
-	seq = dumper->cur_seq;
-	idx = dumper->cur_idx;
-	while (l >= size && seq < dumper->next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	/* count the total bytes after seq */
+	textlen = count_remaining(&iter, end_seq, msgbuf,
+				  PRINTK_RECORD_MAX, 0, time);
+
+	/* move iter forward until length fits into the buffer */
+	while (textlen > size) {
+		ret = prb_iter_next(&iter, msgbuf, PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0 || seq >= end_seq) {
+			prb_iter_init(&iter, &printk_rb, &seq);
+			goto retry;
+		}
 
-		l -= msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
+		msg = (struct printk_log *)msgbuf;
+		textlen -= msg_print_text(msg, true, time, NULL, 0);
 	}
 
-	/* last message in next interation */
-	next_seq = seq;
-	next_idx = idx;
+	/* save end seq for the next interation */
+	new_end_seq = seq + 1;
 
-	l = 0;
-	while (seq < dumper->next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	/* copy messages to buffer */
+	while (l < size) {
+		ret = prb_iter_next(&iter, msgbuf, PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/*
+			 * iterator (and thus also the start position)
+			 * invalid, start over from beginning of list
+			 */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
+
+		if (seq >= end_seq)
+			break;
 
-		l += msg_print_text(msg, syslog, time, buf + l, size - l);
-		idx = log_next(idx);
-		seq++;
+		msg = (struct printk_log *)msgbuf;
+		textlen = msg_print_text(msg, syslog, time, buf + l, size - l);
+		if (textlen > 0)
+			l += textlen;
+		cont = true;
 	}
 
-	dumper->next_seq = next_seq;
-	dumper->next_idx = next_idx;
-	ret = true;
-	logbuf_unlock_irqrestore(flags);
-out:
-	if (len)
+	if (cont && len)
 		*len = l;
-	return ret;
+out:
+	prb_commit(&h);
+	if (new_end_seq)
+		dumper->buffer_end_seq = new_end_seq;
+	return cont;
 }
 EXPORT_SYMBOL_GPL(kmsg_dump_get_buffer);
 
@@ -3428,10 +3198,8 @@
  */
 void kmsg_dump_rewind_nolock(struct kmsg_dumper *dumper)
 {
-	dumper->cur_seq = clear_seq;
-	dumper->cur_idx = clear_idx;
-	dumper->next_seq = log_next_seq;
-	dumper->next_idx = log_next_idx;
+	dumper->line_seq = 0;
+	dumper->buffer_end_seq = 0;
 }
 
 /**
@@ -3444,12 +3212,89 @@
  */
 void kmsg_dump_rewind(struct kmsg_dumper *dumper)
 {
-	unsigned long flags;
-
-	logbuf_lock_irqsave(flags);
 	kmsg_dump_rewind_nolock(dumper);
-	logbuf_unlock_irqrestore(flags);
 }
 EXPORT_SYMBOL_GPL(kmsg_dump_rewind);
 
+static bool console_can_emergency(int level)
+{
+	struct console *con;
+
+	for_each_console(con) {
+		if (!(con->flags & CON_ENABLED))
+			continue;
+		if (con->write_atomic && oops_in_progress)
+			return true;
+		if (con->write && (con->flags & CON_BOOT))
+			return true;
+	}
+	return false;
+}
+
+static void call_emergency_console_drivers(int level, const char *text,
+					   size_t text_len)
+{
+	struct console *con;
+
+	for_each_console(con) {
+		if (!(con->flags & CON_ENABLED))
+			continue;
+		if (con->write_atomic && oops_in_progress) {
+			con->write_atomic(con, text, text_len);
+			continue;
+		}
+		if (con->write && (con->flags & CON_BOOT)) {
+			con->write(con, text, text_len);
+			continue;
+		}
+	}
+}
+
+static void printk_emergency(char *buffer, int level, u64 ts_nsec, u16 cpu,
+			     char *text, u16 text_len)
+{
+	struct printk_log msg;
+	size_t prefix_len;
+
+	if (!console_can_emergency(level))
+		return;
+
+	msg.level = level;
+	msg.ts_nsec = ts_nsec;
+	msg.cpu = cpu;
+	msg.facility = 0;
+
+	/* "text" must have PREFIX_MAX preceding bytes available */
+
+	prefix_len = print_prefix(&msg,
+				  console_msg_format & MSG_FORMAT_SYSLOG,
+				  printk_time, buffer);
+	/* move the prefix forward to the beginning of the message text */
+	text -= prefix_len;
+	memmove(text, buffer, prefix_len);
+	text_len += prefix_len;
+
+	text[text_len++] = '\n';
+
+	call_emergency_console_drivers(level, text, text_len);
+
+	touch_softlockup_watchdog_sync();
+	clocksource_touch_watchdog();
+	rcu_cpu_stall_reset();
+	touch_nmi_watchdog();
+
+	printk_delay(level);
+}
 #endif
+
+void console_atomic_lock(unsigned int *flags)
+{
+	prb_lock(&printk_cpulock, flags);
+}
+EXPORT_SYMBOL(console_atomic_lock);
+
+void console_atomic_unlock(unsigned int flags)
+{
+	prb_unlock(&printk_cpulock, flags);
+}
+EXPORT_SYMBOL(console_atomic_unlock);
diff -Naur a/kernel/printk/printk_safe.c b/kernel/printk/printk_safe.c
--- a/kernel/printk/printk_safe.c	2020-11-23 13:48:35.125948569 +0200
+++ b/kernel/printk/printk_safe.c	1970-01-01 02:00:00.000000000 +0200
@@ -1,414 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-or-later
-/*
- * printk_safe.c - Safe printk for printk-deadlock-prone contexts
- */
-
-#include <linux/preempt.h>
-#include <linux/spinlock.h>
-#include <linux/debug_locks.h>
-#include <linux/kdb.h>
-#include <linux/smp.h>
-#include <linux/cpumask.h>
-#include <linux/irq_work.h>
-#include <linux/printk.h>
-#include <linux/kprobes.h>
-
-#include "internal.h"
-
-/*
- * printk() could not take logbuf_lock in NMI context. Instead,
- * it uses an alternative implementation that temporary stores
- * the strings into a per-CPU buffer. The content of the buffer
- * is later flushed into the main ring buffer via IRQ work.
- *
- * The alternative implementation is chosen transparently
- * by examinig current printk() context mask stored in @printk_context
- * per-CPU variable.
- *
- * The implementation allows to flush the strings also from another CPU.
- * There are situations when we want to make sure that all buffers
- * were handled or when IRQs are blocked.
- */
-
-#define SAFE_LOG_BUF_LEN ((1 << CONFIG_PRINTK_SAFE_LOG_BUF_SHIFT) -	\
-				sizeof(atomic_t) -			\
-				sizeof(atomic_t) -			\
-				sizeof(struct irq_work))
-
-struct printk_safe_seq_buf {
-	atomic_t		len;	/* length of written data */
-	atomic_t		message_lost;
-	struct irq_work		work;	/* IRQ work that flushes the buffer */
-	unsigned char		buffer[SAFE_LOG_BUF_LEN];
-};
-
-static DEFINE_PER_CPU(struct printk_safe_seq_buf, safe_print_seq);
-static DEFINE_PER_CPU(int, printk_context);
-
-#ifdef CONFIG_PRINTK_NMI
-static DEFINE_PER_CPU(struct printk_safe_seq_buf, nmi_print_seq);
-#endif
-
-/* Get flushed in a more safe context. */
-static void queue_flush_work(struct printk_safe_seq_buf *s)
-{
-	if (printk_percpu_data_ready())
-		irq_work_queue(&s->work);
-}
-
-/*
- * Add a message to per-CPU context-dependent buffer. NMI and printk-safe
- * have dedicated buffers, because otherwise printk-safe preempted by
- * NMI-printk would have overwritten the NMI messages.
- *
- * The messages are flushed from irq work (or from panic()), possibly,
- * from other CPU, concurrently with printk_safe_log_store(). Should this
- * happen, printk_safe_log_store() will notice the buffer->len mismatch
- * and repeat the write.
- */
-static __printf(2, 0) int printk_safe_log_store(struct printk_safe_seq_buf *s,
-						const char *fmt, va_list args)
-{
-	int add;
-	size_t len;
-	va_list ap;
-
-again:
-	len = atomic_read(&s->len);
-
-	/* The trailing '\0' is not counted into len. */
-	if (len >= sizeof(s->buffer) - 1) {
-		atomic_inc(&s->message_lost);
-		queue_flush_work(s);
-		return 0;
-	}
-
-	/*
-	 * Make sure that all old data have been read before the buffer
-	 * was reset. This is not needed when we just append data.
-	 */
-	if (!len)
-		smp_rmb();
-
-	va_copy(ap, args);
-	add = vscnprintf(s->buffer + len, sizeof(s->buffer) - len, fmt, ap);
-	va_end(ap);
-	if (!add)
-		return 0;
-
-	/*
-	 * Do it once again if the buffer has been flushed in the meantime.
-	 * Note that atomic_cmpxchg() is an implicit memory barrier that
-	 * makes sure that the data were written before updating s->len.
-	 */
-	if (atomic_cmpxchg(&s->len, len, len + add) != len)
-		goto again;
-
-	queue_flush_work(s);
-	return add;
-}
-
-static inline void printk_safe_flush_line(const char *text, int len)
-{
-	/*
-	 * Avoid any console drivers calls from here, because we may be
-	 * in NMI or printk_safe context (when in panic). The messages
-	 * must go only into the ring buffer at this stage.  Consoles will
-	 * get explicitly called later when a crashdump is not generated.
-	 */
-	printk_deferred("%.*s", len, text);
-}
-
-/* printk part of the temporary buffer line by line */
-static int printk_safe_flush_buffer(const char *start, size_t len)
-{
-	const char *c, *end;
-	bool header;
-
-	c = start;
-	end = start + len;
-	header = true;
-
-	/* Print line by line. */
-	while (c < end) {
-		if (*c == '\n') {
-			printk_safe_flush_line(start, c - start + 1);
-			start = ++c;
-			header = true;
-			continue;
-		}
-
-		/* Handle continuous lines or missing new line. */
-		if ((c + 1 < end) && printk_get_level(c)) {
-			if (header) {
-				c = printk_skip_level(c);
-				continue;
-			}
-
-			printk_safe_flush_line(start, c - start);
-			start = c++;
-			header = true;
-			continue;
-		}
-
-		header = false;
-		c++;
-	}
-
-	/* Check if there was a partial line. Ignore pure header. */
-	if (start < end && !header) {
-		static const char newline[] = KERN_CONT "\n";
-
-		printk_safe_flush_line(start, end - start);
-		printk_safe_flush_line(newline, strlen(newline));
-	}
-
-	return len;
-}
-
-static void report_message_lost(struct printk_safe_seq_buf *s)
-{
-	int lost = atomic_xchg(&s->message_lost, 0);
-
-	if (lost)
-		printk_deferred("Lost %d message(s)!\n", lost);
-}
-
-/*
- * Flush data from the associated per-CPU buffer. The function
- * can be called either via IRQ work or independently.
- */
-static void __printk_safe_flush(struct irq_work *work)
-{
-	static raw_spinlock_t read_lock =
-		__RAW_SPIN_LOCK_INITIALIZER(read_lock);
-	struct printk_safe_seq_buf *s =
-		container_of(work, struct printk_safe_seq_buf, work);
-	unsigned long flags;
-	size_t len;
-	int i;
-
-	/*
-	 * The lock has two functions. First, one reader has to flush all
-	 * available message to make the lockless synchronization with
-	 * writers easier. Second, we do not want to mix messages from
-	 * different CPUs. This is especially important when printing
-	 * a backtrace.
-	 */
-	raw_spin_lock_irqsave(&read_lock, flags);
-
-	i = 0;
-more:
-	len = atomic_read(&s->len);
-
-	/*
-	 * This is just a paranoid check that nobody has manipulated
-	 * the buffer an unexpected way. If we printed something then
-	 * @len must only increase. Also it should never overflow the
-	 * buffer size.
-	 */
-	if ((i && i >= len) || len > sizeof(s->buffer)) {
-		const char *msg = "printk_safe_flush: internal error\n";
-
-		printk_safe_flush_line(msg, strlen(msg));
-		len = 0;
-	}
-
-	if (!len)
-		goto out; /* Someone else has already flushed the buffer. */
-
-	/* Make sure that data has been written up to the @len */
-	smp_rmb();
-	i += printk_safe_flush_buffer(s->buffer + i, len - i);
-
-	/*
-	 * Check that nothing has got added in the meantime and truncate
-	 * the buffer. Note that atomic_cmpxchg() is an implicit memory
-	 * barrier that makes sure that the data were copied before
-	 * updating s->len.
-	 */
-	if (atomic_cmpxchg(&s->len, len, 0) != len)
-		goto more;
-
-out:
-	report_message_lost(s);
-	raw_spin_unlock_irqrestore(&read_lock, flags);
-}
-
-/**
- * printk_safe_flush - flush all per-cpu nmi buffers.
- *
- * The buffers are flushed automatically via IRQ work. This function
- * is useful only when someone wants to be sure that all buffers have
- * been flushed at some point.
- */
-void printk_safe_flush(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-#ifdef CONFIG_PRINTK_NMI
-		__printk_safe_flush(&per_cpu(nmi_print_seq, cpu).work);
-#endif
-		__printk_safe_flush(&per_cpu(safe_print_seq, cpu).work);
-	}
-}
-
-/**
- * printk_safe_flush_on_panic - flush all per-cpu nmi buffers when the system
- *	goes down.
- *
- * Similar to printk_safe_flush() but it can be called even in NMI context when
- * the system goes down. It does the best effort to get NMI messages into
- * the main ring buffer.
- *
- * Note that it could try harder when there is only one CPU online.
- */
-void printk_safe_flush_on_panic(void)
-{
-	/*
-	 * Make sure that we could access the main ring buffer.
-	 * Do not risk a double release when more CPUs are up.
-	 */
-	if (raw_spin_is_locked(&logbuf_lock)) {
-		if (num_online_cpus() > 1)
-			return;
-
-		debug_locks_off();
-		raw_spin_lock_init(&logbuf_lock);
-	}
-
-	printk_safe_flush();
-}
-
-#ifdef CONFIG_PRINTK_NMI
-/*
- * Safe printk() for NMI context. It uses a per-CPU buffer to
- * store the message. NMIs are not nested, so there is always only
- * one writer running. But the buffer might get flushed from another
- * CPU, so we need to be careful.
- */
-static __printf(1, 0) int vprintk_nmi(const char *fmt, va_list args)
-{
-	struct printk_safe_seq_buf *s = this_cpu_ptr(&nmi_print_seq);
-
-	return printk_safe_log_store(s, fmt, args);
-}
-
-void noinstr printk_nmi_enter(void)
-{
-	this_cpu_add(printk_context, PRINTK_NMI_CONTEXT_OFFSET);
-}
-
-void noinstr printk_nmi_exit(void)
-{
-	this_cpu_sub(printk_context, PRINTK_NMI_CONTEXT_OFFSET);
-}
-
-/*
- * Marks a code that might produce many messages in NMI context
- * and the risk of losing them is more critical than eventual
- * reordering.
- *
- * It has effect only when called in NMI context. Then printk()
- * will try to store the messages into the main logbuf directly
- * and use the per-CPU buffers only as a fallback when the lock
- * is not available.
- */
-void printk_nmi_direct_enter(void)
-{
-	if (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)
-		this_cpu_or(printk_context, PRINTK_NMI_DIRECT_CONTEXT_MASK);
-}
-
-void printk_nmi_direct_exit(void)
-{
-	this_cpu_and(printk_context, ~PRINTK_NMI_DIRECT_CONTEXT_MASK);
-}
-
-#else
-
-static __printf(1, 0) int vprintk_nmi(const char *fmt, va_list args)
-{
-	return 0;
-}
-
-#endif /* CONFIG_PRINTK_NMI */
-
-/*
- * Lock-less printk(), to avoid deadlocks should the printk() recurse
- * into itself. It uses a per-CPU buffer to store the message, just like
- * NMI.
- */
-static __printf(1, 0) int vprintk_safe(const char *fmt, va_list args)
-{
-	struct printk_safe_seq_buf *s = this_cpu_ptr(&safe_print_seq);
-
-	return printk_safe_log_store(s, fmt, args);
-}
-
-/* Can be preempted by NMI. */
-void __printk_safe_enter(void)
-{
-	this_cpu_inc(printk_context);
-}
-
-/* Can be preempted by NMI. */
-void __printk_safe_exit(void)
-{
-	this_cpu_dec(printk_context);
-}
-
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args)
-{
-#ifdef CONFIG_KGDB_KDB
-	/* Allow to pass printk() to kdb but avoid a recursion. */
-	if (unlikely(kdb_trap_printk && kdb_printf_cpu < 0))
-		return vkdb_printf(KDB_MSGSRC_PRINTK, fmt, args);
-#endif
-
-	/*
-	 * Try to use the main logbuf even in NMI. But avoid calling console
-	 * drivers that might have their own locks.
-	 */
-	if ((this_cpu_read(printk_context) & PRINTK_NMI_DIRECT_CONTEXT_MASK) &&
-	    raw_spin_trylock(&logbuf_lock)) {
-		int len;
-
-		len = vprintk_store(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
-		raw_spin_unlock(&logbuf_lock);
-		defer_console_output();
-		return len;
-	}
-
-	/* Use extra buffer in NMI when logbuf_lock is taken or in safe mode. */
-	if (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)
-		return vprintk_nmi(fmt, args);
-
-	/* Use extra buffer to prevent a recursion deadlock in safe mode. */
-	if (this_cpu_read(printk_context) & PRINTK_SAFE_CONTEXT_MASK)
-		return vprintk_safe(fmt, args);
-
-	/* No obstacles. */
-	return vprintk_default(fmt, args);
-}
-
-void __init printk_safe_init(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-		struct printk_safe_seq_buf *s;
-
-		s = &per_cpu(safe_print_seq, cpu);
-		init_irq_work(&s->work, __printk_safe_flush);
-
-#ifdef CONFIG_PRINTK_NMI
-		s = &per_cpu(nmi_print_seq, cpu);
-		init_irq_work(&s->work, __printk_safe_flush);
-#endif
-	}
-
-	/* Flush pending messages that did not have scheduled IRQ works. */
-	printk_safe_flush();
-}
diff -Naur a/kernel/ptrace.c b/kernel/ptrace.c
--- a/kernel/ptrace.c	2020-11-23 13:48:34.941944876 +0200
+++ b/kernel/ptrace.c	2021-07-14 15:39:10.890141965 +0300
@@ -180,7 +180,14 @@
 
 	spin_lock_irq(&task->sighand->siglock);
 	if (task_is_traced(task) && !__fatal_signal_pending(task)) {
-		task->state = __TASK_TRACED;
+		unsigned long flags;
+
+		raw_spin_lock_irqsave(&task->pi_lock, flags);
+		if (task->state & __TASK_TRACED)
+			task->state = __TASK_TRACED;
+		else
+			task->saved_state = __TASK_TRACED;
+		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		ret = true;
 	}
 	spin_unlock_irq(&task->sighand->siglock);
diff -Naur a/kernel/rcu/Kconfig b/kernel/rcu/Kconfig
--- a/kernel/rcu/Kconfig	2020-11-23 13:48:35.129948649 +0200
+++ b/kernel/rcu/Kconfig	2021-07-14 15:39:11.146140164 +0300
@@ -186,8 +186,8 @@
 
 config RCU_BOOST
 	bool "Enable RCU priority boosting"
-	depends on RT_MUTEXES && PREEMPT_RCU && RCU_EXPERT
-	default n
+	depends on (RT_MUTEXES && PREEMPT_RCU && RCU_EXPERT) || PREEMPT_RT
+	default y if PREEMPT_RT
 	help
 	  This option boosts the priority of preempted RCU readers that
 	  block the current preemptible RCU grace period for too long.
diff -Naur a/kernel/rcu/rcutorture.c b/kernel/rcu/rcutorture.c
--- a/kernel/rcu/rcutorture.c	2020-11-23 13:48:35.133948729 +0200
+++ b/kernel/rcu/rcutorture.c	2021-07-14 15:39:11.150140136 +0300
@@ -74,10 +74,13 @@
 #define RCUTORTURE_RDR_RBH	 0x08	/*  ... rcu_read_lock_bh(). */
 #define RCUTORTURE_RDR_SCHED	 0x10	/*  ... rcu_read_lock_sched(). */
 #define RCUTORTURE_RDR_RCU	 0x20	/*  ... entering another RCU reader. */
-#define RCUTORTURE_RDR_NBITS	 6	/* Number of bits defined above. */
+#define RCUTORTURE_RDR_ATOM_BH	 0x40	/*  ... disabling bh while atomic */
+#define RCUTORTURE_RDR_ATOM_RBH	 0x80	/*  ... RBH while atomic */
+#define RCUTORTURE_RDR_NBITS	 8	/* Number of bits defined above. */
 #define RCUTORTURE_MAX_EXTEND	 \
 	(RCUTORTURE_RDR_BH | RCUTORTURE_RDR_IRQ | RCUTORTURE_RDR_PREEMPT | \
-	 RCUTORTURE_RDR_RBH | RCUTORTURE_RDR_SCHED)
+	 RCUTORTURE_RDR_RBH | RCUTORTURE_RDR_SCHED | \
+	 RCUTORTURE_RDR_ATOM_BH | RCUTORTURE_RDR_ATOM_RBH)
 #define RCUTORTURE_RDR_MAX_LOOPS 0x7	/* Maximum reader extensions. */
 					/* Must be power of two minus one. */
 #define RCUTORTURE_RDR_MAX_SEGS (RCUTORTURE_RDR_MAX_LOOPS + 3)
@@ -1246,31 +1249,53 @@
 	WARN_ON_ONCE((idxold >> RCUTORTURE_RDR_SHIFT) > 1);
 	rtrsp->rt_readstate = newstate;
 
-	/* First, put new protection in place to avoid critical-section gap. */
+	/*
+	 * First, put new protection in place to avoid critical-section gap.
+	 * Disable preemption around the ATOM disables to ensure that
+	 * in_atomic() is true.
+	 */
 	if (statesnew & RCUTORTURE_RDR_BH)
 		local_bh_disable();
+	if (statesnew & RCUTORTURE_RDR_RBH)
+		rcu_read_lock_bh();
 	if (statesnew & RCUTORTURE_RDR_IRQ)
 		local_irq_disable();
 	if (statesnew & RCUTORTURE_RDR_PREEMPT)
 		preempt_disable();
-	if (statesnew & RCUTORTURE_RDR_RBH)
-		rcu_read_lock_bh();
 	if (statesnew & RCUTORTURE_RDR_SCHED)
 		rcu_read_lock_sched();
+	preempt_disable();
+	if (statesnew & RCUTORTURE_RDR_ATOM_BH)
+		local_bh_disable();
+	if (statesnew & RCUTORTURE_RDR_ATOM_RBH)
+		rcu_read_lock_bh();
+	preempt_enable();
 	if (statesnew & RCUTORTURE_RDR_RCU)
 		idxnew = cur_ops->readlock() << RCUTORTURE_RDR_SHIFT;
 
-	/* Next, remove old protection, irq first due to bh conflict. */
+	/*
+	 * Next, remove old protection, in decreasing order of strength
+	 * to avoid unlock paths that aren't safe in the stronger
+	 * context.  Disable preemption around the ATOM enables in
+	 * case the context was only atomic due to IRQ disabling.
+	 */
+	preempt_disable();
 	if (statesold & RCUTORTURE_RDR_IRQ)
 		local_irq_enable();
-	if (statesold & RCUTORTURE_RDR_BH)
+	if (statesold & RCUTORTURE_RDR_ATOM_BH)
 		local_bh_enable();
+	if (statesold & RCUTORTURE_RDR_ATOM_RBH)
+		rcu_read_unlock_bh();
+	preempt_enable();
 	if (statesold & RCUTORTURE_RDR_PREEMPT)
 		preempt_enable();
-	if (statesold & RCUTORTURE_RDR_RBH)
-		rcu_read_unlock_bh();
 	if (statesold & RCUTORTURE_RDR_SCHED)
 		rcu_read_unlock_sched();
+	if (statesold & RCUTORTURE_RDR_BH)
+		local_bh_enable();
+	if (statesold & RCUTORTURE_RDR_RBH)
+		rcu_read_unlock_bh();
+
 	if (statesold & RCUTORTURE_RDR_RCU) {
 		bool lockit = !statesnew && !(torture_random(trsp) & 0xffff);
 
@@ -1313,6 +1338,12 @@
 	int mask = rcutorture_extend_mask_max();
 	unsigned long randmask1 = torture_random(trsp) >> 8;
 	unsigned long randmask2 = randmask1 >> 3;
+	unsigned long preempts = RCUTORTURE_RDR_PREEMPT | RCUTORTURE_RDR_SCHED;
+	unsigned long preempts_irq = preempts | RCUTORTURE_RDR_IRQ;
+	unsigned long nonatomic_bhs = RCUTORTURE_RDR_BH | RCUTORTURE_RDR_RBH;
+	unsigned long atomic_bhs = RCUTORTURE_RDR_ATOM_BH |
+				   RCUTORTURE_RDR_ATOM_RBH;
+	unsigned long tmp;
 
 	WARN_ON_ONCE(mask >> RCUTORTURE_RDR_SHIFT);
 	/* Mostly only one bit (need preemption!), sometimes lots of bits. */
@@ -1320,11 +1351,49 @@
 		mask = mask & randmask2;
 	else
 		mask = mask & (1 << (randmask2 % RCUTORTURE_RDR_NBITS));
-	/* Can't enable bh w/irq disabled. */
-	if ((mask & RCUTORTURE_RDR_IRQ) &&
-	    ((!(mask & RCUTORTURE_RDR_BH) && (oldmask & RCUTORTURE_RDR_BH)) ||
-	     (!(mask & RCUTORTURE_RDR_RBH) && (oldmask & RCUTORTURE_RDR_RBH))))
-		mask |= RCUTORTURE_RDR_BH | RCUTORTURE_RDR_RBH;
+
+	/*
+	 * Can't enable bh w/irq disabled.
+	 */
+	tmp = atomic_bhs | nonatomic_bhs;
+	if (mask & RCUTORTURE_RDR_IRQ)
+		mask |= oldmask & tmp;
+
+	/*
+	 * Ideally these sequences would be detected in debug builds
+	 * (regardless of RT), but until then don't stop testing
+	 * them on non-RT.
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * Can't release the outermost rcu lock in an irq disabled
+		 * section without preemption also being disabled, if irqs
+		 * had ever been enabled during this RCU critical section
+		 * (could leak a special flag and delay reporting the qs).
+		 */
+		if ((oldmask & RCUTORTURE_RDR_RCU) &&
+		    (mask & RCUTORTURE_RDR_IRQ) &&
+		    !(mask & preempts))
+			mask |= RCUTORTURE_RDR_RCU;
+
+		/* Can't modify atomic bh in non-atomic context */
+		if ((oldmask & atomic_bhs) && (mask & atomic_bhs) &&
+		    !(mask & preempts_irq)) {
+			mask |= oldmask & preempts_irq;
+			if (mask & RCUTORTURE_RDR_IRQ)
+				mask |= oldmask & tmp;
+		}
+		if ((mask & atomic_bhs) && !(mask & preempts_irq))
+			mask |= RCUTORTURE_RDR_PREEMPT;
+
+		/* Can't modify non-atomic bh in atomic context */
+		tmp = nonatomic_bhs;
+		if (oldmask & preempts_irq)
+			mask &= ~tmp;
+		if ((oldmask | mask) & preempts_irq)
+			mask |= oldmask & tmp;
+	}
+
 	return mask ?: RCUTORTURE_RDR_RCU;
 }
 
diff -Naur a/kernel/rcu/tree.c b/kernel/rcu/tree.c
--- a/kernel/rcu/tree.c	2020-11-23 13:48:35.141948890 +0200
+++ b/kernel/rcu/tree.c	2021-07-14 15:39:11.150140136 +0300
@@ -113,8 +113,10 @@
 static bool dump_tree;
 module_param(dump_tree, bool, 0444);
 /* By default, use RCU_SOFTIRQ instead of rcuc kthreads. */
-static bool use_softirq = true;
+static bool use_softirq = !IS_ENABLED(CONFIG_PREEMPT_RT);
+#ifndef CONFIG_PREEMPT_RT
 module_param(use_softirq, bool, 0444);
+#endif
 /* Control rcu_node-tree auto-balancing at boot time. */
 static bool rcu_fanout_exact;
 module_param(rcu_fanout_exact, bool, 0444);
diff -Naur a/kernel/rcu/update.c b/kernel/rcu/update.c
--- a/kernel/rcu/update.c	2020-11-23 13:48:35.153949131 +0200
+++ b/kernel/rcu/update.c	2021-07-14 15:39:11.150140136 +0300
@@ -69,8 +69,10 @@
 #ifndef CONFIG_TINY_RCU
 module_param(rcu_expedited, int, 0);
 module_param(rcu_normal, int, 0);
-static int rcu_normal_after_boot;
+static int rcu_normal_after_boot = IS_ENABLED(CONFIG_PREEMPT_RT);
+#ifndef CONFIG_PREEMPT_RT
 module_param(rcu_normal_after_boot, int, 0);
+#endif
 #endif /* #ifndef CONFIG_TINY_RCU */
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
diff -Naur a/kernel/sched/core.c b/kernel/sched/core.c
--- a/kernel/sched/core.c	2020-11-23 13:48:35.161949291 +0200
+++ b/kernel/sched/core.c	2021-07-14 15:39:11.210139714 +0300
@@ -63,7 +63,11 @@
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task CPU usage in us.
@@ -511,9 +515,15 @@
 #endif
 #endif
 
-static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
+static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task,
+			 bool sleeper)
 {
-	struct wake_q_node *node = &task->wake_q;
+	struct wake_q_node *node;
+
+	if (sleeper)
+		node = &task->wake_q_sleeper;
+	else
+		node = &task->wake_q;
 
 	/*
 	 * Atomically grab the task, if ->wake_q is !nil already it means
@@ -549,7 +559,13 @@
  */
 void wake_q_add(struct wake_q_head *head, struct task_struct *task)
 {
-	if (__wake_q_add(head, task))
+	if (__wake_q_add(head, task, false))
+		get_task_struct(task);
+}
+
+void wake_q_add_sleeper(struct wake_q_head *head, struct task_struct *task)
+{
+	if (__wake_q_add(head, task, true))
 		get_task_struct(task);
 }
 
@@ -572,28 +588,39 @@
  */
 void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)
 {
-	if (!__wake_q_add(head, task))
+	if (!__wake_q_add(head, task, false))
 		put_task_struct(task);
 }
 
-void wake_up_q(struct wake_q_head *head)
+void __wake_up_q(struct wake_q_head *head, bool sleeper)
 {
 	struct wake_q_node *node = head->first;
 
 	while (node != WAKE_Q_TAIL) {
 		struct task_struct *task;
 
-		task = container_of(node, struct task_struct, wake_q);
+		if (sleeper)
+			task = container_of(node, struct task_struct, wake_q_sleeper);
+		else
+			task = container_of(node, struct task_struct, wake_q);
+
 		BUG_ON(!task);
 		/* Task can safely be re-inserted now: */
 		node = node->next;
-		task->wake_q.next = NULL;
 
+		if (sleeper)
+			task->wake_q_sleeper.next = NULL;
+		else
+			task->wake_q.next = NULL;
 		/*
 		 * wake_up_process() executes a full barrier, which pairs with
 		 * the queueing in wake_q_add() so as not to miss wakeups.
 		 */
-		wake_up_process(task);
+		if (sleeper)
+			wake_up_lock_sleeper(task);
+		else
+			wake_up_process(task);
+
 		put_task_struct(task);
 	}
 }
@@ -629,6 +656,48 @@
 		trace_sched_wake_idle_without_ipi(cpu);
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+
+static int tsk_is_polling(struct task_struct *p)
+{
+#ifdef TIF_POLLING_NRFLAG
+	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
+#else
+	return 0;
+#endif
+}
+
+void resched_curr_lazy(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	int cpu;
+
+	if (!sched_feat(PREEMPT_LAZY)) {
+		resched_curr(rq);
+		return;
+	}
+
+	lockdep_assert_held(&rq->lock);
+
+	if (test_tsk_need_resched(curr))
+		return;
+
+	if (test_tsk_need_resched_lazy(curr))
+		return;
+
+	set_tsk_need_resched_lazy(curr);
+
+	cpu = cpu_of(rq);
+	if (cpu == smp_processor_id())
+		return;
+
+	/* NEED_RESCHED_LAZY must be visible before we test polling */
+	smp_mb();
+	if (!tsk_is_polling(curr))
+		smp_send_reschedule(cpu);
+}
+#endif
+
 void resched_cpu(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
@@ -1700,6 +1769,86 @@
 
 #ifdef CONFIG_SMP
 
+#ifdef CONFIG_PREEMPT_RT
+
+static void
+__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
+
+static int __set_cpus_allowed_ptr(struct task_struct *p,
+				  const struct cpumask *new_mask,
+				  u32 flags);
+
+static void migrate_disable_switch(struct rq *rq, struct task_struct *p)
+{
+	if (likely(!p->migration_disabled))
+		return;
+
+	if (p->cpus_ptr != &p->cpus_mask)
+		return;
+
+	/*
+	 * Violates locking rules! see comment in __do_set_cpus_allowed().
+	 */
+	__do_set_cpus_allowed(p, cpumask_of(rq->cpu), SCA_MIGRATE_DISABLE);
+}
+
+void migrate_disable(void)
+{
+	struct task_struct *p = current;
+
+	if (p->migration_disabled) {
+		p->migration_disabled++;
+		return;
+	}
+
+	trace_sched_migrate_disable_tp(p);
+
+	preempt_disable();
+	this_rq()->nr_pinned++;
+	p->migration_disabled = 1;
+	preempt_lazy_disable();
+	preempt_enable();
+}
+EXPORT_SYMBOL_GPL(migrate_disable);
+
+void migrate_enable(void)
+{
+	struct task_struct *p = current;
+
+	if (p->migration_disabled > 1) {
+		p->migration_disabled--;
+		return;
+	}
+
+	/*
+	 * Ensure stop_task runs either before or after this, and that
+	 * __set_cpus_allowed_ptr(SCA_MIGRATE_ENABLE) doesn't schedule().
+	 */
+	preempt_disable();
+	if (p->cpus_ptr != &p->cpus_mask)
+		__set_cpus_allowed_ptr(p, &p->cpus_mask, SCA_MIGRATE_ENABLE);
+	/*
+	 * Mustn't clear migration_disabled() until cpus_ptr points back at the
+	 * regular cpus_mask, otherwise things that race (eg.
+	 * select_fallback_rq) get confused.
+	 */
+	barrier();
+	p->migration_disabled = 0;
+	this_rq()->nr_pinned--;
+	preempt_lazy_enable();
+	preempt_enable();
+
+	trace_sched_migrate_enable_tp(p);
+}
+EXPORT_SYMBOL_GPL(migrate_enable);
+
+static inline bool rq_has_pinned_tasks(struct rq *rq)
+{
+	return rq->nr_pinned;
+}
+
+#endif
+
 /*
  * Per-CPU kthreads are allowed to run on !active && online CPUs, see
  * __set_cpus_allowed_ptr() and select_fallback_rq().
@@ -1709,7 +1858,7 @@
 	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 		return false;
 
-	if (is_per_cpu_kthread(p))
+	if (is_per_cpu_kthread(p) || is_migration_disabled(p))
 		return cpu_online(cpu);
 
 	return cpu_active(cpu);
@@ -1756,6 +1905,7 @@
 struct migration_arg {
 	struct task_struct *task;
 	int dest_cpu;
+	struct completion *done;
 };
 
 /*
@@ -1790,6 +1940,7 @@
 	struct migration_arg *arg = data;
 	struct task_struct *p = arg->task;
 	struct rq *rq = this_rq();
+	bool complete = false;
 	struct rq_flags rf;
 
 	/*
@@ -1812,15 +1963,70 @@
 	 * we're holding p->pi_lock.
 	 */
 	if (task_rq(p) == rq) {
+		if (is_migration_disabled(p))
+			goto out;
+
 		if (task_on_rq_queued(p))
 			rq = __migrate_task(rq, &rf, p, arg->dest_cpu);
 		else
 			p->wake_cpu = arg->dest_cpu;
+
+		if (arg->done) {
+			p->migration_pending = NULL;
+			complete = true;
+		}
 	}
+out:
 	rq_unlock(rq, &rf);
 	raw_spin_unlock(&p->pi_lock);
-
 	local_irq_enable();
+
+	if (complete)
+		complete_all(arg->done);
+
+	return 0;
+}
+
+int push_cpu_stop(void *arg)
+{
+	struct rq *lowest_rq = NULL, *rq = this_rq();
+	struct task_struct *p = arg;
+
+	raw_spin_lock_irq(&p->pi_lock);
+	raw_spin_lock(&rq->lock);
+
+	if (task_rq(p) != rq)
+		goto out_unlock;
+
+	if (is_migration_disabled(p)) {
+		p->migration_flags |= MDF_PUSH;
+		goto out_unlock;
+	}
+
+	p->migration_flags &= ~MDF_PUSH;
+
+	if (p->sched_class->find_lock_rq)
+		lowest_rq = p->sched_class->find_lock_rq(p, rq);
+
+	if (!lowest_rq)
+		goto out_unlock;
+
+	// XXX validate p is still the highest prio task
+	if (task_rq(p) == rq) {
+		deactivate_task(rq, p, 0);
+		set_task_cpu(p, lowest_rq->cpu);
+		activate_task(lowest_rq, p, 0);
+		resched_curr(lowest_rq);
+	}
+
+	double_unlock_balance(rq, lowest_rq);
+
+out_unlock:
+	rq->push_busy = false;
+	raw_spin_unlock(&rq->lock);
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	put_task_struct(p);
 	return 0;
 }
 
@@ -1828,18 +2034,39 @@
  * sched_class::set_cpus_allowed must do the below, but is not required to
  * actually call this function.
  */
-void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask)
+void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
 {
+	if (flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
+		p->cpus_ptr = new_mask;
+		return;
+	}
+
 	cpumask_copy(&p->cpus_mask, new_mask);
 	p->nr_cpus_allowed = cpumask_weight(new_mask);
 }
 
-void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
+static void
+__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
 {
 	struct rq *rq = task_rq(p);
 	bool queued, running;
 
-	lockdep_assert_held(&p->pi_lock);
+	/*
+	 * This here violates the locking rules for affinity, since we're only
+	 * supposed to change these variables while holding both rq->lock and
+	 * p->pi_lock.
+	 *
+	 * HOWEVER, it magically works, because ttwu() is the only code that
+	 * accesses these variables under p->pi_lock and only does so after
+	 * smp_cond_load_acquire(&p->on_cpu, !VAL), and we're in __schedule()
+	 * before finish_task().
+	 *
+	 * XXX do further audits, this smells like something putrid.
+	 */
+	if (flags & SCA_MIGRATE_DISABLE)
+		SCHED_WARN_ON(!p->on_cpu);
+	else
+		lockdep_assert_held(&p->pi_lock);
 
 	queued = task_on_rq_queued(p);
 	running = task_current(rq, p);
@@ -1855,7 +2082,7 @@
 	if (running)
 		put_prev_task(rq, p);
 
-	p->sched_class->set_cpus_allowed(p, new_mask);
+	p->sched_class->set_cpus_allowed(p, new_mask, flags);
 
 	if (queued)
 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
@@ -1863,6 +2090,130 @@
 		set_next_task(rq, p);
 }
 
+void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
+{
+	__do_set_cpus_allowed(p, new_mask, 0);
+}
+
+struct set_affinity_pending {
+	refcount_t		refs;
+	struct completion	done;
+	struct cpu_stop_work	stop_work;
+	struct migration_arg	arg;
+};
+
+/*
+ * This function is wildly self concurrent, consider at least 3 times.
+ */
+static int affine_move_task(struct rq *rq, struct rq_flags *rf,
+			    struct task_struct *p, int dest_cpu, unsigned int flags)
+{
+	struct set_affinity_pending my_pending = { }, *pending = NULL;
+	struct migration_arg arg = {
+		.task = p,
+		.dest_cpu = dest_cpu,
+	};
+	bool complete = false;
+
+	/* Can the task run on the task's current CPU? If so, we're done */
+	if (cpumask_test_cpu(task_cpu(p), &p->cpus_mask)) {
+		struct task_struct *push_task = NULL;
+
+		if ((flags & SCA_MIGRATE_ENABLE) &&
+		    (p->migration_flags & MDF_PUSH) && !rq->push_busy) {
+			rq->push_busy = true;
+			push_task = get_task_struct(p);
+		}
+
+		pending = p->migration_pending;
+		if (pending) {
+			p->migration_pending = NULL;
+			complete = true;
+		}
+		task_rq_unlock(rq, p, rf);
+
+		if (push_task) {
+			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+					    p, &rq->push_work);
+		}
+
+		if (complete)
+			goto do_complete;
+
+		return 0;
+	}
+
+	if (!(flags & SCA_MIGRATE_ENABLE)) {
+		/* serialized by p->pi_lock */
+		if (!p->migration_pending) {
+			refcount_set(&my_pending.refs, 1);
+			init_completion(&my_pending.done);
+			p->migration_pending = &my_pending;
+		} else {
+			pending = p->migration_pending;
+			refcount_inc(&pending->refs);
+		}
+	}
+	pending = p->migration_pending;
+	/*
+	 * - !MIGRATE_ENABLE:
+	 *   we'll have installed a pending if there wasn't one already.
+	 *
+	 * - MIGRATE_ENABLE:
+	 *   we're here because the current CPU isn't matching anymore,
+	 *   the only way that can happen is because of a concurrent
+	 *   set_cpus_allowed_ptr() call, which should then still be
+	 *   pending completion.
+	 *
+	 * Either way, we really should have a @pending here.
+	 */
+	if (WARN_ON_ONCE(!pending))
+		return -EINVAL;
+
+	arg.done = &pending->done;
+
+	if (flags & SCA_MIGRATE_ENABLE) {
+
+		p->migration_flags &= ~MDF_PUSH;
+		task_rq_unlock(rq, p, rf);
+		pending->arg = arg;
+		stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
+				    &pending->arg, &pending->stop_work);
+
+		return 0;
+	}
+
+	if (task_running(rq, p) || p->state == TASK_WAKING) {
+
+		task_rq_unlock(rq, p, rf);
+		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
+
+	} else {
+
+		if (!is_migration_disabled(p)) {
+			if (task_on_rq_queued(p))
+				rq = move_queued_task(rq, rf, p, dest_cpu);
+
+			p->migration_pending = NULL;
+			complete = true;
+		}
+		task_rq_unlock(rq, p, rf);
+
+do_complete:
+		if (complete)
+			complete_all(&pending->done);
+	}
+
+	wait_for_completion(&pending->done);
+
+	if (refcount_dec_and_test(&pending->refs))
+		wake_up_var(&pending->refs);
+
+	wait_var_event(&my_pending.refs, !refcount_read(&my_pending.refs));
+
+	return 0;
+}
+
 /*
  * Change a given task's CPU affinity. Migrate the thread to a
  * proper CPU and schedule it away if the CPU it's executing on
@@ -1873,7 +2224,8 @@
  * call is not atomic; no spinlocks may be held.
  */
 static int __set_cpus_allowed_ptr(struct task_struct *p,
-				  const struct cpumask *new_mask, bool check)
+				  const struct cpumask *new_mask,
+				  u32 flags)
 {
 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
 	unsigned int dest_cpu;
@@ -1884,9 +2236,14 @@
 	rq = task_rq_lock(p, &rf);
 	update_rq_clock(rq);
 
-	if (p->flags & PF_KTHREAD) {
+	if (p->flags & PF_KTHREAD || is_migration_disabled(p)) {
 		/*
-		 * Kernel threads are allowed on online && !active CPUs
+		 * Kernel threads are allowed on online && !active CPUs.
+		 *
+		 * Specifically, migration_disabled() tasks must not fail the
+		 * cpumask_any_and_distribute() pick below, esp. so on
+		 * SCA_MIGRATE_ENABLE, otherwise we'll not call
+		 * set_cpus_allowed_common() and actually reset p->cpus_ptr.
 		 */
 		cpu_valid_mask = cpu_online_mask;
 	}
@@ -1895,12 +2252,12 @@
 	 * Must re-check here, to close a race against __kthread_bind(),
 	 * sched_setaffinity() is not guaranteed to observe the flag.
 	 */
-	if (check && (p->flags & PF_NO_SETAFFINITY)) {
+	if ((flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	if (cpumask_equal(&p->cpus_mask, new_mask))
+	if (!(flags & SCA_MIGRATE_ENABLE) && cpumask_equal(&p->cpus_mask, new_mask))
 		goto out;
 
 	/*
@@ -1914,7 +2271,7 @@
 		goto out;
 	}
 
-	do_set_cpus_allowed(p, new_mask);
+	__do_set_cpus_allowed(p, new_mask, flags);
 
 	if (p->flags & PF_KTHREAD) {
 		/*
@@ -1926,23 +2283,8 @@
 			p->nr_cpus_allowed != 1);
 	}
 
-	/* Can the task run on the task's current CPU? If so, we're done */
-	if (cpumask_test_cpu(task_cpu(p), new_mask))
-		goto out;
+	return affine_move_task(rq, &rf, p, dest_cpu, flags);
 
-	if (task_running(rq, p) || p->state == TASK_WAKING) {
-		struct migration_arg arg = { p, dest_cpu };
-		/* Need help from migration thread: drop lock and wait. */
-		task_rq_unlock(rq, p, &rf);
-		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
-		return 0;
-	} else if (task_on_rq_queued(p)) {
-		/*
-		 * OK, since we're going to drop the lock immediately
-		 * afterwards anyway.
-		 */
-		rq = move_queued_task(rq, &rf, p, dest_cpu);
-	}
 out:
 	task_rq_unlock(rq, p, &rf);
 
@@ -1951,7 +2293,7 @@
 
 int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 {
-	return __set_cpus_allowed_ptr(p, new_mask, false);
+	return __set_cpus_allowed_ptr(p, new_mask, 0);
 }
 EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
@@ -1992,6 +2334,8 @@
 	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing.
 	 */
 	WARN_ON_ONCE(!cpu_online(new_cpu));
+
+	WARN_ON_ONCE(is_migration_disabled(p));
 #endif
 
 	trace_sched_migrate_task(p, new_cpu);
@@ -2124,6 +2468,18 @@
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+static bool check_task_state(struct task_struct *p, long match_state)
+{
+	bool match = false;
+
+	raw_spin_lock_irq(&p->pi_lock);
+	if (p->state == match_state || p->saved_state == match_state)
+		match = true;
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	return match;
+}
+
 /*
  * wait_task_inactive - wait for a thread to unschedule.
  *
@@ -2168,7 +2524,7 @@
 		 * is actually now running somewhere else!
 		 */
 		while (task_running(rq, p)) {
-			if (match_state && unlikely(p->state != match_state))
+			if (match_state && !check_task_state(p, match_state))
 				return 0;
 			cpu_relax();
 		}
@@ -2183,7 +2539,8 @@
 		running = task_running(rq, p);
 		queued = task_on_rq_queued(p);
 		ncsw = 0;
-		if (!match_state || p->state == match_state)
+		if (!match_state || p->state == match_state ||
+		    p->saved_state == match_state)
 			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
 		task_rq_unlock(rq, p, &rf);
 
@@ -2322,6 +2679,12 @@
 			}
 			fallthrough;
 		case possible:
+			/*
+			 * XXX When called from select_task_rq() we only
+			 * hold p->pi_lock and again violate locking order.
+			 *
+			 * More yuck to audit.
+			 */
 			do_set_cpus_allowed(p, cpu_possible_mask);
 			state = fail;
 			break;
@@ -2356,7 +2719,7 @@
 {
 	lockdep_assert_held(&p->pi_lock);
 
-	if (p->nr_cpus_allowed > 1)
+	if (p->nr_cpus_allowed > 1 && !is_migration_disabled(p))
 		cpu = p->sched_class->select_task_rq(p, cpu, sd_flags, wake_flags);
 	else
 		cpu = cpumask_any(p->cpus_ptr);
@@ -2379,6 +2742,7 @@
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
 {
+	static struct lock_class_key stop_pi_lock;
 	struct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };
 	struct task_struct *old_stop = cpu_rq(cpu)->stop;
 
@@ -2394,6 +2758,20 @@
 		sched_setscheduler_nocheck(stop, SCHED_FIFO, &param);
 
 		stop->sched_class = &stop_sched_class;
+
+		/*
+		 * The PI code calls rt_mutex_setprio() with ->pi_lock held to
+		 * adjust the effective priority of a task. As a result,
+		 * rt_mutex_setprio() can trigger (RT) balancing operations,
+		 * which can then trigger wakeups of the stop thread to push
+		 * around the current task.
+		 *
+		 * The stop task itself will never be part of the PI-chain, it
+		 * never blocks, therefore that ->pi_lock recursion is safe.
+		 * Tell lockdep about this by placing the stop->pi_lock in its
+		 * own class.
+		 */
+		lockdep_set_class(&stop->pi_lock, &stop_pi_lock);
 	}
 
 	cpu_rq(cpu)->stop = stop;
@@ -2410,13 +2788,25 @@
 #else
 
 static inline int __set_cpus_allowed_ptr(struct task_struct *p,
-					 const struct cpumask *new_mask, bool check)
+					 const struct cpumask *new_mask,
+					 u32 flags)
 {
 	return set_cpus_allowed_ptr(p, new_mask);
 }
 
 #endif /* CONFIG_SMP */
 
+#if !defined(CONFIG_SMP) || !defined(CONFIG_PREEMPT_RT)
+
+static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
+
+static inline bool rq_has_pinned_tasks(struct rq *rq)
+{
+	return false;
+}
+
+#endif
+
 static void
 ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 {
@@ -2828,7 +3218,7 @@
 	int cpu, success = 0;
 
 	preempt_disable();
-	if (p == current) {
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT) && p == current) {
 		/*
 		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
 		 * == smp_processor_id()'. Together this means we can special
@@ -2858,8 +3248,26 @@
 	 */
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	smp_mb__after_spinlock();
-	if (!(p->state & state))
+	if (!(p->state & state)) {
+		/*
+		 * The task might be running due to a spinlock sleeper
+		 * wakeup. Check the saved state and set it to running
+		 * if the wakeup condition is true.
+		 */
+		if (!(wake_flags & WF_LOCK_SLEEPER)) {
+			if (p->saved_state & state) {
+				p->saved_state = TASK_RUNNING;
+				success = 1;
+			}
+		}
 		goto unlock;
+	}
+	/*
+	 * If this is a regular wakeup, then we can unconditionally
+	 * clear the saved state of a "lock sleeper".
+	 */
+	if (!(wake_flags & WF_LOCK_SLEEPER))
+		p->saved_state = TASK_RUNNING;
 
 	trace_sched_waking(p);
 
@@ -3049,6 +3457,18 @@
 }
 EXPORT_SYMBOL(wake_up_process);
 
+/**
+ * wake_up_lock_sleeper - Wake up a specific process blocked on a "sleeping lock"
+ * @p: The process to be woken up.
+ *
+ * Same as wake_up_process() above, but wake_flags=WF_LOCK_SLEEPER to indicate
+ * the nature of the wakeup.
+ */
+int wake_up_lock_sleeper(struct task_struct *p)
+{
+	return try_to_wake_up(p, TASK_UNINTERRUPTIBLE, WF_LOCK_SLEEPER);
+}
+
 int wake_up_state(struct task_struct *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
@@ -3295,6 +3715,9 @@
 	p->on_cpu = 0;
 #endif
 	init_task_preempt_count(p);
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(p)->preempt_lazy_count = 0;
+#endif
 #ifdef CONFIG_SMP
 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
 	RB_CLEAR_NODE(&p->pushable_dl_tasks);
@@ -3489,6 +3912,91 @@
 #endif
 }
 
+#ifdef CONFIG_SMP
+
+static void do_balance_callbacks(struct rq *rq, struct callback_head *head)
+{
+	void (*func)(struct rq *rq);
+	struct callback_head *next;
+
+	lockdep_assert_held(&rq->lock);
+
+	while (head) {
+		func = (void (*)(struct rq *))head->func;
+		next = head->next;
+		head->next = NULL;
+		head = next;
+
+		func(rq);
+	}
+}
+
+static inline struct callback_head *splice_balance_callbacks(struct rq *rq)
+{
+	struct callback_head *head = rq->balance_callback;
+
+	lockdep_assert_held(&rq->lock);
+	if (head) {
+		rq->balance_callback = NULL;
+		rq->balance_flags &= ~BALANCE_WORK;
+	}
+
+	return head;
+}
+
+static void __balance_callbacks(struct rq *rq)
+{
+	do_balance_callbacks(rq, splice_balance_callbacks(rq));
+}
+
+static inline void balance_callbacks(struct rq *rq, struct callback_head *head)
+{
+	unsigned long flags;
+
+	if (unlikely(head)) {
+		raw_spin_lock_irqsave(&rq->lock, flags);
+		do_balance_callbacks(rq, head);
+		raw_spin_unlock_irqrestore(&rq->lock, flags);
+	}
+}
+
+static bool balance_push(struct rq *rq);
+
+static inline void balance_switch(struct rq *rq)
+{
+	if (unlikely(rq->balance_flags)) {
+		/*
+		 * Run the balance_callbacks, except on hotplug
+		 * when we need to push the current task away.
+		 */
+		if (!IS_ENABLED(CONFIG_HOTPLUG_CPU) ||
+		    !(rq->balance_flags & BALANCE_PUSH) ||
+		    !balance_push(rq))
+			__balance_callbacks(rq);
+	}
+}
+
+#else
+
+static inline void __balance_callbacks(struct rq *rq)
+{
+}
+
+static inline struct callback_head *splice_balance_callbacks(struct rq *rq)
+{
+	return NULL;
+}
+
+static inline void balance_callbacks(struct rq *rq, struct callback_head *head)
+{
+}
+
+static inline void balance_switch(struct rq *rq)
+{
+}
+
+#endif
+
 static inline void
 prepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf)
 {
@@ -3514,6 +4022,7 @@
 	 * prev into current:
 	 */
 	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
+	balance_switch(rq);
 	raw_spin_unlock_irq(&rq->lock);
 }
 
@@ -3631,23 +4140,18 @@
 	 *   provided by mmdrop(),
 	 * - a sync_core for SYNC_CORE.
 	 */
+	/*
+	 * We use mmdrop_delayed() here so we don't have to do the
+	 * full __mmdrop() when we are the last user.
+	 */
 	if (mm) {
 		membarrier_mm_sync_core_before_usermode(mm);
-		mmdrop(mm);
+		mmdrop_delayed(mm);
 	}
 	if (unlikely(prev_state == TASK_DEAD)) {
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
 
-		/*
-		 * Remove function-return probe instances associated with this
-		 * task and put them back on the free list.
-		 */
-		kprobe_flush_task(prev);
-
-		/* Task is done with its stack. */
-		put_task_stack(prev);
-
 		put_task_struct_rcu_user(prev);
 	}
 
@@ -3655,43 +4159,6 @@
 	return rq;
 }
 
-#ifdef CONFIG_SMP
-
-/* rq->lock is NOT held, but preemption is disabled */
-static void __balance_callback(struct rq *rq)
-{
-	struct callback_head *head, *next;
-	void (*func)(struct rq *rq);
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&rq->lock, flags);
-	head = rq->balance_callback;
-	rq->balance_callback = NULL;
-	while (head) {
-		func = (void (*)(struct rq *))head->func;
-		next = head->next;
-		head->next = NULL;
-		head = next;
-
-		func(rq);
-	}
-	raw_spin_unlock_irqrestore(&rq->lock, flags);
-}
-
-static inline void balance_callback(struct rq *rq)
-{
-	if (unlikely(rq->balance_callback))
-		__balance_callback(rq);
-}
-
-#else
-
-static inline void balance_callback(struct rq *rq)
-{
-}
-
-#endif
-
 /**
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
@@ -3711,7 +4178,6 @@
 	 */
 
 	rq = finish_task_switch(prev);
-	balance_callback(rq);
 	preempt_enable();
 
 	if (current->set_child_tid)
@@ -4406,7 +4872,7 @@
  *
  * WARNING: must be called with preemption disabled!
  */
-static void __sched notrace __schedule(bool preempt)
+static void __sched notrace __schedule(bool preempt, bool spinning_lock)
 {
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
@@ -4459,7 +4925,7 @@
 	 *  - ptrace_{,un}freeze_traced() can change ->state underneath us.
 	 */
 	prev_state = prev->state;
-	if (!preempt && prev_state) {
+	if ((!preempt || spinning_lock) && prev_state) {
 		if (signal_pending_state(prev_state, prev)) {
 			prev->state = TASK_RUNNING;
 		} else {
@@ -4494,6 +4960,7 @@
 
 	next = pick_next_task(rq, prev, &rf);
 	clear_tsk_need_resched(prev);
+	clear_tsk_need_resched_lazy(prev);
 	clear_preempt_need_resched();
 
 	if (likely(prev != next)) {
@@ -4519,6 +4986,7 @@
 		 */
 		++*switch_count;
 
+		migrate_disable_switch(rq, prev);
 		psi_sched_switch(prev, next, !task_on_rq_queued(prev));
 
 		trace_sched_switch(preempt, prev, next);
@@ -4527,10 +4995,11 @@
 		rq = context_switch(rq, prev, next, &rf);
 	} else {
 		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
-		rq_unlock_irq(rq, &rf);
-	}
 
-	balance_callback(rq);
+		rq_unpin_lock(rq, &rf);
+		__balance_callbacks(rq);
+		raw_spin_unlock_irq(&rq->lock);
+	}
 }
 
 void __noreturn do_task_dead(void)
@@ -4541,7 +5010,7 @@
 	/* Tell freezer to ignore us: */
 	current->flags |= PF_NOFREEZE;
 
-	__schedule(false);
+	__schedule(false, false);
 	BUG();
 
 	/* Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */
@@ -4571,9 +5040,6 @@
 		preempt_enable_no_resched();
 	}
 
-	if (tsk_is_pi_blocked(tsk))
-		return;
-
 	/*
 	 * If we are going to sleep and we have plugged IO queued,
 	 * make sure to submit it to avoid deadlocks.
@@ -4599,7 +5065,7 @@
 	sched_submit_work(tsk);
 	do {
 		preempt_disable();
-		__schedule(false);
+		__schedule(false, false);
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
 	sched_update_worker(tsk);
@@ -4627,7 +5093,7 @@
 	 */
 	WARN_ON_ONCE(current->state);
 	do {
-		__schedule(false);
+		__schedule(false, false);
 	} while (need_resched());
 }
 
@@ -4680,7 +5146,7 @@
 		 */
 		preempt_disable_notrace();
 		preempt_latency_start(1);
-		__schedule(true);
+		__schedule(true, false);
 		preempt_latency_stop(1);
 		preempt_enable_no_resched_notrace();
 
@@ -4691,6 +5157,30 @@
 	} while (need_resched());
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+/*
+ * If TIF_NEED_RESCHED is then we allow to be scheduled away since this is
+ * set by a RT task. Oterwise we try to avoid beeing scheduled out as long as
+ * preempt_lazy_count counter >0.
+ */
+static __always_inline int preemptible_lazy(void)
+{
+	if (test_thread_flag(TIF_NEED_RESCHED))
+		return 1;
+	if (current_thread_info()->preempt_lazy_count)
+		return 0;
+	return 1;
+}
+
+#else
+
+static inline int preemptible_lazy(void)
+{
+	return 1;
+}
+
+#endif
+
 #ifdef CONFIG_PREEMPTION
 /*
  * This is the entry point to schedule() from in-kernel preemption
@@ -4704,12 +5194,26 @@
 	 */
 	if (likely(!preemptible()))
 		return;
-
+	if (!preemptible_lazy())
+		return;
 	preempt_schedule_common();
 }
 NOKPROBE_SYMBOL(preempt_schedule);
 EXPORT_SYMBOL(preempt_schedule);
 
+#ifdef CONFIG_PREEMPT_RT
+void __sched notrace preempt_schedule_lock(void)
+{
+	do {
+		preempt_disable();
+		__schedule(true, true);
+		sched_preempt_enable_no_resched();
+	} while (need_resched());
+}
+NOKPROBE_SYMBOL(preempt_schedule_lock);
+EXPORT_SYMBOL(preempt_schedule_lock);
+#endif
+
 /**
  * preempt_schedule_notrace - preempt_schedule called by tracing
  *
@@ -4731,6 +5235,9 @@
 	if (likely(!preemptible()))
 		return;
 
+	if (!preemptible_lazy())
+		return;
+
 	do {
 		/*
 		 * Because the function tracer can trace preempt_count_sub()
@@ -4753,7 +5260,7 @@
 		 * an infinite recursion.
 		 */
 		prev_ctx = exception_enter();
-		__schedule(true);
+		__schedule(true, false);
 		exception_exit(prev_ctx);
 
 		preempt_latency_stop(1);
@@ -4782,7 +5289,7 @@
 	do {
 		preempt_disable();
 		local_irq_enable();
-		__schedule(true);
+		__schedule(true, false);
 		local_irq_disable();
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
@@ -4938,9 +5445,11 @@
 out_unlock:
 	/* Avoid rq from going away on us: */
 	preempt_disable();
-	__task_rq_unlock(rq, &rf);
 
-	balance_callback(rq);
+	rq_unpin_lock(rq, &rf);
+	__balance_callbacks(rq);
+	raw_spin_unlock(&rq->lock);
+
 	preempt_enable();
 }
 #else
@@ -5214,6 +5723,7 @@
 	int retval, oldprio, oldpolicy = -1, queued, running;
 	int new_effective_prio, policy = attr->sched_policy;
 	const struct sched_class *prev_class;
+	struct callback_head *head;
 	struct rq_flags rf;
 	int reset_on_fork;
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
@@ -5452,6 +5962,7 @@
 
 	/* Avoid rq from going away on us: */
 	preempt_disable();
+	head = splice_balance_callbacks(rq);
 	task_rq_unlock(rq, p, &rf);
 
 	if (pi) {
@@ -5460,7 +5971,7 @@
 	}
 
 	/* Run balance callbacks after we've adjusted the PI chain: */
-	balance_callback(rq);
+	balance_callbacks(rq, head);
 	preempt_enable();
 
 	return 0;
@@ -5955,7 +6466,7 @@
 	}
 #endif
 again:
-	retval = __set_cpus_allowed_ptr(p, new_mask, true);
+	retval = __set_cpus_allowed_ptr(p, new_mask, SCA_CHECK);
 
 	if (!retval) {
 		cpuset_cpus_allowed(p, cpus_allowed);
@@ -6538,7 +7049,7 @@
 	 *
 	 * And since this is boot we can forgo the serialization.
 	 */
-	set_cpus_allowed_common(idle, cpumask_of(cpu));
+	set_cpus_allowed_common(idle, cpumask_of(cpu), 0);
 #endif
 	/*
 	 * We're having a chicken and egg problem, even though we are
@@ -6565,7 +7076,9 @@
 
 	/* Set the preempt count _outside_ the spinlocks! */
 	init_idle_preempt_count(idle, cpu);
-
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(idle)->preempt_lazy_count = 0;
+#endif
 	/*
 	 * The idle tasks have their own, simple scheduling class:
 	 */
@@ -6670,6 +7183,7 @@
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_HOTPLUG_CPU
+
 /*
  * Ensure that the idle task is using init_mm right before its CPU goes
  * offline.
@@ -6689,119 +7203,129 @@
 	/* finish_cpu(), as ran on the BP, will clean up the active_mm state */
 }
 
-/*
- * Since this CPU is going 'away' for a while, fold any nr_active delta
- * we might have. Assumes we're called after migrate_tasks() so that the
- * nr_active count is stable. We need to take the teardown thread which
- * is calling this into account, so we hand in adjust = 1 to the load
- * calculation.
- *
- * Also see the comment "Global load-average calculations".
- */
-static void calc_load_migrate(struct rq *rq)
+static int __balance_push_cpu_stop(void *arg)
 {
-	long delta = calc_load_fold_active(rq, 1);
-	if (delta)
-		atomic_long_add(delta, &calc_load_tasks);
-}
+	struct task_struct *p = arg;
+	struct rq *rq = this_rq();
+	struct rq_flags rf;
+	int cpu;
 
-static struct task_struct *__pick_migrate_task(struct rq *rq)
-{
-	const struct sched_class *class;
-	struct task_struct *next;
+	raw_spin_lock_irq(&p->pi_lock);
+	rq_lock(rq, &rf);
 
-	for_each_class(class) {
-		next = class->pick_next_task(rq);
-		if (next) {
-			next->sched_class->put_prev_task(rq, next);
-			return next;
-		}
+	update_rq_clock(rq);
+
+	if (task_rq(p) == rq && task_on_rq_queued(p)) {
+		cpu = select_fallback_rq(rq->cpu, p);
+		rq = __migrate_task(rq, &rf, p, cpu);
 	}
 
-	/* The idle class should always have a runnable task */
-	BUG();
+	rq_unlock(rq, &rf);
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	put_task_struct(p);
+
+	return 0;
 }
 
+static DEFINE_PER_CPU(struct cpu_stop_work, push_work);
+
 /*
- * Migrate all tasks from the rq, sleeping tasks will be migrated by
- * try_to_wake_up()->select_task_rq().
- *
- * Called with rq->lock held even though we'er in stop_machine() and
- * there's no concurrency possible, we hold the required locks anyway
- * because of lock validation efforts.
- */
-static void migrate_tasks(struct rq *dead_rq, struct rq_flags *rf)
-{
-	struct rq *rq = dead_rq;
-	struct task_struct *next, *stop = rq->stop;
-	struct rq_flags orf = *rf;
-	int dest_cpu;
+ * Ensure we only run per-cpu kthreads once the CPU goes !active.
+ */
+static bool balance_push(struct rq *rq)
+{
+	struct task_struct *push_task = rq->curr;
+
+	lockdep_assert_held(&rq->lock);
+	SCHED_WARN_ON(rq->cpu != smp_processor_id());
 
 	/*
-	 * Fudge the rq selection such that the below task selection loop
-	 * doesn't get stuck on the currently eligible stop task.
-	 *
-	 * We're currently inside stop_machine() and the rq is either stuck
-	 * in the stop_machine_cpu_stop() loop, or we're executing this code,
-	 * either way we should never end up calling schedule() until we're
-	 * done here.
+	 * Both the cpu-hotplug and stop task are in this case and are
+	 * required to complete the hotplug process.
 	 */
-	rq->stop = NULL;
+	if (is_per_cpu_kthread(push_task) || is_migration_disabled(push_task)) {
+		/*
+		 * If this is the idle task on the outgoing CPU try to wake
+		 * up the hotplug control thread which might wait for the
+		 * last task to vanish. The rcuwait_active() check is
+		 * accurate here because the waiter is pinned on this CPU
+		 * and can't obviously be running in parallel.
+		 *
+		 * On RT kernels this also has to check whether there are
+		 * pinned and scheduled out tasks on the runqueue. They
+		 * need to leave the migrate disabled section first.
+		 */
+		if (!rq->nr_running && !rq_has_pinned_tasks(rq) &&
+		    rcuwait_active(&rq->hotplug_wait)) {
+			raw_spin_unlock(&rq->lock);
+			rcuwait_wake_up(&rq->hotplug_wait);
+			raw_spin_lock(&rq->lock);
+		}
+		return false;
+	}
 
+	get_task_struct(push_task);
 	/*
-	 * put_prev_task() and pick_next_task() sched
-	 * class method both need to have an up-to-date
-	 * value of rq->clock[_task]
+	 * Temporarily drop rq->lock such that we can wake-up the stop task.
+	 * Both preemption and IRQs are still disabled.
 	 */
-	update_rq_clock(rq);
+	raw_spin_unlock(&rq->lock);
+	stop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,
+			    this_cpu_ptr(&push_work));
+	/*
+	 * At this point need_resched() is true and we'll take the loop in
+	 * schedule(). The next pick is obviously going to be the stop task
+	 * which is_per_cpu_kthread() and will push this task away.
+	 */
+	raw_spin_lock(&rq->lock);
 
-	for (;;) {
-		/*
-		 * There's this thread running, bail when that's the only
-		 * remaining thread:
-		 */
-		if (rq->nr_running == 1)
-			break;
+	return true;
+}
 
-		next = __pick_migrate_task(rq);
+static void balance_push_set(int cpu, bool on)
+{
+	struct rq *rq = cpu_rq(cpu);
+	struct rq_flags rf;
 
-		/*
-		 * Rules for changing task_struct::cpus_mask are holding
-		 * both pi_lock and rq->lock, such that holding either
-		 * stabilizes the mask.
-		 *
-		 * Drop rq->lock is not quite as disastrous as it usually is
-		 * because !cpu_active at this point, which means load-balance
-		 * will not interfere. Also, stop-machine.
-		 */
-		rq_unlock(rq, rf);
-		raw_spin_lock(&next->pi_lock);
-		rq_relock(rq, rf);
+	rq_lock_irqsave(rq, &rf);
+	if (on)
+		rq->balance_flags |= BALANCE_PUSH;
+	else
+		rq->balance_flags &= ~BALANCE_PUSH;
+	rq_unlock_irqrestore(rq, &rf);
+}
 
-		/*
-		 * Since we're inside stop-machine, _nothing_ should have
-		 * changed the task, WARN if weird stuff happened, because in
-		 * that case the above rq->lock drop is a fail too.
-		 */
-		if (WARN_ON(task_rq(next) != rq || !task_on_rq_queued(next))) {
-			raw_spin_unlock(&next->pi_lock);
-			continue;
-		}
+/*
+ * Invoked from a CPUs hotplug control thread after the CPU has been marked
+ * inactive. All tasks which are not per CPU kernel threads are either
+ * pushed off this CPU now via balance_push() or placed on a different CPU
+ * during wakeup. Wait until the CPU is quiescent.
+ */
+static void balance_hotplug_wait(void)
+{
+	struct rq *rq = this_rq();
 
-		/* Find suitable destination for @next, with force if needed. */
-		dest_cpu = select_fallback_rq(dead_rq->cpu, next);
-		rq = __migrate_task(rq, rf, next, dest_cpu);
-		if (rq != dead_rq) {
-			rq_unlock(rq, rf);
-			rq = dead_rq;
-			*rf = orf;
-			rq_relock(rq, rf);
-		}
-		raw_spin_unlock(&next->pi_lock);
-	}
+	rcuwait_wait_event(&rq->hotplug_wait,
+			   rq->nr_running == 1 && !rq_has_pinned_tasks(rq),
+			   TASK_UNINTERRUPTIBLE);
+}
+
+#else
 
-	rq->stop = stop;
+static inline bool balance_push(struct rq *rq)
+{
+	return false;
+}
+
+static void balance_push_set(int cpu, bool on)
+{
 }
+
+static inline void balance_hotplug_wait(void)
+{
+}
+
 #endif /* CONFIG_HOTPLUG_CPU */
 
 void set_rq_online(struct rq *rq)
@@ -6887,6 +7411,8 @@
 	struct rq *rq = cpu_rq(cpu);
 	struct rq_flags rf;
 
+	balance_push_set(cpu, false);
+
 #ifdef CONFIG_SCHED_SMT
 	/*
 	 * When going up, increment the number of cores with SMT present.
@@ -6922,6 +7448,8 @@
 
 int sched_cpu_deactivate(unsigned int cpu)
 {
+	struct rq *rq = cpu_rq(cpu);
+	struct rq_flags rf;
 	int ret;
 
 	set_cpu_active(cpu, false);
@@ -6934,6 +7462,16 @@
 	 */
 	synchronize_rcu();
 
+	balance_push_set(cpu, true);
+
+	rq_lock_irqsave(rq, &rf);
+	if (rq->rd) {
+		update_rq_clock(rq);
+		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
+		set_rq_offline(rq);
+	}
+	rq_unlock_irqrestore(rq, &rf);
+
 #ifdef CONFIG_SCHED_SMT
 	/*
 	 * When going down, decrement the number of cores with SMT present.
@@ -6947,6 +7485,7 @@
 
 	ret = cpuset_cpu_inactive(cpu);
 	if (ret) {
+		balance_push_set(cpu, false);
 		set_cpu_active(cpu, true);
 		return ret;
 	}
@@ -6970,6 +7509,41 @@
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
+
+/*
+ * Invoked immediately before the stopper thread is invoked to bring the
+ * CPU down completely. At this point all per CPU kthreads except the
+ * hotplug thread (current) and the stopper thread (inactive) have been
+ * either parked or have been unbound from the outgoing CPU. Ensure that
+ * any of those which might be on the way out are gone.
+ *
+ * If after this point a bound task is being woken on this CPU then the
+ * responsible hotplug callback has failed to do it's job.
+ * sched_cpu_dying() will catch it with the appropriate fireworks.
+ */
+int sched_cpu_wait_empty(unsigned int cpu)
+{
+	balance_hotplug_wait();
+	return 0;
+}
+
+/*
+ * Since this CPU is going 'away' for a while, fold any nr_active delta we
+ * might have. Called from the CPU stopper task after ensuring that the
+ * stopper is the last running task on the CPU, so nr_active count is
+ * stable. We need to take the teardown thread which is calling this into
+ * account, so we hand in adjust = 1 to the load calculation.
+ *
+ * Also see the comment "Global load-average calculations".
+ */
+static void calc_load_migrate(struct rq *rq)
+{
+	long delta = calc_load_fold_active(rq, 1);
+
+	if (delta)
+		atomic_long_add(delta, &calc_load_tasks);
+}
+
 int sched_cpu_dying(unsigned int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
@@ -6979,12 +7553,7 @@
 	sched_tick_stop(cpu);
 
 	rq_lock_irqsave(rq, &rf);
-	if (rq->rd) {
-		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
-		set_rq_offline(rq);
-	}
-	migrate_tasks(rq, &rf);
-	BUG_ON(rq->nr_running != 1);
+	BUG_ON(rq->nr_running != 1 || rq_has_pinned_tasks(rq));
 	rq_unlock_irqrestore(rq, &rf);
 
 	calc_load_migrate(rq);
@@ -7191,6 +7760,9 @@
 
 		rq_csd_init(rq, &rq->nohz_csd, nohz_csd_func);
 #endif
+#ifdef CONFIG_HOTPLUG_CPU
+		rcuwait_init(&rq->hotplug_wait);
+#endif
 #endif /* CONFIG_SMP */
 		hrtick_rq_init(rq);
 		atomic_set(&rq->nr_iowait, 0);
@@ -7231,7 +7803,7 @@
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 static inline int preempt_count_equals(int preempt_offset)
 {
-	int nested = preempt_count() + rcu_preempt_depth();
+	int nested = preempt_count() + sched_rcu_preempt_depth();
 
 	return (nested == preempt_offset);
 }
diff -Naur a/kernel/sched/cpudeadline.c b/kernel/sched/cpudeadline.c
--- a/kernel/sched/cpudeadline.c	2020-11-23 13:48:35.161949291 +0200
+++ b/kernel/sched/cpudeadline.c	2021-07-14 15:39:11.210139714 +0300
@@ -120,7 +120,7 @@
 	const struct sched_dl_entity *dl_se = &p->dl;
 
 	if (later_mask &&
-	    cpumask_and(later_mask, cp->free_cpus, p->cpus_ptr)) {
+	    cpumask_and(later_mask, cp->free_cpus, &p->cpus_mask)) {
 		unsigned long cap, max_cap = 0;
 		int cpu, max_cpu = -1;
 
@@ -151,7 +151,7 @@
 
 		WARN_ON(best_cpu != -1 && !cpu_present(best_cpu));
 
-		if (cpumask_test_cpu(best_cpu, p->cpus_ptr) &&
+		if (cpumask_test_cpu(best_cpu, &p->cpus_mask) &&
 		    dl_time_before(dl_se->deadline, cp->elements[0].dl)) {
 			if (later_mask)
 				cpumask_set_cpu(best_cpu, later_mask);
diff -Naur a/kernel/sched/cpupri.c b/kernel/sched/cpupri.c
--- a/kernel/sched/cpupri.c	2020-11-23 13:48:35.165949371 +0200
+++ b/kernel/sched/cpupri.c	2021-07-14 15:39:11.210139714 +0300
@@ -73,11 +73,11 @@
 	if (skip)
 		return 0;
 
-	if (cpumask_any_and(p->cpus_ptr, vec->mask) >= nr_cpu_ids)
+	if (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)
 		return 0;
 
 	if (lowest_mask) {
-		cpumask_and(lowest_mask, p->cpus_ptr, vec->mask);
+		cpumask_and(lowest_mask, &p->cpus_mask, vec->mask);
 
 		/*
 		 * We have to ensure that we have at least one bit
diff -Naur a/kernel/sched/deadline.c b/kernel/sched/deadline.c
--- a/kernel/sched/deadline.c	2020-11-23 13:48:35.169949452 +0200
+++ b/kernel/sched/deadline.c	2021-07-14 15:39:11.214139686 +0300
@@ -543,7 +543,7 @@
 
 static inline bool need_pull_dl_task(struct rq *rq, struct task_struct *prev)
 {
-	return dl_task(prev);
+	return rq->online && dl_task(prev);
 }
 
 static DEFINE_PER_CPU(struct callback_head, dl_push_head);
@@ -1888,7 +1888,7 @@
 static int pick_dl_task(struct rq *rq, struct task_struct *p, int cpu)
 {
 	if (!task_running(rq, p) &&
-	    cpumask_test_cpu(cpu, p->cpus_ptr))
+	    cpumask_test_cpu(cpu, &p->cpus_mask))
 		return 1;
 	return 0;
 }
@@ -2001,7 +2001,7 @@
 	if (this_cpu != -1)
 		return this_cpu;
 
-	cpu = cpumask_any(later_mask);
+	cpu = cpumask_any_distribute(later_mask);
 	if (cpu < nr_cpu_ids)
 		return cpu;
 
@@ -2038,7 +2038,7 @@
 		/* Retry if something changed. */
 		if (double_lock_balance(rq, later_rq)) {
 			if (unlikely(task_rq(task) != rq ||
-				     !cpumask_test_cpu(later_rq->cpu, task->cpus_ptr) ||
+				     !cpumask_test_cpu(later_rq->cpu, &task->cpus_mask) ||
 				     task_running(rq, task) ||
 				     !dl_task(task) ||
 				     !task_on_rq_queued(task))) {
@@ -2182,7 +2182,7 @@
 static void pull_dl_task(struct rq *this_rq)
 {
 	int this_cpu = this_rq->cpu, cpu;
-	struct task_struct *p;
+	struct task_struct *p, *push_task;
 	bool resched = false;
 	struct rq *src_rq;
 	u64 dmin = LONG_MAX;
@@ -2212,6 +2212,7 @@
 			continue;
 
 		/* Might drop this_rq->lock */
+		push_task = NULL;
 		double_lock_balance(this_rq, src_rq);
 
 		/*
@@ -2243,17 +2244,28 @@
 					   src_rq->curr->dl.deadline))
 				goto skip;
 
-			resched = true;
-
-			deactivate_task(src_rq, p, 0);
-			set_task_cpu(p, this_cpu);
-			activate_task(this_rq, p, 0);
-			dmin = p->dl.deadline;
+			if (is_migration_disabled(p)) {
+				trace_sched_migrate_pull_tp(p);
+				push_task = get_push_task(src_rq);
+			} else {
+				deactivate_task(src_rq, p, 0);
+				set_task_cpu(p, this_cpu);
+				activate_task(this_rq, p, 0);
+				dmin = p->dl.deadline;
+				resched = true;
+			}
 
 			/* Is there any other task even earlier? */
 		}
 skip:
 		double_unlock_balance(this_rq, src_rq);
+
+		if (push_task) {
+			raw_spin_unlock(&this_rq->lock);
+			stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+					    push_task, &src_rq->push_work);
+			raw_spin_lock(&this_rq->lock);
+		}
 	}
 
 	if (resched)
@@ -2277,7 +2289,8 @@
 }
 
 static void set_cpus_allowed_dl(struct task_struct *p,
-				const struct cpumask *new_mask)
+				const struct cpumask *new_mask,
+				u32 flags)
 {
 	struct root_domain *src_rd;
 	struct rq *rq;
@@ -2306,7 +2319,7 @@
 		raw_spin_unlock(&src_dl_b->lock);
 	}
 
-	set_cpus_allowed_common(p, new_mask);
+	set_cpus_allowed_common(p, new_mask, flags);
 }
 
 /* Assumes rq->lock is held */
@@ -2323,9 +2336,6 @@
 /* Assumes rq->lock is held */
 static void rq_offline_dl(struct rq *rq)
 {
-	if (rq->dl.overloaded)
-		dl_clear_overload(rq);
-
 	cpudl_clear(&rq->rd->cpudl, rq->cpu);
 	cpudl_clear_freecpu(&rq->rd->cpudl, rq->cpu);
 }
@@ -2499,6 +2509,7 @@
 	.rq_online              = rq_online_dl,
 	.rq_offline             = rq_offline_dl,
 	.task_woken		= task_woken_dl,
+	.find_lock_rq		= find_lock_later_rq,
 #endif
 
 	.task_tick		= task_tick_dl,
diff -Naur a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c	2020-11-23 13:48:35.177949612 +0200
+++ b/kernel/sched/fair.c	2021-07-14 15:39:11.214139686 +0300
@@ -4357,7 +4357,7 @@
 	ideal_runtime = sched_slice(cfs_rq, curr);
 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
 	if (delta_exec > ideal_runtime) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		/*
 		 * The current task ran long enough, ensure it doesn't get
 		 * re-elected due to buddy favours.
@@ -4381,7 +4381,7 @@
 		return;
 
 	if (delta > ideal_runtime)
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static void
@@ -4524,7 +4524,7 @@
 	 * validating it and just reschedule.
 	 */
 	if (queued) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		return;
 	}
 	/*
@@ -4661,7 +4661,7 @@
 	 * hierarchy can be throttled
 	 */
 	if (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static __always_inline
@@ -5396,7 +5396,7 @@
 
 		if (delta < 0) {
 			if (rq->curr == p)
-				resched_curr(rq);
+				resched_curr_lazy(rq);
 			return;
 		}
 		hrtick_start(rq, delta);
@@ -6953,7 +6953,7 @@
 	return;
 
 preempt:
-	resched_curr(rq);
+	resched_curr_lazy(rq);
 	/*
 	 * Only set the backward buddy when the current task is still
 	 * on the rq. This can happen when a wakeup gets interleaved
@@ -10694,7 +10694,7 @@
 		 * 'current' within the tree based on its new key value.
 		 */
 		swap(curr->vruntime, se->vruntime);
-		resched_curr(rq);
+		resched_curr_lazy(rq);
 	}
 
 	se->vruntime -= cfs_rq->min_vruntime;
@@ -10721,7 +10721,7 @@
 	 */
 	if (rq->curr == p) {
 		if (p->prio > oldprio)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	} else
 		check_preempt_curr(rq, p, 0);
 }
diff -Naur a/kernel/sched/features.h b/kernel/sched/features.h
--- a/kernel/sched/features.h	2020-11-23 13:48:35.177949612 +0200
+++ b/kernel/sched/features.h	2021-07-14 15:39:11.214139686 +0300
@@ -45,11 +45,19 @@
  */
 SCHED_FEAT(NONTASK_CAPACITY, true)
 
+#ifdef CONFIG_PREEMPT_RT
+SCHED_FEAT(TTWU_QUEUE, false)
+# ifdef CONFIG_PREEMPT_LAZY
+SCHED_FEAT(PREEMPT_LAZY, true)
+# endif
+#else
+
 /*
  * Queue remote wakeups on the target CPU and process them
  * using the scheduler IPI. Reduces rq->lock contention/bounces.
  */
 SCHED_FEAT(TTWU_QUEUE, true)
+#endif
 
 /*
  * When doing wakeups, attempt to limit superfluous scans of the LLC domain.
diff -Naur a/kernel/sched/rt.c b/kernel/sched/rt.c
--- a/kernel/sched/rt.c	2020-11-23 13:48:35.185949773 +0200
+++ b/kernel/sched/rt.c	2021-07-14 15:39:11.214139686 +0300
@@ -265,7 +265,7 @@
 static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)
 {
 	/* Try to pull RT tasks here if we lower this rq's prio */
-	return rq->rt.highest_prio.curr > prev->prio;
+	return rq->online && rq->rt.highest_prio.curr > prev->prio;
 }
 
 static inline int rt_overloaded(struct rq *rq)
@@ -1658,7 +1658,7 @@
 static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)
 {
 	if (!task_running(rq, p) &&
-	    cpumask_test_cpu(cpu, p->cpus_ptr))
+	    cpumask_test_cpu(cpu, &p->cpus_mask))
 		return 1;
 
 	return 0;
@@ -1752,8 +1752,8 @@
 				return this_cpu;
 			}
 
-			best_cpu = cpumask_first_and(lowest_mask,
-						     sched_domain_span(sd));
+			best_cpu = cpumask_any_and_distribute(lowest_mask,
+							      sched_domain_span(sd));
 			if (best_cpu < nr_cpu_ids) {
 				rcu_read_unlock();
 				return best_cpu;
@@ -1770,7 +1770,7 @@
 	if (this_cpu != -1)
 		return this_cpu;
 
-	cpu = cpumask_any(lowest_mask);
+	cpu = cpumask_any_distribute(lowest_mask);
 	if (cpu < nr_cpu_ids)
 		return cpu;
 
@@ -1811,7 +1811,7 @@
 			 * Also make sure that it wasn't scheduled on its rq.
 			 */
 			if (unlikely(task_rq(task) != rq ||
-				     !cpumask_test_cpu(lowest_rq->cpu, task->cpus_ptr) ||
+				     !cpumask_test_cpu(lowest_rq->cpu, &task->cpus_mask) ||
 				     task_running(rq, task) ||
 				     !rt_task(task) ||
 				     !task_on_rq_queued(task))) {
@@ -1859,7 +1859,7 @@
  * running task can migrate over to a CPU that is running a task
  * of lesser priority.
  */
-static int push_rt_task(struct rq *rq)
+static int push_rt_task(struct rq *rq, bool pull)
 {
 	struct task_struct *next_task;
 	struct rq *lowest_rq;
@@ -1873,6 +1873,39 @@
 		return 0;
 
 retry:
+	if (is_migration_disabled(next_task)) {
+		struct task_struct *push_task = NULL;
+		int cpu;
+
+		if (!pull)
+			return 0;
+
+		trace_sched_migrate_pull_tp(next_task);
+
+		if (rq->push_busy)
+			return 0;
+
+		cpu = find_lowest_rq(rq->curr);
+		if (cpu == -1 || cpu == rq->cpu)
+			return 0;
+
+		/*
+		 * Given we found a CPU with lower priority than @next_task,
+		 * therefore it should be running. However we cannot migrate it
+		 * to this other CPU, instead attempt to push the current
+		 * running task on this CPU away.
+		 */
+		push_task = get_push_task(rq);
+		if (push_task) {
+			raw_spin_unlock(&rq->lock);
+			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+					    push_task, &rq->push_work);
+			raw_spin_lock(&rq->lock);
+		}
+
+		return 0;
+	}
+
 	if (WARN_ON(next_task == rq->curr))
 		return 0;
 
@@ -1927,12 +1960,10 @@
 	deactivate_task(rq, next_task, 0);
 	set_task_cpu(next_task, lowest_rq->cpu);
 	activate_task(lowest_rq, next_task, 0);
-	ret = 1;
-
 	resched_curr(lowest_rq);
+	ret = 1;
 
 	double_unlock_balance(rq, lowest_rq);
-
 out:
 	put_task_struct(next_task);
 
@@ -1942,7 +1973,7 @@
 static void push_rt_tasks(struct rq *rq)
 {
 	/* push_rt_task will return true if it moved an RT */
-	while (push_rt_task(rq))
+	while (push_rt_task(rq, false))
 		;
 }
 
@@ -2095,7 +2126,8 @@
 	 */
 	if (has_pushable_tasks(rq)) {
 		raw_spin_lock(&rq->lock);
-		push_rt_tasks(rq);
+		while (push_rt_task(rq, true))
+			;
 		raw_spin_unlock(&rq->lock);
 	}
 
@@ -2120,7 +2152,7 @@
 {
 	int this_cpu = this_rq->cpu, cpu;
 	bool resched = false;
-	struct task_struct *p;
+	struct task_struct *p, *push_task;
 	struct rq *src_rq;
 	int rt_overload_count = rt_overloaded(this_rq);
 
@@ -2167,6 +2199,7 @@
 		 * double_lock_balance, and another CPU could
 		 * alter this_rq
 		 */
+		push_task = NULL;
 		double_lock_balance(this_rq, src_rq);
 
 		/*
@@ -2194,11 +2227,15 @@
 			if (p->prio < src_rq->curr->prio)
 				goto skip;
 
-			resched = true;
-
-			deactivate_task(src_rq, p, 0);
-			set_task_cpu(p, this_cpu);
-			activate_task(this_rq, p, 0);
+			if (is_migration_disabled(p)) {
+				trace_sched_migrate_pull_tp(p);
+				push_task = get_push_task(src_rq);
+			} else {
+				deactivate_task(src_rq, p, 0);
+				set_task_cpu(p, this_cpu);
+				activate_task(this_rq, p, 0);
+				resched = true;
+			}
 			/*
 			 * We continue with the search, just in
 			 * case there's an even higher prio task
@@ -2208,6 +2245,13 @@
 		}
 skip:
 		double_unlock_balance(this_rq, src_rq);
+
+		if (push_task) {
+			raw_spin_unlock(&this_rq->lock);
+			stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+					    push_task, &src_rq->push_work);
+			raw_spin_lock(&this_rq->lock);
+		}
 	}
 
 	if (resched)
@@ -2245,9 +2289,6 @@
 /* Assumes rq->lock is held */
 static void rq_offline_rt(struct rq *rq)
 {
-	if (rq->rt.overloaded)
-		rt_clear_overload(rq);
-
 	__disable_runtime(rq);
 
 	cpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_INVALID);
@@ -2449,6 +2490,7 @@
 	.rq_offline             = rq_offline_rt,
 	.task_woken		= task_woken_rt,
 	.switched_from		= switched_from_rt,
+	.find_lock_rq		= find_lock_lowest_rq,
 #endif
 
 	.task_tick		= task_tick_rt,
diff -Naur a/kernel/sched/sched.h b/kernel/sched/sched.h
--- a/kernel/sched/sched.h	2020-11-23 13:48:35.189949854 +0200
+++ b/kernel/sched/sched.h	2021-07-14 15:39:11.214139686 +0300
@@ -973,6 +973,7 @@
 	unsigned long		cpu_capacity_orig;
 
 	struct callback_head	*balance_callback;
+	unsigned char		balance_flags;
 
 	unsigned char		nohz_idle_balance;
 	unsigned char		idle_balance;
@@ -1003,6 +1004,10 @@
 
 	/* This is used to determine avg_idle's max value */
 	u64			max_idle_balance_cost;
+
+#ifdef CONFIG_HOTPLUG_CPU
+	struct rcuwait		hotplug_wait;
+#endif
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
@@ -1048,6 +1053,12 @@
 	/* Must be inspected within a rcu lock section */
 	struct cpuidle_state	*idle_state;
 #endif
+
+#if defined(CONFIG_PREEMPT_RT) && defined(CONFIG_SMP)
+	unsigned int		nr_pinned;
+#endif
+	unsigned int		push_busy;
+	struct cpu_stop_work	push_work;
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -1075,6 +1086,16 @@
 #endif
 }
 
+#define MDF_PUSH	0x01
+
+static inline bool is_migration_disabled(struct task_struct *p)
+{
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	return p->migration_disabled;
+#else
+	return false;
+#endif
+}
 
 #ifdef CONFIG_SCHED_SMT
 extern void __update_idle_core(struct rq *rq);
@@ -1221,6 +1242,9 @@
 	rq->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);
 	rf->clock_update_flags = 0;
 #endif
+#ifdef CONFIG_SMP
+	SCHED_WARN_ON(rq->balance_callback);
+#endif
 }
 
 static inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)
@@ -1382,6 +1406,9 @@
 
 #ifdef CONFIG_SMP
 
+#define BALANCE_WORK	0x01
+#define BALANCE_PUSH	0x02
+
 static inline void
 queue_balance_callback(struct rq *rq,
 		       struct callback_head *head,
@@ -1389,12 +1416,13 @@
 {
 	lockdep_assert_held(&rq->lock);
 
-	if (unlikely(head->next))
+	if (unlikely(head->next || (rq->balance_flags & BALANCE_PUSH)))
 		return;
 
 	head->func = (void (*)(struct callback_head *))func;
 	head->next = rq->balance_callback;
 	rq->balance_callback = head;
+	rq->balance_flags |= BALANCE_WORK;
 }
 
 #define rcu_dereference_check_sched_domain(p) \
@@ -1714,6 +1742,7 @@
 #define WF_FORK			0x02		/* Child wakeup after fork */
 #define WF_MIGRATED		0x04		/* Internal use, task got migrated */
 #define WF_ON_CPU		0x08		/* Wakee is on_cpu */
+#define WF_LOCK_SLEEPER		0x10		/* Wakeup spinlock "sleeper" */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution
@@ -1795,10 +1824,13 @@
 	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
-				 const struct cpumask *newmask);
+				 const struct cpumask *newmask,
+				 u32 flags);
 
 	void (*rq_online)(struct rq *rq);
 	void (*rq_offline)(struct rq *rq);
+
+	struct rq *(*find_lock_rq)(struct task_struct *p, struct rq *rq);
 #endif
 
 	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
@@ -1882,13 +1914,35 @@
 extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
 extern struct task_struct *pick_next_task_idle(struct rq *rq);
 
+#define SCA_CHECK		0x01
+#define SCA_MIGRATE_DISABLE	0x02
+#define SCA_MIGRATE_ENABLE	0x04
+
 #ifdef CONFIG_SMP
 
 extern void update_group_capacity(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
 
-extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask);
+extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
+
+static inline struct task_struct *get_push_task(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	lockdep_assert_held(&rq->lock);
+
+	if (rq->push_busy)
+		return NULL;
+
+	if (p->nr_cpus_allowed == 1)
+		return NULL;
+
+	rq->push_busy = true;
+	return get_task_struct(p);
+}
+
+extern int push_cpu_stop(void *arg);
 
 #endif
 
@@ -1932,6 +1986,15 @@
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
+#ifdef CONFIG_PREEMPT_LAZY
+extern void resched_curr_lazy(struct rq *rq);
+#else
+static inline void resched_curr_lazy(struct rq *rq)
+{
+	resched_curr(rq);
+}
+#endif
+
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
diff -Naur a/kernel/sched/swait.c b/kernel/sched/swait.c
--- a/kernel/sched/swait.c	2020-11-23 13:48:35.189949854 +0200
+++ b/kernel/sched/swait.c	2021-07-14 15:39:11.214139686 +0300
@@ -64,6 +64,7 @@
 	struct swait_queue *curr;
 	LIST_HEAD(tmp);
 
+	WARN_ON(irqs_disabled());
 	raw_spin_lock_irq(&q->lock);
 	list_splice_init(&q->task_list, &tmp);
 	while (!list_empty(&tmp)) {
diff -Naur a/kernel/sched/topology.c b/kernel/sched/topology.c
--- a/kernel/sched/topology.c	2020-11-23 13:48:35.193949933 +0200
+++ b/kernel/sched/topology.c	2021-07-14 15:39:11.218139658 +0300
@@ -500,6 +500,7 @@
 	rd->rto_cpu = -1;
 	raw_spin_lock_init(&rd->rto_lock);
 	init_irq_work(&rd->rto_push_work, rto_push_irq_work_func);
+	atomic_or(IRQ_WORK_HARD_IRQ, &rd->rto_push_work.flags);
 #endif
 
 	init_dl_bw(&rd->dl_bw);
diff -Naur a/kernel/sched/wait.c b/kernel/sched/wait.c
--- a/kernel/sched/wait.c	2020-11-23 13:48:35.193949933 +0200
+++ b/kernel/sched/wait.c	2020-11-23 13:50:10.403439585 +0200
@@ -3,9 +3,21 @@
  * Generic waiting primitives.
  *
  * (C) 2004 Nadia Yvette Chambers, Oracle
+ *
+ *	2020/10/16
+ *		Laurentiu-Cristian Duca (laurentiu [dot] duca [at] gmail [dot] com)
+ *		Added RTnet select() and poll() system calls helpers.
  */
 #include "sched.h"
 
+void __init_waitqueue_head_rtnet(struct wait_queue_head_rtnet *wq_head, 
+								 const char *name, struct lock_class_key *key)
+{
+	raw_spin_lock_init(&wq_head->lock);
+	INIT_LIST_HEAD(&wq_head->head);
+}
+EXPORT_SYMBOL(__init_waitqueue_head_rtnet);
+
 void __init_waitqueue_head(struct wait_queue_head *wq_head, const char *name, struct lock_class_key *key)
 {
 	spin_lock_init(&wq_head->lock);
@@ -15,6 +27,17 @@
 
 EXPORT_SYMBOL(__init_waitqueue_head);
 
+void add_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry)
+{
+	unsigned long flags;
+
+	wq_entry->flags &= ~WQ_FLAG_EXCLUSIVE;
+	raw_spin_lock_irqsave(&wq_head->lock, flags);
+	__add_wait_queue_rtnet(wq_head, wq_entry);
+	raw_spin_unlock_irqrestore(&wq_head->lock, flags);
+}
+EXPORT_SYMBOL(add_wait_queue_rtnet);
+
 void add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
 {
 	unsigned long flags;
@@ -37,6 +60,16 @@
 }
 EXPORT_SYMBOL(add_wait_queue_exclusive);
 
+void remove_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&wq_head->lock, flags);
+	__remove_wait_queue_rtnet(wq_head, wq_entry);
+	raw_spin_unlock_irqrestore(&wq_head->lock, flags);
+}
+EXPORT_SYMBOL(remove_wait_queue_rtnet);
+
 void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
 {
 	unsigned long flags;
@@ -63,6 +96,59 @@
  * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns
  * zero in this (rare) case, and we handle it by continuing to scan the queue.
  */
+static int __wake_up_common_rtnet(struct wait_queue_head_rtnet *wq_head, unsigned int mode,
+			int nr_exclusive, int wake_flags, void *key,
+			wait_queue_entry_t *bookmark)
+{
+	wait_queue_entry_t *curr, *next;
+	int cnt = 0;
+
+	lockdep_assert_held(&wq_head->lock);
+
+	if (bookmark && (bookmark->flags & WQ_FLAG_BOOKMARK)) {
+		curr = list_next_entry(bookmark, entry);
+
+		list_del(&bookmark->entry);
+		bookmark->flags = 0;
+	} else
+		curr = list_first_entry(&wq_head->head, wait_queue_entry_t, entry);
+
+	if (&curr->entry == &wq_head->head)
+		return nr_exclusive;
+
+	list_for_each_entry_safe_from(curr, next, &wq_head->head, entry) {
+		unsigned flags = curr->flags;
+		int ret;
+
+		if (flags & WQ_FLAG_BOOKMARK)
+			continue;
+
+		ret = curr->func(curr, mode, wake_flags, key);
+		if (ret < 0)
+			break;
+		if (ret && (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
+			break;
+
+		if (bookmark && (++cnt > WAITQUEUE_WALK_BREAK_CNT) &&
+				(&next->entry != &wq_head->head)) {
+			bookmark->flags = WQ_FLAG_BOOKMARK;
+			list_add_tail(&bookmark->entry, &next->entry);
+			break;
+		}
+	}
+
+	return nr_exclusive;
+}
+
+/*
+ * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just
+ * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve
+ * number) then we wake all the non-exclusive tasks and one exclusive task.
+ *
+ * There are circumstances in which we can try to wake a task which has already
+ * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns
+ * zero in this (rare) case, and we handle it by continuing to scan the queue.
+ */
 static int __wake_up_common(struct wait_queue_head *wq_head, unsigned int mode,
 			int nr_exclusive, int wake_flags, void *key,
 			wait_queue_entry_t *bookmark)
@@ -107,6 +193,25 @@
 	return nr_exclusive;
 }
 
+static void __wake_up_common_lock_rtnet(struct wait_queue_head_rtnet *wq_head, unsigned int mode,
+			int nr_exclusive, int wake_flags, void *key)
+{
+	unsigned long flags;
+	wait_queue_entry_t bookmark;
+
+	bookmark.flags = 0;
+	bookmark.private = NULL;
+	bookmark.func = NULL;
+	INIT_LIST_HEAD(&bookmark.entry);
+
+	do {
+		raw_spin_lock_irqsave(&wq_head->lock, flags);
+		nr_exclusive = __wake_up_common_rtnet(wq_head, mode, nr_exclusive,
+						wake_flags, key, &bookmark);
+		raw_spin_unlock_irqrestore(&wq_head->lock, flags);
+	} while (bookmark.flags & WQ_FLAG_BOOKMARK);
+}
+
 static void __wake_up_common_lock(struct wait_queue_head *wq_head, unsigned int mode,
 			int nr_exclusive, int wake_flags, void *key)
 {
@@ -166,6 +271,32 @@
 EXPORT_SYMBOL_GPL(__wake_up_locked_key_bookmark);
 
 /**
+ * __wake_up_sync_key_rtnet - wake up threads blocked on a waitqueue.
+ * @wq_head: the waitqueue
+ * @mode: which threads
+ * @key: opaque value to be passed to wakeup targets
+ *
+ * The sync wakeup differs that the waker knows that it will schedule
+ * away soon, so while the target thread will be woken up, it will not
+ * be migrated to another CPU - ie. the two threads are 'synchronized'
+ * with each other. This can prevent needless bouncing between CPUs.
+ *
+ * On UP it can prevent extra preemption.
+ *
+ * If this function wakes up a task, it executes a full memory barrier before
+ * accessing the task state.
+ */
+void __wake_up_sync_key_rtnet(struct wait_queue_head_rtnet *wq_head, unsigned int mode,
+			void *key)
+{
+	if (unlikely(!wq_head))
+		return;
+
+	__wake_up_common_lock_rtnet(wq_head, mode, 1, WF_SYNC, key);
+}
+EXPORT_SYMBOL_GPL(__wake_up_sync_key_rtnet);
+
+/**
  * __wake_up_sync_key - wake up threads blocked on a waitqueue.
  * @wq_head: the waitqueue
  * @mode: which threads
diff -Naur a/kernel/signal.c b/kernel/signal.c
--- a/kernel/signal.c	2020-11-23 13:48:34.949945037 +0200
+++ b/kernel/signal.c	2021-07-14 15:39:10.890141965 +0300
@@ -20,6 +20,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
+#include <linux/sched/rt.h>
 #include <linux/file.h>
 #include <linux/fs.h>
 #include <linux/proc_fs.h>
@@ -403,13 +404,30 @@
 	}
 }
 
+static inline struct sigqueue *get_task_cache(struct task_struct *t)
+{
+	struct sigqueue *q = t->sigqueue_cache;
+
+	if (cmpxchg(&t->sigqueue_cache, q, NULL) != q)
+		return NULL;
+	return q;
+}
+
+static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
+{
+	if (cmpxchg(&t->sigqueue_cache, NULL, q) == NULL)
+		return 0;
+	return 1;
+}
+
 /*
  * allocate a new signal queue record
  * - this may be called without locks if and only if t == current, otherwise an
  *   appropriate lock must be held to stop the target task from exiting
  */
 static struct sigqueue *
-__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)
+__sigqueue_do_alloc(int sig, struct task_struct *t, gfp_t flags,
+		    int override_rlimit, int fromslab)
 {
 	struct sigqueue *q = NULL;
 	struct user_struct *user;
@@ -431,7 +449,10 @@
 	rcu_read_unlock();
 
 	if (override_rlimit || likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) {
-		q = kmem_cache_alloc(sigqueue_cachep, flags);
+		if (!fromslab)
+			q = get_task_cache(t);
+		if (!q)
+			q = kmem_cache_alloc(sigqueue_cachep, flags);
 	} else {
 		print_dropped_signal(sig);
 	}
@@ -448,6 +469,13 @@
 	return q;
 }
 
+static struct sigqueue *
+__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags,
+		 int override_rlimit)
+{
+	return __sigqueue_do_alloc(sig, t, flags, override_rlimit, 0);
+}
+
 static void __sigqueue_free(struct sigqueue *q)
 {
 	if (q->flags & SIGQUEUE_PREALLOC)
@@ -457,6 +485,21 @@
 	kmem_cache_free(sigqueue_cachep, q);
 }
 
+static void sigqueue_free_current(struct sigqueue *q)
+{
+	struct user_struct *up;
+
+	if (q->flags & SIGQUEUE_PREALLOC)
+		return;
+
+	up = q->user;
+	if (rt_prio(current->normal_prio) && !put_task_cache(current, q)) {
+		if (atomic_dec_and_test(&up->sigpending))
+			free_uid(up);
+	} else
+		  __sigqueue_free(q);
+}
+
 void flush_sigqueue(struct sigpending *queue)
 {
 	struct sigqueue *q;
@@ -470,6 +513,21 @@
 }
 
 /*
+ * Called from __exit_signal. Flush tsk->pending and
+ * tsk->sigqueue_cache
+ */
+void flush_task_sigqueue(struct task_struct *tsk)
+{
+	struct sigqueue *q;
+
+	flush_sigqueue(&tsk->pending);
+
+	q = get_task_cache(tsk);
+	if (q)
+		kmem_cache_free(sigqueue_cachep, q);
+}
+
+/*
  * Flush all pending signals for this kthread.
  */
 void flush_signals(struct task_struct *t)
@@ -593,7 +651,7 @@
 			(info->si_code == SI_TIMER) &&
 			(info->si_sys_private);
 
-		__sigqueue_free(first);
+		sigqueue_free_current(first);
 	} else {
 		/*
 		 * Ok, it wasn't in the queue.  This must be
@@ -630,6 +688,8 @@
 	bool resched_timer = false;
 	int signr;
 
+	WARN_ON_ONCE(tsk != current);
+
 	/* We only dequeue private signals from ourselves, we don't let
 	 * signalfd steal them
 	 */
@@ -1313,6 +1373,34 @@
 	struct k_sigaction *action;
 	int sig = info->si_signo;
 
+	/*
+	 * On some archs, PREEMPT_RT has to delay sending a signal from a trap
+	 * since it can not enable preemption, and the signal code's spin_locks
+	 * turn into mutexes. Instead, it must set TIF_NOTIFY_RESUME which will
+	 * send the signal on exit of the trap.
+	 */
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+	if (in_atomic()) {
+		struct task_struct *t = current;
+
+		if (WARN_ON_ONCE(t->forced_info.si_signo))
+			return 0;
+
+		if (is_si_special(info)) {
+			WARN_ON_ONCE(info != SEND_SIG_PRIV);
+			t->forced_info.si_signo = info->si_signo;
+			t->forced_info.si_errno = 0;
+			t->forced_info.si_code = SI_KERNEL;
+			t->forced_info.si_pid = 0;
+			t->forced_info.si_uid = 0;
+		} else {
+			t->forced_info = *info;
+		}
+
+		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
+		return 0;
+	}
+#endif
 	spin_lock_irqsave(&t->sighand->siglock, flags);
 	action = &t->sighand->action[sig-1];
 	ignored = action->sa.sa_handler == SIG_IGN;
@@ -1806,7 +1894,8 @@
  */
 struct sigqueue *sigqueue_alloc(void)
 {
-	struct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);
+	/* Preallocated sigqueue objects always from the slabcache ! */
+	struct sigqueue *q = __sigqueue_do_alloc(-1, current, GFP_KERNEL, 0, 1);
 
 	if (q)
 		q->flags |= SIGQUEUE_PREALLOC;
@@ -2202,16 +2291,8 @@
 		if (gstop_done && ptrace_reparented(current))
 			do_notify_parent_cldstop(current, false, why);
 
-		/*
-		 * Don't want to allow preemption here, because
-		 * sys_ptrace() needs this task to be inactive.
-		 *
-		 * XXX: implement read_unlock_no_resched().
-		 */
-		preempt_disable();
 		read_unlock(&tasklist_lock);
 		cgroup_enter_frozen();
-		preempt_enable_no_resched();
 		freezable_schedule();
 		cgroup_leave_frozen(true);
 	} else {
diff -Naur a/kernel/softirq.c b/kernel/softirq.c
--- a/kernel/softirq.c	2020-11-23 13:48:34.953945117 +0200
+++ b/kernel/softirq.c	2021-07-14 15:39:10.890141965 +0300
@@ -13,6 +13,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/interrupt.h>
 #include <linux/init.h>
+#include <linux/local_lock.h>
 #include <linux/mm.h>
 #include <linux/notifier.h>
 #include <linux/percpu.h>
@@ -92,15 +93,225 @@
 		!__kthread_should_park(tsk);
 }
 
+#ifdef CONFIG_TRACE_IRQFLAGS
+DEFINE_PER_CPU(int, hardirqs_enabled);
+DEFINE_PER_CPU(int, hardirq_context);
+EXPORT_PER_CPU_SYMBOL_GPL(hardirqs_enabled);
+EXPORT_PER_CPU_SYMBOL_GPL(hardirq_context);
+#endif
+
 /*
- * preempt_count and SOFTIRQ_OFFSET usage:
- * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving
- *   softirq processing.
- * - preempt_count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)
+ * SOFTIRQ_OFFSET usage:
+ *
+ * On !RT kernels 'count' is the preempt counter, on RT kernels this applies
+ * to a per CPU counter and to task::softirqs_disabled_cnt.
+ *
+ * - count is changed by SOFTIRQ_OFFSET on entering or leaving softirq
+ *   processing.
+ *
+ * - count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)
  *   on local_bh_disable or local_bh_enable.
+ *
  * This lets us distinguish between whether we are currently processing
  * softirq and whether we just have bh disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+
+/*
+ * RT accounts for BH disabled sections in task::softirqs_disabled_cnt and
+ * also in per CPU softirq_ctrl::cnt. This is necessary to allow tasks in a
+ * softirq disabled section to be preempted.
+ *
+ * The per task counter is used for softirq_count(), in_softirq() and
+ * in_serving_softirqs() because these counts are only valid when the task
+ * holding softirq_ctrl::lock is running.
+ *
+ * The per CPU counter prevents pointless wakeups of ksoftirqd in case that
+ * the task which is in a softirq disabled section is preempted or blocks.
+ */
+struct softirq_ctrl {
+	local_lock_t	lock;
+	int		cnt;
+};
+
+static DEFINE_PER_CPU(struct softirq_ctrl, softirq_ctrl) = {
+	.lock	= INIT_LOCAL_LOCK(softirq_ctrl.lock),
+};
+
+/**
+ * local_bh_blocked() - Check for idle whether BH processing is blocked
+ *
+ * Returns false if the per CPU softirq::cnt is 0 otherwise true.
+ *
+ * This is invoked from the idle task to guard against false positive
+ * softirq pending warnings, which would happen when the task which holds
+ * softirq_ctrl::lock was the only running task on the CPU and blocks on
+ * some other lock.
+ */
+bool local_bh_blocked(void)
+{
+	return this_cpu_read(softirq_ctrl.cnt) != 0;
+}
+
+void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
+{
+	unsigned long flags;
+	int newcnt;
+
+	WARN_ON_ONCE(in_irq());
+
+	/* First entry of a task into a BH disabled section? */
+	if (!current->softirq_disable_cnt) {
+		if (preemptible()) {
+			local_lock(&softirq_ctrl.lock);
+			rcu_read_lock();
+		} else {
+			DEBUG_LOCKS_WARN_ON(this_cpu_read(softirq_ctrl.cnt));
+		}
+	}
+
+	preempt_disable();
+	/*
+	 * Track the per CPU softirq disabled state. On RT this is per CPU
+	 * state to allow preemption of bottom half disabled sections.
+	 */
+	newcnt = this_cpu_add_return(softirq_ctrl.cnt, cnt);
+	/*
+	 * Reflect the result in the task state to prevent recursion on the
+	 * local lock and to make softirq_count() & al work.
+	 */
+	current->softirq_disable_cnt = newcnt;
+
+	if (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && newcnt == cnt) {
+		raw_local_irq_save(flags);
+		lockdep_softirqs_off(ip);
+		raw_local_irq_restore(flags);
+	}
+	preempt_enable();
+}
+EXPORT_SYMBOL(__local_bh_disable_ip);
+
+static void __local_bh_enable(unsigned int cnt, bool unlock)
+{
+	unsigned long flags;
+	int newcnt;
+
+	DEBUG_LOCKS_WARN_ON(current->softirq_disable_cnt !=
+			    this_cpu_read(softirq_ctrl.cnt));
+
+	preempt_disable();
+	if (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && softirq_count() == cnt) {
+		raw_local_irq_save(flags);
+		lockdep_softirqs_on(_RET_IP_);
+		raw_local_irq_restore(flags);
+	}
+
+	newcnt = this_cpu_sub_return(softirq_ctrl.cnt, cnt);
+	current->softirq_disable_cnt = newcnt;
+	preempt_enable();
+
+	if (!newcnt && unlock) {
+		rcu_read_unlock();
+		local_unlock(&softirq_ctrl.lock);
+	}
+}
+
+void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
+{
+	bool preempt_on = preemptible();
+	unsigned long flags;
+	u32 pending;
+	int curcnt;
+
+	WARN_ON_ONCE(in_irq());
+	lockdep_assert_irqs_enabled();
+
+	local_irq_save(flags);
+	curcnt = this_cpu_read(softirq_ctrl.cnt);
+
+	/*
+	 * If this is not reenabling soft interrupts, no point in trying to
+	 * run pending ones.
+	 */
+	if (curcnt != cnt)
+		goto out;
+
+	pending = local_softirq_pending();
+	if (!pending || ksoftirqd_running(pending))
+		goto out;
+
+	/*
+	 * If this was called from non preemptible context, wake up the
+	 * softirq daemon.
+	 */
+	if (!preempt_on) {
+		wakeup_softirqd();
+		goto out;
+	}
+
+	/*
+	 * Adjust softirq count to SOFTIRQ_OFFSET which makes
+	 * in_serving_softirq() become true.
+	 */
+	cnt = SOFTIRQ_OFFSET;
+	__local_bh_enable(cnt, false);
+	__do_softirq();
+
+out:
+	__local_bh_enable(cnt, preempt_on);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(__local_bh_enable_ip);
+
+/*
+ * Invoked from irq_enter_rcu() to prevent that tick_irq_enter()
+ * pointlessly wakes the softirq daemon. That's handled in __irq_exit_rcu().
+ * None of the above logic in the regular bh_disable/enable functions is
+ * required here.
+ */
+static inline void local_bh_disable_irq_enter(void)
+{
+	this_cpu_add(softirq_ctrl.cnt, SOFTIRQ_DISABLE_OFFSET);
+}
+
+static inline void local_bh_enable_irq_enter(void)
+{
+	this_cpu_sub(softirq_ctrl.cnt, SOFTIRQ_DISABLE_OFFSET);
+}
+
+/*
+ * Invoked from ksoftirqd_run() outside of the interrupt disabled section
+ * to acquire the per CPU local lock for reentrancy protection.
+ */
+static inline void ksoftirqd_run_begin(void)
+{
+	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+	local_irq_disable();
+}
+
+/* Counterpart to ksoftirqd_run_begin() */
+static inline void ksoftirqd_run_end(void)
+{
+	__local_bh_enable(SOFTIRQ_OFFSET, true);
+	WARN_ON_ONCE(in_interrupt());
+	local_irq_enable();
+}
+
+static inline void softirq_handle_begin(void) { }
+static inline void softirq_handle_end(void) { }
+
+static inline void invoke_softirq(void)
+{
+	if (!this_cpu_read(softirq_ctrl.cnt))
+		wakeup_softirqd();
+}
+
+static inline bool should_wake_ksoftirqd(void)
+{
+	return !this_cpu_read(softirq_ctrl.cnt);
+}
+
+#else /* CONFIG_PREEMPT_RT */
 
 /*
  * This one is for softirq.c-internal use,
@@ -108,11 +319,6 @@
  */
 #ifdef CONFIG_TRACE_IRQFLAGS
 
-DEFINE_PER_CPU(int, hardirqs_enabled);
-DEFINE_PER_CPU(int, hardirq_context);
-EXPORT_PER_CPU_SYMBOL_GPL(hardirqs_enabled);
-EXPORT_PER_CPU_SYMBOL_GPL(hardirq_context);
-
 void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 {
 	unsigned long flags;
@@ -203,6 +409,67 @@
 }
 EXPORT_SYMBOL(__local_bh_enable_ip);
 
+static inline void local_bh_disable_irq_enter(void)
+{
+	local_bh_disable();
+}
+
+static inline void local_bh_enable_irq_enter(void)
+{
+	_local_bh_enable();
+}
+
+static inline void softirq_handle_begin(void)
+{
+	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+}
+
+static inline void softirq_handle_end(void)
+{
+	__local_bh_enable(SOFTIRQ_OFFSET);
+	WARN_ON_ONCE(in_interrupt());
+}
+
+static inline void ksoftirqd_run_begin(void)
+{
+	local_irq_disable();
+}
+
+static inline void ksoftirqd_run_end(void)
+{
+	local_irq_enable();
+}
+
+static inline void invoke_softirq(void)
+{
+	if (ksoftirqd_running(local_softirq_pending()))
+		return;
+
+	if (!force_irqthreads) {
+#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
+		/*
+		 * We can safely execute softirq on the current stack if
+		 * it is the irq stack, because it should be near empty
+		 * at this stage.
+		 */
+		__do_softirq();
+#else
+		/*
+		 * Otherwise, irq_exit() is called on the task stack that can
+		 * be potentially deep already. So call softirq in its own stack
+		 * to prevent from any overrun.
+		 */
+		do_softirq_own_stack();
+#endif
+	} else {
+		wakeup_softirqd();
+	}
+}
+
+static inline bool should_wake_ksoftirqd(void) { return true; }
+
+#endif /* !CONFIG_PREEMPT_RT */
+
 /*
  * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,
  * but break the loop if need_resched() is set or after 2 ms.
@@ -272,7 +539,7 @@
 	pending = local_softirq_pending();
 	account_irq_enter_time(current);
 
-	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+	softirq_handle_begin();
 	in_hardirq = lockdep_softirq_start();
 
 restart:
@@ -307,8 +574,10 @@
 		pending >>= softirq_bit;
 	}
 
-	if (__this_cpu_read(ksoftirqd) == current)
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT) &&
+	    __this_cpu_read(ksoftirqd) == current)
 		rcu_softirq_qs();
+
 	local_irq_disable();
 
 	pending = local_softirq_pending();
@@ -322,11 +591,11 @@
 
 	lockdep_softirq_end(in_hardirq);
 	account_irq_exit_time(current);
-	__local_bh_enable(SOFTIRQ_OFFSET);
-	WARN_ON_ONCE(in_interrupt());
+	softirq_handle_end();
 	current_restore_flags(old_flags, PF_MEMALLOC);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 asmlinkage __visible void do_softirq(void)
 {
 	__u32 pending;
@@ -344,6 +613,7 @@
 
 	local_irq_restore(flags);
 }
+#endif
 
 /**
  * irq_enter_rcu - Enter an interrupt context with RCU watching
@@ -355,9 +625,9 @@
 		 * Prevent raise_softirq from needlessly waking up ksoftirqd
 		 * here, as softirq will be serviced on return from interrupt.
 		 */
-		local_bh_disable();
+		local_bh_disable_irq_enter();
 		tick_irq_enter();
-		_local_bh_enable();
+		local_bh_enable_irq_enter();
 	}
 	__irq_enter();
 }
@@ -371,32 +641,6 @@
 	irq_enter_rcu();
 }
 
-static inline void invoke_softirq(void)
-{
-	if (ksoftirqd_running(local_softirq_pending()))
-		return;
-
-	if (!force_irqthreads) {
-#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
-		/*
-		 * We can safely execute softirq on the current stack if
-		 * it is the irq stack, because it should be near empty
-		 * at this stage.
-		 */
-		__do_softirq();
-#else
-		/*
-		 * Otherwise, irq_exit() is called on the task stack that can
-		 * be potentially deep already. So call softirq in its own stack
-		 * to prevent from any overrun.
-		 */
-		do_softirq_own_stack();
-#endif
-	} else {
-		wakeup_softirqd();
-	}
-}
-
 static inline void tick_irq_exit(void)
 {
 #ifdef CONFIG_NO_HZ_COMMON
@@ -466,7 +710,7 @@
 	 * Otherwise we wake up ksoftirqd to make sure we
 	 * schedule the softirq soon.
 	 */
-	if (!in_interrupt())
+	if (!in_interrupt() && should_wake_ksoftirqd())
 		wakeup_softirqd();
 }
 
@@ -606,6 +850,29 @@
 }
 EXPORT_SYMBOL(tasklet_init);
 
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)
+
+void tasklet_unlock_wait(struct tasklet_struct *t)
+{
+	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) {
+		if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+			/*
+			 * Prevent a live lock when current preempted soft
+			 * interrupt processing or prevents ksoftirqd from
+			 * running. If the tasklet runs on a different CPU
+			 * then this has no effect other than doing the BH
+			 * disable/enable dance for nothing.
+			 */
+			local_bh_disable();
+			local_bh_enable();
+		} else {
+			cpu_relax();
+		}
+	}
+}
+EXPORT_SYMBOL(tasklet_unlock_wait);
+#endif
+
 void tasklet_kill(struct tasklet_struct *t)
 {
 	if (in_interrupt())
@@ -613,7 +880,20 @@
 
 	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
 		do {
-			yield();
+			if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+				/*
+				 * Prevent a live lock when current
+				 * preempted soft interrupt processing or
+				 * prevents ksoftirqd from running. If the
+				 * tasklet runs on a different CPU then
+				 * this has no effect other than doing the
+				 * BH disable/enable dance for nothing.
+				 */
+				local_bh_disable();
+				local_bh_enable();
+			} else {
+				yield();
+			}
 		} while (test_bit(TASKLET_STATE_SCHED, &t->state));
 	}
 	tasklet_unlock_wait(t);
@@ -643,18 +923,18 @@
 
 static void run_ksoftirqd(unsigned int cpu)
 {
-	local_irq_disable();
+	ksoftirqd_run_begin();
 	if (local_softirq_pending()) {
 		/*
 		 * We can safely run softirq on inline stack, as we are not deep
 		 * in the task stack here.
 		 */
 		__do_softirq();
-		local_irq_enable();
+		ksoftirqd_run_end();
 		cond_resched();
 		return;
 	}
-	local_irq_enable();
+	ksoftirqd_run_end();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
diff -Naur a/kernel/stop_machine.c b/kernel/stop_machine.c
--- a/kernel/stop_machine.c	2020-11-23 13:48:34.953945117 +0200
+++ b/kernel/stop_machine.c	2021-07-14 15:39:10.890141965 +0300
@@ -42,11 +42,23 @@
 	struct list_head	works;		/* list of pending works */
 
 	struct cpu_stop_work	stop_work;	/* for stop_cpus */
+	unsigned long		caller;
+	cpu_stop_fn_t		fn;
 };
 
 static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
 static bool stop_machine_initialized = false;
 
+void print_stop_info(const char *log_lvl, struct task_struct *task)
+{
+	struct cpu_stopper *stopper = this_cpu_ptr(&cpu_stopper);
+
+	if (task != stopper->thread)
+		return;
+
+	printk("%sStopper: %pS <- %pS\n", log_lvl, stopper->fn, (void *)stopper->caller);
+}
+
 /* static data for stop_cpus */
 static DEFINE_MUTEX(stop_cpus_mutex);
 static bool stop_cpus_in_progress;
@@ -123,7 +135,7 @@
 int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)
 {
 	struct cpu_stop_done done;
-	struct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };
+	struct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };
 
 	cpu_stop_init_done(&done, 1);
 	if (!cpu_stop_queue_work(cpu, &work))
@@ -331,7 +343,8 @@
 	work1 = work2 = (struct cpu_stop_work){
 		.fn = multi_cpu_stop,
 		.arg = &msdata,
-		.done = &done
+		.done = &done,
+		.caller = _RET_IP_,
 	};
 
 	cpu_stop_init_done(&done, 2);
@@ -367,7 +380,7 @@
 bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,
 			struct cpu_stop_work *work_buf)
 {
-	*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };
+	*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };
 	return cpu_stop_queue_work(cpu, work_buf);
 }
 
@@ -487,6 +500,8 @@
 		int ret;
 
 		/* cpu stop callbacks must not sleep, make in_atomic() == T */
+		stopper->caller = work->caller;
+		stopper->fn = fn;
 		preempt_count_inc();
 		ret = fn(arg);
 		if (done) {
@@ -495,6 +510,8 @@
 			cpu_stop_signal_done(done);
 		}
 		preempt_count_dec();
+		stopper->fn = NULL;
+		stopper->caller = 0;
 		WARN_ONCE(preempt_count(),
 			  "cpu_stop: %ps(%p) leaked preempt count\n", fn, arg);
 		goto repeat;
diff -Naur a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
--- a/kernel/time/hrtimer.c	2020-11-23 13:48:35.201950094 +0200
+++ b/kernel/time/hrtimer.c	2021-07-14 15:39:11.254139405 +0300
@@ -1828,7 +1828,7 @@
 	 * expiry.
 	 */
 	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
-		if (task_is_realtime(current) && !(mode & HRTIMER_MODE_SOFT))
+		if ((task_is_realtime(current) && !(mode & HRTIMER_MODE_SOFT)) || system_state != SYSTEM_RUNNING)
 			mode |= HRTIMER_MODE_HARD;
 	}
 
@@ -1993,6 +1993,36 @@
 }
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Sleep for 1 ms in hope whoever holds what we want will let it go.
+ */
+void cpu_chill(void)
+{
+	unsigned int freeze_flag = current->flags & PF_NOFREEZE;
+	struct task_struct *self = current;
+	ktime_t chill_time;
+
+	raw_spin_lock_irq(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock_irq(&self->pi_lock);
+
+	chill_time = ktime_set(0, NSEC_PER_MSEC);
+
+	current->flags |= PF_NOFREEZE;
+	schedule_hrtimeout(&chill_time, HRTIMER_MODE_REL_HARD);
+	if (!freeze_flag)
+		current->flags &= ~PF_NOFREEZE;
+
+	raw_spin_lock_irq(&self->pi_lock);
+	__set_current_state_no_track(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	raw_spin_unlock_irq(&self->pi_lock);
+}
+EXPORT_SYMBOL(cpu_chill);
+#endif
+
 /*
  * Functions related to boot-time initialization:
  */
diff -Naur a/kernel/time/sched_clock.c b/kernel/time/sched_clock.c
--- a/kernel/time/sched_clock.c	2020-11-23 13:48:35.205950175 +0200
+++ b/kernel/time/sched_clock.c	2021-07-14 15:39:11.254139405 +0300
@@ -35,7 +35,7 @@
  * into a single 64-byte cache line.
  */
 struct clock_data {
-	seqcount_t		seq;
+	seqcount_latch_t	seq;
 	struct clock_read_data	read_data[2];
 	ktime_t			wrap_kt;
 	unsigned long		rate;
@@ -76,7 +76,7 @@
 
 int sched_clock_read_retry(unsigned int seq)
 {
-	return read_seqcount_retry(&cd.seq, seq);
+	return read_seqcount_latch_retry(&cd.seq, seq);
 }
 
 unsigned long long notrace sched_clock(void)
@@ -258,7 +258,7 @@
  */
 static u64 notrace suspended_sched_clock_read(void)
 {
-	unsigned int seq = raw_read_seqcount(&cd.seq);
+	unsigned int seq = raw_read_seqcount_latch(&cd.seq);
 
 	return cd.read_data[seq & 1].epoch_cyc;
 }
diff -Naur a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
--- a/kernel/time/tick-sched.c	2020-11-23 13:48:35.209950255 +0200
+++ b/kernel/time/tick-sched.c	2021-07-14 15:39:11.254139405 +0300
@@ -925,7 +925,7 @@
 	if (unlikely(local_softirq_pending())) {
 		static int ratelimit;
 
-		if (ratelimit < 10 &&
+		if (ratelimit < 10 && !local_bh_blocked() &&
 		    (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
 			pr_warn("NOHZ: local_softirq_pending %02x\n",
 				(unsigned int) local_softirq_pending());
diff -Naur a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
--- a/kernel/time/timekeeping.c	2020-11-23 13:48:35.213950335 +0200
+++ b/kernel/time/timekeeping.c	2021-07-14 15:39:11.254139405 +0300
@@ -64,7 +64,7 @@
  * See @update_fast_timekeeper() below.
  */
 struct tk_fast {
-	seqcount_raw_spinlock_t	seq;
+	seqcount_latch_t	seq;
 	struct tk_read_base	base[2];
 };
 
@@ -81,13 +81,13 @@
 };
 
 static struct tk_fast tk_fast_mono ____cacheline_aligned = {
-	.seq     = SEQCNT_RAW_SPINLOCK_ZERO(tk_fast_mono.seq, &timekeeper_lock),
+	.seq     = SEQCNT_LATCH_ZERO(tk_fast_mono.seq),
 	.base[0] = { .clock = &dummy_clock, },
 	.base[1] = { .clock = &dummy_clock, },
 };
 
 static struct tk_fast tk_fast_raw  ____cacheline_aligned = {
-	.seq     = SEQCNT_RAW_SPINLOCK_ZERO(tk_fast_raw.seq, &timekeeper_lock),
+	.seq     = SEQCNT_LATCH_ZERO(tk_fast_raw.seq),
 	.base[0] = { .clock = &dummy_clock, },
 	.base[1] = { .clock = &dummy_clock, },
 };
@@ -467,7 +467,7 @@
 					tk_clock_read(tkr),
 					tkr->cycle_last,
 					tkr->mask));
-	} while (read_seqcount_retry(&tkf->seq, seq));
+	} while (read_seqcount_latch_retry(&tkf->seq, seq));
 
 	return now;
 }
@@ -533,7 +533,7 @@
 					tk_clock_read(tkr),
 					tkr->cycle_last,
 					tkr->mask));
-	} while (read_seqcount_retry(&tkf->seq, seq));
+	} while (read_seqcount_latch_retry(&tkf->seq, seq));
 
 	return now;
 }
diff -Naur a/kernel/time/timer.c b/kernel/time/timer.c
--- a/kernel/time/timer.c	2020-11-23 13:48:35.217950416 +0200
+++ b/kernel/time/timer.c	2021-07-14 15:39:11.254139405 +0300
@@ -1765,6 +1765,8 @@
 {
 	struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);
 
+	irq_work_tick_soft();
+
 	__run_timers(base);
 	if (IS_ENABLED(CONFIG_NO_HZ_COMMON))
 		__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));
diff -Naur a/kernel/trace/trace.c b/kernel/trace/trace.c
--- a/kernel/trace/trace.c	2020-11-23 13:48:35.241950897 +0200
+++ b/kernel/trace/trace.c	2021-07-14 15:39:11.290139152 +0300
@@ -2437,6 +2437,15 @@
 }
 EXPORT_SYMBOL_GPL(trace_handle_return);
 
+static unsigned short migration_disable_value(struct task_struct *tsk)
+{
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	return tsk ? tsk->migration_disabled : 0;
+#else
+	return 0;
+#endif
+}
+
 void
 tracing_generic_entry_update(struct trace_entry *entry, unsigned short type,
 			     unsigned long flags, int pc)
@@ -2444,6 +2453,7 @@
 	struct task_struct *tsk = current;
 
 	entry->preempt_count		= pc & 0xff;
+	entry->preempt_lazy_count	= preempt_lazy_count();
 	entry->pid			= (tsk) ? tsk->pid : 0;
 	entry->type			= type;
 	entry->flags =
@@ -2455,8 +2465,11 @@
 		((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |
 		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
 		((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |
-		(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |
+		(tif_need_resched_now() ? TRACE_FLAG_NEED_RESCHED : 0) |
+		(need_resched_lazy() ? TRACE_FLAG_NEED_RESCHED_LAZY : 0) |
 		(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);
+
+	entry->migrate_disable = migration_disable_value(tsk);
 }
 EXPORT_SYMBOL_GPL(tracing_generic_entry_update);
 
@@ -3784,14 +3797,17 @@
 
 static void print_lat_help_header(struct seq_file *m)
 {
-	seq_puts(m, "#                    _------=> CPU#            \n"
-		    "#                   / _-----=> irqs-off        \n"
-		    "#                  | / _----=> need-resched    \n"
-		    "#                  || / _---=> hardirq/softirq \n"
-		    "#                  ||| / _--=> preempt-depth   \n"
-		    "#                  |||| /     delay            \n"
-		    "#  cmd     pid     ||||| time  |   caller      \n"
-		    "#     \\   /        |||||  \\    |   /         \n");
+	seq_puts(m, "#                    _--------=> CPU#            \n"
+		    "#                   / _-------=> irqs-off        \n"
+		    "#                  | / _------=> need-resched    \n"
+		    "#                  || / _-----=> need-resched-lazy\n"
+		    "#                  ||| / _----=> hardirq/softirq \n"
+		    "#                  |||| / _---=> preempt-depth   \n"
+		    "#                  ||||| / _--=> preempt-lazy-depth\n"
+		    "#                  |||||| / _-=> migrate-disable \n"
+		    "#                  ||||||| /     delay           \n"
+		    "#  cmd     pid     |||||||| time  |   caller     \n"
+		    "#     \\   /        ||||||||  \\    |    /       \n");
 }
 
 static void print_event_info(struct array_buffer *buf, struct seq_file *m)
@@ -3825,13 +3841,16 @@
 
 	print_event_info(buf, m);
 
-	seq_printf(m, "#                            %.*s  _-----=> irqs-off\n", prec, space);
-	seq_printf(m, "#                            %.*s / _----=> need-resched\n", prec, space);
-	seq_printf(m, "#                            %.*s| / _---=> hardirq/softirq\n", prec, space);
-	seq_printf(m, "#                            %.*s|| / _--=> preempt-depth\n", prec, space);
-	seq_printf(m, "#                            %.*s||| /     delay\n", prec, space);
-	seq_printf(m, "#           TASK-PID  %.*s CPU#  ||||   TIMESTAMP  FUNCTION\n", prec, "     TGID   ");
-	seq_printf(m, "#              | |    %.*s   |   ||||      |         |\n", prec, "       |    ");
+	seq_printf(m, "#                            %.*s  _-------=> irqs-off\n", prec, space);
+	seq_printf(m, "#                            %.*s / _------=> need-resched\n", prec, space);
+	seq_printf(m, "#                            %.*s| / _-----=> need-resched-lazy\n", prec, space);
+	seq_printf(m, "#                            %.*s|| / _----=> hardirq/softirq\n", prec, space);
+	seq_printf(m, "#                            %.*s||| / _---=> preempt-depth\n", prec, space);
+	seq_printf(m, "#                            %.*s|||| / _--=> preempt-lazy-depth\n", prec, space);
+	seq_printf(m, "#                            %.*s||||| / _-=> migrate-disable\n", prec, space);
+	seq_printf(m, "#                            %.*s|||||| /     delay\n", prec, space);
+	seq_printf(m, "#           TASK-PID  %.*s CPU#  |||||||  TIMESTAMP  FUNCTION\n", prec, "     TGID   ");
+	seq_printf(m, "#              | |    %.*s   |   |||||||      |         |\n", prec, "       |    ");
 }
 
 void
@@ -9249,7 +9268,6 @@
 	tracing_off();
 
 	local_irq_save(flags);
-	printk_nmi_direct_enter();
 
 	/* Simulate the iterator */
 	trace_init_global_iter(&iter);
@@ -9329,7 +9347,6 @@
 		atomic_dec(&per_cpu_ptr(iter.array_buffer->data, cpu)->disabled);
 	}
 	atomic_dec(&dump_running);
-	printk_nmi_direct_exit();
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(ftrace_dump);
diff -Naur a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
--- a/kernel/trace/trace_events.c	2020-11-23 13:48:35.249951058 +0200
+++ b/kernel/trace/trace_events.c	2021-07-14 15:39:11.290139152 +0300
@@ -182,6 +182,8 @@
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
+	__common_field(unsigned char, migrate_disable);
+	__common_field(unsigned char, preempt_lazy_count);
 
 	return ret;
 }
diff -Naur a/kernel/trace/trace.h b/kernel/trace/trace.h
--- a/kernel/trace/trace.h	2020-11-23 13:48:35.245950978 +0200
+++ b/kernel/trace/trace.h	2021-07-14 15:39:11.290139152 +0300
@@ -143,6 +143,7 @@
  *  NEED_RESCHED	- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
+ *  NEED_RESCHED_LAZY	- lazy reschedule is requested
  */
 enum trace_flag_type {
 	TRACE_FLAG_IRQS_OFF		= 0x01,
@@ -152,6 +153,7 @@
 	TRACE_FLAG_SOFTIRQ		= 0x10,
 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
 	TRACE_FLAG_NMI			= 0x40,
+	TRACE_FLAG_NEED_RESCHED_LAZY	= 0x80,
 };
 
 #define TRACE_BUF_SIZE		1024
diff -Naur a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c
--- a/kernel/trace/trace_output.c	2020-11-23 13:48:35.269951459 +0200
+++ b/kernel/trace/trace_output.c	2021-07-14 15:39:11.294139124 +0300
@@ -441,6 +441,7 @@
 {
 	char hardsoft_irq;
 	char need_resched;
+	char need_resched_lazy;
 	char irqs_off;
 	int hardirq;
 	int softirq;
@@ -471,6 +472,9 @@
 		break;
 	}
 
+	need_resched_lazy =
+		(entry->flags & TRACE_FLAG_NEED_RESCHED_LAZY) ? 'L' : '.';
+
 	hardsoft_irq =
 		(nmi && hardirq)     ? 'Z' :
 		nmi                  ? 'z' :
@@ -479,14 +483,25 @@
 		softirq              ? 's' :
 		                       '.' ;
 
-	trace_seq_printf(s, "%c%c%c",
-			 irqs_off, need_resched, hardsoft_irq);
+	trace_seq_printf(s, "%c%c%c%c",
+			 irqs_off, need_resched, need_resched_lazy,
+			 hardsoft_irq);
 
 	if (entry->preempt_count)
 		trace_seq_printf(s, "%x", entry->preempt_count);
 	else
 		trace_seq_putc(s, '.');
 
+	if (entry->preempt_lazy_count)
+		trace_seq_printf(s, "%x", entry->preempt_lazy_count);
+	else
+		trace_seq_putc(s, '.');
+
+	if (entry->migrate_disable)
+		trace_seq_printf(s, "%x", entry->migrate_disable);
+	else
+		trace_seq_putc(s, '.');
+
 	return !trace_seq_has_overflowed(s);
 }
 
diff -Naur a/kernel/workqueue.c b/kernel/workqueue.c
--- a/kernel/workqueue.c	2020-11-23 13:48:34.977945599 +0200
+++ b/kernel/workqueue.c	2021-07-14 15:39:10.890141965 +0300
@@ -4905,6 +4905,10 @@
 		pool->flags |= POOL_DISASSOCIATED;
 
 		raw_spin_unlock_irq(&pool->lock);
+
+		for_each_pool_worker(worker, pool)
+			WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_active_mask) < 0);
+
 		mutex_unlock(&wq_pool_attach_mutex);
 
 		/*
diff -Naur a/lib/bust_spinlocks.c b/lib/bust_spinlocks.c
--- a/lib/bust_spinlocks.c	2020-11-23 13:48:35.313952342 +0200
+++ b/lib/bust_spinlocks.c	2021-07-14 15:39:11.458137971 +0300
@@ -26,7 +26,6 @@
 		unblank_screen();
 #endif
 		console_unblank();
-		if (--oops_in_progress == 0)
-			wake_up_klogd();
+		--oops_in_progress;
 	}
 }
diff -Naur a/lib/cpumask.c b/lib/cpumask.c
--- a/lib/cpumask.c	2020-11-23 13:48:35.317952423 +0200
+++ b/lib/cpumask.c	2021-07-14 15:39:11.458137971 +0300
@@ -267,3 +267,21 @@
 	return next;
 }
 EXPORT_SYMBOL(cpumask_any_and_distribute);
+
+int cpumask_any_distribute(const struct cpumask *srcp)
+{
+	int next, prev;
+
+	/* NOTE: our first selection will skip 0. */
+	prev = __this_cpu_read(distribute_cpu_mask_prev);
+
+	next = cpumask_next(prev, srcp);
+	if (next >= nr_cpu_ids)
+		next = cpumask_first(srcp);
+
+	if (next < nr_cpu_ids)
+		__this_cpu_write(distribute_cpu_mask_prev, next);
+
+	return next;
+}
+EXPORT_SYMBOL(cpumask_any_distribute);
diff -Naur a/lib/debugobjects.c b/lib/debugobjects.c
--- a/lib/debugobjects.c	2020-11-23 13:48:35.349953065 +0200
+++ b/lib/debugobjects.c	2021-07-14 15:39:11.458137971 +0300
@@ -537,7 +537,10 @@
 	struct debug_obj *obj;
 	unsigned long flags;
 
-	fill_pool();
+#ifdef CONFIG_PREEMPT_RT
+	if (preempt_count() == 0 && !irqs_disabled())
+#endif
+		fill_pool();
 
 	db = get_bucket((unsigned long) addr);
 
diff -Naur a/lib/dump_stack.c b/lib/dump_stack.c
--- a/lib/dump_stack.c	2020-11-23 13:48:35.357953225 +0200
+++ b/lib/dump_stack.c	2021-07-14 15:39:11.458137971 +0300
@@ -12,6 +12,7 @@
 #include <linux/atomic.h>
 #include <linux/kexec.h>
 #include <linux/utsname.h>
+#include <linux/stop_machine.h>
 
 static char dump_stack_arch_desc_str[128];
 
@@ -57,6 +58,7 @@
 		       log_lvl, dump_stack_arch_desc_str);
 
 	print_worker_info(log_lvl, current);
+	print_stop_info(log_lvl, current);
 }
 
 /**
diff -Naur a/lib/irq_poll.c b/lib/irq_poll.c
--- a/lib/irq_poll.c	2020-11-23 13:48:35.373953546 +0200
+++ b/lib/irq_poll.c	2021-07-14 15:39:11.458137971 +0300
@@ -37,6 +37,7 @@
 	list_add_tail(&iop->list, this_cpu_ptr(&blk_cpu_iopoll));
 	raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(irq_poll_sched);
 
@@ -72,6 +73,7 @@
 	local_irq_save(flags);
 	__irq_poll_complete(iop);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(irq_poll_complete);
 
@@ -96,6 +98,7 @@
 		}
 
 		local_irq_enable();
+		preempt_check_resched_rt();
 
 		/* Even though interrupts have been re-enabled, this
 		 * access is safe because interrupts can only add new
@@ -133,6 +136,7 @@
 		__raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 
 	local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 /**
@@ -196,6 +200,7 @@
 			 this_cpu_ptr(&blk_cpu_iopoll));
 	__raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 	return 0;
 }
diff -Naur a/lib/Kconfig.debug b/lib/Kconfig.debug
--- a/lib/Kconfig.debug	2020-11-23 13:48:35.297952021 +0200
+++ b/lib/Kconfig.debug	2021-07-14 15:39:11.458137971 +0300
@@ -61,6 +61,23 @@
 	  will be used as the loglevel. IOW passing "quiet" will be the
 	  equivalent of passing "loglevel=<CONSOLE_LOGLEVEL_QUIET>"
 
+config CONSOLE_LOGLEVEL_EMERGENCY
+	int "Emergency console loglevel (1-15)"
+	range 1 15
+	default "5"
+	help
+	  The loglevel to determine if a console message is an emergency
+	  message.
+
+	  If supported by the console driver, emergency messages will be
+	  flushed to the console immediately. This can cause significant system
+	  latencies so the value should be set such that only significant
+	  messages are classified as emergency messages.
+
+	  Setting a default here is equivalent to passing in
+	  emergency_loglevel=<x> in the kernel bootargs. emergency_loglevel=<x>
+	  continues to override whatever value is specified here as well.
+
 config MESSAGE_LOGLEVEL_DEFAULT
 	int "Default message log level (1-7)"
 	range 1 7
@@ -1332,7 +1349,7 @@
 
 config DEBUG_LOCKING_API_SELFTESTS
 	bool "Locking API boot-time self-tests"
-	depends on DEBUG_KERNEL
+	depends on DEBUG_KERNEL && !PREEMPT_RT
 	help
 	  Say Y here if you want the kernel to run a short self-test during
 	  bootup. The self-test checks whether common types of locking bugs
diff -Naur a/lib/locking-selftest.c b/lib/locking-selftest.c
--- a/lib/locking-selftest.c	2020-11-23 13:48:35.389953868 +0200
+++ b/lib/locking-selftest.c	2021-07-14 15:39:11.458137971 +0300
@@ -742,6 +742,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_rlock)
 
@@ -757,9 +759,12 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
+#ifndef CONFIG_PREEMPT_RT
 /*
  * Enabling hardirqs with a softirq-safe lock held:
  */
@@ -792,6 +797,8 @@
 #undef E1
 #undef E2
 
+#endif
+
 /*
  * Enabling irqs with an irq-safe lock held:
  */
@@ -815,6 +822,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_rlock)
 
@@ -830,6 +839,8 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
@@ -861,6 +872,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_rlock)
 
@@ -876,6 +889,8 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
@@ -909,6 +924,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_rlock)
 
@@ -924,10 +941,14 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
 
+#ifndef CONFIG_PREEMPT_RT
+
 /*
  * read-lock / write-lock irq inversion.
  *
@@ -990,6 +1011,10 @@
 #undef E2
 #undef E3
 
+#endif
+
+#ifndef CONFIG_PREEMPT_RT
+
 /*
  * read-lock / write-lock recursion that is actually safe.
  */
@@ -1028,6 +1053,8 @@
 #undef E2
 #undef E3
 
+#endif
+
 /*
  * read-lock / write-lock recursion that is unsafe.
  */
@@ -2058,6 +2085,7 @@
 
 	printk("  --------------------------------------------------------------------------\n");
 
+#ifndef CONFIG_PREEMPT_RT
 	/*
 	 * irq-context testcases:
 	 */
@@ -2070,6 +2098,28 @@
 
 	DO_TESTCASE_6x2("irq read-recursion", irq_read_recursion);
 //	DO_TESTCASE_6x2B("irq read-recursion #2", irq_read_recursion2);
+#else
+	/* On -rt, we only do hardirq context test for raw spinlock */
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 12);
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 12);
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 321);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 321);
+#endif
 
 	ww_tests();
 
diff -Naur a/lib/Makefile b/lib/Makefile
--- a/lib/Makefile	2020-11-23 13:48:35.301952101 +0200
+++ b/lib/Makefile	2021-07-14 15:39:11.458137971 +0300
@@ -32,7 +32,7 @@
 
 lib-y := ctype.o string.o vsprintf.o cmdline.o \
 	 rbtree.o radix-tree.o timerqueue.o xarray.o \
-	 idr.o extable.o sha1.o irq_regs.o argv_split.o \
+	 idr.o extable.o sha1.o irq_regs.o argv_split.o printk_ringbuffer.o \
 	 flex_proportions.o ratelimit.o show_mem.o \
 	 is_single_threaded.o plist.o decompress.o kobject_uevent.o \
 	 earlycpio.o seq_buf.o siphash.o dec_and_lock.o \
diff -Naur a/lib/nmi_backtrace.c b/lib/nmi_backtrace.c
--- a/lib/nmi_backtrace.c	2020-11-23 13:48:35.393953948 +0200
+++ b/lib/nmi_backtrace.c	2021-07-14 15:39:11.458137971 +0300
@@ -75,12 +75,6 @@
 		touch_softlockup_watchdog();
 	}
 
-	/*
-	 * Force flush any remote buffers that might be stuck in IRQ context
-	 * and therefore could not run their irq_work.
-	 */
-	printk_safe_flush();
-
 	clear_bit_unlock(0, &backtrace_flag);
 	put_cpu();
 }
diff -Naur a/lib/printk_ringbuffer.c b/lib/printk_ringbuffer.c
--- a/lib/printk_ringbuffer.c	1970-01-01 02:00:00.000000000 +0200
+++ b/lib/printk_ringbuffer.c	2021-07-14 15:39:11.458137971 +0300
@@ -0,0 +1,589 @@
+// SPDX-License-Identifier: GPL-2.0
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/printk_ringbuffer.h>
+
+#define PRB_SIZE(rb) (1 << rb->size_bits)
+#define PRB_SIZE_BITMASK(rb) (PRB_SIZE(rb) - 1)
+#define PRB_INDEX(rb, lpos) (lpos & PRB_SIZE_BITMASK(rb))
+#define PRB_WRAPS(rb, lpos) (lpos >> rb->size_bits)
+#define PRB_WRAP_LPOS(rb, lpos, xtra) \
+	((PRB_WRAPS(rb, lpos) + xtra) << rb->size_bits)
+#define PRB_DATA_SIZE(e) (e->size - sizeof(struct prb_entry))
+#define PRB_DATA_ALIGN sizeof(long)
+
+static bool __prb_trylock(struct prb_cpulock *cpu_lock,
+			  unsigned int *cpu_store)
+{
+	unsigned long *flags;
+	unsigned int cpu;
+
+	cpu = get_cpu();
+
+	*cpu_store = atomic_read(&cpu_lock->owner);
+	/* memory barrier to ensure the current lock owner is visible */
+	smp_rmb();
+	if (*cpu_store == -1) {
+		flags = per_cpu_ptr(cpu_lock->irqflags, cpu);
+		local_irq_save(*flags);
+		if (atomic_try_cmpxchg_acquire(&cpu_lock->owner,
+					       cpu_store, cpu)) {
+			return true;
+		}
+		local_irq_restore(*flags);
+	} else if (*cpu_store == cpu) {
+		return true;
+	}
+
+	put_cpu();
+	return false;
+}
+
+/*
+ * prb_lock: Perform a processor-reentrant spin lock.
+ * @cpu_lock: A pointer to the lock object.
+ * @cpu_store: A "flags" pointer to store lock status information.
+ *
+ * If no processor has the lock, the calling processor takes the lock and
+ * becomes the owner. If the calling processor is already the owner of the
+ * lock, this function succeeds immediately. If lock is locked by another
+ * processor, this function spins until the calling processor becomes the
+ * owner.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store)
+{
+	for (;;) {
+		if (__prb_trylock(cpu_lock, cpu_store))
+			break;
+		cpu_relax();
+	}
+}
+
+/*
+ * prb_unlock: Perform a processor-reentrant spin unlock.
+ * @cpu_lock: A pointer to the lock object.
+ * @cpu_store: A "flags" object storing lock status information.
+ *
+ * Release the lock. The calling processor must be the owner of the lock.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_unlock(struct prb_cpulock *cpu_lock, unsigned int cpu_store)
+{
+	unsigned long *flags;
+	unsigned int cpu;
+
+	cpu = atomic_read(&cpu_lock->owner);
+	atomic_set_release(&cpu_lock->owner, cpu_store);
+
+	if (cpu_store == -1) {
+		flags = per_cpu_ptr(cpu_lock->irqflags, cpu);
+		local_irq_restore(*flags);
+	}
+
+	put_cpu();
+}
+
+static struct prb_entry *to_entry(struct printk_ringbuffer *rb,
+				  unsigned long lpos)
+{
+	char *buffer = rb->buffer;
+	buffer += PRB_INDEX(rb, lpos);
+	return (struct prb_entry *)buffer;
+}
+
+static int calc_next(struct printk_ringbuffer *rb, unsigned long tail,
+		     unsigned long lpos, int size, unsigned long *calced_next)
+{
+	unsigned long next_lpos;
+	int ret = 0;
+again:
+	next_lpos = lpos + size;
+	if (next_lpos - tail > PRB_SIZE(rb))
+		return -1;
+
+	if (PRB_WRAPS(rb, lpos) != PRB_WRAPS(rb, next_lpos)) {
+		lpos = PRB_WRAP_LPOS(rb, next_lpos, 0);
+		ret |= 1;
+		goto again;
+	}
+
+	*calced_next = next_lpos;
+	return ret;
+}
+
+static bool push_tail(struct printk_ringbuffer *rb, unsigned long tail)
+{
+	unsigned long new_tail;
+	struct prb_entry *e;
+	unsigned long head;
+
+	if (tail != atomic_long_read(&rb->tail))
+		return true;
+
+	e = to_entry(rb, tail);
+	if (e->size != -1)
+		new_tail = tail + e->size;
+	else
+		new_tail = PRB_WRAP_LPOS(rb, tail, 1);
+
+	/* make sure the new tail does not overtake the head */
+	head = atomic_long_read(&rb->head);
+	if (head - new_tail > PRB_SIZE(rb))
+		return false;
+
+	atomic_long_cmpxchg(&rb->tail, tail, new_tail);
+	return true;
+}
+
+/*
+ * prb_commit: Commit a reserved entry to the ring buffer.
+ * @h: An entry handle referencing the data entry to commit.
+ *
+ * Commit data that has been reserved using prb_reserve(). Once the data
+ * block has been committed, it can be invalidated at any time. If a writer
+ * is interested in using the data after committing, the writer should make
+ * its own copy first or use the prb_iter_ reader functions to access the
+ * data in the ring buffer.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_commit(struct prb_handle *h)
+{
+	struct printk_ringbuffer *rb = h->rb;
+	bool changed = false;
+	struct prb_entry *e;
+	unsigned long head;
+	unsigned long res;
+
+	for (;;) {
+		if (atomic_read(&rb->ctx) != 1) {
+			/* the interrupted context will fixup head */
+			atomic_dec(&rb->ctx);
+			break;
+		}
+		/* assign sequence numbers before moving head */
+		head = atomic_long_read(&rb->head);
+		res = atomic_long_read(&rb->reserve);
+		while (head != res) {
+			e = to_entry(rb, head);
+			if (e->size == -1) {
+				head = PRB_WRAP_LPOS(rb, head, 1);
+				continue;
+			}
+			while (atomic_long_read(&rb->lost)) {
+				atomic_long_dec(&rb->lost);
+				rb->seq++;
+			}
+			e->seq = ++rb->seq;
+			head += e->size;
+			changed = true;
+		}
+		atomic_long_set_release(&rb->head, res);
+
+		atomic_dec(&rb->ctx);
+
+		if (atomic_long_read(&rb->reserve) == res)
+			break;
+		atomic_inc(&rb->ctx);
+	}
+
+	prb_unlock(rb->cpulock, h->cpu);
+
+	if (changed) {
+		atomic_long_inc(&rb->wq_counter);
+		if (wq_has_sleeper(rb->wq)) {
+#ifdef CONFIG_IRQ_WORK
+			irq_work_queue(rb->wq_work);
+#else
+			if (!in_nmi())
+				wake_up_interruptible_all(rb->wq);
+#endif
+		}
+	}
+}
+
+/*
+ * prb_reserve: Reserve an entry within a ring buffer.
+ * @h: An entry handle to be setup and reference an entry.
+ * @rb: A ring buffer to reserve data within.
+ * @size: The number of bytes to reserve.
+ *
+ * Reserve an entry of at least @size bytes to be used by the caller. If
+ * successful, the data region of the entry belongs to the caller and cannot
+ * be invalidated by any other task/context. For this reason, the caller
+ * should call prb_commit() as quickly as possible in order to avoid preventing
+ * other tasks/contexts from reserving data in the case that the ring buffer
+ * has wrapped.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns a pointer to the reserved entry (and @h is setup to reference that
+ * entry) or NULL if it was not possible to reserve data.
+ */
+char *prb_reserve(struct prb_handle *h, struct printk_ringbuffer *rb,
+		  unsigned int size)
+{
+	unsigned long tail, res1, res2;
+	int ret;
+
+	if (size == 0)
+		return NULL;
+	size += sizeof(struct prb_entry);
+	size += PRB_DATA_ALIGN - 1;
+	size &= ~(PRB_DATA_ALIGN - 1);
+	if (size >= PRB_SIZE(rb))
+		return NULL;
+
+	h->rb = rb;
+	prb_lock(rb->cpulock, &h->cpu);
+
+	atomic_inc(&rb->ctx);
+
+	do {
+		for (;;) {
+			tail = atomic_long_read(&rb->tail);
+			res1 = atomic_long_read(&rb->reserve);
+			ret = calc_next(rb, tail, res1, size, &res2);
+			if (ret >= 0)
+				break;
+			if (!push_tail(rb, tail)) {
+				prb_commit(h);
+				return NULL;
+			}
+		}
+	} while (!atomic_long_try_cmpxchg_acquire(&rb->reserve, &res1, res2));
+
+	h->entry = to_entry(rb, res1);
+
+	if (ret) {
+		/* handle wrap */
+		h->entry->size = -1;
+		h->entry = to_entry(rb, PRB_WRAP_LPOS(rb, res2, 0));
+	}
+
+	h->entry->size = size;
+
+	return &h->entry->data[0];
+}
+
+/*
+ * prb_iter_copy: Copy an iterator.
+ * @dest: The iterator to copy to.
+ * @src: The iterator to copy from.
+ *
+ * Make a deep copy of an iterator. This is particularly useful for making
+ * backup copies of an iterator in case a form of rewinding it needed.
+ *
+ * It is safe to call this function from any context and state. But
+ * note that this function is not atomic. Callers should not make copies
+ * to/from iterators that can be accessed by other tasks/contexts.
+ */
+void prb_iter_copy(struct prb_iterator *dest, struct prb_iterator *src)
+{
+	memcpy(dest, src, sizeof(*dest));
+}
+
+/*
+ * prb_iter_init: Initialize an iterator for a ring buffer.
+ * @iter: The iterator to initialize.
+ * @rb: A ring buffer to that @iter should iterate.
+ * @seq: The sequence number of the position preceding the first record.
+ *       May be NULL.
+ *
+ * Initialize an iterator to be used with a specified ring buffer. If @seq
+ * is non-NULL, it will be set such that prb_iter_next() will provide a
+ * sequence value of "@seq + 1" if no records were missed.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_iter_init(struct prb_iterator *iter, struct printk_ringbuffer *rb,
+		   u64 *seq)
+{
+	memset(iter, 0, sizeof(*iter));
+	iter->rb = rb;
+	iter->lpos = PRB_INIT;
+
+	if (!seq)
+		return;
+
+	for (;;) {
+		struct prb_iterator tmp_iter;
+		int ret;
+
+		prb_iter_copy(&tmp_iter, iter);
+
+		ret = prb_iter_next(&tmp_iter, NULL, 0, seq);
+		if (ret < 0)
+			continue;
+
+		if (ret == 0)
+			*seq = 0;
+		else
+			(*seq)--;
+		break;
+	}
+}
+
+static bool is_valid(struct printk_ringbuffer *rb, unsigned long lpos)
+{
+	unsigned long head, tail;
+
+	tail = atomic_long_read(&rb->tail);
+	head = atomic_long_read(&rb->head);
+	head -= tail;
+	lpos -= tail;
+
+	if (lpos >= head)
+		return false;
+	return true;
+}
+
+/*
+ * prb_iter_data: Retrieve the record data at the current position.
+ * @iter: Iterator tracking the current position.
+ * @buf: A buffer to store the data of the record. May be NULL.
+ * @size: The size of @buf. (Ignored if @buf is NULL.)
+ * @seq: The sequence number of the record. May be NULL.
+ *
+ * If @iter is at a record, provide the data and/or sequence number of that
+ * record (if specified by the caller).
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns >=0 if the current record contains valid data (returns 0 if @buf
+ * is NULL or returns the size of the data block if @buf is non-NULL) or
+ * -EINVAL if @iter is now invalid.
+ */
+int prb_iter_data(struct prb_iterator *iter, char *buf, int size, u64 *seq)
+{
+	struct printk_ringbuffer *rb = iter->rb;
+	unsigned long lpos = iter->lpos;
+	unsigned int datsize = 0;
+	struct prb_entry *e;
+
+	if (buf || seq) {
+		e = to_entry(rb, lpos);
+		if (!is_valid(rb, lpos))
+			return -EINVAL;
+		/* memory barrier to ensure valid lpos */
+		smp_rmb();
+		if (buf) {
+			datsize = PRB_DATA_SIZE(e);
+			/* memory barrier to ensure load of datsize */
+			smp_rmb();
+			if (!is_valid(rb, lpos))
+				return -EINVAL;
+			if (PRB_INDEX(rb, lpos) + datsize >
+			    PRB_SIZE(rb) - PRB_DATA_ALIGN) {
+				return -EINVAL;
+			}
+			if (size > datsize)
+				size = datsize;
+			memcpy(buf, &e->data[0], size);
+		}
+		if (seq)
+			*seq = e->seq;
+		/* memory barrier to ensure loads of entry data */
+		smp_rmb();
+	}
+
+	if (!is_valid(rb, lpos))
+		return -EINVAL;
+
+	return datsize;
+}
+
+/*
+ * prb_iter_next: Advance to the next record.
+ * @iter: Iterator tracking the current position.
+ * @buf: A buffer to store the data of the next record. May be NULL.
+ * @size: The size of @buf. (Ignored if @buf is NULL.)
+ * @seq: The sequence number of the next record. May be NULL.
+ *
+ * If a next record is available, @iter is advanced and (if specified)
+ * the data and/or sequence number of that record are provided.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns 1 if @iter was advanced, 0 if @iter is at the end of the list, or
+ * -EINVAL if @iter is now invalid.
+ */
+int prb_iter_next(struct prb_iterator *iter, char *buf, int size, u64 *seq)
+{
+	struct printk_ringbuffer *rb = iter->rb;
+	unsigned long next_lpos;
+	struct prb_entry *e;
+	unsigned int esize;
+
+	if (iter->lpos == PRB_INIT) {
+		next_lpos = atomic_long_read(&rb->tail);
+	} else {
+		if (!is_valid(rb, iter->lpos))
+			return -EINVAL;
+		/* memory barrier to ensure valid lpos */
+		smp_rmb();
+		e = to_entry(rb, iter->lpos);
+		esize = e->size;
+		/* memory barrier to ensure load of size */
+		smp_rmb();
+		if (!is_valid(rb, iter->lpos))
+			return -EINVAL;
+		next_lpos = iter->lpos + esize;
+	}
+	if (next_lpos == atomic_long_read(&rb->head))
+		return 0;
+	if (!is_valid(rb, next_lpos))
+		return -EINVAL;
+	/* memory barrier to ensure valid lpos */
+	smp_rmb();
+
+	iter->lpos = next_lpos;
+	e = to_entry(rb, iter->lpos);
+	esize = e->size;
+	/* memory barrier to ensure load of size */
+	smp_rmb();
+	if (!is_valid(rb, iter->lpos))
+		return -EINVAL;
+	if (esize == -1)
+		iter->lpos = PRB_WRAP_LPOS(rb, iter->lpos, 1);
+
+	if (prb_iter_data(iter, buf, size, seq) < 0)
+		return -EINVAL;
+
+	return 1;
+}
+
+/*
+ * prb_iter_wait_next: Advance to the next record, blocking if none available.
+ * @iter: Iterator tracking the current position.
+ * @buf: A buffer to store the data of the next record. May be NULL.
+ * @size: The size of @buf. (Ignored if @buf is NULL.)
+ * @seq: The sequence number of the next record. May be NULL.
+ *
+ * If a next record is already available, this function works like
+ * prb_iter_next(). Otherwise block interruptible until a next record is
+ * available.
+ *
+ * When a next record is available, @iter is advanced and (if specified)
+ * the data and/or sequence number of that record are provided.
+ *
+ * This function might sleep.
+ *
+ * Returns 1 if @iter was advanced, -EINVAL if @iter is now invalid, or
+ * -ERESTARTSYS if interrupted by a signal.
+ */
+int prb_iter_wait_next(struct prb_iterator *iter, char *buf, int size, u64 *seq)
+{
+	unsigned long last_seen;
+	int ret;
+
+	for (;;) {
+		last_seen = atomic_long_read(&iter->rb->wq_counter);
+
+		ret = prb_iter_next(iter, buf, size, seq);
+		if (ret != 0)
+			break;
+
+		ret = wait_event_interruptible(*iter->rb->wq,
+			last_seen != atomic_long_read(&iter->rb->wq_counter));
+		if (ret < 0)
+			break;
+	}
+
+	return ret;
+}
+
+/*
+ * prb_iter_seek: Seek forward to a specific record.
+ * @iter: Iterator to advance.
+ * @seq: Record number to advance to.
+ *
+ * Advance @iter such that a following call to prb_iter_data() will provide
+ * the contents of the specified record. If a record is specified that does
+ * not yet exist, advance @iter to the end of the record list.
+ *
+ * Note that iterators cannot be rewound. So if a record is requested that
+ * exists but is previous to @iter in position, @iter is considered invalid.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns 1 on succces, 0 if specified record does not yet exist (@iter is
+ * now at the end of the list), or -EINVAL if @iter is now invalid.
+ */
+int prb_iter_seek(struct prb_iterator *iter, u64 seq)
+{
+	u64 cur_seq;
+	int ret;
+
+	/* first check if the iterator is already at the wanted seq */
+	if (seq == 0) {
+		if (iter->lpos == PRB_INIT)
+			return 1;
+		else
+			return -EINVAL;
+	}
+	if (iter->lpos != PRB_INIT) {
+		if (prb_iter_data(iter, NULL, 0, &cur_seq) >= 0) {
+			if (cur_seq == seq)
+				return 1;
+			if (cur_seq > seq)
+				return -EINVAL;
+		}
+	}
+
+	/* iterate to find the wanted seq */
+	for (;;) {
+		ret = prb_iter_next(iter, NULL, 0, &cur_seq);
+		if (ret <= 0)
+			break;
+
+		if (cur_seq == seq)
+			break;
+
+		if (cur_seq > seq) {
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+/*
+ * prb_buffer_size: Get the size of the ring buffer.
+ * @rb: The ring buffer to get the size of.
+ *
+ * Return the number of bytes used for the ring buffer entry storage area.
+ * Note that this area stores both entry header and entry data. Therefore
+ * this represents an upper bound to the amount of data that can be stored
+ * in the ring buffer.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns the size in bytes of the entry storage area.
+ */
+int prb_buffer_size(struct printk_ringbuffer *rb)
+{
+	return PRB_SIZE(rb);
+}
+
+/*
+ * prb_inc_lost: Increment the seq counter to signal a lost record.
+ * @rb: The ring buffer to increment the seq of.
+ *
+ * Increment the seq counter so that a seq number is intentially missing
+ * for the readers. This allows readers to identify that a record is
+ * missing. A writer will typically use this function if prb_reserve()
+ * fails.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_inc_lost(struct printk_ringbuffer *rb)
+{
+	atomic_long_inc(&rb->lost);
+}
diff -Naur a/lib/scatterlist.c b/lib/scatterlist.c
--- a/lib/scatterlist.c	2020-11-23 13:48:35.409954269 +0200
+++ b/lib/scatterlist.c	2021-07-14 15:39:11.458137971 +0300
@@ -811,7 +811,7 @@
 			flush_kernel_dcache_page(miter->page);
 
 		if (miter->__flags & SG_MITER_ATOMIC) {
-			WARN_ON_ONCE(preemptible());
+			WARN_ON_ONCE(!pagefault_disabled());
 			kunmap_atomic(miter->addr);
 		} else
 			kunmap(miter->page);
diff -Naur a/lib/smp_processor_id.c b/lib/smp_processor_id.c
--- a/lib/smp_processor_id.c	2020-11-23 13:48:35.413954349 +0200
+++ b/lib/smp_processor_id.c	2021-07-14 15:39:11.462137943 +0300
@@ -26,6 +26,11 @@
 	if (current->nr_cpus_allowed == 1)
 		goto out;
 
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	if (current->migration_disabled)
+		goto out;
+#endif
+
 	/*
 	 * It is valid to assume CPU-locality during early bootup:
 	 */
diff -Naur a/localversion-rt b/localversion-rt
--- a/localversion-rt	1970-01-01 02:00:00.000000000 +0200
+++ b/localversion-rt	2021-07-14 15:38:50.910284828 +0300
@@ -0,0 +1 @@
+-rt16
diff -Naur a/mm/highmem.c b/mm/highmem.c
--- a/mm/highmem.c	2020-11-23 13:48:35.777961655 +0200
+++ b/mm/highmem.c	2021-07-14 15:39:11.886134962 +0300
@@ -31,8 +31,11 @@
 #include <asm/tlbflush.h>
 #include <linux/vmalloc.h>
 
+#ifndef CONFIG_PREEMPT_RT
 #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
 DEFINE_PER_CPU(int, __kmap_atomic_idx);
+EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
+#endif
 #endif
 
 /*
@@ -108,8 +111,6 @@
 atomic_long_t _totalhigh_pages __read_mostly;
 EXPORT_SYMBOL(_totalhigh_pages);
 
-EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
-
 unsigned int nr_free_highpages (void)
 {
 	struct zone *zone;
diff -Naur a/mm/Kconfig b/mm/Kconfig
--- a/mm/Kconfig	2020-11-23 13:48:35.757961254 +0200
+++ b/mm/Kconfig	2021-07-14 15:39:11.882134990 +0300
@@ -387,7 +387,7 @@
 
 config TRANSPARENT_HUGEPAGE
 	bool "Transparent Hugepage Support"
-	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE
+	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE && !PREEMPT_RT
 	select COMPACTION
 	select XARRAY_MULTI
 	help
diff -Naur a/mm/memcontrol.c b/mm/memcontrol.c
--- a/mm/memcontrol.c	2020-11-23 13:48:35.813962378 +0200
+++ b/mm/memcontrol.c	2021-07-14 15:39:11.886134962 +0300
@@ -63,6 +63,7 @@
 #include <net/sock.h>
 #include <net/ip.h>
 #include "slab.h"
+#include <linux/local_lock.h>
 
 #include <linux/uaccess.h>
 
@@ -90,6 +91,13 @@
 static DECLARE_WAIT_QUEUE_HEAD(memcg_cgwb_frn_waitq);
 #endif
 
+struct event_lock {
+	local_lock_t l;
+};
+static DEFINE_PER_CPU(struct event_lock, event_lock) = {
+	.l      = INIT_LOCAL_LOCK(l),
+};
+
 /* Whether legacy memory+swap accounting is active */
 static bool do_memsw_account(void)
 {
@@ -2154,6 +2162,7 @@
 EXPORT_SYMBOL(unlock_page_memcg);
 
 struct memcg_stock_pcp {
+	local_lock_t lock;
 	struct mem_cgroup *cached; /* this never be root cgroup */
 	unsigned int nr_pages;
 
@@ -2205,7 +2214,7 @@
 	if (nr_pages > MEMCG_CHARGE_BATCH)
 		return ret;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (memcg == stock->cached && stock->nr_pages >= nr_pages) {
@@ -2213,7 +2222,7 @@
 		ret = true;
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 
 	return ret;
 }
@@ -2248,14 +2257,14 @@
 	 * The only protection from memory hotplug vs. drain_stock races is
 	 * that we always operate on local CPU stock here with IRQ disabled
 	 */
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	drain_obj_stock(stock);
 	drain_stock(stock);
 	clear_bit(FLUSHING_CACHED_CHARGE, &stock->flags);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 }
 
 /*
@@ -2267,7 +2276,7 @@
 	struct memcg_stock_pcp *stock;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (stock->cached != memcg) { /* reset if necessary */
@@ -2280,7 +2289,7 @@
 	if (stock->nr_pages > MEMCG_CHARGE_BATCH)
 		drain_stock(stock);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 }
 
 /*
@@ -2300,7 +2309,7 @@
 	 * as well as workers from this path always operate on the local
 	 * per-cpu data. CPU up doesn't touch memcg_stock at all.
 	 */
-	curcpu = get_cpu();
+	curcpu = get_cpu_light();
 	for_each_online_cpu(cpu) {
 		struct memcg_stock_pcp *stock = &per_cpu(memcg_stock, cpu);
 		struct mem_cgroup *memcg;
@@ -2323,7 +2332,7 @@
 				schedule_work_on(cpu, &stock->work);
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 	mutex_unlock(&percpu_charge_mutex);
 }
 
@@ -3084,7 +3093,7 @@
 	unsigned long flags;
 	bool ret = false;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (objcg == stock->cached_objcg && stock->nr_bytes >= nr_bytes) {
@@ -3092,7 +3101,7 @@
 		ret = true;
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 
 	return ret;
 }
@@ -3151,7 +3160,7 @@
 	struct memcg_stock_pcp *stock;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&memcg_stock.lock, flags);
 
 	stock = this_cpu_ptr(&memcg_stock);
 	if (stock->cached_objcg != objcg) { /* reset if necessary */
@@ -3165,7 +3174,7 @@
 	if (stock->nr_bytes > PAGE_SIZE)
 		drain_obj_stock(stock);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&memcg_stock.lock, flags);
 }
 
 int obj_cgroup_charge(struct obj_cgroup *objcg, gfp_t gfp, size_t size)
@@ -5681,12 +5690,12 @@
 
 	ret = 0;
 
-	local_irq_disable();
+	local_lock_irq(&event_lock.l);
 	mem_cgroup_charge_statistics(to, page, nr_pages);
 	memcg_check_events(to, page);
 	mem_cgroup_charge_statistics(from, page, -nr_pages);
 	memcg_check_events(from, page);
-	local_irq_enable();
+	local_unlock_irq(&event_lock.l);
 out_unlock:
 	unlock_page(page);
 out:
@@ -6722,10 +6731,10 @@
 	css_get(&memcg->css);
 	commit_charge(page, memcg);
 
-	local_irq_disable();
+	local_lock_irq(&event_lock.l);
 	mem_cgroup_charge_statistics(memcg, page, nr_pages);
 	memcg_check_events(memcg, page);
-	local_irq_enable();
+	local_unlock_irq(&event_lock.l);
 
 	if (PageSwapCache(page)) {
 		swp_entry_t entry = { .val = page_private(page) };
@@ -6769,11 +6778,11 @@
 		memcg_oom_recover(ug->memcg);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(&event_lock.l, flags);
 	__count_memcg_events(ug->memcg, PGPGOUT, ug->pgpgout);
 	__this_cpu_add(ug->memcg->vmstats_percpu->nr_page_events, ug->nr_pages);
 	memcg_check_events(ug->memcg, ug->dummy_page);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&event_lock.l, flags);
 
 	/* drop reference from uncharge_page */
 	css_put(&ug->memcg->css);
@@ -6927,10 +6936,10 @@
 	css_get(&memcg->css);
 	commit_charge(newpage, memcg);
 
-	local_irq_save(flags);
+	local_lock_irqsave(&event_lock.l, flags);
 	mem_cgroup_charge_statistics(memcg, newpage, nr_pages);
 	memcg_check_events(memcg, newpage);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&event_lock.l, flags);
 }
 
 DEFINE_STATIC_KEY_FALSE(memcg_sockets_enabled_key);
@@ -7050,9 +7059,13 @@
 	cpuhp_setup_state_nocalls(CPUHP_MM_MEMCQ_DEAD, "mm/memctrl:dead", NULL,
 				  memcg_hotplug_cpu_dead);
 
-	for_each_possible_cpu(cpu)
-		INIT_WORK(&per_cpu_ptr(&memcg_stock, cpu)->work,
-			  drain_local_stock);
+	for_each_possible_cpu(cpu) {
+		struct memcg_stock_pcp *stock;
+
+		stock = per_cpu_ptr(&memcg_stock, cpu);
+		INIT_WORK(&stock->work, drain_local_stock);
+		local_lock_init(&stock->lock);
+	}
 
 	for_each_node(node) {
 		struct mem_cgroup_tree_per_node *rtpn;
@@ -7101,6 +7114,7 @@
 	struct mem_cgroup *memcg, *swap_memcg;
 	unsigned int nr_entries;
 	unsigned short oldid;
+	unsigned long flags;
 
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 	VM_BUG_ON_PAGE(page_count(page), page);
@@ -7146,9 +7160,13 @@
 	 * important here to have the interrupts disabled because it is the
 	 * only synchronisation we have for updating the per-CPU variables.
 	 */
+	local_lock_irqsave(&event_lock.l, flags);
+#ifndef CONFIG_PREEMPT_RT
 	VM_BUG_ON(!irqs_disabled());
+#endif
 	mem_cgroup_charge_statistics(memcg, page, -nr_entries);
 	memcg_check_events(memcg, page);
+	local_unlock_irqrestore(&event_lock.l, flags);
 
 	css_put(&memcg->css);
 }
diff -Naur a/mm/page_alloc.c b/mm/page_alloc.c
--- a/mm/page_alloc.c	2020-11-23 13:48:35.873963582 +0200
+++ b/mm/page_alloc.c	2021-07-14 15:39:11.886134962 +0300
@@ -61,6 +61,7 @@
 #include <linux/hugetlb.h>
 #include <linux/sched/rt.h>
 #include <linux/sched/mm.h>
+#include <linux/local_lock.h>
 #include <linux/page_owner.h>
 #include <linux/kthread.h>
 #include <linux/memcontrol.h>
@@ -357,6 +358,13 @@
 EXPORT_SYMBOL(nr_online_nodes);
 #endif
 
+struct pa_lock {
+	local_lock_t l;
+};
+static DEFINE_PER_CPU(struct pa_lock, pa_lock) = {
+	.l	= INIT_LOCAL_LOCK(l),
+};
+
 int page_group_by_mobility_disabled __read_mostly;
 
 #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
@@ -1283,7 +1291,7 @@
 }
 
 /*
- * Frees a number of pages from the PCP lists
+ * Frees a number of pages which have been collected from the pcp lists.
  * Assumes all pages on list are in same zone, and of same order.
  * count is the number of pages to free.
  *
@@ -1293,15 +1301,56 @@
  * And clear the zone's pages_scanned counter, to hold off the "all pages are
  * pinned" detection logic.
  */
-static void free_pcppages_bulk(struct zone *zone, int count,
-					struct per_cpu_pages *pcp)
+static void free_pcppages_bulk(struct zone *zone, struct list_head *head,
+			       bool zone_retry)
+{
+	bool isolated_pageblocks;
+	struct page *page, *tmp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&zone->lock, flags);
+	isolated_pageblocks = has_isolate_pageblock(zone);
+
+	/*
+	 * Use safe version since after __free_one_page(),
+	 * page->lru.next will not point to original list.
+	 */
+	list_for_each_entry_safe(page, tmp, head, lru) {
+		int mt = get_pcppage_migratetype(page);
+
+		if (page_zone(page) != zone) {
+			/*
+			 * free_unref_page_list() sorts pages by zone. If we end
+			 * up with pages from a different NUMA nodes belonging
+			 * to the same ZONE index then we need to redo with the
+			 * correct ZONE pointer. Skip the page for now, redo it
+			 * on the next iteration.
+			 */
+			WARN_ON_ONCE(zone_retry == false);
+			if (zone_retry)
+				continue;
+		}
+
+		/* MIGRATE_ISOLATE page should not go to pcplists */
+		VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
+		/* Pageblock could have been isolated meanwhile */
+		if (unlikely(isolated_pageblocks))
+			mt = get_pageblock_migratetype(page);
+
+		list_del(&page->lru);
+		__free_one_page(page, page_to_pfn(page), zone, 0, mt, true);
+		trace_mm_page_pcpu_drain(page, 0, mt);
+	}
+	spin_unlock_irqrestore(&zone->lock, flags);
+}
+
+static void isolate_pcp_pages(int count, struct per_cpu_pages *pcp,
+			      struct list_head *dst)
 {
 	int migratetype = 0;
 	int batch_free = 0;
 	int prefetch_nr = 0;
-	bool isolated_pageblocks;
-	struct page *page, *tmp;
-	LIST_HEAD(head);
+	struct page *page;
 
 	/*
 	 * Ensure proper count is passed which otherwise would stuck in the
@@ -1338,7 +1387,7 @@
 			if (bulkfree_pcp_prepare(page))
 				continue;
 
-			list_add_tail(&page->lru, &head);
+			list_add_tail(&page->lru, dst);
 
 			/*
 			 * We are going to put the page back to the global
@@ -1353,26 +1402,6 @@
 				prefetch_buddy(page);
 		} while (--count && --batch_free && !list_empty(list));
 	}
-
-	spin_lock(&zone->lock);
-	isolated_pageblocks = has_isolate_pageblock(zone);
-
-	/*
-	 * Use safe version since after __free_one_page(),
-	 * page->lru.next will not point to original list.
-	 */
-	list_for_each_entry_safe(page, tmp, &head, lru) {
-		int mt = get_pcppage_migratetype(page);
-		/* MIGRATE_ISOLATE page should not go to pcplists */
-		VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
-		/* Pageblock could have been isolated meanwhile */
-		if (unlikely(isolated_pageblocks))
-			mt = get_pageblock_migratetype(page);
-
-		__free_one_page(page, page_to_pfn(page), zone, 0, mt, true);
-		trace_mm_page_pcpu_drain(page, 0, mt);
-	}
-	spin_unlock(&zone->lock);
 }
 
 static void free_one_page(struct zone *zone,
@@ -1473,10 +1502,10 @@
 		return;
 
 	migratetype = get_pfnblock_migratetype(page, pfn);
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	__count_vm_events(PGFREE, 1 << order);
 	free_one_page(page_zone(page), page, pfn, order, migratetype);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 }
 
 void __free_pages_core(struct page *page, unsigned int order)
@@ -2877,13 +2906,18 @@
 {
 	unsigned long flags;
 	int to_drain, batch;
+	LIST_HEAD(dst);
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	batch = READ_ONCE(pcp->batch);
 	to_drain = min(pcp->count, batch);
 	if (to_drain > 0)
-		free_pcppages_bulk(zone, to_drain, pcp);
-	local_irq_restore(flags);
+		isolate_pcp_pages(to_drain, pcp, &dst);
+
+	local_unlock_irqrestore(&pa_lock.l, flags);
+
+	if (to_drain > 0)
+		free_pcppages_bulk(zone, &dst, false);
 }
 #endif
 
@@ -2899,14 +2933,21 @@
 	unsigned long flags;
 	struct per_cpu_pageset *pset;
 	struct per_cpu_pages *pcp;
+	LIST_HEAD(dst);
+	int count;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	pset = per_cpu_ptr(zone->pageset, cpu);
 
 	pcp = &pset->pcp;
-	if (pcp->count)
-		free_pcppages_bulk(zone, pcp->count, pcp);
-	local_irq_restore(flags);
+	count = pcp->count;
+	if (count)
+		isolate_pcp_pages(count, pcp, &dst);
+
+	local_unlock_irqrestore(&pa_lock.l, flags);
+
+	if (count)
+		free_pcppages_bulk(zone, &dst, false);
 }
 
 /*
@@ -2954,9 +2995,9 @@
 	 * cpu which is allright but we also have to make sure to not move to
 	 * a different one.
 	 */
-	preempt_disable();
+	migrate_disable();
 	drain_local_pages(drain->zone);
-	preempt_enable();
+	migrate_enable();
 }
 
 /*
@@ -3105,7 +3146,8 @@
 	return true;
 }
 
-static void free_unref_page_commit(struct page *page, unsigned long pfn)
+static void free_unref_page_commit(struct page *page, unsigned long pfn,
+				   struct list_head *dst)
 {
 	struct zone *zone = page_zone(page);
 	struct per_cpu_pages *pcp;
@@ -3134,7 +3176,8 @@
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
 		unsigned long batch = READ_ONCE(pcp->batch);
-		free_pcppages_bulk(zone, batch, pcp);
+
+		isolate_pcp_pages(batch, pcp, dst);
 	}
 }
 
@@ -3145,13 +3188,17 @@
 {
 	unsigned long flags;
 	unsigned long pfn = page_to_pfn(page);
+	struct zone *zone = page_zone(page);
+	LIST_HEAD(dst);
 
 	if (!free_unref_page_prepare(page, pfn))
 		return;
 
-	local_irq_save(flags);
-	free_unref_page_commit(page, pfn);
-	local_irq_restore(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
+	free_unref_page_commit(page, pfn, &dst);
+	local_unlock_irqrestore(&pa_lock.l, flags);
+	if (!list_empty(&dst))
+		free_pcppages_bulk(zone, &dst, false);
 }
 
 /*
@@ -3162,6 +3209,11 @@
 	struct page *page, *next;
 	unsigned long flags, pfn;
 	int batch_count = 0;
+	struct list_head dsts[__MAX_NR_ZONES];
+	int i;
+
+	for (i = 0; i < __MAX_NR_ZONES; i++)
+		INIT_LIST_HEAD(&dsts[i]);
 
 	/* Prepare pages for freeing */
 	list_for_each_entry_safe(page, next, list, lru) {
@@ -3171,25 +3223,42 @@
 		set_page_private(page, pfn);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	list_for_each_entry_safe(page, next, list, lru) {
 		unsigned long pfn = page_private(page);
+		enum zone_type type;
 
 		set_page_private(page, 0);
 		trace_mm_page_free_batched(page);
-		free_unref_page_commit(page, pfn);
+		type = page_zonenum(page);
+		free_unref_page_commit(page, pfn, &dsts[type]);
 
 		/*
 		 * Guard against excessive IRQ disabled times when we get
 		 * a large list of pages to free.
 		 */
 		if (++batch_count == SWAP_CLUSTER_MAX) {
-			local_irq_restore(flags);
+			local_unlock_irqrestore(&pa_lock.l, flags);
 			batch_count = 0;
-			local_irq_save(flags);
+			local_lock_irqsave(&pa_lock.l, flags);
 		}
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
+
+	for (i = 0; i < __MAX_NR_ZONES; ) {
+		struct page *page;
+		struct zone *zone;
+
+		if (list_empty(&dsts[i])) {
+			i++;
+			continue;
+		}
+
+		page = list_first_entry(&dsts[i], struct page, lru);
+		zone = page_zone(page);
+
+		free_pcppages_bulk(zone, &dsts[i], true);
+	}
 }
 
 /*
@@ -3343,7 +3412,7 @@
 	struct page *page;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
 	list = &pcp->lists[migratetype];
 	page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);
@@ -3351,7 +3420,7 @@
 		__count_zid_vm_events(PGALLOC, page_zonenum(page), 1);
 		zone_statistics(preferred_zone, zone);
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 	return page;
 }
 
@@ -3385,7 +3454,8 @@
 	 * allocate greater than order-1 page units with __GFP_NOFAIL.
 	 */
 	WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));
-	spin_lock_irqsave(&zone->lock, flags);
+	local_lock_irqsave(&pa_lock.l, flags);
+	spin_lock(&zone->lock);
 
 	do {
 		page = NULL;
@@ -3411,7 +3481,7 @@
 
 	__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
 	zone_statistics(preferred_zone, zone);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 
 out:
 	/* Separate test+clear to avoid unnecessary atomics */
@@ -3424,7 +3494,7 @@
 	return page;
 
 failed:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 	return NULL;
 }
 
@@ -8697,7 +8767,7 @@
 	struct per_cpu_pageset *pset;
 
 	/* avoid races with drain_pages()  */
-	local_irq_save(flags);
+	local_lock_irqsave(&pa_lock.l, flags);
 	if (zone->pageset != &boot_pageset) {
 		for_each_online_cpu(cpu) {
 			pset = per_cpu_ptr(zone->pageset, cpu);
@@ -8706,7 +8776,7 @@
 		free_percpu(zone->pageset);
 		zone->pageset = &boot_pageset;
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(&pa_lock.l, flags);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
diff -Naur a/mm/shmem.c b/mm/shmem.c
--- a/mm/shmem.c	2020-11-23 13:48:35.909964305 +0200
+++ b/mm/shmem.c	2021-07-14 15:39:11.890134934 +0300
@@ -278,10 +278,10 @@
 	ino_t ino;
 
 	if (!(sb->s_flags & SB_KERNMOUNT)) {
-		spin_lock(&sbinfo->stat_lock);
+		raw_spin_lock(&sbinfo->stat_lock);
 		if (sbinfo->max_inodes) {
 			if (!sbinfo->free_inodes) {
-				spin_unlock(&sbinfo->stat_lock);
+				raw_spin_unlock(&sbinfo->stat_lock);
 				return -ENOSPC;
 			}
 			sbinfo->free_inodes--;
@@ -304,7 +304,7 @@
 			}
 			*inop = ino;
 		}
-		spin_unlock(&sbinfo->stat_lock);
+		raw_spin_unlock(&sbinfo->stat_lock);
 	} else if (inop) {
 		/*
 		 * __shmem_file_setup, one of our callers, is lock-free: it
@@ -319,13 +319,14 @@
 		 * to worry about things like glibc compatibility.
 		 */
 		ino_t *next_ino;
+
 		next_ino = per_cpu_ptr(sbinfo->ino_batch, get_cpu());
 		ino = *next_ino;
 		if (unlikely(ino % SHMEM_INO_BATCH == 0)) {
-			spin_lock(&sbinfo->stat_lock);
+			raw_spin_lock(&sbinfo->stat_lock);
 			ino = sbinfo->next_ino;
 			sbinfo->next_ino += SHMEM_INO_BATCH;
-			spin_unlock(&sbinfo->stat_lock);
+			raw_spin_unlock(&sbinfo->stat_lock);
 			if (unlikely(is_zero_ino(ino)))
 				ino++;
 		}
@@ -341,9 +342,9 @@
 {
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 	if (sbinfo->max_inodes) {
-		spin_lock(&sbinfo->stat_lock);
+		raw_spin_lock(&sbinfo->stat_lock);
 		sbinfo->free_inodes++;
-		spin_unlock(&sbinfo->stat_lock);
+		raw_spin_unlock(&sbinfo->stat_lock);
 	}
 }
 
@@ -1479,10 +1480,10 @@
 {
 	struct mempolicy *mpol = NULL;
 	if (sbinfo->mpol) {
-		spin_lock(&sbinfo->stat_lock);	/* prevent replace/use races */
+		raw_spin_lock(&sbinfo->stat_lock);	/* prevent replace/use races */
 		mpol = sbinfo->mpol;
 		mpol_get(mpol);
-		spin_unlock(&sbinfo->stat_lock);
+		raw_spin_unlock(&sbinfo->stat_lock);
 	}
 	return mpol;
 }
@@ -3582,9 +3583,10 @@
 	struct shmem_options *ctx = fc->fs_private;
 	struct shmem_sb_info *sbinfo = SHMEM_SB(fc->root->d_sb);
 	unsigned long inodes;
+	struct mempolicy *mpol = NULL;
 	const char *err;
 
-	spin_lock(&sbinfo->stat_lock);
+	raw_spin_lock(&sbinfo->stat_lock);
 	inodes = sbinfo->max_inodes - sbinfo->free_inodes;
 	if ((ctx->seen & SHMEM_SEEN_BLOCKS) && ctx->blocks) {
 		if (!sbinfo->max_blocks) {
@@ -3629,14 +3631,15 @@
 	 * Preserve previous mempolicy unless mpol remount option was specified.
 	 */
 	if (ctx->mpol) {
-		mpol_put(sbinfo->mpol);
+		mpol = sbinfo->mpol;
 		sbinfo->mpol = ctx->mpol;	/* transfers initial ref */
 		ctx->mpol = NULL;
 	}
-	spin_unlock(&sbinfo->stat_lock);
+	raw_spin_unlock(&sbinfo->stat_lock);
+	mpol_put(mpol);
 	return 0;
 out:
-	spin_unlock(&sbinfo->stat_lock);
+	raw_spin_unlock(&sbinfo->stat_lock);
 	return invalfc(fc, "%s", err);
 }
 
@@ -3753,7 +3756,7 @@
 	sbinfo->mpol = ctx->mpol;
 	ctx->mpol = NULL;
 
-	spin_lock_init(&sbinfo->stat_lock);
+	raw_spin_lock_init(&sbinfo->stat_lock);
 	if (percpu_counter_init(&sbinfo->used_blocks, 0, GFP_KERNEL))
 		goto failed;
 	spin_lock_init(&sbinfo->shrinklist_lock);
diff -Naur a/mm/slab.c b/mm/slab.c
--- a/mm/slab.c	2020-11-23 13:48:35.913964384 +0200
+++ b/mm/slab.c	2021-07-14 15:39:11.890134934 +0300
@@ -233,7 +233,7 @@
 	parent->shared = NULL;
 	parent->alien = NULL;
 	parent->colour_next = 0;
-	spin_lock_init(&parent->list_lock);
+	raw_spin_lock_init(&parent->list_lock);
 	parent->free_objects = 0;
 	parent->free_touched = 0;
 }
@@ -558,9 +558,9 @@
 	page_node = page_to_nid(page);
 	n = get_node(cachep, page_node);
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	free_block(cachep, &objp, 1, page_node, &list);
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 
 	slabs_destroy(cachep, &list);
 }
@@ -698,7 +698,7 @@
 	struct kmem_cache_node *n = get_node(cachep, node);
 
 	if (ac->avail) {
-		spin_lock(&n->list_lock);
+		raw_spin_lock(&n->list_lock);
 		/*
 		 * Stuff objects into the remote nodes shared array first.
 		 * That way we could avoid the overhead of putting the objects
@@ -709,7 +709,7 @@
 
 		free_block(cachep, ac->entry, ac->avail, node, list);
 		ac->avail = 0;
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 	}
 }
 
@@ -782,9 +782,9 @@
 		slabs_destroy(cachep, &list);
 	} else {
 		n = get_node(cachep, page_node);
-		spin_lock(&n->list_lock);
+		raw_spin_lock(&n->list_lock);
 		free_block(cachep, &objp, 1, page_node, &list);
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 		slabs_destroy(cachep, &list);
 	}
 	return 1;
@@ -825,10 +825,10 @@
 	 */
 	n = get_node(cachep, node);
 	if (n) {
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		n->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +
 				cachep->num;
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		return 0;
 	}
@@ -907,7 +907,7 @@
 		goto fail;
 
 	n = get_node(cachep, node);
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	if (n->shared && force_change) {
 		free_block(cachep, n->shared->entry,
 				n->shared->avail, node, &list);
@@ -925,7 +925,7 @@
 		new_alien = NULL;
 	}
 
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 	slabs_destroy(cachep, &list);
 
 	/*
@@ -964,7 +964,7 @@
 		if (!n)
 			continue;
 
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 
 		/* Free limit for this kmem_cache_node */
 		n->free_limit -= cachep->batchcount;
@@ -975,7 +975,7 @@
 		nc->avail = 0;
 
 		if (!cpumask_empty(mask)) {
-			spin_unlock_irq(&n->list_lock);
+			raw_spin_unlock_irq(&n->list_lock);
 			goto free_slab;
 		}
 
@@ -989,7 +989,7 @@
 		alien = n->alien;
 		n->alien = NULL;
 
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		kfree(shared);
 		if (alien) {
@@ -1173,7 +1173,7 @@
 	/*
 	 * Do not assume that spinlocks can be initialized via memcpy:
 	 */
-	spin_lock_init(&ptr->list_lock);
+	raw_spin_lock_init(&ptr->list_lock);
 
 	MAKE_ALL_LISTS(cachep, ptr, nodeid);
 	cachep->node[nodeid] = ptr;
@@ -1344,11 +1344,11 @@
 	for_each_kmem_cache_node(cachep, node, n) {
 		unsigned long total_slabs, free_slabs, free_objs;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		total_slabs = n->total_slabs;
 		free_slabs = n->free_slabs;
 		free_objs = n->free_objects;
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		pr_warn("  node %d: slabs: %ld/%ld, objs: %ld/%ld\n",
 			node, total_slabs - free_slabs, total_slabs,
@@ -2106,7 +2106,7 @@
 {
 #ifdef CONFIG_SMP
 	check_irq_off();
-	assert_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
+	assert_raw_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
 #endif
 }
 
@@ -2114,7 +2114,7 @@
 {
 #ifdef CONFIG_SMP
 	check_irq_off();
-	assert_spin_locked(&get_node(cachep, node)->list_lock);
+	assert_raw_spin_locked(&get_node(cachep, node)->list_lock);
 #endif
 }
 
@@ -2154,9 +2154,9 @@
 	check_irq_off();
 	ac = cpu_cache_get(cachep);
 	n = get_node(cachep, node);
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	free_block(cachep, ac->entry, ac->avail, node, &list);
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	ac->avail = 0;
 	slabs_destroy(cachep, &list);
 }
@@ -2174,9 +2174,9 @@
 			drain_alien_cache(cachep, n->alien);
 
 	for_each_kmem_cache_node(cachep, node, n) {
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		drain_array_locked(cachep, n->shared, node, true, &list);
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		slabs_destroy(cachep, &list);
 	}
@@ -2198,10 +2198,10 @@
 	nr_freed = 0;
 	while (nr_freed < tofree && !list_empty(&n->slabs_free)) {
 
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		p = n->slabs_free.prev;
 		if (p == &n->slabs_free) {
-			spin_unlock_irq(&n->list_lock);
+			raw_spin_unlock_irq(&n->list_lock);
 			goto out;
 		}
 
@@ -2214,7 +2214,7 @@
 		 * to the cache.
 		 */
 		n->free_objects -= cache->num;
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 		slab_destroy(cache, page);
 		nr_freed++;
 	}
@@ -2652,7 +2652,7 @@
 	INIT_LIST_HEAD(&page->slab_list);
 	n = get_node(cachep, page_to_nid(page));
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	n->total_slabs++;
 	if (!page->active) {
 		list_add_tail(&page->slab_list, &n->slabs_free);
@@ -2662,7 +2662,7 @@
 
 	STATS_INC_GROWN(cachep);
 	n->free_objects += cachep->num - page->active;
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 
 	fixup_objfreelist_debug(cachep, &list);
 }
@@ -2828,7 +2828,7 @@
 {
 	struct page *page;
 
-	assert_spin_locked(&n->list_lock);
+	assert_raw_spin_locked(&n->list_lock);
 	page = list_first_entry_or_null(&n->slabs_partial, struct page,
 					slab_list);
 	if (!page) {
@@ -2855,10 +2855,10 @@
 	if (!gfp_pfmemalloc_allowed(flags))
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	page = get_first_slab(n, true);
 	if (!page) {
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 		return NULL;
 	}
 
@@ -2867,7 +2867,7 @@
 
 	fixup_slab_list(cachep, n, page, &list);
 
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 
 	return obj;
@@ -2926,7 +2926,7 @@
 	if (!n->free_objects && (!shared || !shared->avail))
 		goto direct_grow;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	shared = READ_ONCE(n->shared);
 
 	/* See if we can refill from the shared array */
@@ -2950,7 +2950,7 @@
 must_grow:
 	n->free_objects -= ac->avail;
 alloc_done:
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 
 direct_grow:
@@ -3175,7 +3175,7 @@
 	BUG_ON(!n);
 
 	check_irq_off();
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	page = get_first_slab(n, false);
 	if (!page)
 		goto must_grow;
@@ -3193,12 +3193,12 @@
 
 	fixup_slab_list(cachep, n, page, &list);
 
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 	return obj;
 
 must_grow:
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	page = cache_grow_begin(cachep, gfp_exact_node(flags), nodeid);
 	if (page) {
 		/* This slab isn't counted yet so don't update free_objects */
@@ -3376,7 +3376,7 @@
 
 	check_irq_off();
 	n = get_node(cachep, node);
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	if (n->shared) {
 		struct array_cache *shared_array = n->shared;
 		int max = shared_array->limit - shared_array->avail;
@@ -3405,7 +3405,7 @@
 		STATS_SET_FREEABLE(cachep, i);
 	}
 #endif
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	ac->avail -= batchcount;
 	memmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);
 	slabs_destroy(cachep, &list);
@@ -3834,9 +3834,9 @@
 
 		node = cpu_to_mem(cpu);
 		n = get_node(cachep, node);
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		free_block(cachep, ac->entry, ac->avail, node, &list);
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 		slabs_destroy(cachep, &list);
 	}
 	free_percpu(prev);
@@ -3931,9 +3931,9 @@
 		return;
 	}
 
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	drain_array_locked(cachep, ac, node, false, &list);
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 
 	slabs_destroy(cachep, &list);
 }
@@ -4017,7 +4017,7 @@
 
 	for_each_kmem_cache_node(cachep, node, n) {
 		check_irq_on();
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 
 		total_slabs += n->total_slabs;
 		free_slabs += n->free_slabs;
@@ -4026,7 +4026,7 @@
 		if (n->shared)
 			shared_avail += n->shared->avail;
 
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 	}
 	num_objs = total_slabs * cachep->num;
 	active_slabs = total_slabs - free_slabs;
diff -Naur a/mm/slab.h b/mm/slab.h
--- a/mm/slab.h	2020-11-23 13:48:35.917964465 +0200
+++ b/mm/slab.h	2021-07-14 15:39:11.890134934 +0300
@@ -530,7 +530,7 @@
  * The slab lists for all objects.
  */
 struct kmem_cache_node {
-	spinlock_t list_lock;
+	raw_spinlock_t list_lock;
 
 #ifdef CONFIG_SLAB
 	struct list_head slabs_partial;	/* partial list first, better asm code */
diff -Naur a/mm/slub.c b/mm/slub.c
--- a/mm/slub.c	2020-11-23 13:48:35.921964546 +0200
+++ b/mm/slub.c	2021-07-14 15:39:11.890134934 +0300
@@ -434,7 +434,7 @@
 
 #ifdef CONFIG_SLUB_DEBUG
 static unsigned long object_map[BITS_TO_LONGS(MAX_OBJS_PER_PAGE)];
-static DEFINE_SPINLOCK(object_map_lock);
+static DEFINE_RAW_SPINLOCK(object_map_lock);
 
 /*
  * Determine a map of object in use on a page.
@@ -450,7 +450,7 @@
 
 	VM_BUG_ON(!irqs_disabled());
 
-	spin_lock(&object_map_lock);
+	raw_spin_lock(&object_map_lock);
 
 	bitmap_zero(object_map, page->objects);
 
@@ -463,7 +463,7 @@
 static void put_map(unsigned long *map) __releases(&object_map_lock)
 {
 	VM_BUG_ON(map != object_map);
-	spin_unlock(&object_map_lock);
+	raw_spin_unlock(&object_map_lock);
 }
 
 static inline unsigned int size_from_object(struct kmem_cache *s)
@@ -1213,7 +1213,7 @@
 	unsigned long flags;
 	int ret = 0;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	slab_lock(page);
 
 	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
@@ -1248,7 +1248,7 @@
 			 bulk_cnt, cnt);
 
 	slab_unlock(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	if (!ret)
 		slab_fix(s, "Object at 0x%p not freed", object);
 	return ret;
@@ -1496,6 +1496,12 @@
 }
 #endif /* CONFIG_SLUB_DEBUG */
 
+struct slub_free_list {
+	raw_spinlock_t		lock;
+	struct list_head	list;
+};
+static DEFINE_PER_CPU(struct slub_free_list, slub_free_list);
+
 /*
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
@@ -1739,10 +1745,18 @@
 	void *start, *p, *next;
 	int idx;
 	bool shuffle;
+	bool enableirqs = false;
 
 	flags &= gfp_allowed_mask;
 
 	if (gfpflags_allow_blocking(flags))
+		enableirqs = true;
+
+#ifdef CONFIG_PREEMPT_RT
+	if (system_state > SYSTEM_BOOTING && system_state < SYSTEM_SUSPEND)
+		enableirqs = true;
+#endif
+	if (enableirqs)
 		local_irq_enable();
 
 	flags |= s->allocflags;
@@ -1801,7 +1815,7 @@
 	page->frozen = 1;
 
 out:
-	if (gfpflags_allow_blocking(flags))
+	if (enableirqs)
 		local_irq_disable();
 	if (!page)
 		return NULL;
@@ -1844,6 +1858,16 @@
 	__free_pages(page, order);
 }
 
+static void free_delayed(struct list_head *h)
+{
+	while (!list_empty(h)) {
+		struct page *page = list_first_entry(h, struct page, lru);
+
+		list_del(&page->lru);
+		__free_slab(page->slab_cache, page);
+	}
+}
+
 static void rcu_free_slab(struct rcu_head *h)
 {
 	struct page *page = container_of(h, struct page, rcu_head);
@@ -1855,6 +1879,12 @@
 {
 	if (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {
 		call_rcu(&page->rcu_head, rcu_free_slab);
+	} else if (irqs_disabled()) {
+		struct slub_free_list *f = this_cpu_ptr(&slub_free_list);
+
+		raw_spin_lock(&f->lock);
+		list_add(&page->lru, &f->list);
+		raw_spin_unlock(&f->lock);
 	} else
 		__free_slab(s, page);
 }
@@ -1962,7 +1992,7 @@
 	if (!n || !n->nr_partial)
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	list_for_each_entry_safe(page, page2, &n->partial, slab_list) {
 		void *t;
 
@@ -1987,7 +2017,7 @@
 			break;
 
 	}
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	return object;
 }
 
@@ -2241,7 +2271,7 @@
 			 * that acquire_slab() will see a slab page that
 			 * is frozen
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	} else {
 		m = M_FULL;
@@ -2252,7 +2282,7 @@
 			 * slabs from diagnostic functions will not see
 			 * any frozen slabs.
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	}
 
@@ -2276,7 +2306,7 @@
 		goto redo;
 
 	if (lock)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	if (m == M_PARTIAL)
 		stat(s, tail);
@@ -2315,10 +2345,10 @@
 		n2 = get_node(s, page_to_nid(page));
 		if (n != n2) {
 			if (n)
-				spin_unlock(&n->list_lock);
+				raw_spin_unlock(&n->list_lock);
 
 			n = n2;
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 
 		do {
@@ -2347,7 +2377,7 @@
 	}
 
 	if (n)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	while (discard_page) {
 		page = discard_page;
@@ -2384,14 +2414,21 @@
 			pobjects = oldpage->pobjects;
 			pages = oldpage->pages;
 			if (drain && pobjects > slub_cpu_partial(s)) {
+				struct slub_free_list *f;
 				unsigned long flags;
+				LIST_HEAD(tofree);
 				/*
 				 * partial array is full. Move the existing
 				 * set to the per node partial list.
 				 */
 				local_irq_save(flags);
 				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+				f = this_cpu_ptr(&slub_free_list);
+				raw_spin_lock(&f->lock);
+				list_splice_init(&f->list, &tofree);
+				raw_spin_unlock(&f->lock);
 				local_irq_restore(flags);
+				free_delayed(&tofree);
 				oldpage = NULL;
 				pobjects = 0;
 				pages = 0;
@@ -2459,7 +2496,19 @@
 
 static void flush_all(struct kmem_cache *s)
 {
+	LIST_HEAD(tofree);
+	int cpu;
+
 	on_each_cpu_cond(has_cpu_slab, flush_cpu_slab, s, 1);
+	for_each_online_cpu(cpu) {
+		struct slub_free_list *f;
+
+		f = &per_cpu(slub_free_list, cpu);
+		raw_spin_lock_irq(&f->lock);
+		list_splice_init(&f->list, &tofree);
+		raw_spin_unlock_irq(&f->lock);
+		free_delayed(&tofree);
+	}
 }
 
 /*
@@ -2514,10 +2563,10 @@
 	unsigned long x = 0;
 	struct page *page;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	list_for_each_entry(page, &n->partial, slab_list)
 		x += get_count(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return x;
 }
 #endif /* CONFIG_SLUB_DEBUG || CONFIG_SYSFS */
@@ -2656,8 +2705,10 @@
  * already disabled (which is the case for bulk allocation).
  */
 static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
-			  unsigned long addr, struct kmem_cache_cpu *c)
+			  unsigned long addr, struct kmem_cache_cpu *c,
+			  struct list_head *to_free)
 {
+	struct slub_free_list *f;
 	void *freelist;
 	struct page *page;
 
@@ -2723,6 +2774,13 @@
 	VM_BUG_ON(!c->page->frozen);
 	c->freelist = get_freepointer(s, freelist);
 	c->tid = next_tid(c->tid);
+
+out:
+	f = this_cpu_ptr(&slub_free_list);
+	raw_spin_lock(&f->lock);
+	list_splice_init(&f->list, to_free);
+	raw_spin_unlock(&f->lock);
+
 	return freelist;
 
 new_slab:
@@ -2738,7 +2796,7 @@
 
 	if (unlikely(!freelist)) {
 		slab_out_of_memory(s, gfpflags, node);
-		return NULL;
+		goto out;
 	}
 
 	page = c->page;
@@ -2751,7 +2809,7 @@
 		goto new_slab;	/* Slab failed checks. Next slab needed */
 
 	deactivate_slab(s, page, get_freepointer(s, freelist), c);
-	return freelist;
+	goto out;
 }
 
 /*
@@ -2763,6 +2821,7 @@
 {
 	void *p;
 	unsigned long flags;
+	LIST_HEAD(tofree);
 
 	local_irq_save(flags);
 #ifdef CONFIG_PREEMPTION
@@ -2774,8 +2833,9 @@
 	c = this_cpu_ptr(s->cpu_slab);
 #endif
 
-	p = ___slab_alloc(s, gfpflags, node, addr, c);
+	p = ___slab_alloc(s, gfpflags, node, addr, c, &tofree);
 	local_irq_restore(flags);
+	free_delayed(&tofree);
 	return p;
 }
 
@@ -2809,6 +2869,10 @@
 	unsigned long tid;
 	struct obj_cgroup *objcg = NULL;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && IS_ENABLED(CONFIG_DEBUG_ATOMIC_SLEEP))
+		WARN_ON_ONCE(!preemptible() &&
+			     (system_state > SYSTEM_BOOTING && system_state < SYSTEM_SUSPEND));
+
 	s = slab_pre_alloc_hook(s, &objcg, 1, gfpflags);
 	if (!s)
 		return NULL;
@@ -2975,7 +3039,7 @@
 
 	do {
 		if (unlikely(n)) {
-			spin_unlock_irqrestore(&n->list_lock, flags);
+			raw_spin_unlock_irqrestore(&n->list_lock, flags);
 			n = NULL;
 		}
 		prior = page->freelist;
@@ -3007,7 +3071,7 @@
 				 * Otherwise the list_lock will synchronize with
 				 * other processors updating the list of slabs.
 				 */
-				spin_lock_irqsave(&n->list_lock, flags);
+				raw_spin_lock_irqsave(&n->list_lock, flags);
 
 			}
 		}
@@ -3048,7 +3112,7 @@
 		add_partial(n, page, DEACTIVATE_TO_TAIL);
 		stat(s, FREE_ADD_PARTIAL);
 	}
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return;
 
 slab_empty:
@@ -3063,7 +3127,7 @@
 		remove_full(s, n, page);
 	}
 
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	stat(s, FREE_SLAB);
 	discard_slab(s, page);
 }
@@ -3270,9 +3334,14 @@
 			  void **p)
 {
 	struct kmem_cache_cpu *c;
+	LIST_HEAD(to_free);
 	int i;
 	struct obj_cgroup *objcg = NULL;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && IS_ENABLED(CONFIG_DEBUG_ATOMIC_SLEEP))
+		WARN_ON_ONCE(!preemptible() &&
+			     (system_state > SYSTEM_BOOTING && system_state < SYSTEM_SUSPEND));
+
 	/* memcg and kmem_cache debug support */
 	s = slab_pre_alloc_hook(s, &objcg, size, flags);
 	if (unlikely(!s))
@@ -3303,7 +3372,7 @@
 			 * of re-populating per CPU c->freelist
 			 */
 			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
-					    _RET_IP_, c);
+					    _RET_IP_, c, &to_free);
 			if (unlikely(!p[i]))
 				goto error;
 
@@ -3318,6 +3387,7 @@
 	}
 	c->tid = next_tid(c->tid);
 	local_irq_enable();
+	free_delayed(&to_free);
 
 	/* Clear memory outside IRQ disabled fastpath loop */
 	if (unlikely(slab_want_init_on_alloc(flags, s))) {
@@ -3332,6 +3402,7 @@
 	return i;
 error:
 	local_irq_enable();
+	free_delayed(&to_free);
 	slab_post_alloc_hook(s, objcg, flags, i, p);
 	__kmem_cache_free_bulk(s, i, p);
 	return 0;
@@ -3467,7 +3538,7 @@
 init_kmem_cache_node(struct kmem_cache_node *n)
 {
 	n->nr_partial = 0;
-	spin_lock_init(&n->list_lock);
+	raw_spin_lock_init(&n->list_lock);
 	INIT_LIST_HEAD(&n->partial);
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_set(&n->nr_slabs, 0);
@@ -3868,7 +3939,7 @@
 	struct page *page, *h;
 
 	BUG_ON(irqs_disabled());
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	list_for_each_entry_safe(page, h, &n->partial, slab_list) {
 		if (!page->inuse) {
 			remove_partial(n, page);
@@ -3878,7 +3949,7 @@
 			  "Objects remaining in %s on __kmem_cache_shutdown()");
 		}
 	}
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 
 	list_for_each_entry_safe(page, h, &discard, slab_list)
 		discard_slab(s, page);
@@ -4149,7 +4220,7 @@
 		for (i = 0; i < SHRINK_PROMOTE_MAX; i++)
 			INIT_LIST_HEAD(promote + i);
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 
 		/*
 		 * Build lists of slabs to discard or promote.
@@ -4180,7 +4251,7 @@
 		for (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)
 			list_splice(promote + i, &n->partial);
 
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		/* Release empty slabs */
 		list_for_each_entry_safe(page, t, &discard, slab_list)
@@ -4355,6 +4426,12 @@
 {
 	static __initdata struct kmem_cache boot_kmem_cache,
 		boot_kmem_cache_node;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		raw_spin_lock_init(&per_cpu(slub_free_list, cpu).lock);
+		INIT_LIST_HEAD(&per_cpu(slub_free_list, cpu).list);
+	}
 
 	if (debug_guardpage_minorder())
 		slub_max_order = 0;
@@ -4542,7 +4619,7 @@
 	struct page *page;
 	unsigned long flags;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 
 	list_for_each_entry(page, &n->partial, slab_list) {
 		validate_slab(s, page);
@@ -4564,7 +4641,7 @@
 		       s->name, count, atomic_long_read(&n->nr_slabs));
 
 out:
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return count;
 }
 
@@ -4743,12 +4820,12 @@
 		if (!atomic_long_read(&n->nr_slabs))
 			continue;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		list_for_each_entry(page, &n->partial, slab_list)
 			process_slab(&t, s, page, alloc);
 		list_for_each_entry(page, &n->full, slab_list)
 			process_slab(&t, s, page, alloc);
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	}
 
 	for (i = 0; i < t.count; i++) {
diff -Naur a/mm/swap.c b/mm/swap.c
--- a/mm/swap.c	2020-11-23 13:48:35.925964626 +0200
+++ b/mm/swap.c	2021-07-14 15:39:11.890134934 +0300
@@ -763,10 +763,20 @@
  */
 void lru_add_drain_all(void)
 {
-	static seqcount_t seqcount = SEQCNT_ZERO(seqcount);
-	static DEFINE_MUTEX(lock);
+	/*
+	 * lru_drain_gen - Global pages generation number
+	 *
+	 * (A) Definition: global lru_drain_gen = x implies that all generations
+	 *     0 < n <= x are already *scheduled* for draining.
+	 *
+	 * This is an optimization for the highly-contended use case where a
+	 * user space workload keeps constantly generating a flow of pages for
+	 * each CPU.
+	 */
+	static unsigned int lru_drain_gen;
 	static struct cpumask has_work;
-	int cpu, seq;
+	static DEFINE_MUTEX(lock);
+	unsigned cpu, this_gen;
 
 	/*
 	 * Make sure nobody triggers this path before mm_percpu_wq is fully
@@ -775,21 +785,54 @@
 	if (WARN_ON(!mm_percpu_wq))
 		return;
 
-	seq = raw_read_seqcount_latch(&seqcount);
+	/*
+	 * Guarantee pagevec counter stores visible by this CPU are visible to
+	 * other CPUs before loading the current drain generation.
+	 */
+	smp_mb();
+
+	/*
+	 * (B) Locally cache global LRU draining generation number
+	 *
+	 * The read barrier ensures that the counter is loaded before the mutex
+	 * is taken. It pairs with smp_mb() inside the mutex critical section
+	 * at (D).
+	 */
+	this_gen = smp_load_acquire(&lru_drain_gen);
 
 	mutex_lock(&lock);
 
 	/*
-	 * Piggyback on drain started and finished while we waited for lock:
-	 * all pages pended at the time of our enter were drained from vectors.
+	 * (C) Exit the draining operation if a newer generation, from another
+	 * lru_add_drain_all(), was already scheduled for draining. Check (A).
 	 */
-	if (__read_seqcount_retry(&seqcount, seq))
+	if (unlikely(this_gen != lru_drain_gen))
 		goto done;
 
-	raw_write_seqcount_latch(&seqcount);
+	/*
+	 * (D) Increment global generation number
+	 *
+	 * Pairs with smp_load_acquire() at (B), outside of the critical
+	 * section. Use a full memory barrier to guarantee that the new global
+	 * drain generation number is stored before loading pagevec counters.
+	 *
+	 * This pairing must be done here, before the for_each_online_cpu loop
+	 * below which drains the page vectors.
+	 *
+	 * Let x, y, and z represent some system CPU numbers, where x < y < z.
+	 * Assume CPU #z is is in the middle of the for_each_online_cpu loop
+	 * below and has already reached CPU #y's per-cpu data. CPU #x comes
+	 * along, adds some pages to its per-cpu vectors, then calls
+	 * lru_add_drain_all().
+	 *
+	 * If the paired barrier is done at any later step, e.g. after the
+	 * loop, CPU #x will just exit at (C) and miss flushing out all of its
+	 * added pages.
+	 */
+	WRITE_ONCE(lru_drain_gen, lru_drain_gen + 1);
+	smp_mb();
 
 	cpumask_clear(&has_work);
-
 	for_each_online_cpu(cpu) {
 		struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
 
@@ -801,7 +844,7 @@
 		    need_activate_page_drain(cpu)) {
 			INIT_WORK(work, lru_add_drain_per_cpu);
 			queue_work_on(cpu, mm_percpu_wq, work);
-			cpumask_set_cpu(cpu, &has_work);
+			__cpumask_set_cpu(cpu, &has_work);
 		}
 	}
 
@@ -816,7 +859,7 @@
 {
 	lru_add_drain();
 }
-#endif
+#endif /* CONFIG_SMP */
 
 /**
  * release_pages - batched put_page()
diff -Naur a/mm/vmalloc.c b/mm/vmalloc.c
--- a/mm/vmalloc.c	2020-11-23 13:48:35.937964867 +0200
+++ b/mm/vmalloc.c	2021-07-14 15:39:11.890134934 +0300
@@ -1544,7 +1544,7 @@
 	struct vmap_block *vb;
 	struct vmap_area *va;
 	unsigned long vb_idx;
-	int node, err;
+	int node, err, cpu;
 	void *vaddr;
 
 	node = numa_node_id();
@@ -1581,11 +1581,12 @@
 		return ERR_PTR(err);
 	}
 
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = this_cpu_ptr(&vmap_block_queue);
 	spin_lock(&vbq->lock);
 	list_add_tail_rcu(&vb->free_list, &vbq->free);
 	spin_unlock(&vbq->lock);
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 
 	return vaddr;
 }
@@ -1650,6 +1651,7 @@
 	struct vmap_block *vb;
 	void *vaddr = NULL;
 	unsigned int order;
+	int cpu;
 
 	BUG_ON(offset_in_page(size));
 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
@@ -1664,7 +1666,8 @@
 	order = get_order(size);
 
 	rcu_read_lock();
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = this_cpu_ptr(&vmap_block_queue);
 	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
 		unsigned long pages_off;
 
@@ -1687,7 +1690,7 @@
 		break;
 	}
 
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 	rcu_read_unlock();
 
 	/* Allocate new block if nothing was found */
diff -Naur a/mm/vmstat.c b/mm/vmstat.c
--- a/mm/vmstat.c	2020-11-23 13:48:35.945965027 +0200
+++ b/mm/vmstat.c	2021-07-14 15:39:11.894134906 +0300
@@ -321,6 +321,7 @@
 	long x;
 	long t;
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -330,6 +331,7 @@
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_zone_page_state);
 
@@ -346,6 +348,7 @@
 		delta >>= PAGE_SHIFT;
 	}
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -355,6 +358,7 @@
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_node_page_state);
 
@@ -387,6 +391,7 @@
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -395,6 +400,7 @@
 		zone_page_state_add(v + overstep, zone, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
@@ -405,6 +411,7 @@
 
 	VM_WARN_ON_ONCE(vmstat_item_in_bytes(item));
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -413,6 +420,7 @@
 		node_page_state_add(v + overstep, pgdat, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
@@ -433,6 +441,7 @@
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -441,6 +450,7 @@
 		zone_page_state_add(v - overstep, zone, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
@@ -451,6 +461,7 @@
 
 	VM_WARN_ON_ONCE(vmstat_item_in_bytes(item));
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -459,6 +470,7 @@
 		node_page_state_add(v - overstep, pgdat, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
diff -Naur a/mm/workingset.c b/mm/workingset.c
--- a/mm/workingset.c	2020-11-23 13:48:35.945965027 +0200
+++ b/mm/workingset.c	2021-07-14 15:39:11.894134906 +0300
@@ -432,6 +432,8 @@
 
 void workingset_update_node(struct xa_node *node)
 {
+	struct address_space *mapping;
+
 	/*
 	 * Track non-empty nodes that contain only shadow entries;
 	 * unlink those that contain pages or are being freed.
@@ -440,7 +442,8 @@
 	 * already where they should be. The list_empty() test is safe
 	 * as node->private_list is protected by the i_pages lock.
 	 */
-	VM_WARN_ON_ONCE(!irqs_disabled());  /* For __inc_lruvec_page_state */
+	mapping = container_of(node->array, struct address_space, i_pages);
+	lockdep_assert_held(&mapping->i_pages.xa_lock);
 
 	if (node->count && node->count == node->nr_values) {
 		if (list_empty(&node->private_list)) {
diff -Naur a/mm/zsmalloc.c b/mm/zsmalloc.c
--- a/mm/zsmalloc.c	2020-11-23 13:48:35.953965188 +0200
+++ b/mm/zsmalloc.c	2021-07-14 15:39:11.894134906 +0300
@@ -57,6 +57,7 @@
 #include <linux/wait.h>
 #include <linux/pagemap.h>
 #include <linux/fs.h>
+#include <linux/local_lock.h>
 
 #define ZSPAGE_MAGIC	0x58
 
@@ -77,6 +78,20 @@
 
 #define ZS_HANDLE_SIZE (sizeof(unsigned long))
 
+#ifdef CONFIG_PREEMPT_RT
+
+struct zsmalloc_handle {
+	unsigned long addr;
+	struct mutex lock;
+};
+
+#define ZS_HANDLE_ALLOC_SIZE (sizeof(struct zsmalloc_handle))
+
+#else
+
+#define ZS_HANDLE_ALLOC_SIZE (sizeof(unsigned long))
+#endif
+
 /*
  * Object location (<PFN>, <obj_idx>) is encoded as
  * a single (unsigned long) handle value.
@@ -293,6 +308,7 @@
 };
 
 struct mapping_area {
+	local_lock_t	lock;
 #ifdef CONFIG_ZSMALLOC_PGTABLE_MAPPING
 	struct vm_struct *vm; /* vm area for mapping object that span pages */
 #else
@@ -326,7 +342,7 @@
 
 static int create_cache(struct zs_pool *pool)
 {
-	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_SIZE,
+	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_ALLOC_SIZE,
 					0, 0, NULL);
 	if (!pool->handle_cachep)
 		return 1;
@@ -350,9 +366,26 @@
 
 static unsigned long cache_alloc_handle(struct zs_pool *pool, gfp_t gfp)
 {
-	return (unsigned long)kmem_cache_alloc(pool->handle_cachep,
-			gfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));
+	void *p;
+
+	p = kmem_cache_alloc(pool->handle_cachep,
+			     gfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));
+#ifdef CONFIG_PREEMPT_RT
+	if (p) {
+		struct zsmalloc_handle *zh = p;
+
+		mutex_init(&zh->lock);
+	}
+#endif
+	return (unsigned long)p;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+static struct zsmalloc_handle *zs_get_pure_handle(unsigned long handle)
+{
+	return (void *)(handle &~((1 << OBJ_TAG_BITS) - 1));
 }
+#endif
 
 static void cache_free_handle(struct zs_pool *pool, unsigned long handle)
 {
@@ -372,12 +405,18 @@
 
 static void record_obj(unsigned long handle, unsigned long obj)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	WRITE_ONCE(zh->addr, obj);
+#else
 	/*
 	 * lsb of @obj represents handle lock while other bits
 	 * represent object value the handle is pointing so
 	 * updating shouldn't do store tearing.
 	 */
 	WRITE_ONCE(*(unsigned long *)handle, obj);
+#endif
 }
 
 /* zpool driver */
@@ -459,7 +498,10 @@
 #endif /* CONFIG_ZPOOL */
 
 /* per-cpu VM mapping areas for zspage accesses that cross page boundaries */
-static DEFINE_PER_CPU(struct mapping_area, zs_map_area);
+static DEFINE_PER_CPU(struct mapping_area, zs_map_area) = {
+	/* XXX remove this and use a spin_lock_t in pin_tag() */
+	.lock	= INIT_LOCAL_LOCK(lock),
+};
 
 static bool is_zspage_isolated(struct zspage *zspage)
 {
@@ -869,7 +911,13 @@
 
 static unsigned long handle_to_obj(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return zh->addr;
+#else
 	return *(unsigned long *)handle;
+#endif
 }
 
 static unsigned long obj_to_head(struct page *page, void *obj)
@@ -883,22 +931,46 @@
 
 static inline int testpin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_is_locked(&zh->lock);
+#else
 	return bit_spin_is_locked(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static inline int trypin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_trylock(&zh->lock);
+#else
 	return bit_spin_trylock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void pin_tag(unsigned long handle) __acquires(bitlock)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_lock(&zh->lock);
+#else
 	bit_spin_lock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void unpin_tag(unsigned long handle) __releases(bitlock)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_unlock(&zh->lock);
+#else
 	bit_spin_unlock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void reset_page(struct page *page)
@@ -1326,7 +1398,8 @@
 	class = pool->size_class[class_idx];
 	off = (class->size * obj_idx) & ~PAGE_MASK;
 
-	area = &get_cpu_var(zs_map_area);
+	local_lock(&zs_map_area.lock);
+	area = this_cpu_ptr(&zs_map_area);
 	area->vm_mm = mm;
 	if (off + class->size <= PAGE_SIZE) {
 		/* this object is contained entirely within a page */
@@ -1380,7 +1453,7 @@
 
 		__zs_unmap_object(area, pages, off, class->size);
 	}
-	put_cpu_var(zs_map_area);
+	local_unlock(&zs_map_area.lock);
 
 	migrate_read_unlock(zspage);
 	unpin_tag(handle);
diff -Naur a/mm/zswap.c b/mm/zswap.c
--- a/mm/zswap.c	2020-11-23 13:48:35.953965188 +0200
+++ b/mm/zswap.c	2021-07-14 15:39:11.894134906 +0300
@@ -18,6 +18,7 @@
 #include <linux/highmem.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
+#include <linux/local_lock.h>
 #include <linux/types.h>
 #include <linux/atomic.h>
 #include <linux/frontswap.h>
@@ -387,27 +388,35 @@
 /*********************************
 * per-cpu code
 **********************************/
-static DEFINE_PER_CPU(u8 *, zswap_dstmem);
+struct zswap_comp {
+	/* Used for per-CPU dstmem and tfm */
+	local_lock_t lock;
+	u8 *dstmem;
+};
+
+static DEFINE_PER_CPU(struct zswap_comp, zswap_comp);
 
 static int zswap_dstmem_prepare(unsigned int cpu)
 {
+	struct zswap_comp *zcomp;
 	u8 *dst;
 
 	dst = kmalloc_node(PAGE_SIZE * 2, GFP_KERNEL, cpu_to_node(cpu));
 	if (!dst)
 		return -ENOMEM;
 
-	per_cpu(zswap_dstmem, cpu) = dst;
+	zcomp = per_cpu_ptr(&zswap_comp, cpu);
+	zcomp->dstmem = dst;
 	return 0;
 }
 
 static int zswap_dstmem_dead(unsigned int cpu)
 {
-	u8 *dst;
+	struct zswap_comp *zcomp;
 
-	dst = per_cpu(zswap_dstmem, cpu);
-	kfree(dst);
-	per_cpu(zswap_dstmem, cpu) = NULL;
+	zcomp = per_cpu_ptr(&zswap_comp, cpu);
+	kfree(zcomp->dstmem);
+	zcomp->dstmem = NULL;
 
 	return 0;
 }
@@ -919,10 +928,11 @@
 		dlen = PAGE_SIZE;
 		src = (u8 *)zhdr + sizeof(struct zswap_header);
 		dst = kmap_atomic(page);
-		tfm = *get_cpu_ptr(entry->pool->tfm);
+		local_lock(&zswap_comp.lock);
+		tfm = *this_cpu_ptr(entry->pool->tfm);
 		ret = crypto_comp_decompress(tfm, src, entry->length,
 					     dst, &dlen);
-		put_cpu_ptr(entry->pool->tfm);
+		local_unlock(&zswap_comp.lock);
 		kunmap_atomic(dst);
 		BUG_ON(ret);
 		BUG_ON(dlen != PAGE_SIZE);
@@ -1074,12 +1084,12 @@
 	}
 
 	/* compress */
-	dst = get_cpu_var(zswap_dstmem);
-	tfm = *get_cpu_ptr(entry->pool->tfm);
+	local_lock(&zswap_comp.lock);
+	dst = *this_cpu_ptr(&zswap_comp.dstmem);
+	tfm = *this_cpu_ptr(entry->pool->tfm);
 	src = kmap_atomic(page);
 	ret = crypto_comp_compress(tfm, src, PAGE_SIZE, dst, &dlen);
 	kunmap_atomic(src);
-	put_cpu_ptr(entry->pool->tfm);
 	if (ret) {
 		ret = -EINVAL;
 		goto put_dstmem;
@@ -1103,7 +1113,7 @@
 	memcpy(buf, &zhdr, hlen);
 	memcpy(buf + hlen, dst, dlen);
 	zpool_unmap_handle(entry->pool->zpool, handle);
-	put_cpu_var(zswap_dstmem);
+	local_unlock(&zswap_comp.lock);
 
 	/* populate entry */
 	entry->offset = offset;
@@ -1131,7 +1141,7 @@
 	return 0;
 
 put_dstmem:
-	put_cpu_var(zswap_dstmem);
+	local_unlock(&zswap_comp.lock);
 	zswap_pool_put(entry->pool);
 freepage:
 	zswap_entry_cache_free(entry);
@@ -1176,9 +1186,10 @@
 	if (zpool_evictable(entry->pool->zpool))
 		src += sizeof(struct zswap_header);
 	dst = kmap_atomic(page);
-	tfm = *get_cpu_ptr(entry->pool->tfm);
+	local_lock(&zswap_comp.lock);
+	tfm = *this_cpu_ptr(entry->pool->tfm);
 	ret = crypto_comp_decompress(tfm, src, entry->length, dst, &dlen);
-	put_cpu_ptr(entry->pool->tfm);
+	local_unlock(&zswap_comp.lock);
 	kunmap_atomic(dst);
 	zpool_unmap_handle(entry->pool->zpool, entry->handle);
 	BUG_ON(ret);
diff -Naur a/net/core/dev.c b/net/core/dev.c
--- a/net/core/dev.c	2020-11-23 13:48:36.281971772 +0200
+++ b/net/core/dev.c	2021-07-14 15:39:12.106133416 +0300
@@ -219,14 +219,14 @@
 static inline void rps_lock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_lock(&sd->input_pkt_queue.lock);
+	raw_spin_lock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
 static inline void rps_unlock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_unlock(&sd->input_pkt_queue.lock);
+	raw_spin_unlock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
@@ -3034,6 +3034,7 @@
 	sd->output_queue_tailp = &q->next_sched;
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 void __netif_schedule(struct Qdisc *q)
@@ -3096,6 +3097,7 @@
 	__this_cpu_write(softnet_data.completion_queue, skb);
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__dev_kfree_skb_irq);
 
@@ -3762,7 +3764,11 @@
 	 * This permits qdisc->running owner to get the lock more
 	 * often and dequeue packets faster.
 	 */
+#ifdef CONFIG_PREEMPT_RT
+	contended = true;
+#else
 	contended = qdisc_is_running(q);
+#endif
 	if (unlikely(contended))
 		spin_lock(&q->busylock);
 
@@ -4558,6 +4564,7 @@
 	rps_unlock(sd);
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 
 	atomic_long_inc(&skb->dev->rx_dropped);
 	kfree_skb(skb);
@@ -4773,7 +4780,7 @@
 		struct rps_dev_flow voidflow, *rflow = &voidflow;
 		int cpu;
 
-		preempt_disable();
+		migrate_disable();
 		rcu_read_lock();
 
 		cpu = get_rps_cpu(skb->dev, skb, &rflow);
@@ -4783,14 +4790,14 @@
 		ret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
 
 		rcu_read_unlock();
-		preempt_enable();
+		migrate_enable();
 	} else
 #endif
 	{
 		unsigned int qtail;
 
-		ret = enqueue_to_backlog(skb, get_cpu(), &qtail);
-		put_cpu();
+		ret = enqueue_to_backlog(skb, get_cpu_light(), &qtail);
+		put_cpu_light();
 	}
 	return ret;
 }
@@ -4829,11 +4836,9 @@
 
 	trace_netif_rx_ni_entry(skb);
 
-	preempt_disable();
+	local_bh_disable();
 	err = netif_rx_internal(skb);
-	if (local_softirq_pending())
-		do_softirq();
-	preempt_enable();
+	local_bh_enable();
 	trace_netif_rx_ni_exit(err);
 
 	return err;
@@ -6202,12 +6207,14 @@
 		sd->rps_ipi_list = NULL;
 
 		local_irq_enable();
+		preempt_check_resched_rt();
 
 		/* Send pending IPI's to kick RPS processing on remote cpus. */
 		net_rps_send_ipi(remsd);
 	} else
 #endif
 		local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 static bool sd_has_rps_ipi_waiting(struct softnet_data *sd)
@@ -6285,6 +6292,7 @@
 	local_irq_save(flags);
 	____napi_schedule(this_cpu_ptr(&softnet_data), n);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__napi_schedule);
 
@@ -10711,6 +10719,7 @@
 
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 #ifdef CONFIG_RPS
 	remsd = oldsd->rps_ipi_list;
@@ -10724,7 +10733,7 @@
 		netif_rx_ni(skb);
 		input_queue_head_incr(oldsd);
 	}
-	while ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {
+	while ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {
 		netif_rx_ni(skb);
 		input_queue_head_incr(oldsd);
 	}
@@ -11040,7 +11049,7 @@
 
 		INIT_WORK(flush, flush_backlog);
 
-		skb_queue_head_init(&sd->input_pkt_queue);
+		skb_queue_head_init_raw(&sd->input_pkt_queue);
 		skb_queue_head_init(&sd->process_queue);
 #ifdef CONFIG_XFRM_OFFLOAD
 		skb_queue_head_init(&sd->xfrm_backlog);
diff -Naur a/net/core/gen_estimator.c b/net/core/gen_estimator.c
--- a/net/core/gen_estimator.c	2020-11-23 13:48:36.305972254 +0200
+++ b/net/core/gen_estimator.c	2021-07-14 15:39:12.106133416 +0300
@@ -42,7 +42,7 @@
 struct net_rate_estimator {
 	struct gnet_stats_basic_packed	*bstats;
 	spinlock_t		*stats_lock;
-	seqcount_t		*running;
+	net_seqlock_t		*running;
 	struct gnet_stats_basic_cpu __percpu *cpu_bstats;
 	u8			ewma_log;
 	u8			intvl_log; /* period : (250ms << intvl_log) */
@@ -125,7 +125,7 @@
 		      struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 		      struct net_rate_estimator __rcu **rate_est,
 		      spinlock_t *lock,
-		      seqcount_t *running,
+		      net_seqlock_t *running,
 		      struct nlattr *opt)
 {
 	struct gnet_estimator *parm = nla_data(opt);
@@ -223,7 +223,7 @@
 			  struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 			  struct net_rate_estimator __rcu **rate_est,
 			  spinlock_t *lock,
-			  seqcount_t *running, struct nlattr *opt)
+			  net_seqlock_t *running, struct nlattr *opt)
 {
 	return gen_new_estimator(bstats, cpu_bstats, rate_est,
 				 lock, running, opt);
diff -Naur a/net/core/gen_stats.c b/net/core/gen_stats.c
--- a/net/core/gen_stats.c	2020-11-23 13:48:36.305972254 +0200
+++ b/net/core/gen_stats.c	2021-07-14 15:39:12.106133416 +0300
@@ -137,7 +137,7 @@
 }
 
 void
-__gnet_stats_copy_basic(const seqcount_t *running,
+__gnet_stats_copy_basic(net_seqlock_t *running,
 			struct gnet_stats_basic_packed *bstats,
 			struct gnet_stats_basic_cpu __percpu *cpu,
 			struct gnet_stats_basic_packed *b)
@@ -150,15 +150,15 @@
 	}
 	do {
 		if (running)
-			seq = read_seqcount_begin(running);
+			seq = net_seq_begin(running);
 		bstats->bytes = b->bytes;
 		bstats->packets = b->packets;
-	} while (running && read_seqcount_retry(running, seq));
+	} while (running && net_seq_retry(running, seq));
 }
 EXPORT_SYMBOL(__gnet_stats_copy_basic);
 
 static int
-___gnet_stats_copy_basic(const seqcount_t *running,
+___gnet_stats_copy_basic(net_seqlock_t *running,
 			 struct gnet_dump *d,
 			 struct gnet_stats_basic_cpu __percpu *cpu,
 			 struct gnet_stats_basic_packed *b,
@@ -204,7 +204,7 @@
  * if the room in the socket buffer was not sufficient.
  */
 int
-gnet_stats_copy_basic(const seqcount_t *running,
+gnet_stats_copy_basic(net_seqlock_t *running,
 		      struct gnet_dump *d,
 		      struct gnet_stats_basic_cpu __percpu *cpu,
 		      struct gnet_stats_basic_packed *b)
@@ -228,7 +228,7 @@
  * if the room in the socket buffer was not sufficient.
  */
 int
-gnet_stats_copy_basic_hw(const seqcount_t *running,
+gnet_stats_copy_basic_hw(net_seqlock_t *running,
 			 struct gnet_dump *d,
 			 struct gnet_stats_basic_cpu __percpu *cpu,
 			 struct gnet_stats_basic_packed *b)
diff -Naur a/net/core/sock.c b/net/core/sock.c
--- a/net/core/sock.c	2020-11-23 13:48:36.337972897 +0200
+++ b/net/core/sock.c	2021-07-14 15:39:12.106133416 +0300
@@ -3049,12 +3049,11 @@
 	if (sk->sk_lock.owned)
 		__lock_sock(sk);
 	sk->sk_lock.owned = 1;
-	spin_unlock(&sk->sk_lock.slock);
+	spin_unlock_bh(&sk->sk_lock.slock);
 	/*
 	 * The sk_lock has mutex_lock() semantics here:
 	 */
 	mutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);
-	local_bh_enable();
 }
 EXPORT_SYMBOL(lock_sock_nested);
 
@@ -3103,12 +3102,11 @@
 
 	__lock_sock(sk);
 	sk->sk_lock.owned = 1;
-	spin_unlock(&sk->sk_lock.slock);
+	spin_unlock_bh(&sk->sk_lock.slock);
 	/*
 	 * The sk_lock has mutex_lock() semantics here:
 	 */
 	mutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);
-	local_bh_enable();
 	return true;
 }
 EXPORT_SYMBOL(lock_sock_fast);
diff -Naur a/net/ipv4/inet_hashtables.c b/net/ipv4/inet_hashtables.c
--- a/net/ipv4/inet_hashtables.c	2020-11-23 13:48:36.449975145 +0200
+++ b/net/ipv4/inet_hashtables.c	2021-07-14 15:39:12.306132013 +0300
@@ -585,7 +585,9 @@
 	int err = 0;
 
 	if (sk->sk_state != TCP_LISTEN) {
+		local_bh_disable();
 		inet_ehash_nolisten(sk, osk);
+		local_bh_enable();
 		return 0;
 	}
 	WARN_ON(!sk_unhashed(sk));
@@ -617,11 +619,8 @@
 {
 	int err = 0;
 
-	if (sk->sk_state != TCP_CLOSE) {
-		local_bh_disable();
+	if (sk->sk_state != TCP_CLOSE)
 		err = __inet_hash(sk, NULL);
-		local_bh_enable();
-	}
 
 	return err;
 }
@@ -632,17 +631,20 @@
 	struct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;
 	struct inet_listen_hashbucket *ilb = NULL;
 	spinlock_t *lock;
+	bool state_listen;
 
 	if (sk_unhashed(sk))
 		return;
 
 	if (sk->sk_state == TCP_LISTEN) {
+		state_listen = true;
 		ilb = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)];
-		lock = &ilb->lock;
+		spin_lock(&ilb->lock);
 	} else {
+		state_listen = false;
 		lock = inet_ehash_lockp(hashinfo, sk->sk_hash);
+		spin_lock_bh(lock);
 	}
-	spin_lock_bh(lock);
 	if (sk_unhashed(sk))
 		goto unlock;
 
@@ -655,7 +657,10 @@
 	__sk_nulls_del_node_init_rcu(sk);
 	sock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);
 unlock:
-	spin_unlock_bh(lock);
+	if (state_listen)
+		spin_unlock(&ilb->lock);
+	else
+		spin_unlock_bh(lock);
 }
 EXPORT_SYMBOL_GPL(inet_unhash);
 
diff -Naur a/net/ipv6/inet6_hashtables.c b/net/ipv6/inet6_hashtables.c
--- a/net/ipv6/inet6_hashtables.c	2020-11-23 13:48:36.549977153 +0200
+++ b/net/ipv6/inet6_hashtables.c	2021-07-14 15:39:12.550130300 +0300
@@ -335,11 +335,8 @@
 {
 	int err = 0;
 
-	if (sk->sk_state != TCP_CLOSE) {
-		local_bh_disable();
+	if (sk->sk_state != TCP_CLOSE)
 		err = __inet_hash(sk, NULL);
-		local_bh_enable();
-	}
 
 	return err;
 }
diff -Naur a/net/Kconfig b/net/Kconfig
--- a/net/Kconfig	2020-11-23 13:48:35.985965830 +0200
+++ b/net/Kconfig	2021-07-14 15:39:11.918134737 +0300
@@ -26,6 +26,8 @@
 
 if NET
 
+source "net/rtnet/Kconfig"
+
 config WANT_COMPAT_NETLINK_MESSAGES
 	bool
 	help
@@ -282,7 +284,7 @@
 
 config NET_RX_BUSY_POLL
 	bool
-	default y
+	default y if !PREEMPT_RT
 
 config BQL
 	bool
diff -Naur a/net/Makefile b/net/Makefile
--- a/net/Makefile	2020-11-23 13:48:35.985965830 +0200
+++ b/net/Makefile	2021-07-14 15:39:11.922134709 +0300
@@ -6,11 +6,15 @@
 # Rewritten to use lists instead of if-statements.
 #
 
+ccflags-y += -Inet/rtnet/stack/include
+
 obj-$(CONFIG_NET)		:= devres.o socket.o core/
 
 tmp-$(CONFIG_COMPAT) 		:= compat.o
 obj-$(CONFIG_NET)		+= $(tmp-y)
 
+obj-$(CONFIG_RTNET)  += rtnet/
+
 # LLC has to be linked before the files in net/802/
 obj-$(CONFIG_LLC)		+= llc/
 obj-$(CONFIG_NET)		+= ethernet/ 802/ sched/ netlink/ bpf/ ethtool/
diff -Naur a/net/rtnet/addons/cap.c b/net/rtnet/addons/cap.c
--- a/net/rtnet/addons/cap.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/addons/cap.c	2021-07-14 15:39:13.438124072 +0300
@@ -0,0 +1,518 @@
+/***
+ *
+ *  rtcap/rtcap.c
+ *
+ *  Real-Time Capturing Interface
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/sched.h>
+
+#include <rtdev.h>
+#include <rtnet_chrdev.h>
+#include <rtnet_port.h> /* for netdev_priv() */
+
+MODULE_LICENSE("GPL");
+
+static unsigned int rtcap_rtskbs = 128;
+module_param(rtcap_rtskbs, uint, 0444);
+MODULE_PARM_DESC(rtcap_rtskbs, "Number of real-time socket buffers per "
+		 "real-time device");
+
+#define TAP_DEV             1
+#define RTMAC_TAP_DEV       2
+#define XMIT_HOOK           4
+
+static rtdm_nrtsig_t        cap_signal;
+static struct rtskb_queue   cap_queue;
+static struct rtskb_pool   cap_pool;
+
+static struct tap_device_t {
+    struct net_device       *tap_dev;
+    struct net_device       *rtmac_tap_dev;
+    struct net_device_stats tap_dev_stats;
+    int                     present;
+    int                     (*orig_xmit)(struct rtskb *skb,
+					 struct rtnet_device *dev);
+} tap_device[MAX_RT_DEVICES];
+
+
+
+void rtcap_rx_hook(struct rtskb *rtskb)
+{
+    if ((rtskb->cap_comp_skb = rtskb_pool_dequeue(&cap_pool)) == 0) {
+	tap_device[rtskb->rtdev->ifindex].tap_dev_stats.rx_dropped++;
+	return;
+    }
+
+    if (cap_queue.first == NULL)
+	cap_queue.first = rtskb;
+    else
+	cap_queue.last->cap_next = rtskb;
+    cap_queue.last  = rtskb;
+    rtskb->cap_next = NULL;
+
+    rtskb->cap_flags |= RTSKB_CAP_SHARED;
+
+    rtdm_nrtsig_pend(&cap_signal);
+}
+
+
+
+int rtcap_xmit_hook(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    struct tap_device_t *tap_dev = &tap_device[rtskb->rtdev->ifindex];
+    rtdm_lockctx_t      context;
+
+
+    if ((rtskb->cap_comp_skb = rtskb_pool_dequeue(&cap_pool)) == 0) {
+	tap_dev->tap_dev_stats.rx_dropped++;
+	return tap_dev->orig_xmit(rtskb, rtdev);
+    }
+
+    rtskb->cap_next  = NULL;
+    rtskb->cap_start = rtskb->data;
+    rtskb->cap_len   = rtskb->len;
+    rtskb->cap_flags |= RTSKB_CAP_SHARED;
+
+    rtskb->time_stamp = rtdm_clock_read();
+
+    rtdm_lock_get_irqsave(&rtcap_lock, context);
+
+    if (cap_queue.first == NULL)
+	cap_queue.first = rtskb;
+    else
+	cap_queue.last->cap_next = rtskb;
+    cap_queue.last = rtskb;
+
+    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+    rtdm_nrtsig_pend(&cap_signal);
+
+    return tap_dev->orig_xmit(rtskb, rtdev);
+}
+
+
+
+int rtcap_loopback_xmit_hook(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    struct tap_device_t *tap_dev = &tap_device[rtskb->rtdev->ifindex];
+
+
+    rtskb->time_stamp = rtdm_clock_read();
+
+    return tap_dev->orig_xmit(rtskb, rtdev);
+}
+
+
+
+void rtcap_kfree_rtskb(struct rtskb *rtskb)
+{
+    rtdm_lockctx_t  context;
+    struct rtskb    *comp_skb;
+
+
+    rtdm_lock_get_irqsave(&rtcap_lock, context);
+
+    if (rtskb->cap_flags & RTSKB_CAP_SHARED) {
+	rtskb->cap_flags &= ~RTSKB_CAP_SHARED;
+
+	comp_skb = rtskb->cap_comp_skb;
+
+	rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+	rtskb_pool_queue_tail(comp_skb->pool, comp_skb);
+
+	return;
+    }
+
+    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+    rtskb->chain_end = rtskb;
+    rtskb_pool_queue_tail(rtskb->pool, rtskb);
+}
+
+
+
+static void convert_timestamp(nanosecs_abs_t timestamp, struct sk_buff *skb)
+{
+# ifdef CONFIG_KTIME_SCALAR
+    skb->tstamp.tv64 = timestamp;
+# else /* !CONFIG_KTIME_SCALAR */
+    unsigned long rem;
+
+    rem = do_div(timestamp, NSEC_PER_SEC);
+    skb->tstamp = ktime_set((long)timestamp, rem);
+# endif /* !CONFIG_KTIME_SCALAR */
+}
+
+
+
+static void rtcap_signal_handler(rtdm_nrtsig_t *nrtsig, void *arg)
+{
+    struct rtskb            *rtskb;
+    struct sk_buff          *skb;
+    struct sk_buff          *rtmac_skb;
+    struct net_device_stats *stats;
+    int                     ifindex;
+    int                     active;
+    rtdm_lockctx_t          context;
+
+
+    while (1)
+    {
+	rtdm_lock_get_irqsave(&rtcap_lock, context);
+
+	if ((rtskb = cap_queue.first) == NULL) {
+	    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+	    break;
+	}
+
+	cap_queue.first = rtskb->cap_next;
+
+	rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+	ifindex = rtskb->rtdev->ifindex;
+	active  = tap_device[ifindex].present;
+
+	if (active) {
+		if ((tap_device[ifindex].tap_dev->flags & IFF_UP) == 0)
+			active &= ~TAP_DEV;
+		if (active & RTMAC_TAP_DEV &&
+		    !(tap_device[ifindex].rtmac_tap_dev->flags & IFF_UP))
+			active &= ~RTMAC_TAP_DEV;
+	}
+
+	if (active == 0) {
+	    tap_device[ifindex].tap_dev_stats.rx_dropped++;
+	    rtcap_kfree_rtskb(rtskb);
+	    continue;
+	}
+
+	skb = dev_alloc_skb(rtskb->cap_len);
+	if (skb) {
+	    memcpy(skb_put(skb, rtskb->cap_len),
+		   rtskb->cap_start, rtskb->cap_len);
+
+	    if (active & TAP_DEV) {
+		skb->dev      = tap_device[ifindex].tap_dev;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		convert_timestamp(rtskb->time_stamp, skb);
+
+		rtmac_skb = NULL;
+		if ((rtskb->cap_flags & RTSKB_CAP_RTMAC_STAMP) &&
+		    (active & RTMAC_TAP_DEV)) {
+		    rtmac_skb = skb_clone(skb, GFP_ATOMIC);
+		    if (rtmac_skb != NULL)
+			convert_timestamp(rtskb->cap_rtmac_stamp, rtmac_skb);
+		}
+
+		rtcap_kfree_rtskb(rtskb);
+
+		stats = &tap_device[ifindex].tap_dev_stats;
+		stats->rx_packets++;
+		stats->rx_bytes += skb->len;
+
+		if (rtmac_skb != NULL) {
+		    rtmac_skb->dev = tap_device[ifindex].rtmac_tap_dev;
+		    netif_rx(rtmac_skb);
+		}
+		netif_rx(skb);
+	    } else if (rtskb->cap_flags & RTSKB_CAP_RTMAC_STAMP) {
+		skb->dev      = tap_device[ifindex].rtmac_tap_dev;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		convert_timestamp(rtskb->cap_rtmac_stamp, skb);
+
+		rtcap_kfree_rtskb(rtskb);
+
+		stats = &tap_device[ifindex].tap_dev_stats;
+		stats->rx_packets++;
+		stats->rx_bytes += skb->len;
+
+		netif_rx(skb);
+	    } else {
+		dev_kfree_skb(skb);
+		rtcap_kfree_rtskb(rtskb);
+	    }
+	} else {
+	    printk(KERN_WARNING "RTcap: unable to allocate linux skb\n");
+	    rtcap_kfree_rtskb(rtskb);
+	}
+    }
+}
+
+
+
+static int tap_dev_open(struct net_device *dev)
+{
+    int err;
+
+    err = try_module_get(THIS_MODULE);
+    if (err == 0)
+	return -EIDRM;
+
+    memcpy(dev->dev_addr,
+	   (*(struct rtnet_device **)netdev_priv(dev))->dev_addr,
+	   MAX_ADDR_LEN);
+
+    return 0;
+}
+
+static int tap_dev_stop(struct net_device *dev)
+{
+    module_put(THIS_MODULE);
+    return 0;
+}
+
+static int tap_dev_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+    netif_stop_queue(dev);
+    return 1;
+}
+
+
+
+static struct net_device_stats *tap_dev_get_stats(struct net_device *dev)
+{
+    struct rtnet_device *rtdev = *(struct rtnet_device **)netdev_priv(dev);
+
+    return &tap_device[rtdev->ifindex].tap_dev_stats;
+}
+
+
+
+static int tap_dev_change_mtu(struct net_device *dev, int new_mtu)
+{
+    return -EINVAL;
+}
+
+
+
+static const struct net_device_ops tap_netdev_ops = {
+    .ndo_open       = tap_dev_open,
+    .ndo_stop       = tap_dev_stop,
+    .ndo_start_xmit = tap_dev_xmit,
+    .ndo_get_stats  = tap_dev_get_stats,
+    .ndo_change_mtu = tap_dev_change_mtu,
+};
+
+static void tap_dev_setup(struct net_device *dev)
+{
+    ether_setup(dev);
+
+    dev->netdev_ops      = &tap_netdev_ops;
+    dev->mtu             = 1500;
+    dev->flags           &= ~IFF_MULTICAST;
+}
+
+
+
+void cleanup_tap_devices(void)
+{
+    int                 i;
+    struct rtnet_device *rtdev;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++)
+	if ((tap_device[i].present & TAP_DEV) != 0) {
+	    if ((tap_device[i].present & XMIT_HOOK) != 0) {
+		rtdev = *(struct rtnet_device **)
+		    netdev_priv(tap_device[i].tap_dev);
+
+		mutex_lock(&rtdev->nrt_lock);
+		rtdev->hard_start_xmit = tap_device[i].orig_xmit;
+		if (rtdev->features & NETIF_F_LLTX)
+		    rtdev->start_xmit = tap_device[i].orig_xmit;
+		mutex_unlock(&rtdev->nrt_lock);
+
+		rtdev_dereference(rtdev);
+	    }
+
+	    if ((tap_device[i].present & RTMAC_TAP_DEV) != 0) {
+		unregister_netdev(tap_device[i].rtmac_tap_dev);
+		free_netdev(tap_device[i].rtmac_tap_dev);
+	    }
+
+	    unregister_netdev(tap_device[i].tap_dev);
+	    free_netdev(tap_device[i].tap_dev);
+	}
+}
+
+
+
+int __init rtcap_init(void)
+{
+    struct rtnet_device *rtdev;
+    struct net_device   *dev;
+    int                 ret;
+    int                 devices = 0;
+    int                 i;
+
+
+    printk(KERN_INFO "RTcap: real-time capturing interface\n");
+
+    rtskb_queue_init(&cap_queue);
+
+    rtdm_nrtsig_init(&cap_signal, rtcap_signal_handler, NULL);
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	tap_device[i].present = 0;
+
+	rtdev = rtdev_get_by_index(i);
+	if (rtdev != NULL) {
+	    mutex_lock(&rtdev->nrt_lock);
+
+	    if (test_bit(PRIV_FLAG_UP, &rtdev->priv_flags)) {
+		mutex_unlock(&rtdev->nrt_lock);
+		printk(KERN_WARNING "RTcap: %s busy, skipping device!\n", rtdev->name);
+		rtdev_dereference(rtdev);
+		continue;
+	    }
+
+	    if (rtdev->mac_priv != NULL) {
+		mutex_unlock(&rtdev->nrt_lock);
+
+		printk(KERN_ERR "RTcap: RTmac discipline already active on device %s. "
+		       "Load RTcap before RTmac!\n", rtdev->name);
+
+		rtdev_dereference(rtdev);
+		continue;
+	    }
+
+	    memset(&tap_device[i].tap_dev_stats, 0,
+		   sizeof(struct net_device_stats));
+
+	    dev = alloc_netdev(sizeof(struct rtnet_device *), rtdev->name,
+			    NET_NAME_UNKNOWN, tap_dev_setup);
+	    if (!dev) {
+		ret = -ENOMEM;
+		goto error3;
+	    }
+
+	    tap_device[i].tap_dev = dev;
+	    *(struct rtnet_device **)netdev_priv(dev) = rtdev;
+
+	    ret = register_netdev(dev);
+	    if (ret < 0)
+		goto error3;
+
+	    tap_device[i].present = TAP_DEV;
+
+	    tap_device[i].orig_xmit = rtdev->hard_start_xmit;
+
+	    if ((rtdev->flags & IFF_LOOPBACK) == 0) {
+		dev = alloc_netdev(sizeof(struct rtnet_device *), rtdev->name,
+				NET_NAME_UNKNOWN, tap_dev_setup);
+		if (!dev) {
+		    ret = -ENOMEM;
+		    goto error3;
+		}
+
+		tap_device[i].rtmac_tap_dev = dev;
+		*(struct rtnet_device **)netdev_priv(dev) = rtdev;
+		strncat(dev->name, "-mac", IFNAMSIZ-strlen(dev->name));
+
+		ret = register_netdev(dev);
+		if (ret < 0)
+		    goto error3;
+
+		tap_device[i].present |= RTMAC_TAP_DEV;
+
+		rtdev->hard_start_xmit = rtcap_xmit_hook;
+	    } else
+		rtdev->hard_start_xmit = rtcap_loopback_xmit_hook;
+
+	    /* If the device requires no xmit_lock, start_xmit points equals
+	     * hard_start_xmit => we have to update this as well
+	     */
+	    if (rtdev->features & NETIF_F_LLTX)
+		rtdev->start_xmit = rtdev->hard_start_xmit;
+
+	    tap_device[i].present |= XMIT_HOOK;
+
+	    mutex_unlock(&rtdev->nrt_lock);
+
+	    devices++;
+	}
+    }
+
+    if (devices == 0) {
+	printk(KERN_WARNING "RTcap: no real-time devices found!\n");
+	ret = -ENODEV;
+	goto error2;
+    }
+
+    if (rtskb_module_pool_init(&cap_pool, rtcap_rtskbs * devices) <
+	    rtcap_rtskbs * devices) {
+	rtskb_pool_release(&cap_pool);
+	ret = -ENOMEM;
+	goto error2;
+    }
+
+    /* register capturing handlers with RTnet core
+     * (adding the handler need no locking) */
+    rtcap_handler = rtcap_rx_hook;
+
+    return 0;
+
+  error3:
+    mutex_unlock(&rtdev->nrt_lock);
+    rtdev_dereference(rtdev);
+    printk(KERN_ERR "RTcap: unable to register %s!\n", dev->name);
+
+  error2:
+    cleanup_tap_devices();
+    rtdm_nrtsig_destroy(&cap_signal);
+
+    return ret;
+}
+
+
+
+void rtcap_cleanup(void)
+{
+    rtdm_lockctx_t  context;
+
+
+    rtdm_nrtsig_destroy(&cap_signal);
+
+    /* unregister capturing handlers
+     * (take lock to avoid any unloading code before handler was left) */
+    rtdm_lock_get_irqsave(&rtcap_lock, context);
+    rtcap_handler = NULL;
+    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+    /* empty queue (should be already empty) */
+    rtcap_signal_handler(0, NULL /* we ignore them anyway */);
+
+    cleanup_tap_devices();
+
+    rtskb_pool_release(&cap_pool);
+
+    printk(KERN_INFO "RTcap: unloaded\n");
+}
+
+
+
+module_init(rtcap_init);
+module_exit(rtcap_cleanup);
diff -Naur a/net/rtnet/addons/Kconfig b/net/rtnet/addons/Kconfig
--- a/net/rtnet/addons/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/addons/Kconfig	2021-07-14 15:39:13.434124100 +0300
@@ -0,0 +1,44 @@
+menu "Add-Ons"
+    depends on RTNET
+
+config RTNET_ADDON_RTCAP
+    depends on RTNET && m
+    select ETHERNET
+    tristate "Real-Time Capturing Support"
+    default n
+    help
+    This feature allows to capture real-time packets traversing the RTnet
+    stack. It can both be used to sniff passively on a network (in this
+    case you may want to enable the promisc mode of your real-time NIC via
+    rtifconfig) and to log the traffic the node receives and transmits
+    during normal operation. RTcap consists of additional hooks in the
+    RTnet stack and a separate module as interface to standard network
+    analysis tools like Ethereal.
+
+    For further information see Documentation/README.rtcap.
+
+config RTNET_ADDON_PROXY
+    depends on RTNET_RTIPV4
+    select ETHERNET
+    bool "IP protocol proxy for Linux"
+    default n
+    help
+    Enables a forward-to-Linux module for all IP protocols that are not
+    handled by the IPv4 implemenation of RTnet (TCP, UDP, etc.). Only use
+    when you know what you are doing - it can easily break your real-time
+    requirements!
+
+    See Documentation/README.rtnetproxy for further information.
+
+config RTNET_ADDON_PROXY_ARP
+    depends on RTNET_ADDON_PROXY
+    bool "Enable ARP handling via protocol proxy"
+    default n
+    help
+    Enables ARP support for the IP protocol proxy. Incoming ARP replies
+    are then delivered to both, the RTnet and the Linux network stack,
+    but only answered by Linux. The IP protocol proxy gets attached to
+    the RTnet device specified by the module parameter "rtdev_attach",
+    rteth0 by default.
+
+endmenu
diff -Naur a/net/rtnet/addons/Makefile b/net/rtnet/addons/Makefile
--- a/net/rtnet/addons/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/addons/Makefile	2021-07-14 15:39:13.434124100 +0300
@@ -0,0 +1,9 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_ADDON_RTCAP) += rtcap.o
+
+rtcap-y := cap.o
+
+obj-$(CONFIG_RTNET_ADDON_PROXY) += rtnetproxy.o
+
+rtnetproxy-y := proxy.o
diff -Naur a/net/rtnet/addons/proxy.c b/net/rtnet/addons/proxy.c
--- a/net/rtnet/addons/proxy.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/addons/proxy.c	2021-07-14 15:39:13.434124100 +0300
@@ -0,0 +1,516 @@
+/* rtnetproxy.c: a Linux network driver that uses the RTnet driver to
+ * transport IP data from/to Linux kernel mode.
+ * This allows the usage of TCP/IP from linux space using via the RTNET
+ * network adapter.
+ *
+ *
+ * Usage:
+ *
+ * insmod rtnetproxy.o    (only after having rtnet up and running)
+ *
+ * ifconfig rtproxy up IP_ADDRESS netmask NETMASK
+ *
+ * Use it like any other network device from linux.
+ *
+ * Restrictions:
+ * Only IPV4 based protocols are supported, UDP and ICMP can be send out
+ * but not received - as these are handled directly by rtnet!
+ *
+ *
+ *
+ * Based on the linux net driver dummy.c by Nick Holloway
+ *
+ *
+ * Changelog:
+ *
+ * 08-Nov-2002  Mathias Koehrer - Clear separation between rtai context and
+ *                                standard linux driver context.
+ *                                Data exchange via ringbuffers.
+ *                                A RTAI thread is used for rtnet transmission.
+ *
+ * 05-Nov-2002  Mathias Koehrer - Initial version!
+ *                                Development based on rtnet 0.2.6,
+ *                                rtai-24.1.10, kernel 2.4.19
+ *
+ *
+ * Mathias Koehrer - mathias_koehrer@yahoo.de
+*/
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+#include <linux/sched/task.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <net/ip.h>
+
+#include <linux/if_ether.h>	/* For the statistics structure. */
+#include <linux/if_arp.h>	/* For ARPHRD_ETHER */
+
+#include <rtdev.h>
+#include <rtskb.h>
+#include <ipv4/ip_input.h>
+#include <ipv4/route.h>
+#include <rtnet_port.h>
+#include <rtnet_rtdm.h>
+
+static struct net_device *dev_rtnetproxy;
+
+/* **************************************************************************
+ *  SKB pool management (JK):
+ * ************************************************************************ */
+#define DEFAULT_PROXY_RTSKBS       64
+
+static unsigned int proxy_rtskbs = DEFAULT_PROXY_RTSKBS;
+module_param(proxy_rtskbs, uint, 0444);
+MODULE_PARM_DESC(proxy_rtskbs,
+		 "Number of realtime socket buffers in proxy pool");
+
+static struct rtskb_pool rtskb_pool;
+
+static struct rtskb_queue tx_queue;
+static struct rtskb_queue rx_queue;
+
+/* handle for non-real-time signal */
+static rtdm_event_t rtnetproxy_rx_signal;
+static struct task_struct *signal_handler_task;
+static int stop_signal_handler_task = 0;
+
+/* Thread for transmission */
+static struct task_struct *tx_task;
+static int stop_tx_task = 0;
+
+static rtdm_event_t rtnetproxy_tx_event;
+static struct net_device *the_ndev;
+
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+static char *rtdev_attach = "rteth0";
+module_param(rtdev_attach, charp, 0444);
+MODULE_PARM_DESC(rtdev_attach, "Attach to the specified RTnet device");
+
+struct rtnet_device *rtnetproxy_rtdev;
+#endif
+
+/* ************************************************************************
+ * ************************************************************************
+ *   T R A N S M I T
+ * ************************************************************************
+ * ************************************************************************ */
+
+static int rtnetproxy_tx_loop(void *arg)
+{
+	struct rtnet_device *rtdev;
+	struct rtskb *rtskb;
+
+	while (!stop_tx_task) {
+		if (rtdm_event_wait_one(&rtnetproxy_tx_event) < 0)
+			break;
+
+		if (stop_tx_task)
+			break;
+
+		while ((rtskb = rtskb_dequeue(&tx_queue)) != NULL) {
+			rtdev = rtskb->rtdev;
+			rtdev_xmit_proxy(rtskb);
+			rtdev_dereference(rtdev);
+		}
+		netif_wake_queue(the_ndev);
+	}
+
+	do_exit(0);
+	return 0;
+}
+
+/* ************************************************************************
+ *  hard_xmit
+ *
+ *  This function runs in linux kernel context and is executed whenever
+ *  there is a frame to be sent out.
+ * ************************************************************************ */
+static int rtnetproxy_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct ethhdr *eth = (struct ethhdr *)skb->data;
+	struct rtskb *rtskb;
+	int len = skb->len;
+#ifndef CONFIG_RTNET_ADDON_PROXY_ARP
+	struct dest_route rt;
+	struct iphdr *iph;
+	u32 saddr, daddr;
+#endif
+
+	switch (ntohs(eth->h_proto)) {
+	case ETH_P_IP:
+		if (len < sizeof(struct ethhdr) + sizeof(struct iphdr))
+			goto drop1;
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	case ETH_P_ARP:
+#endif
+		break;
+	default:
+ drop1:
+		dev->stats.tx_dropped++;
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	}
+
+	rtskb = alloc_rtskb(len, &rtskb_pool);
+	if (!rtskb) {
+		trace_printk("%s %d NETDEV_TX_BUSY\n", __func__, __LINE__);
+		return NETDEV_TX_BUSY;
+	}
+
+	memcpy(rtskb_put(rtskb, len), skb->data, len);
+
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	dev_kfree_skb(skb);
+
+	rtskb->rtdev = rtnetproxy_rtdev;
+	if (rtdev_reference(rtnetproxy_rtdev) == 0) {
+		dev->stats.tx_dropped++;
+		kfree_rtskb(rtskb);
+		trace_printk("%s %d NETDEV_TX_BUSY\n", __func__, __LINE__);
+		return NETDEV_TX_BUSY;
+	}
+
+#else				/* !CONFIG_RTNET_ADDON_PROXY_ARP */
+	iph = (struct iphdr *)(skb->data + sizeof(struct ethhdr));
+	saddr = iph->saddr;
+	daddr = iph->daddr;
+
+	dev_kfree_skb(skb);
+
+	if (rt_ip_route_output(&rt, daddr, INADDR_ANY) < 0) {
+ drop2:
+		dev->stats.tx_dropped++;
+		kfree_rtskb(rtskb);
+		return NETDEV_TX_OK;
+	}
+	if (rt.rtdev->local_ip != saddr) {
+		rtdev_dereference(rt.rtdev);
+		goto drop2;
+	}
+
+	eth = (struct ethhdr *)rtskb->data;
+	memcpy(eth->h_source, rt.rtdev->dev_addr, rt.rtdev->addr_len);
+	memcpy(eth->h_dest, rt.dev_addr, rt.rtdev->addr_len);
+
+	rtskb->rtdev = rt.rtdev;
+#endif				/* CONFIG_RTNET_ADDON_PROXY_ARP */
+
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += len;
+
+	rtskb_queue_tail(&tx_queue, rtskb);
+	netif_stop_queue(dev);
+	the_ndev = dev;
+	smp_mb();
+	rtdm_event_signal(&rtnetproxy_tx_event);
+
+	return NETDEV_TX_OK;
+}
+
+/* ************************************************************************
+ * ************************************************************************
+ *   R E C E I V E
+ * ************************************************************************
+ * ************************************************************************ */
+
+/* ************************************************************************
+ * This function runs in real-time context.
+ *
+ * It is called from inside rtnet whenever a packet has been received that
+ * has to be processed by rtnetproxy.
+ * ************************************************************************ */
+static void rtnetproxy_recv(struct rtskb *rtskb)
+{
+	/* Acquire rtskb (JK) */
+	if (rtskb_acquire(rtskb, &rtskb_pool) != 0) {
+		dev_rtnetproxy->stats.rx_dropped++;
+		printk(KERN_ERR "rtnetproxy_recv: No free rtskb in pool\n");
+		kfree_rtskb(rtskb);
+		return;
+	}
+
+	rtskb_queue_tail(&rx_queue, rtskb);
+	rtdm_event_signal_one(&rtnetproxy_rx_signal);
+}
+
+/* ************************************************************************
+ * This function runs in kernel mode.
+ * It is activated from rtnetproxy_signal_handler whenever rtnet received a
+ * frame to be processed by rtnetproxy.
+ * ************************************************************************ */
+static inline void rtnetproxy_kernel_recv(struct rtskb *rtskb)
+{
+	struct sk_buff *skb;
+	struct net_device *dev = dev_rtnetproxy;
+
+	int header_len = rtskb->rtdev->hard_header_len;
+	int len = rtskb->len + header_len;
+
+	/* Copy the realtime skb (rtskb) to the standard skb: */
+	skb = dev_alloc_skb(len + 2);
+	skb_reserve(skb, 2);
+
+	memcpy(skb_put(skb, len), rtskb->data - header_len, len);
+
+	/* Set some relevant entries in the skb: */
+	skb->protocol = eth_type_trans(skb, dev);
+	skb->dev = dev;
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+	skb->pkt_type = PACKET_HOST;	/* Extremely important! Why?!? */
+
+	/* the rtskb stamp is useless (different clock), get new one */
+	__net_timestamp(skb);
+
+#if 0
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)
+	dev->last_rx = jiffies;
+#endif
+#endif
+	dev->stats.rx_bytes += skb->len;
+	dev->stats.rx_packets++;
+
+	netif_rx(skb);		/* pass it to the received stuff */
+
+}
+
+/* ************************************************************************
+ * This function runs in kernel mode.
+ * It is activated from rtnetproxy_recv whenever rtnet received a frame to
+ * be processed by rtnetproxy.
+ * ************************************************************************ */
+static int rtnetproxy_signal_handler(void *arg)
+{
+	struct rtskb *rtskb;
+
+	while (!stop_signal_handler_task) {
+		if (rtdm_event_wait_one(&rtnetproxy_rx_signal) < 0)
+			break;
+
+		if (stop_signal_handler_task)
+			break;
+
+		while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+			rtnetproxy_kernel_recv(rtskb);
+			kfree_rtskb(rtskb);
+		}
+	}
+
+	do_exit(0);
+	return 0;
+}
+
+/* ************************************************************************
+ * ************************************************************************
+ *   G E N E R A L
+ * ************************************************************************
+ * ************************************************************************ */
+
+static void fake_multicast_support(struct net_device *dev)
+{
+}
+
+#ifdef CONFIG_NET_FASTROUTE
+static int rtnetproxy_accept_fastpath(struct net_device *dev,
+				      struct dst_entry *dst)
+{
+	return -1;
+}
+#endif
+
+static int rtnetproxy_open(struct net_device *dev)
+{
+	struct sched_param task_param = {.sched_priority =
+		    (RTDM_TASK_LOWEST_PRIORITY + 1) };
+#if 0
+	int err = try_module_get(THIS_MODULE);
+	if (err == 0)
+		return -EIDRM;
+#endif
+
+	rtdm_event_init(&rtnetproxy_rx_signal, 0);
+	stop_signal_handler_task = 0;
+	signal_handler_task =
+	    kthread_create(rtnetproxy_signal_handler, NULL,
+			   "rtnetproxy_signal_handler");
+	if (!signal_handler_task) {
+		printk(KERN_ERR "%s signal_handler_task can not be created\n",
+		       __func__);
+		return PTR_ERR(signal_handler_task);
+	}
+	sched_setscheduler(signal_handler_task, SCHED_FIFO, &task_param);
+	wake_up_process(signal_handler_task);
+
+	/* Init the task for transmission */
+	rtdm_event_init(&rtnetproxy_tx_event, 0);
+	stop_tx_task = 0;
+	tx_task = kthread_create(rtnetproxy_tx_loop, NULL, "rtnetproxy");
+	if (!tx_task) {
+		printk(KERN_ERR "%s tx_handler_task can not be created\n",
+		       __func__);
+		return PTR_ERR(tx_task);
+	}
+	sched_setscheduler(tx_task, SCHED_FIFO, &task_param);
+	wake_up_process(tx_task);
+
+	rtskb_queue_init(&tx_queue);
+	rtskb_queue_init(&rx_queue);
+
+	/* Register with RTnet */
+	rt_ip_fallback_handler = rtnetproxy_recv;
+
+	return 0;
+}
+
+static int rtnetproxy_stop(struct net_device *dev)
+{
+	struct rtskb *rtskb;
+
+	/* Unregister with RTnet */
+	rt_ip_fallback_handler = NULL;
+
+	stop_tx_task = 1;
+	smp_mb();
+	rtdm_event_signal_one(&rtnetproxy_tx_event);
+	rtdm_event_destroy(&rtnetproxy_tx_event);
+	/* wait for the thread termination */
+	kthread_stop(tx_task);
+#if 0
+	/* release the task structure */
+	put_task_struct(tx_task);
+#endif
+
+	/* free the non-real-time signal */
+	stop_signal_handler_task = 1;
+	smp_mb();
+	rtdm_event_signal_one(&rtnetproxy_rx_signal);
+	kthread_stop(signal_handler_task);
+#if 0
+	put_task_struct(signal_handler_task);
+#endif
+
+	while ((rtskb = rtskb_dequeue(&tx_queue)) != NULL) {
+		rtdev_dereference(rtskb->rtdev);
+		kfree_rtskb(rtskb);
+	}
+
+	while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+		kfree_rtskb(rtskb);
+	}
+
+#if 0
+	module_put(THIS_MODULE);
+#endif
+	return 0;
+}
+
+static const struct net_device_ops rtnetproxy_netdev_ops = {
+	.ndo_open = rtnetproxy_open,
+	.ndo_stop = rtnetproxy_stop,
+	.ndo_start_xmit = rtnetproxy_xmit,
+	.ndo_set_rx_mode = fake_multicast_support,
+};
+
+/* ************************************************************************
+ *  device init
+ * ************************************************************************ */
+static void __init rtnetproxy_init(struct net_device *dev)
+{
+	/* Fill in device structure with ethernet-generic values. */
+	ether_setup(dev);
+
+	dev->tx_queue_len = 0;
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	memcpy(dev->dev_addr, rtnetproxy_rtdev->dev_addr, MAX_ADDR_LEN);
+#else
+	dev->flags |= IFF_NOARP;
+#endif
+	dev->flags &= ~IFF_MULTICAST;
+
+	dev->netdev_ops = &rtnetproxy_netdev_ops;
+}
+
+/* ************************************************************************
+ * ************************************************************************
+ *   I N I T
+ * ************************************************************************
+ * ************************************************************************ */
+static int __init rtnetproxy_init_module(void)
+{
+	int err = 0;
+
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	if ((rtnetproxy_rtdev = rtdev_get_by_name(rtdev_attach)) == NULL) {
+		printk(KERN_ERR "Couldn't attach to %s\n", rtdev_attach);
+		return -EINVAL;
+	}
+	printk(KERN_INFO "RTproxy attached to %s\n", rtdev_attach);
+#endif
+
+	/* Initialize the proxy's rtskb pool (JK) */
+	if (rtskb_module_pool_init(&rtskb_pool, proxy_rtskbs) < proxy_rtskbs) {
+		err = -ENOMEM;
+		goto err1;
+	}
+
+	dev_rtnetproxy =
+	    alloc_netdev(0, "rtproxy", NET_NAME_UNKNOWN, rtnetproxy_init);
+	if (!dev_rtnetproxy) {
+		err = -ENOMEM;
+		goto err1;
+	}
+
+	err = register_netdev(dev_rtnetproxy);
+	if (err < 0)
+		goto err3;
+
+	printk(KERN_INFO "rtnetproxy installed as \"%s\"\n",
+	       dev_rtnetproxy->name);
+
+	return 0;
+
+#if 0
+ err4:
+	unregister_netdev(dev_rtnetproxy);
+#endif
+
+ err3:
+	rtdm_event_destroy(&rtnetproxy_rx_signal);
+
+	free_netdev(dev_rtnetproxy);
+
+ err1:
+	rtskb_pool_release(&rtskb_pool);
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	rtdev_dereference(rtnetproxy_rtdev);
+#endif
+	return err;
+}
+
+static void __exit rtnetproxy_cleanup_module(void)
+{
+	/* Unregister the fallback at rtnet */
+	rt_ip_fallback_handler = NULL;
+
+	/* Unregister the net device: */
+	unregister_netdev(dev_rtnetproxy);
+	free_netdev(dev_rtnetproxy);
+
+	rtskb_pool_release(&rtskb_pool);
+
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	rtdev_dereference(rtnetproxy_rtdev);
+#endif
+}
+
+module_init(rtnetproxy_init_module);
+module_exit(rtnetproxy_cleanup_module);
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/drivers/Kconfig b/net/rtnet/drivers/Kconfig
--- a/net/rtnet/drivers/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/Kconfig	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,23 @@
+menu "Drivers"
+    depends on RTNET
+
+comment "Common PCI Drivers"
+    depends on PCI
+
+config RTNET_DRV_LOOPBACK
+    depends on RTNET
+    bool "Loopback"
+    default y
+
+config RTNET_DRV_TI_CPSW
+    depends on RTNET && ARM
+    bool "TI CPSW"
+    help
+    Texas Instruments am335x real-time network driver
+
+source "net/rtnet/drivers/realtek/Kconfig"
+source "net/rtnet/drivers/microchip/Kconfig"
+source "net/rtnet/drivers/rpi-4/Kconfig"
+source "net/rtnet/drivers/orange-pi-one/Kconfig"
+
+endmenu
diff -Naur a/net/rtnet/drivers/loopback.c b/net/rtnet/drivers/loopback.c
--- a/net/rtnet/drivers/loopback.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/loopback.c	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,148 @@
+/* loopback.c
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ * extended by Jose Carlos Billalabeitia and Jan Kiszka
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+
+#include <linux/netdevice.h>
+
+#include <rtnet_port.h>
+#include <stack_mgr.h>
+
+MODULE_AUTHOR("Maintainer: Jan Kiszka <Jan.Kiszka@web.de>");
+MODULE_DESCRIPTION("RTnet loopback driver");
+MODULE_LICENSE("GPL");
+
+static struct rtnet_device* rt_loopback_dev;
+
+/***
+ *  rt_loopback_open
+ *  @rtdev
+ */
+static int rt_loopback_open (struct rtnet_device *rtdev)
+{
+    rt_stack_connect(rtdev, &STACK_manager);
+    rtnetif_start_queue(rtdev);
+
+    return 0;
+}
+
+
+/***
+ *  rt_loopback_close
+ *  @rtdev
+ */
+static int rt_loopback_close (struct rtnet_device *rtdev)
+{
+    rtnetif_stop_queue(rtdev);
+    rt_stack_disconnect(rtdev);
+
+    return 0;
+}
+
+
+/***
+ *  rt_loopback_xmit - begin packet transmission
+ *  @skb: packet to be sent
+ *  @dev: network device to which packet is sent
+ *
+ */
+static int rt_loopback_xmit(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    /* write transmission stamp - in case any protocol ever gets the idea to
+       ask the lookback device for this service... */
+    if (rtskb->xmit_stamp)
+	*rtskb->xmit_stamp =
+	    cpu_to_be64(ktime_get() + *rtskb->xmit_stamp);
+
+    /* make sure that critical fields are re-intialised */
+    rtskb->chain_end = rtskb;
+
+    /* parse the Ethernet header as usual */
+    rtskb->protocol = rt_eth_type_trans(rtskb, rtdev);
+
+    rt_stack_deliver(rtskb);
+
+    return 0;
+}
+
+
+/***
+ *  loopback_init
+ */
+static int __init loopback_init(void)
+{
+    int err;
+    struct rtnet_device *rtdev;
+
+    printk(KERN_INFO "initializing loopback...\n");
+
+    if ((rtdev = rt_alloc_etherdev(0, 1)) == NULL) {
+	printk(KERN_ERR "%s rt_alloc_etherdev error\n", __func__);
+	return -ENODEV;
+    }
+
+    rt_rtdev_connect(rtdev, &RTDEV_manager);
+
+    strcpy(rtdev->name, "rtlo");
+
+    rtdev->vers = RTDEV_VERS_2_0;
+    rtdev->open = &rt_loopback_open;
+    rtdev->stop = &rt_loopback_close;
+    rtdev->hard_start_xmit = &rt_loopback_xmit;
+    rtdev->flags |= IFF_LOOPBACK;
+    rtdev->flags &= ~IFF_BROADCAST;
+    rtdev->features |= NETIF_F_LLTX;
+
+    if ((err = rt_register_rtnetdev(rtdev)) != 0)
+    {
+	printk(KERN_ERR "%s rt_register_rtnetdev error %d\n", __func__, err);
+	rtdev_free(rtdev);
+	return err;
+    }
+
+    rt_loopback_dev = rtdev;
+
+    return 0;
+}
+
+
+/***
+ *  loopback_cleanup
+ */
+static void __exit loopback_cleanup(void)
+{
+    struct rtnet_device *rtdev = rt_loopback_dev;
+
+    printk(KERN_INFO "removing loopback...\n");
+
+    rt_unregister_rtnetdev(rtdev);
+    rt_rtdev_disconnect(rtdev);
+
+    rtdev_free(rtdev);
+}
+
+module_init(loopback_init);
+module_exit(loopback_cleanup);
diff -Naur a/net/rtnet/drivers/Makefile b/net/rtnet/drivers/Makefile
--- a/net/rtnet/drivers/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/Makefile	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,16 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_DRV_LOOPBACK) += loopback.o
+
+obj-$(CONFIG_RTNET_DRV_TI_CPSW) += ticpsw/ \
+					   rt_davinci_mdio.o \
+					   rt_smsc.o
+
+obj-$(CONFIG_RTNET_DRV_ENC28J60) += microchip/
+
+obj-$(CONFIG_RTNET_DRV_REALTEK) += realtek/
+
+obj-$(CONFIG_RTNET_DRV_BCMGENET) += rpi-4/
+
+obj-$(CONFIG_RTNET_DWMAC_SUN8I) += orange-pi-one/
+
diff -Naur a/net/rtnet/drivers/microchip/enc28j60.c b/net/rtnet/drivers/microchip/enc28j60.c
--- a/net/rtnet/drivers/microchip/enc28j60.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/microchip/enc28j60.c	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,1804 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Microchip ENC28J60 ethernet driver (MAC + PHY)
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2007 Eurek srl
+ * Author: Claudio Lanconelli <lanconelli.claudio@eptar.com>
+ * based on enc28j60.c written by David Anders for 2.4 kernel version
+ *
+ * $Id: enc28j60.c,v 1.22 2007/12/20 10:47:01 claudio Exp $
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/fcntl.h>
+#include <linux/interrupt.h>
+#include <linux/property.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/tcp.h>
+#include <linux/skbuff.h>
+#include <linux/delay.h>
+#include <linux/spi/spi.h>
+#include <linux/timekeeping.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/ip.h>
+#include <uapi/linux/ip.h>
+
+/* *** RTnet *** */
+#include <rtnet_port.h>
+#include <rtnet_multiple_queues.h>
+
+#include "enc28j60_hw.h"
+
+#define DRV_NAME	"enc28j60"
+#define DRV_VERSION	"1.02"
+
+#define SPI_OPLEN	1
+
+#define ENC28J60_MSG_DEFAULT	\
+	(NETIF_MSG_PROBE | NETIF_MSG_IFUP | NETIF_MSG_IFDOWN | NETIF_MSG_LINK)
+
+/* Buffer size required for the largest SPI transfer (i.e., reading a
+ * frame).
+ */
+#define SPI_TRANSFER_BUF_LEN	(4 + MAX_FRAMELEN)
+
+#define TX_TIMEOUT		(4 * HZ)
+
+/* Max TX retries in case of collision as suggested by errata datasheet */
+#define MAX_TX_RETRYCOUNT	16
+
+enum {
+	RXFILTER_NORMAL,
+	RXFILTER_MULTI,
+	RXFILTER_PROMISC
+};
+
+/* Driver local data */
+struct enc28j60_net {
+	struct rtnet_device *netdev;
+	struct spi_device *spi;
+	struct rt_mutex lock;
+	struct rtskb *tx_skb;
+	struct work_struct tx_work;
+	struct work_struct irq_work;
+	struct work_struct setrx_work;
+	struct work_struct restart_work;
+	u8 bank;		/* current register bank selected */
+	u16 next_pk_ptr;	/* next packet pointer within FIFO */
+	u16 max_pk_counter;	/* statistics: max packet counter */
+	u16 tx_retry_count;
+	bool hw_enable;
+	bool full_duplex;
+	int rxfilter;
+	u32 msg_enable;
+	u8 spi_transfer_buf[SPI_TRANSFER_BUF_LEN];
+#if 0
+	struct rtskb_pool skb_pool;
+#endif
+	rtdm_event_t tx_event, rx_event;
+	struct task_struct		*rx_task, *tx_task;
+	
+	struct net_device_stats stats;
+};
+
+/* use ethtool to change the level for any given device */
+static struct {
+	u32 msg_enable;
+} debug = { -1 };
+
+#define RX_RING_SIZE	64
+static unsigned int enc28j60_rtskb_pool_size = RX_RING_SIZE * 2;
+module_param(enc28j60_rtskb_pool_size, uint, 0444);
+MODULE_PARM_DESC(enc28j60_rtskb_pool_size, "Number of realtime socket buffers in proxy pool");
+
+static int stop_tx_task = 0, stop_rx_task = 0;
+
+/*
+ * SPI read buffer
+ * Wait for the SPI transfer and copy received data to destination.
+ */
+static int
+spi_read_buf(struct enc28j60_net *priv, int len, u8 *data)
+{
+	struct device *dev = &priv->spi->dev;
+	u8 *rx_buf = priv->spi_transfer_buf + 4;
+	u8 *tx_buf = priv->spi_transfer_buf;
+	struct spi_transfer tx = {
+		.tx_buf = tx_buf,
+		.len = SPI_OPLEN,
+	};
+	struct spi_transfer rx = {
+		.rx_buf = rx_buf,
+		.len = len,
+	};
+	struct spi_message msg;
+	int ret;
+
+	tx_buf[0] = ENC28J60_READ_BUF_MEM;
+
+	spi_message_init(&msg);
+	spi_message_add_tail(&tx, &msg);
+	spi_message_add_tail(&rx, &msg);
+
+	ret = spi_sync(priv->spi, &msg);
+	if (ret == 0) {
+		memcpy(data, rx_buf, len);
+		ret = msg.status;
+	}
+	if (ret && netif_msg_drv(priv))
+		dev_printk(KERN_DEBUG, dev, "%s() failed: ret = %d\n",
+			   __func__, ret);
+
+	return ret;
+}
+
+/*
+ * SPI write buffer
+ */
+static int spi_write_buf(struct enc28j60_net *priv, int len, const u8 *data)
+{
+	struct device *dev = &priv->spi->dev;
+	int ret;
+
+	if (len > SPI_TRANSFER_BUF_LEN - 1 || len <= 0)
+		ret = -EINVAL;
+	else {
+		priv->spi_transfer_buf[0] = ENC28J60_WRITE_BUF_MEM;
+		memcpy(&priv->spi_transfer_buf[1], data, len);
+		ret = spi_write(priv->spi, priv->spi_transfer_buf, len + 1);
+		if (ret && netif_msg_drv(priv))
+			dev_printk(KERN_DEBUG, dev, "%s() failed: ret = %d\n",
+				   __func__, ret);
+	}
+	return ret;
+}
+
+/*
+ * basic SPI read operation
+ */
+static u8 spi_read_op(struct enc28j60_net *priv, u8 op, u8 addr)
+{
+	struct device *dev = &priv->spi->dev;
+	u8 tx_buf[2];
+	u8 rx_buf[4];
+	u8 val = 0;
+	int ret;
+	int slen = SPI_OPLEN;
+
+	/* do dummy read if needed */
+	if (addr & SPRD_MASK)
+		slen++;
+
+	tx_buf[0] = op | (addr & ADDR_MASK);
+	ret = spi_write_then_read(priv->spi, tx_buf, 1, rx_buf, slen);
+	if (ret)
+		dev_printk(KERN_DEBUG, dev, "%s() failed: ret = %d\n",
+			   __func__, ret);
+	else
+		val = rx_buf[slen - 1];
+
+	return val;
+}
+
+/*
+ * basic SPI write operation
+ */
+static int spi_write_op(struct enc28j60_net *priv, u8 op, u8 addr, u8 val)
+{
+	struct device *dev = &priv->spi->dev;
+	int ret;
+
+	priv->spi_transfer_buf[0] = op | (addr & ADDR_MASK);
+	priv->spi_transfer_buf[1] = val;
+	ret = spi_write(priv->spi, priv->spi_transfer_buf, 2);
+	if (ret && netif_msg_drv(priv))
+		dev_printk(KERN_DEBUG, dev, "%s() failed: ret = %d\n",
+			   __func__, ret);
+	return ret;
+}
+
+static void enc28j60_soft_reset(struct enc28j60_net *priv)
+{
+	spi_write_op(priv, ENC28J60_SOFT_RESET, 0, ENC28J60_SOFT_RESET);
+	/* Errata workaround #1, CLKRDY check is unreliable,
+	 * delay at least 1 ms instead */
+	udelay(2000);
+}
+
+/*
+ * select the current register bank if necessary
+ */
+static void enc28j60_set_bank(struct enc28j60_net *priv, u8 addr)
+{
+	u8 b = (addr & BANK_MASK) >> 5;
+
+	/* These registers (EIE, EIR, ESTAT, ECON2, ECON1)
+	 * are present in all banks, no need to switch bank.
+	 */
+	if (addr >= EIE && addr <= ECON1)
+		return;
+
+	/* Clear or set each bank selection bit as needed */
+	if ((b & ECON1_BSEL0) != (priv->bank & ECON1_BSEL0)) {
+		if (b & ECON1_BSEL0)
+			spi_write_op(priv, ENC28J60_BIT_FIELD_SET, ECON1,
+					ECON1_BSEL0);
+		else
+			spi_write_op(priv, ENC28J60_BIT_FIELD_CLR, ECON1,
+					ECON1_BSEL0);
+	}
+	if ((b & ECON1_BSEL1) != (priv->bank & ECON1_BSEL1)) {
+		if (b & ECON1_BSEL1)
+			spi_write_op(priv, ENC28J60_BIT_FIELD_SET, ECON1,
+					ECON1_BSEL1);
+		else
+			spi_write_op(priv, ENC28J60_BIT_FIELD_CLR, ECON1,
+					ECON1_BSEL1);
+	}
+	priv->bank = b;
+}
+
+/*
+ * Register access routines through the SPI bus.
+ * Every register access comes in two flavours:
+ * - nolock_xxx: caller needs to invoke rt_mutex_lock, usually to access
+ *   atomically more than one register
+ * - locked_xxx: caller doesn't need to invoke rt_mutex_lock, single access
+ *
+ * Some registers can be accessed through the bit field clear and
+ * bit field set to avoid a read modify write cycle.
+ */
+
+/*
+ * Register bit field Set
+ */
+static void nolock_reg_bfset(struct enc28j60_net *priv, u8 addr, u8 mask)
+{
+	enc28j60_set_bank(priv, addr);
+	spi_write_op(priv, ENC28J60_BIT_FIELD_SET, addr, mask);
+}
+
+static void locked_reg_bfset(struct enc28j60_net *priv, u8 addr, u8 mask)
+{
+	rt_mutex_lock(&priv->lock);
+	nolock_reg_bfset(priv, addr, mask);
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * Register bit field Clear
+ */
+static void nolock_reg_bfclr(struct enc28j60_net *priv, u8 addr, u8 mask)
+{
+	enc28j60_set_bank(priv, addr);
+	spi_write_op(priv, ENC28J60_BIT_FIELD_CLR, addr, mask);
+}
+
+static void locked_reg_bfclr(struct enc28j60_net *priv, u8 addr, u8 mask)
+{
+	rt_mutex_lock(&priv->lock);
+	nolock_reg_bfclr(priv, addr, mask);
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * Register byte read
+ */
+static int nolock_regb_read(struct enc28j60_net *priv, u8 address)
+{
+	enc28j60_set_bank(priv, address);
+	return spi_read_op(priv, ENC28J60_READ_CTRL_REG, address);
+}
+
+static int locked_regb_read(struct enc28j60_net *priv, u8 address)
+{
+	int ret;
+
+	rt_mutex_lock(&priv->lock);
+	ret = nolock_regb_read(priv, address);
+	rt_mutex_unlock(&priv->lock);
+
+	return ret;
+}
+
+/*
+ * Register word read
+ */
+static int nolock_regw_read(struct enc28j60_net *priv, u8 address)
+{
+	int rl, rh;
+
+	enc28j60_set_bank(priv, address);
+	rl = spi_read_op(priv, ENC28J60_READ_CTRL_REG, address);
+	rh = spi_read_op(priv, ENC28J60_READ_CTRL_REG, address + 1);
+
+	return (rh << 8) | rl;
+}
+
+static int locked_regw_read(struct enc28j60_net *priv, u8 address)
+{
+	int ret;
+
+	rt_mutex_lock(&priv->lock);
+	ret = nolock_regw_read(priv, address);
+	rt_mutex_unlock(&priv->lock);
+
+	return ret;
+}
+
+/*
+ * Register byte write
+ */
+static void nolock_regb_write(struct enc28j60_net *priv, u8 address, u8 data)
+{
+	enc28j60_set_bank(priv, address);
+	spi_write_op(priv, ENC28J60_WRITE_CTRL_REG, address, data);
+}
+
+static void locked_regb_write(struct enc28j60_net *priv, u8 address, u8 data)
+{
+	rt_mutex_lock(&priv->lock);
+	nolock_regb_write(priv, address, data);
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * Register word write
+ */
+static void nolock_regw_write(struct enc28j60_net *priv, u8 address, u16 data)
+{
+	enc28j60_set_bank(priv, address);
+	spi_write_op(priv, ENC28J60_WRITE_CTRL_REG, address, (u8) data);
+	spi_write_op(priv, ENC28J60_WRITE_CTRL_REG, address + 1,
+		     (u8) (data >> 8));
+}
+
+static void locked_regw_write(struct enc28j60_net *priv, u8 address, u16 data)
+{
+	rt_mutex_lock(&priv->lock);
+	nolock_regw_write(priv, address, data);
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * Buffer memory read
+ * Select the starting address and execute a SPI buffer read.
+ */
+static void enc28j60_mem_read(struct enc28j60_net *priv, u16 addr, int len,
+			      u8 *data)
+{
+	rt_mutex_lock(&priv->lock);
+	nolock_regw_write(priv, ERDPTL, addr);
+#ifdef CONFIG_RTNET_ENC28J60_WRITEVERIFY
+	if (netif_msg_drv(priv)) {
+		struct device *dev = &priv->spi->dev;
+		u16 reg;
+
+		reg = nolock_regw_read(priv, ERDPTL);
+		if (reg != addr)
+			dev_printk(KERN_DEBUG, dev,
+				   "%s() error writing ERDPT (0x%04x - 0x%04x)\n",
+				   __func__, reg, addr);
+	}
+#endif
+	spi_read_buf(priv, len, data);
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * Write packet to enc28j60 TX buffer memory
+ */
+static void
+enc28j60_packet_write(struct enc28j60_net *priv, int len, const u8 *data)
+{
+	struct device *dev = &priv->spi->dev;
+
+	rt_mutex_lock(&priv->lock);
+	/* Set the write pointer to start of transmit buffer area */
+	nolock_regw_write(priv, EWRPTL, TXSTART_INIT);
+#ifdef CONFIG_RTNET_ENC28J60_WRITEVERIFY
+	if (netif_msg_drv(priv)) {
+		u16 reg;
+		reg = nolock_regw_read(priv, EWRPTL);
+		if (reg != TXSTART_INIT)
+			dev_printk(KERN_DEBUG, dev,
+				   "%s() ERWPT:0x%04x != 0x%04x\n",
+				   __func__, reg, TXSTART_INIT);
+	}
+#endif
+	/* Set the TXND pointer to correspond to the packet size given */
+	nolock_regw_write(priv, ETXNDL, TXSTART_INIT + len);
+	/* write per-packet control byte */
+	spi_write_op(priv, ENC28J60_WRITE_BUF_MEM, 0, 0x00);
+	if (netif_msg_hw(priv))
+		dev_printk(KERN_DEBUG, dev,
+			   "%s() after control byte ERWPT:0x%04x\n",
+			   __func__, nolock_regw_read(priv, EWRPTL));
+	/* copy the packet into the transmit buffer */
+	spi_write_buf(priv, len, data);
+	if (netif_msg_hw(priv))
+		dev_printk(KERN_DEBUG, dev,
+			   "%s() after write packet ERWPT:0x%04x, len=%d\n",
+			   __func__, nolock_regw_read(priv, EWRPTL), len);
+	rt_mutex_unlock(&priv->lock);
+}
+
+static int poll_ready(struct enc28j60_net *priv, u8 reg, u8 mask, u8 val)
+{
+	struct device *dev = &priv->spi->dev;
+	unsigned long timeout = jiffies + msecs_to_jiffies(20);
+
+	/* 20 msec timeout read */
+	while ((nolock_regb_read(priv, reg) & mask) != val) {
+		if (time_after(jiffies, timeout)) {
+			if (netif_msg_drv(priv))
+				dev_dbg(dev, "reg %02x ready timeout!\n", reg);
+			return -ETIMEDOUT;
+		}
+		cpu_relax();
+	}
+	return 0;
+}
+
+/*
+ * Wait until the PHY operation is complete.
+ */
+static int wait_phy_ready(struct enc28j60_net *priv)
+{
+	return poll_ready(priv, MISTAT, MISTAT_BUSY, 0) ? 0 : 1;
+}
+
+/*
+ * PHY register read
+ * PHY registers are not accessed directly, but through the MII.
+ */
+static u16 enc28j60_phy_read(struct enc28j60_net *priv, u8 address)
+{
+	u16 ret;
+
+	rt_mutex_lock(&priv->lock);
+	/* set the PHY register address */
+	nolock_regb_write(priv, MIREGADR, address);
+	/* start the register read operation */
+	nolock_regb_write(priv, MICMD, MICMD_MIIRD);
+	/* wait until the PHY read completes */
+	wait_phy_ready(priv);
+	/* quit reading */
+	nolock_regb_write(priv, MICMD, 0x00);
+	/* return the data */
+	ret = nolock_regw_read(priv, MIRDL);
+	rt_mutex_unlock(&priv->lock);
+
+	return ret;
+}
+
+static int enc28j60_phy_write(struct enc28j60_net *priv, u8 address, u16 data)
+{
+	int ret;
+
+	rt_mutex_lock(&priv->lock);
+	/* set the PHY register address */
+	nolock_regb_write(priv, MIREGADR, address);
+	/* write the PHY data */
+	nolock_regw_write(priv, MIWRL, data);
+	/* wait until the PHY write completes and return */
+	ret = wait_phy_ready(priv);
+	rt_mutex_unlock(&priv->lock);
+
+	return ret;
+}
+
+/*
+ * Program the hardware MAC address from dev->dev_addr.
+ */
+static int enc28j60_set_hw_macaddr(struct rtnet_device *ndev)
+{
+	int ret;
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+	struct device *dev = &priv->spi->dev;
+
+	rt_mutex_lock(&priv->lock);
+	if (!priv->hw_enable) {
+		if (netif_msg_drv(priv))
+			dev_info(dev, "%s: Setting MAC address to %pM\n",
+				 ndev->name, ndev->dev_addr);
+		/* NOTE: MAC address in ENC28J60 is byte-backward */
+		nolock_regb_write(priv, MAADR5, ndev->dev_addr[0]);
+		nolock_regb_write(priv, MAADR4, ndev->dev_addr[1]);
+		nolock_regb_write(priv, MAADR3, ndev->dev_addr[2]);
+		nolock_regb_write(priv, MAADR2, ndev->dev_addr[3]);
+		nolock_regb_write(priv, MAADR1, ndev->dev_addr[4]);
+		nolock_regb_write(priv, MAADR0, ndev->dev_addr[5]);
+		ret = 0;
+	} else {
+		if (netif_msg_drv(priv))
+			dev_printk(KERN_DEBUG, dev,
+				   "%s() Hardware must be disabled to set Mac address\n",
+				   __func__);
+		ret = -EBUSY;
+	}
+	rt_mutex_unlock(&priv->lock);
+	return ret;
+}
+
+#if 0
+/*
+ * Store the new hardware address in dev->dev_addr, and update the MAC.
+ */
+static int enc28j60_set_mac_address(struct rtnet_device *dev, void *addr)
+{
+	struct sockaddr *address = addr;
+
+	if (rtnetif_running(dev))
+		return -EBUSY;
+	if (!is_valid_ether_addr(address->sa_data))
+		return -EADDRNOTAVAIL;
+
+	ether_addr_copy(dev->dev_addr, address->sa_data);
+	return enc28j60_set_hw_macaddr(dev);
+}
+#endif
+
+/*
+ * Debug routine to dump useful register contents
+ */
+static void enc28j60_dump_regs(struct enc28j60_net *priv, const char *msg)
+{
+	struct device *dev = &priv->spi->dev;
+
+	rt_mutex_lock(&priv->lock);
+	dev_printk(KERN_DEBUG, dev,
+		   " %s\n"
+		   "HwRevID: 0x%02x\n"
+		   "Cntrl: ECON1 ECON2 ESTAT  EIR  EIE\n"
+		   "       0x%02x  0x%02x  0x%02x  0x%02x  0x%02x\n"
+		   "MAC  : MACON1 MACON3 MACON4\n"
+		   "       0x%02x   0x%02x   0x%02x\n"
+		   "Rx   : ERXST  ERXND  ERXWRPT ERXRDPT ERXFCON EPKTCNT MAMXFL\n"
+		   "       0x%04x 0x%04x 0x%04x  0x%04x  "
+		   "0x%02x    0x%02x    0x%04x\n"
+		   "Tx   : ETXST  ETXND  MACLCON1 MACLCON2 MAPHSUP\n"
+		   "       0x%04x 0x%04x 0x%02x     0x%02x     0x%02x\n",
+		   msg, nolock_regb_read(priv, EREVID),
+		   nolock_regb_read(priv, ECON1), nolock_regb_read(priv, ECON2),
+		   nolock_regb_read(priv, ESTAT), nolock_regb_read(priv, EIR),
+		   nolock_regb_read(priv, EIE), nolock_regb_read(priv, MACON1),
+		   nolock_regb_read(priv, MACON3), nolock_regb_read(priv, MACON4),
+		   nolock_regw_read(priv, ERXSTL), nolock_regw_read(priv, ERXNDL),
+		   nolock_regw_read(priv, ERXWRPTL),
+		   nolock_regw_read(priv, ERXRDPTL),
+		   nolock_regb_read(priv, ERXFCON),
+		   nolock_regb_read(priv, EPKTCNT),
+		   nolock_regw_read(priv, MAMXFLL), nolock_regw_read(priv, ETXSTL),
+		   nolock_regw_read(priv, ETXNDL),
+		   nolock_regb_read(priv, MACLCON1),
+		   nolock_regb_read(priv, MACLCON2),
+		   nolock_regb_read(priv, MAPHSUP));
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * ERXRDPT need to be set always at odd addresses, refer to errata datasheet
+ */
+static u16 erxrdpt_workaround(u16 next_packet_ptr, u16 start, u16 end)
+{
+	u16 erxrdpt;
+
+	if ((next_packet_ptr - 1 < start) || (next_packet_ptr - 1 > end))
+		erxrdpt = end;
+	else
+		erxrdpt = next_packet_ptr - 1;
+
+	return erxrdpt;
+}
+
+/*
+ * Calculate wrap around when reading beyond the end of the RX buffer
+ */
+static u16 rx_packet_start(u16 ptr)
+{
+	if (ptr + RSV_SIZE > RXEND_INIT)
+		return (ptr + RSV_SIZE) - (RXEND_INIT - RXSTART_INIT + 1);
+	else
+		return ptr + RSV_SIZE;
+}
+
+static void nolock_rxfifo_init(struct enc28j60_net *priv, u16 start, u16 end)
+{
+	struct device *dev = &priv->spi->dev;
+	u16 erxrdpt;
+
+	if (start > 0x1FFF || end > 0x1FFF || start > end) {
+		if (netif_msg_drv(priv))
+			dev_err(dev, "%s(%d, %d) RXFIFO bad parameters!\n",
+				__func__, start, end);
+		return;
+	}
+	/* set receive buffer start + end */
+	priv->next_pk_ptr = start;
+	nolock_regw_write(priv, ERXSTL, start);
+	erxrdpt = erxrdpt_workaround(priv->next_pk_ptr, start, end);
+	nolock_regw_write(priv, ERXRDPTL, erxrdpt);
+	nolock_regw_write(priv, ERXNDL, end);
+}
+
+static void nolock_txfifo_init(struct enc28j60_net *priv, u16 start, u16 end)
+{
+	struct device *dev = &priv->spi->dev;
+
+	if (start > 0x1FFF || end > 0x1FFF || start > end) {
+		if (netif_msg_drv(priv))
+			dev_err(dev, "%s(%d, %d) TXFIFO bad parameters!\n",
+				__func__, start, end);
+		return;
+	}
+	/* set transmit buffer start + end */
+	nolock_regw_write(priv, ETXSTL, start);
+	nolock_regw_write(priv, ETXNDL, end);
+}
+
+/*
+ * Low power mode shrinks power consumption about 100x, so we'd like
+ * the chip to be in that mode whenever it's inactive. (However, we
+ * can't stay in low power mode during suspend with WOL active.)
+ */
+static void enc28j60_lowpower(struct enc28j60_net *priv, bool is_low)
+{
+	struct device *dev = &priv->spi->dev;
+
+	if (netif_msg_drv(priv))
+		dev_dbg(dev, "%s power...\n", is_low ? "low" : "high");
+
+	rt_mutex_lock(&priv->lock);
+	if (is_low) {
+		nolock_reg_bfclr(priv, ECON1, ECON1_RXEN);
+		poll_ready(priv, ESTAT, ESTAT_RXBUSY, 0);
+		poll_ready(priv, ECON1, ECON1_TXRTS, 0);
+		/* ECON2_VRPS was set during initialization */
+		nolock_reg_bfset(priv, ECON2, ECON2_PWRSV);
+	} else {
+		nolock_reg_bfclr(priv, ECON2, ECON2_PWRSV);
+		poll_ready(priv, ESTAT, ESTAT_CLKRDY, ESTAT_CLKRDY);
+		/* caller sets ECON1_RXEN */
+	}
+	rt_mutex_unlock(&priv->lock);
+}
+
+static int enc28j60_hw_init(struct enc28j60_net *priv)
+{
+	struct device *dev = &priv->spi->dev;
+	u8 reg;
+
+	if (netif_msg_drv(priv))
+		printk(KERN_DEBUG "%s() - %s\n", __func__,
+			   priv->full_duplex ? "FullDuplex" : "HalfDuplex");
+
+	rt_mutex_lock(&priv->lock);
+	/* first reset the chip */
+	enc28j60_soft_reset(priv);
+	/* Clear ECON1 */
+	spi_write_op(priv, ENC28J60_WRITE_CTRL_REG, ECON1, 0x00);
+	priv->bank = 0;
+	priv->hw_enable = false;
+	priv->tx_retry_count = 0;
+	priv->max_pk_counter = 0;
+	priv->rxfilter = RXFILTER_NORMAL;
+	/* enable address auto increment and voltage regulator powersave */
+	nolock_regb_write(priv, ECON2, ECON2_AUTOINC | ECON2_VRPS);
+
+	nolock_rxfifo_init(priv, RXSTART_INIT, RXEND_INIT);
+	nolock_txfifo_init(priv, TXSTART_INIT, TXEND_INIT);
+	rt_mutex_unlock(&priv->lock);
+
+	/*
+	 * Check the RevID.
+	 * If it's 0x00 or 0xFF probably the enc28j60 is not mounted or
+	 * damaged.
+	 */
+	reg = locked_regb_read(priv, EREVID);
+	if (netif_msg_drv(priv))
+		dev_info(dev, "chip RevID: 0x%02x\n", reg);
+	if (reg == 0x00 || reg == 0xff) {
+		if (netif_msg_drv(priv))
+			printk(KERN_DEBUG "%s() Invalid RevId %d\n",
+				   __func__, reg);
+		return 0;
+	}
+
+	/* default filter mode: (unicast OR broadcast) AND crc valid */
+	locked_regb_write(priv, ERXFCON,
+			    ERXFCON_UCEN | ERXFCON_CRCEN | ERXFCON_BCEN);
+
+	/* enable MAC receive */
+	locked_regb_write(priv, MACON1,
+			    MACON1_MARXEN | MACON1_TXPAUS | MACON1_RXPAUS);
+	/* enable automatic padding and CRC operations */
+	if (priv->full_duplex) {
+		locked_regb_write(priv, MACON3,
+				    MACON3_PADCFG0 | MACON3_TXCRCEN |
+				    MACON3_FRMLNEN | MACON3_FULDPX);
+		/* set inter-frame gap (non-back-to-back) */
+		locked_regb_write(priv, MAIPGL, 0x12);
+		/* set inter-frame gap (back-to-back) */
+		locked_regb_write(priv, MABBIPG, 0x15);
+	} else {
+		locked_regb_write(priv, MACON3,
+				    MACON3_PADCFG0 | MACON3_TXCRCEN |
+				    MACON3_FRMLNEN);
+		locked_regb_write(priv, MACON4, 1 << 6);	/* DEFER bit */
+		/* set inter-frame gap (non-back-to-back) */
+		locked_regw_write(priv, MAIPGL, 0x0C12);
+		/* set inter-frame gap (back-to-back) */
+		locked_regb_write(priv, MABBIPG, 0x12);
+	}
+	/*
+	 * MACLCON1 (default)
+	 * MACLCON2 (default)
+	 * Set the maximum packet size which the controller will accept.
+	 */
+	locked_regw_write(priv, MAMXFLL, MAX_FRAMELEN);
+
+	/* Configure LEDs */
+	if (!enc28j60_phy_write(priv, PHLCON, ENC28J60_LAMPS_MODE))
+		return 0;
+
+	if (priv->full_duplex) {
+		if (!enc28j60_phy_write(priv, PHCON1, PHCON1_PDPXMD))
+			return 0;
+		if (!enc28j60_phy_write(priv, PHCON2, 0x00))
+			return 0;
+	} else {
+		if (!enc28j60_phy_write(priv, PHCON1, 0x00))
+			return 0;
+		if (!enc28j60_phy_write(priv, PHCON2, PHCON2_HDLDIS))
+			return 0;
+	}
+	if (netif_msg_hw(priv))
+		enc28j60_dump_regs(priv, "Hw initialized.");
+
+	return 1;
+}
+
+static void enc28j60_hw_enable(struct enc28j60_net *priv)
+{
+	struct device *dev = &priv->spi->dev;
+
+	/* enable interrupts */
+	if (netif_msg_hw(priv))
+		dev_printk(KERN_DEBUG, dev, "%s() enabling interrupts.\n",
+			   __func__);
+
+	enc28j60_phy_write(priv, PHIE, PHIE_PGEIE | PHIE_PLNKIE);
+
+	rt_mutex_lock(&priv->lock);
+	nolock_reg_bfclr(priv, EIR, EIR_DMAIF | EIR_LINKIF |
+			 EIR_TXIF | EIR_TXERIF | EIR_RXERIF | EIR_PKTIF);
+	nolock_regb_write(priv, EIE, EIE_INTIE | EIE_PKTIE | EIE_LINKIE |
+			  EIE_TXIE | EIE_TXERIE | EIE_RXERIE);
+
+	/* enable receive logic */
+	nolock_reg_bfset(priv, ECON1, ECON1_RXEN);
+	priv->hw_enable = true;
+	rt_mutex_unlock(&priv->lock);
+}
+
+static void enc28j60_hw_disable(struct enc28j60_net *priv)
+{
+	rt_mutex_lock(&priv->lock);
+	/* disable interrupts and packet reception */
+	nolock_regb_write(priv, EIE, 0x00);
+	nolock_reg_bfclr(priv, ECON1, ECON1_RXEN);
+	priv->hw_enable = false;
+	rt_mutex_unlock(&priv->lock);
+}
+#if 0
+static int
+enc28j60_setlink(struct rtnet_device *ndev, u8 autoneg, u16 speed, u8 duplex)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+	int ret = 0;
+
+	if (!priv->hw_enable) {
+		/* link is in low power mode now; duplex setting
+		 * will take effect on next enc28j60_hw_init().
+		 */
+		if (autoneg == AUTONEG_DISABLE && speed == SPEED_10)
+			priv->full_duplex = (duplex == DUPLEX_FULL);
+		else {
+			if (netif_msg_link(priv))
+				printk(KERN_WARNING "%s unsupported link setting\n", __func__);
+			ret = -EOPNOTSUPP;
+		}
+	} else {
+		if (netif_msg_link(priv))
+			printk(KERN_WARNING "%s Warning: hw must be disabled to set link mode\n",
+				   __func__);
+		ret = -EBUSY;
+	}
+	return ret;
+}
+#endif
+
+/*
+ * Read the Transmit Status Vector
+ */
+static void enc28j60_read_tsv(struct enc28j60_net *priv, u8 tsv[TSV_SIZE])
+{
+	struct device *dev = &priv->spi->dev;
+	int endptr;
+
+	endptr = locked_regw_read(priv, ETXNDL);
+	if (netif_msg_hw(priv))
+		dev_printk(KERN_DEBUG, dev, "reading TSV at addr:0x%04x\n",
+			   endptr + 1);
+	enc28j60_mem_read(priv, endptr + 1, TSV_SIZE, tsv);
+}
+
+static void enc28j60_dump_tsv(struct enc28j60_net *priv, const char *msg,
+			      u8 tsv[TSV_SIZE])
+{
+	struct device *dev = &priv->spi->dev;
+	u16 tmp1, tmp2;
+
+	printk(KERN_DEBUG "%s - TSV:\n", msg);
+	tmp1 = tsv[1];
+	tmp1 <<= 8;
+	tmp1 |= tsv[0];
+
+	tmp2 = tsv[5];
+	tmp2 <<= 8;
+	tmp2 |= tsv[4];
+
+	dev_printk(KERN_DEBUG, dev,
+		   "ByteCount: %d, CollisionCount: %d, TotByteOnWire: %d\n",
+		   tmp1, tsv[2] & 0x0f, tmp2);
+	dev_printk(KERN_DEBUG, dev,
+		   "TxDone: %d, CRCErr:%d, LenChkErr: %d, LenOutOfRange: %d\n",
+		   TSV_GETBIT(tsv, TSV_TXDONE),
+		   TSV_GETBIT(tsv, TSV_TXCRCERROR),
+		   TSV_GETBIT(tsv, TSV_TXLENCHKERROR),
+		   TSV_GETBIT(tsv, TSV_TXLENOUTOFRANGE));
+	dev_printk(KERN_DEBUG, dev,
+		   "Multicast: %d, Broadcast: %d, PacketDefer: %d, ExDefer: %d\n",
+		   TSV_GETBIT(tsv, TSV_TXMULTICAST),
+		   TSV_GETBIT(tsv, TSV_TXBROADCAST),
+		   TSV_GETBIT(tsv, TSV_TXPACKETDEFER),
+		   TSV_GETBIT(tsv, TSV_TXEXDEFER));
+	dev_printk(KERN_DEBUG, dev,
+		   "ExCollision: %d, LateCollision: %d, Giant: %d, Underrun: %d\n",
+		   TSV_GETBIT(tsv, TSV_TXEXCOLLISION),
+		   TSV_GETBIT(tsv, TSV_TXLATECOLLISION),
+		   TSV_GETBIT(tsv, TSV_TXGIANT), TSV_GETBIT(tsv, TSV_TXUNDERRUN));
+	dev_printk(KERN_DEBUG, dev,
+		   "ControlFrame: %d, PauseFrame: %d, BackPressApp: %d, VLanTagFrame: %d\n",
+		   TSV_GETBIT(tsv, TSV_TXCONTROLFRAME),
+		   TSV_GETBIT(tsv, TSV_TXPAUSEFRAME),
+		   TSV_GETBIT(tsv, TSV_BACKPRESSUREAPP),
+		   TSV_GETBIT(tsv, TSV_TXVLANTAGFRAME));
+}
+
+/*
+ * Receive Status vector
+ */
+static void enc28j60_dump_rsv(struct enc28j60_net *priv, const char *msg,
+			      u16 pk_ptr, int len, u16 sts)
+{
+	struct device *dev = &priv->spi->dev;
+
+	printk(KERN_DEBUG "%s - NextPk: 0x%04x - RSV:\n", msg, pk_ptr);
+	printk(KERN_DEBUG "ByteCount: %d, DribbleNibble: %d\n",
+		   len, RSV_GETBIT(sts, RSV_DRIBBLENIBBLE));
+	dev_printk(KERN_DEBUG, dev,
+		   "RxOK: %d, CRCErr:%d, LenChkErr: %d, LenOutOfRange: %d\n",
+		   RSV_GETBIT(sts, RSV_RXOK),
+		   RSV_GETBIT(sts, RSV_CRCERROR),
+		   RSV_GETBIT(sts, RSV_LENCHECKERR),
+		   RSV_GETBIT(sts, RSV_LENOUTOFRANGE));
+	dev_printk(KERN_DEBUG, dev,
+		   "Multicast: %d, Broadcast: %d, LongDropEvent: %d, CarrierEvent: %d\n",
+		   RSV_GETBIT(sts, RSV_RXMULTICAST),
+		   RSV_GETBIT(sts, RSV_RXBROADCAST),
+		   RSV_GETBIT(sts, RSV_RXLONGEVDROPEV),
+		   RSV_GETBIT(sts, RSV_CARRIEREV));
+	dev_printk(KERN_DEBUG, dev,
+		   "ControlFrame: %d, PauseFrame: %d, UnknownOp: %d, VLanTagFrame: %d\n",
+		   RSV_GETBIT(sts, RSV_RXCONTROLFRAME),
+		   RSV_GETBIT(sts, RSV_RXPAUSEFRAME),
+		   RSV_GETBIT(sts, RSV_RXUNKNOWNOPCODE),
+		   RSV_GETBIT(sts, RSV_RXTYPEVLAN));
+
+}
+
+static void dump_packet(const char *msg, int len, const char *data)
+{
+	printk(KERN_DEBUG DRV_NAME ": %s - packet len:%d\n", msg, len);
+	print_hex_dump(KERN_DEBUG, "pk data: ", DUMP_PREFIX_OFFSET, 16, 1,
+			data, len, true);
+}
+
+/*
+ * Hardware receive function.
+ * Read the buffer memory, update the FIFO pointer to free the buffer,
+ * check the status vector and decrement the packet counter.
+ */
+static void enc28j60_hw_rx(struct rtnet_device *ndev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+#if 0
+	struct device *dev = &priv->spi->dev;
+#endif
+	struct rtskb *skb = NULL;
+	u16 erxrdpt, next_packet, rxstat;
+	u8 rsv[RSV_SIZE];
+	int len;
+
+	if (netif_msg_rx_status(priv))
+		printk(KERN_DEBUG "%s RX pk_addr:0x%04x\n", 
+			   __func__, priv->next_pk_ptr);
+
+	if (unlikely(priv->next_pk_ptr > RXEND_INIT)) {
+		if (netif_msg_rx_err(priv))
+			printk(KERN_ERR "%s() Invalid packet address!! 0x%04x\n",
+				   __func__, priv->next_pk_ptr);
+		/* packet address corrupted: reset RX logic */
+		rt_mutex_lock(&priv->lock);
+		nolock_reg_bfclr(priv, ECON1, ECON1_RXEN);
+		nolock_reg_bfset(priv, ECON1, ECON1_RXRST);
+		nolock_reg_bfclr(priv, ECON1, ECON1_RXRST);
+		nolock_rxfifo_init(priv, RXSTART_INIT, RXEND_INIT);
+		nolock_reg_bfclr(priv, EIR, EIR_RXERIF);
+		nolock_reg_bfset(priv, ECON1, ECON1_RXEN);
+		rt_mutex_unlock(&priv->lock);
+		priv->stats.rx_errors++;
+		return;
+	}
+	/* Read next packet pointer and rx status vector */
+	enc28j60_mem_read(priv, priv->next_pk_ptr, sizeof(rsv), rsv);
+
+	next_packet = rsv[1];
+	next_packet <<= 8;
+	next_packet |= rsv[0];
+
+	len = rsv[3];
+	len <<= 8;
+	len |= rsv[2];
+
+	rxstat = rsv[5];
+	rxstat <<= 8;
+	rxstat |= rsv[4];
+
+	if (netif_msg_rx_status(priv))
+		enc28j60_dump_rsv(priv, __func__, next_packet, len, rxstat);
+
+	if (!RSV_GETBIT(rxstat, RSV_RXOK) || len > MAX_FRAMELEN) {
+		trace_printk("%s %d rx error: len=%d\n", __func__, __LINE__, len);
+		if (netif_msg_rx_err(priv))
+			printk(KERN_ERR "%s Rx Error (%04x)\n", 
+				   __func__, rxstat);
+		priv->stats.rx_errors++;
+		if (RSV_GETBIT(rxstat, RSV_CRCERROR))
+			priv->stats.rx_crc_errors++;
+		if (RSV_GETBIT(rxstat, RSV_LENCHECKERR))
+			priv->stats.rx_frame_errors++;
+		if (len > MAX_FRAMELEN)
+			priv->stats.rx_over_errors++;
+	} else {
+		skb = rtnetdev_alloc_rtskb(ndev, len + NET_IP_ALIGN);
+		if (!skb) {
+			trace_printk("%s out of memory for Rx'd frame\n", __func__);
+			if (netif_msg_rx_err(priv))
+				printk(KERN_ERR "%s out of memory for Rx'd frame\n", __func__);
+			priv->stats.rx_dropped++;
+		} else {
+			rtskb_reserve(skb, NET_IP_ALIGN);
+			/* copy the packet from the receive buffer */
+			enc28j60_mem_read(priv,
+				rx_packet_start(priv->next_pk_ptr),
+				len, rtskb_put(skb, len));
+			if (netif_msg_pktdata(priv))
+				dump_packet(__func__, skb->len, skb->data);
+			skb->protocol = rt_eth_type_trans(skb, ndev);
+			skb->time_stamp = ktime_get();
+			skb->rtdev = ndev;
+			/* update statistics */
+			priv->stats.rx_packets++;
+			priv->stats.rx_bytes += len;
+			rtnetif_rx(skb);
+		}
+	}
+	/*
+	 * Move the RX read pointer to the start of the next
+	 * received packet.
+	 * This frees the memory we just read out.
+	 */
+	erxrdpt = erxrdpt_workaround(next_packet, RXSTART_INIT, RXEND_INIT);
+	if (netif_msg_hw(priv))
+		printk(KERN_DEBUG "%s() ERXRDPT:0x%04x\n",
+			   __func__, erxrdpt);
+
+	rt_mutex_lock(&priv->lock);
+	nolock_regw_write(priv, ERXRDPTL, erxrdpt);
+#ifdef CONFIG_RTNET_ENC28J60_WRITEVERIFY
+	{
+		u16 reg;
+		reg = nolock_regw_read(priv, ERXRDPTL);
+		if (reg != erxrdpt) {
+			trace_printk("%s() ERXRDPT verify error (0x%04x - 0x%04x)\n",
+			   __func__, reg, erxrdpt);			
+			if (netif_msg_drv(priv))
+				printk(KERN_DEBUG
+			   		"%s() ERXRDPT verify error (0x%04x - 0x%04x)\n",
+			   		__func__, reg, erxrdpt);
+		}
+	}
+#endif
+	priv->next_pk_ptr = next_packet;
+	/* we are done with this packet, decrement the packet counter */
+	nolock_reg_bfset(priv, ECON2, ECON2_PKTDEC);
+	rt_mutex_unlock(&priv->lock);
+}
+
+/*
+ * Calculate free space in RxFIFO
+ */
+static int enc28j60_get_free_rxfifo(struct enc28j60_net *priv)
+{
+	int epkcnt, erxst, erxnd, erxwr, erxrd;
+	int free_space;
+
+	rt_mutex_lock(&priv->lock);
+	epkcnt = nolock_regb_read(priv, EPKTCNT);
+	if (epkcnt >= 255)
+		free_space = -1;
+	else {
+		erxst = nolock_regw_read(priv, ERXSTL);
+		erxnd = nolock_regw_read(priv, ERXNDL);
+		erxwr = nolock_regw_read(priv, ERXWRPTL);
+		erxrd = nolock_regw_read(priv, ERXRDPTL);
+
+		if (erxwr > erxrd)
+			free_space = (erxnd - erxst) - (erxwr - erxrd);
+		else if (erxwr == erxrd)
+			free_space = (erxnd - erxst);
+		else
+			free_space = erxrd - erxwr - 1;
+	}
+	rt_mutex_unlock(&priv->lock);
+	if (netif_msg_rx_status(priv))
+		printk(KERN_DEBUG "%s() free_space = %d\n",
+			      __func__, free_space);
+	return free_space;
+}
+
+/*
+ * Access the PHY to determine link status
+ */
+static void enc28j60_check_link_status(struct rtnet_device *ndev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+	struct device *dev = &priv->spi->dev;
+	u16 reg;
+	int duplex;
+
+	reg = enc28j60_phy_read(priv, PHSTAT2);
+	if (netif_msg_hw(priv))
+		dev_printk(KERN_DEBUG, dev,
+			   "%s() PHSTAT1: %04x, PHSTAT2: %04x\n", __func__,
+			   enc28j60_phy_read(priv, PHSTAT1), reg);
+	duplex = reg & PHSTAT2_DPXSTAT;
+
+	if (reg & PHSTAT2_LSTAT) {
+		rtnetif_carrier_on(ndev);
+		if (netif_msg_ifup(priv))
+			printk(KERN_INFO "%s link up - %s\n", __func__,
+				    duplex ? "Full duplex" : "Half duplex");
+	} else {
+		if (netif_msg_ifdown(priv))
+			printk(KERN_INFO "%s link down\n", __func__);
+		rtnetif_carrier_off(ndev);
+	}
+}
+
+static void enc28j60_tx_clear(struct rtnet_device *ndev, bool err)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+
+	if (err)
+		priv->stats.tx_errors++;
+	else
+		priv->stats.tx_packets++;
+
+	if (priv->tx_skb) {
+		if (!err)
+			priv->stats.tx_bytes += priv->tx_skb->len;
+		dev_kfree_rtskb(priv->tx_skb);
+		priv->tx_skb = NULL;
+	}
+	locked_reg_bfclr(priv, ECON1, ECON1_TXRTS);
+	rtnetif_wake_queue(ndev);
+}
+
+/*
+ * RX handler
+ * Ignore PKTIF because is unreliable! (Look at the errata datasheet)
+ * Check EPKTCNT is the suggested workaround.
+ * We don't need to clear interrupt flag, automatically done when
+ * enc28j60_hw_rx() decrements the packet counter.
+ * Returns how many packet processed.
+ */
+static int enc28j60_rx_interrupt(struct rtnet_device *ndev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+	int pk_counter, ret;
+	int old_packet_cnt;
+
+	pk_counter = locked_regb_read(priv, EPKTCNT);
+	if (pk_counter && netif_msg_intr(priv))
+		printk(KERN_DEBUG "%s intRX, pk_cnt: %d\n", __func__,
+			      pk_counter);
+	if (pk_counter > priv->max_pk_counter) {
+		/* update statistics */
+		priv->max_pk_counter = pk_counter;
+		if (netif_msg_rx_status(priv) && priv->max_pk_counter > 1)
+			printk(KERN_DEBUG "%s RX max_pk_cnt: %d\n", __func__,
+				      priv->max_pk_counter);
+	}
+	ret = pk_counter;
+	old_packet_cnt = priv->stats.rx_packets;
+	while (pk_counter-- > 0)
+		enc28j60_hw_rx(ndev);
+	if (old_packet_cnt != priv->stats.rx_packets) {
+        rt_mark_stack_mgr(ndev);
+	}
+	
+	return ret;
+}
+
+static int enc28j60_irq_work_handler(void *vpriv)
+{
+	struct enc28j60_net *priv = vpriv;
+	struct rtnet_device *ndev = priv->netdev;
+	int intflags, loop;
+
+    while (!stop_rx_task) {
+        if (rtdm_event_wait_one(&priv->rx_event) < 0) {
+			printk(KERN_ERR "%s rtdm_event_wait_one error\n", __func__);
+            break;
+		}
+
+		if(stop_rx_task)
+			break;
+		
+		/* disable further interrupts */
+		locked_reg_bfclr(priv, EIE, EIE_INTIE);
+
+		do {
+			loop = 0;
+			intflags = locked_regb_read(priv, EIR);
+			/* DMA interrupt handler (not currently used) */
+			if ((intflags & EIR_DMAIF) != 0) {
+				loop++;
+				if (netif_msg_intr(priv))
+					printk(KERN_DEBUG "%s intDMA(%d)\n", __func__,
+							  loop);
+				locked_reg_bfclr(priv, EIR, EIR_DMAIF);
+			}
+			/* LINK changed handler */
+			if ((intflags & EIR_LINKIF) != 0) {
+				loop++;
+				if (netif_msg_intr(priv))
+					printk(KERN_DEBUG "%s intLINK(%d)\n", __func__,
+							  loop);
+				enc28j60_check_link_status(ndev);
+				/* read PHIR to clear the flag */
+				enc28j60_phy_read(priv, PHIR);
+			}
+			/* TX complete handler */
+			if (((intflags & EIR_TXIF) != 0) &&
+				((intflags & EIR_TXERIF) == 0)) {
+				bool err = false;
+				loop++;
+				if (netif_msg_intr(priv))
+					printk(KERN_DEBUG "%s intTX(%d)\n", __func__,
+							  loop);
+				priv->tx_retry_count = 0;
+				if (locked_regb_read(priv, ESTAT) & ESTAT_TXABRT) {
+					if (netif_msg_tx_err(priv))
+						printk(KERN_WARNING "%s Tx Error (aborted)\n", __func__);
+					err = true;
+				}
+				if (netif_msg_tx_done(priv)) {
+					u8 tsv[TSV_SIZE];
+					enc28j60_read_tsv(priv, tsv);
+					enc28j60_dump_tsv(priv, "Tx Done", tsv);
+				}
+				enc28j60_tx_clear(ndev, err);
+				locked_reg_bfclr(priv, EIR, EIR_TXIF);
+			}
+			/* TX Error handler */
+			if ((intflags & EIR_TXERIF) != 0) {
+				u8 tsv[TSV_SIZE];
+
+				loop++;
+				if (netif_msg_intr(priv))
+					printk(KERN_DEBUG "%s intTXErr(%d)\n",  __func__,
+							  loop);
+				locked_reg_bfclr(priv, ECON1, ECON1_TXRTS);
+				enc28j60_read_tsv(priv, tsv);
+				if (netif_msg_tx_err(priv))
+					enc28j60_dump_tsv(priv, "Tx Error", tsv);
+				/* Reset TX logic */
+				rt_mutex_lock(&priv->lock);
+				nolock_reg_bfset(priv, ECON1, ECON1_TXRST);
+				nolock_reg_bfclr(priv, ECON1, ECON1_TXRST);
+				nolock_txfifo_init(priv, TXSTART_INIT, TXEND_INIT);
+				rt_mutex_unlock(&priv->lock);
+				/* Transmit Late collision check for retransmit */
+				if (TSV_GETBIT(tsv, TSV_TXLATECOLLISION)) {
+					if (netif_msg_tx_err(priv))
+						printk(KERN_DEBUG 
+								  "%s LateCollision TXErr (%d)\n", __func__,
+								  priv->tx_retry_count);
+					if (priv->tx_retry_count++ < MAX_TX_RETRYCOUNT)
+						locked_reg_bfset(priv, ECON1,
+								   ECON1_TXRTS);
+					else
+						enc28j60_tx_clear(ndev, true);
+				} else
+					enc28j60_tx_clear(ndev, true);
+				locked_reg_bfclr(priv, EIR, EIR_TXERIF | EIR_TXIF);
+			}
+			/* RX Error handler */
+			if ((intflags & EIR_RXERIF) != 0) {
+				loop++;
+				if (netif_msg_intr(priv))
+					printk(KERN_DEBUG "%s intRXErr(%d)\n", __func__,
+							  loop);
+				/* Check free FIFO space to flag RX overrun */
+				if (enc28j60_get_free_rxfifo(priv) <= 0) {
+					if (netif_msg_rx_err(priv))
+						printk(KERN_DEBUG "%s RX Overrun\n", __func__);
+					priv->stats.rx_dropped++;
+				}
+				locked_reg_bfclr(priv, EIR, EIR_RXERIF);
+			}
+			/* RX handler */
+			if (enc28j60_rx_interrupt(ndev))
+				loop++;
+		} while (loop);
+
+		/* re-enable interrupts */
+		locked_reg_bfset(priv, EIE, EIE_INTIE);
+	}
+	
+	do_exit(0);
+    return 0;
+}
+
+/*
+ * Hardware transmit function.
+ * Fill the buffer memory and send the contents of the transmit buffer
+ * onto the network
+ */
+static void enc28j60_hw_tx(struct enc28j60_net *priv)
+{
+	BUG_ON(!priv->tx_skb);
+
+	if (netif_msg_tx_queued(priv))
+		printk(KERN_DEBUG "%s Tx Packet Len:%d\n", __func__,
+			      priv->tx_skb->len);
+
+	if (netif_msg_pktdata(priv))
+		dump_packet(__func__,
+			    priv->tx_skb->len, priv->tx_skb->data);
+      
+	/* get and patch time stamp just before the transmission */
+	 /* xmit_stamp is used in tdma */
+     if (priv->tx_skb->xmit_stamp)
+		  *priv->tx_skb->xmit_stamp = cpu_to_be64(ktime_get() + *priv->tx_skb->xmit_stamp);
+	enc28j60_packet_write(priv, priv->tx_skb->len, priv->tx_skb->data);
+
+#ifdef CONFIG_RTNET_ENC28J60_WRITEVERIFY
+	/* readback and verify written data */
+	if (netif_msg_drv(priv)) {
+		struct device *dev = &priv->spi->dev;
+		int test_len, k;
+		u8 test_buf[64]; /* limit the test to the first 64 bytes */
+		int okflag;
+
+		test_len = priv->tx_skb->len;
+		if (test_len > sizeof(test_buf))
+			test_len = sizeof(test_buf);
+
+		/* + 1 to skip control byte */
+		enc28j60_mem_read(priv, TXSTART_INIT + 1, test_len, test_buf);
+		okflag = 1;
+		for (k = 0; k < test_len; k++) {
+			if (priv->tx_skb->data[k] != test_buf[k]) {
+				dev_printk(KERN_DEBUG, dev,
+					   "%s Error, %d location differ: 0x%02x-0x%02x\n",
+					   __func__, k, priv->tx_skb->data[k], test_buf[k]);
+				okflag = 0;
+			}
+		}
+		if (!okflag)
+			printk(KERN_DEBUG "Tx write buffer, verify ERROR!\n");
+	}
+#endif
+	/* set TX request flag */
+	locked_reg_bfset(priv, ECON1, ECON1_TXRTS);
+}
+
+static netdev_tx_t enc28j60_send_packet(struct rtskb *skb,
+					struct rtnet_device *dev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+	int ret = NETDEV_TX_OK;
+	
+	/* If some error occurs while trying to transmit this
+	 * packet, you should return '1' from this function.
+	 * In such a case you _may not_ do anything to the
+	 * SKB, it is still owned by the network queueing
+	 * layer when an error is returned. This means you
+	 * may not modify any SKB fields, you may not free
+	 * the SKB, etc.
+	 */
+	rtnetif_stop_queue(dev);
+
+	/* Remember the skb for deferred processing */
+	priv->tx_skb = skb;
+    rtdm_event_signal_one(&priv->tx_event);
+	
+	return ret;
+}
+
+static int enc28j60_tx_send_packets(void *vpriv)
+{
+	struct enc28j60_net *priv = vpriv;
+	
+    while (!stop_tx_task) {
+        if (rtdm_event_wait_one(&priv->tx_event) < 0) {
+			printk(KERN_ERR "%s rtdm_event_wait_one error\n", __func__);
+            break;
+		}
+
+		if(stop_tx_task)
+			break;
+		
+		/* actual delivery of data */
+		enc28j60_hw_tx(priv);
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+static irqreturn_t enc28j60_irq(int irq, void *dev_id)
+{
+	struct enc28j60_net *priv = dev_id;
+	int ret = IRQ_HANDLED;
+	
+	/*
+	 * Can't do anything in interrupt context because we need to
+	 * block (spi_sync() is blocking) so fire of the interrupt
+	 * handling workqueue.
+	 * Remember that we access enc28j60 registers through SPI bus
+	 * via spi_sync() call.
+	 */
+    rtdm_event_signal_one(&priv->rx_event);
+
+	return ret;
+}
+
+#if 0
+static void enc28j60_tx_timeout(struct rtnet_device *ndev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(ndev);
+
+	if (netif_msg_timer(priv))
+		printk(KERN_WARNING "%s tx timeout\n", __func__);
+
+	priv->stats.tx_errors++;
+	/* can't restart safely under softirq */
+	schedule_work(&priv->restart_work);
+}
+#endif
+
+/*
+ * Open/initialize the board. This is called (in the current kernel)
+ * sometime after booting when the 'ifconfig' program is run.
+ *
+ * This routine should set everything up anew at each open, even
+ * registers that "should" only need to be set once at boot, so that
+ * there is non-reboot way to recover if something goes wrong.
+ */
+static int enc28j60_net_open(struct rtnet_device *dev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+
+	if (!is_valid_ether_addr(dev->dev_addr)) {
+		if (netif_msg_ifup(priv))
+			printk(KERN_WARNING "%s invalid MAC address %pM\n", 
+				   __func__, dev->dev_addr);
+		return -EADDRNOTAVAIL;
+	}
+	/* Reset the hardware here (and take it out of low power mode) */
+	enc28j60_lowpower(priv, false);
+	enc28j60_hw_disable(priv);
+	if (!enc28j60_hw_init(priv)) {
+		if (netif_msg_ifup(priv))
+			printk(KERN_WARNING "%s hw_reset() failed\n", __func__);
+		return -EINVAL;
+	}
+	/* Update the MAC address (in case user has changed it) */
+	enc28j60_set_hw_macaddr(dev);
+	/* Enable interrupts */
+	enc28j60_hw_enable(priv);
+	/* check link status */
+	enc28j60_check_link_status(dev);
+	/* We are now ready to accept transmit requests from
+	 * the queueing layer of the networking.
+	 */
+	rtnetif_start_queue(dev);
+
+	return 0;
+}
+
+/* The inverse routine to net_open(). */
+static int enc28j60_net_close(struct rtnet_device *dev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+
+	enc28j60_hw_disable(priv);
+	enc28j60_lowpower(priv, true);
+	rtnetif_stop_queue(dev);
+
+	return 0;
+}
+
+#if 0
+/*
+ * Set or clear the multicast filter for this adapter
+ * num_addrs == -1	Promiscuous mode, receive all packets
+ * num_addrs == 0	Normal mode, filter out multicast packets
+ * num_addrs > 0	Multicast mode, receive normal and MC packets
+ */
+static void enc28j60_set_multicast_list(struct rtnet_device *dev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+	int oldfilter = priv->rxfilter;
+
+	if (dev->flags & IFF_PROMISC) {
+		if (netif_msg_link(priv))
+			printk(KERN_DEBUG "%s promiscuous mode\n", __func__);
+		priv->rxfilter = RXFILTER_PROMISC;
+	} else if ((dev->flags & IFF_ALLMULTI)) { // || !netdev_mc_empty(dev)) {
+		if (netif_msg_link(priv))
+			printk(KERN_DEBUG "%s %smulticast mode\n", __func__,
+				    (dev->flags & IFF_ALLMULTI) ? "all-" : "");
+		priv->rxfilter = RXFILTER_MULTI;
+	} else {
+		if (netif_msg_link(priv))
+			printk(KERN_DEBUG "%s normal mode\n", __func__);
+		priv->rxfilter = RXFILTER_NORMAL;
+	}
+
+	if (oldfilter != priv->rxfilter)
+		schedule_work(&priv->setrx_work);
+}
+#endif
+
+static void enc28j60_setrx_work_handler(struct work_struct *work)
+{
+	struct enc28j60_net *priv =
+		container_of(work, struct enc28j60_net, setrx_work);
+	struct device *dev = &priv->spi->dev;
+
+	if (priv->rxfilter == RXFILTER_PROMISC) {
+		if (netif_msg_drv(priv))
+			dev_printk(KERN_DEBUG, dev, "promiscuous mode\n");
+		locked_regb_write(priv, ERXFCON, 0x00);
+	} else if (priv->rxfilter == RXFILTER_MULTI) {
+		if (netif_msg_drv(priv))
+			dev_printk(KERN_DEBUG, dev, "multicast mode\n");
+		locked_regb_write(priv, ERXFCON,
+					ERXFCON_UCEN | ERXFCON_CRCEN |
+					ERXFCON_BCEN | ERXFCON_MCEN);
+	} else {
+		if (netif_msg_drv(priv))
+			dev_printk(KERN_DEBUG, dev, "normal mode\n");
+		locked_regb_write(priv, ERXFCON,
+					ERXFCON_UCEN | ERXFCON_CRCEN |
+					ERXFCON_BCEN);
+	}
+}
+
+static void enc28j60_restart_work_handler(struct work_struct *work)
+{
+	struct enc28j60_net *priv =
+			container_of(work, struct enc28j60_net, restart_work);
+	struct rtnet_device *ndev = priv->netdev;
+	int ret;
+
+	rtnl_lock();
+	if (rtnetif_running(ndev)) {
+		enc28j60_net_close(ndev);
+		ret = enc28j60_net_open(ndev);
+		if (unlikely(ret)) {
+			printk(KERN_WARNING "%s could not restart %d\n", 
+				   __func__, ret);
+			rtdev_close(ndev);
+		}
+	}
+	rtnl_unlock();
+}
+
+/* ......................... ETHTOOL SUPPORT ........................... */
+
+#if 0
+static void
+enc28j60_get_drvinfo(struct rtnet_device *dev, struct ethtool_drvinfo *info)
+{
+	strlcpy(info->driver, DRV_NAME, sizeof(info->driver));
+	strlcpy(info->version, DRV_VERSION, sizeof(info->version));
+	strlcpy(info->bus_info,
+		dev_name(dev->dev.parent), sizeof(info->bus_info));
+}
+
+static int
+enc28j60_get_link_ksettings(struct rtnet_device *dev,
+			    struct ethtool_link_ksettings *cmd)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+
+	ethtool_link_ksettings_zero_link_mode(cmd, supported);
+	ethtool_link_ksettings_add_link_mode(cmd, supported, 10baseT_Half);
+	ethtool_link_ksettings_add_link_mode(cmd, supported, 10baseT_Full);
+	ethtool_link_ksettings_add_link_mode(cmd, supported, TP);
+
+	cmd->base.speed = SPEED_10;
+	cmd->base.duplex = priv->full_duplex ? DUPLEX_FULL : DUPLEX_HALF;
+	cmd->base.port	= PORT_TP;
+	cmd->base.autoneg = AUTONEG_DISABLE;
+
+	return 0;
+}
+
+static int
+enc28j60_set_link_ksettings(struct rtnet_device *dev,
+			    const struct ethtool_link_ksettings *cmd)
+{
+	return enc28j60_setlink(dev, cmd->base.autoneg,
+				cmd->base.speed, cmd->base.duplex);
+}
+
+static u32 enc28j60_get_msglevel(struct rtnet_device *dev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+	return priv->msg_enable;
+}
+
+static void enc28j60_set_msglevel(struct rtnet_device *dev, u32 val)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+	priv->msg_enable = val;
+}
+
+static const struct ethtool_ops enc28j60_ethtool_ops = {
+	.get_drvinfo	= enc28j60_get_drvinfo,
+	.get_msglevel	= enc28j60_get_msglevel,
+	.set_msglevel	= enc28j60_set_msglevel,
+	.get_link_ksettings = enc28j60_get_link_ksettings,
+	.set_link_ksettings = enc28j60_set_link_ksettings,
+};
+#endif
+
+static int enc28j60_chipset_init(struct rtnet_device *dev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(dev);
+
+	return enc28j60_hw_init(priv);
+}
+
+static struct net_device_stats* enc28j60_get_stats(struct rtnet_device *rtdev)
+{
+	struct enc28j60_net *priv = rtnetdev_priv(rtdev);
+	return &priv->stats;
+}
+
+#if 0
+static const struct rtnet_device_ops enc28j60_netdev_ops = {
+	.ndo_open		= enc28j60_net_open,
+	.ndo_stop		= enc28j60_net_close,
+	.ndo_start_xmit		= enc28j60_send_packet,
+	.ndo_set_rx_mode	= enc28j60_set_multicast_list,
+	.ndo_set_mac_address	= enc28j60_set_mac_address,
+	.ndo_tx_timeout		= enc28j60_tx_timeout,
+	.ndo_validate_addr	= eth_validate_addr,
+};
+#endif
+
+static int enc28j60_probe(struct spi_device *spi)
+{
+	unsigned char macaddr[ETH_ALEN];
+	struct rtnet_device *dev;
+	struct enc28j60_net *priv;
+	int ret = 0;
+	/* soft interrupts have priority MAX_RT_PRIO / 2 in preempt_rt */
+	struct sched_param tx_schedule_param = 
+		{ .sched_priority = RTNET_TX_THREAD_RT_PRIO };
+	struct sched_param rx_schedule_param = 
+		{ .sched_priority = RTNET_RX_THREAD_RT_PRIO };
+	
+	if (netif_msg_drv(&debug))
+		dev_info(&spi->dev, "Ethernet driver %s loaded\n", DRV_VERSION);
+
+    dev = rt_alloc_etherdev(sizeof (struct enc28j60_net), enc28j60_rtskb_pool_size);
+    if (dev == NULL) {
+        ret = -ENOMEM;
+		goto error_alloc;
+	}
+    rtdev_alloc_name(dev, "rteth%d");
+    rt_rtdev_connect(dev, &RTDEV_manager);
+    dev->vers = RTDEV_VERS_2_0;
+	priv = rtnetdev_priv(dev);
+
+	/* The Rtl8139-specific entries in the device structure. */
+	dev->open = enc28j60_net_open;
+	dev->stop = enc28j60_net_close;
+	dev->hard_header = &rt_eth_header;
+	dev->hard_start_xmit = enc28j60_send_packet;
+	dev->start_xmit = dev->hard_start_xmit;
+	dev->get_stats = enc28j60_get_stats;
+	
+	priv->netdev = dev;	/* priv to netdev reference */
+	priv->spi = spi;	/* priv to spi reference */
+	priv->msg_enable = netif_msg_init(debug.msg_enable, ENC28J60_MSG_DEFAULT);
+	rt_mutex_init(&priv->lock);
+	INIT_WORK(&priv->setrx_work, enc28j60_setrx_work_handler);
+	INIT_WORK(&priv->restart_work, enc28j60_restart_work_handler);
+	spi_set_drvdata(spi, priv);	/* spi to priv reference */
+#if 0
+	SET_NETDEV_DEV(dev, &spi->dev);
+#endif
+	if (!enc28j60_chipset_init(dev)) {
+		if (netif_msg_probe(priv))
+			printk(KERN_WARNING "%s chip not found\n", __func__);
+		ret = -EIO;
+		goto error_irq;
+	}
+
+	if (device_get_mac_address(&spi->dev, macaddr, sizeof(macaddr)))
+		ether_addr_copy(dev->dev_addr, macaddr);
+	else
+		eth_random_addr((u8 *) dev->dev_addr);
+	enc28j60_set_hw_macaddr(dev);
+
+	/* Board setup must set the relevant edge trigger type;
+	 * level triggers won't currently work.
+	 */
+	ret = request_irq(spi->irq, enc28j60_irq, IRQF_NO_THREAD, DRV_NAME, priv);
+	if (ret < 0) {
+		if (netif_msg_probe(priv))
+			dev_err(&spi->dev, "request irq %d failed (ret = %d)\n",
+				spi->irq, ret);
+		goto error_irq;
+	}
+	rt_stack_connect(dev, &STACK_manager);
+	
+	dev->if_port = IF_PORT_10BASET;
+	dev->irq = spi->irq;
+#if 0
+	dev->netdev_ops = &enc28j60_netdev_ops;
+	dev->watchdog_timeo = TX_TIMEOUT;
+	dev->ethtool_ops = &enc28j60_ethtool_ops;
+#endif
+
+	enc28j60_lowpower(priv, true);
+
+	ret = rt_register_rtnetdev(dev);
+	if (ret) {
+		if (netif_msg_probe(priv))
+			dev_err(&spi->dev, "register netdev failed (ret = %d)\n",
+				ret);
+		goto error_register;
+	}
+
+#if 0
+    if (rtskb_pool_init(&priv->skb_pool, enc28j60_rtskb_pool_size)
+			< enc28j60_rtskb_pool_size) {
+        ret = -ENOMEM;
+		goto error_register;
+    }
+#endif
+	
+	/* setup the rx task */
+	rtdm_event_init(&priv->rx_event, 0);
+    stop_rx_task = 0;
+    priv->rx_task = kthread_create(enc28j60_irq_work_handler, priv, "enc28j60-rx_task");
+    if (!priv->rx_task) {
+		ret = -ENOMEM;
+		goto error_register;
+	}
+    sched_setscheduler(priv->rx_task, SCHED_FIFO, &rx_schedule_param);
+    wake_up_process(priv->rx_task);
+
+	/* setup the tx task */
+	rtdm_event_init(&priv->tx_event, 0);
+    stop_tx_task = 0;
+    priv->tx_task = kthread_create(enc28j60_tx_send_packets, priv, "enc28j60-tx_task");
+    if (!priv->tx_task) {
+		ret = -ENOMEM;
+		goto error_register;
+	}
+    sched_setscheduler(priv->tx_task, SCHED_FIFO, &tx_schedule_param);
+    wake_up_process(priv->tx_task);
+	
+	return 0;
+
+error_register:
+#if 0
+	rtskb_pool_release(&priv->skb_pool);
+#endif
+	rt_unregister_rtnetdev(priv->netdev);
+	free_irq(spi->irq, priv);
+error_irq:
+	rtdev_free(dev);
+error_alloc:
+	return ret;
+}
+
+static int enc28j60_remove(struct spi_device *spi)
+{
+	struct enc28j60_net *priv = spi_get_drvdata(spi);
+
+#if 0
+	stop_rx_task = 1;
+	stop_tx_task = 1;
+	wmb();
+	rtdm_event_signal_one(&rx_event);
+	rtdm_event_signal_one(&tx_event);
+#endif	
+	rt_unregister_rtnetdev(priv->netdev);
+	free_irq(spi->irq, priv);
+#if 0
+	rtskb_pool_release(&priv->skb_pool);
+#endif
+	rt_rtdev_disconnect(priv->netdev);
+	rtdev_free(priv->netdev);
+
+	return 0;
+}
+
+static const struct of_device_id enc28j60_dt_ids[] = {
+	{ .compatible = "microchip,enc28j60" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, enc28j60_dt_ids);
+
+static struct spi_driver enc28j60_driver = {
+	.driver = {
+		.name = DRV_NAME,
+		.of_match_table = enc28j60_dt_ids,
+	 },
+	.probe = enc28j60_probe,
+	.remove = enc28j60_remove,
+};
+module_spi_driver(enc28j60_driver);
+
+MODULE_DESCRIPTION(DRV_NAME " ethernet driver");
+MODULE_AUTHOR("Claudio Lanconelli <lanconelli.claudio@eptar.com>");
+MODULE_LICENSE("GPL");
+module_param_named(debug, debug.msg_enable, int, 0);
+MODULE_PARM_DESC(debug, "Debug verbosity level in amount of bits set (0=none, ..., 31=all)");
+MODULE_ALIAS("spi:" DRV_NAME);
diff -Naur a/net/rtnet/drivers/microchip/enc28j60_hw.h b/net/rtnet/drivers/microchip/enc28j60_hw.h
--- a/net/rtnet/drivers/microchip/enc28j60_hw.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/microchip/enc28j60_hw.h	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,310 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * enc28j60_hw.h: EDTP FrameThrower style enc28j60 registers
+ *
+ * $Id: enc28j60_hw.h,v 1.9 2007/12/14 11:59:16 claudio Exp $
+ */
+
+#ifndef _ENC28J60_HW_H
+#define _ENC28J60_HW_H
+
+/*
+ * ENC28J60 Control Registers
+ * Control register definitions are a combination of address,
+ * bank number, and Ethernet/MAC/PHY indicator bits.
+ * - Register address	(bits 0-4)
+ * - Bank number	(bits 5-6)
+ * - MAC/MII indicator	(bit 7)
+ */
+#define ADDR_MASK	0x1F
+#define BANK_MASK	0x60
+#define SPRD_MASK	0x80
+/* All-bank registers */
+#define EIE		0x1B
+#define EIR		0x1C
+#define ESTAT		0x1D
+#define ECON2		0x1E
+#define ECON1		0x1F
+/* Bank 0 registers */
+#define ERDPTL		(0x00|0x00)
+#define ERDPTH		(0x01|0x00)
+#define EWRPTL		(0x02|0x00)
+#define EWRPTH		(0x03|0x00)
+#define ETXSTL		(0x04|0x00)
+#define ETXSTH		(0x05|0x00)
+#define ETXNDL		(0x06|0x00)
+#define ETXNDH		(0x07|0x00)
+#define ERXSTL		(0x08|0x00)
+#define ERXSTH		(0x09|0x00)
+#define ERXNDL		(0x0A|0x00)
+#define ERXNDH		(0x0B|0x00)
+#define ERXRDPTL	(0x0C|0x00)
+#define ERXRDPTH	(0x0D|0x00)
+#define ERXWRPTL	(0x0E|0x00)
+#define ERXWRPTH	(0x0F|0x00)
+#define EDMASTL		(0x10|0x00)
+#define EDMASTH		(0x11|0x00)
+#define EDMANDL		(0x12|0x00)
+#define EDMANDH		(0x13|0x00)
+#define EDMADSTL	(0x14|0x00)
+#define EDMADSTH	(0x15|0x00)
+#define EDMACSL		(0x16|0x00)
+#define EDMACSH		(0x17|0x00)
+/* Bank 1 registers */
+#define EHT0		(0x00|0x20)
+#define EHT1		(0x01|0x20)
+#define EHT2		(0x02|0x20)
+#define EHT3		(0x03|0x20)
+#define EHT4		(0x04|0x20)
+#define EHT5		(0x05|0x20)
+#define EHT6		(0x06|0x20)
+#define EHT7		(0x07|0x20)
+#define EPMM0		(0x08|0x20)
+#define EPMM1		(0x09|0x20)
+#define EPMM2		(0x0A|0x20)
+#define EPMM3		(0x0B|0x20)
+#define EPMM4		(0x0C|0x20)
+#define EPMM5		(0x0D|0x20)
+#define EPMM6		(0x0E|0x20)
+#define EPMM7		(0x0F|0x20)
+#define EPMCSL		(0x10|0x20)
+#define EPMCSH		(0x11|0x20)
+#define EPMOL		(0x14|0x20)
+#define EPMOH		(0x15|0x20)
+#define EWOLIE		(0x16|0x20)
+#define EWOLIR		(0x17|0x20)
+#define ERXFCON		(0x18|0x20)
+#define EPKTCNT		(0x19|0x20)
+/* Bank 2 registers */
+#define MACON1		(0x00|0x40|SPRD_MASK)
+/* #define MACON2	(0x01|0x40|SPRD_MASK) */
+#define MACON3		(0x02|0x40|SPRD_MASK)
+#define MACON4		(0x03|0x40|SPRD_MASK)
+#define MABBIPG		(0x04|0x40|SPRD_MASK)
+#define MAIPGL		(0x06|0x40|SPRD_MASK)
+#define MAIPGH		(0x07|0x40|SPRD_MASK)
+#define MACLCON1	(0x08|0x40|SPRD_MASK)
+#define MACLCON2	(0x09|0x40|SPRD_MASK)
+#define MAMXFLL		(0x0A|0x40|SPRD_MASK)
+#define MAMXFLH		(0x0B|0x40|SPRD_MASK)
+#define MAPHSUP		(0x0D|0x40|SPRD_MASK)
+#define MICON		(0x11|0x40|SPRD_MASK)
+#define MICMD		(0x12|0x40|SPRD_MASK)
+#define MIREGADR	(0x14|0x40|SPRD_MASK)
+#define MIWRL		(0x16|0x40|SPRD_MASK)
+#define MIWRH		(0x17|0x40|SPRD_MASK)
+#define MIRDL		(0x18|0x40|SPRD_MASK)
+#define MIRDH		(0x19|0x40|SPRD_MASK)
+/* Bank 3 registers */
+#define MAADR1		(0x00|0x60|SPRD_MASK)
+#define MAADR0		(0x01|0x60|SPRD_MASK)
+#define MAADR3		(0x02|0x60|SPRD_MASK)
+#define MAADR2		(0x03|0x60|SPRD_MASK)
+#define MAADR5		(0x04|0x60|SPRD_MASK)
+#define MAADR4		(0x05|0x60|SPRD_MASK)
+#define EBSTSD		(0x06|0x60)
+#define EBSTCON		(0x07|0x60)
+#define EBSTCSL		(0x08|0x60)
+#define EBSTCSH		(0x09|0x60)
+#define MISTAT		(0x0A|0x60|SPRD_MASK)
+#define EREVID		(0x12|0x60)
+#define ECOCON		(0x15|0x60)
+#define EFLOCON		(0x17|0x60)
+#define EPAUSL		(0x18|0x60)
+#define EPAUSH		(0x19|0x60)
+/* PHY registers */
+#define PHCON1		0x00
+#define PHSTAT1		0x01
+#define PHHID1		0x02
+#define PHHID2		0x03
+#define PHCON2		0x10
+#define PHSTAT2		0x11
+#define PHIE		0x12
+#define PHIR		0x13
+#define PHLCON		0x14
+
+/* ENC28J60 EIE Register Bit Definitions */
+#define EIE_INTIE	0x80
+#define EIE_PKTIE	0x40
+#define EIE_DMAIE	0x20
+#define EIE_LINKIE	0x10
+#define EIE_TXIE	0x08
+/* #define EIE_WOLIE	0x04 (reserved) */
+#define EIE_TXERIE	0x02
+#define EIE_RXERIE	0x01
+/* ENC28J60 EIR Register Bit Definitions */
+#define EIR_PKTIF	0x40
+#define EIR_DMAIF	0x20
+#define EIR_LINKIF	0x10
+#define EIR_TXIF	0x08
+/* #define EIR_WOLIF	0x04 (reserved) */
+#define EIR_TXERIF	0x02
+#define EIR_RXERIF	0x01
+/* ENC28J60 ESTAT Register Bit Definitions */
+#define ESTAT_INT	0x80
+#define ESTAT_LATECOL	0x10
+#define ESTAT_RXBUSY	0x04
+#define ESTAT_TXABRT	0x02
+#define ESTAT_CLKRDY	0x01
+/* ENC28J60 ECON2 Register Bit Definitions */
+#define ECON2_AUTOINC	0x80
+#define ECON2_PKTDEC	0x40
+#define ECON2_PWRSV	0x20
+#define ECON2_VRPS	0x08
+/* ENC28J60 ECON1 Register Bit Definitions */
+#define ECON1_TXRST	0x80
+#define ECON1_RXRST	0x40
+#define ECON1_DMAST	0x20
+#define ECON1_CSUMEN	0x10
+#define ECON1_TXRTS	0x08
+#define ECON1_RXEN	0x04
+#define ECON1_BSEL1	0x02
+#define ECON1_BSEL0	0x01
+/* ENC28J60 MACON1 Register Bit Definitions */
+#define MACON1_LOOPBK	0x10
+#define MACON1_TXPAUS	0x08
+#define MACON1_RXPAUS	0x04
+#define MACON1_PASSALL	0x02
+#define MACON1_MARXEN	0x01
+/* ENC28J60 MACON2 Register Bit Definitions */
+#define MACON2_MARST	0x80
+#define MACON2_RNDRST	0x40
+#define MACON2_MARXRST	0x08
+#define MACON2_RFUNRST	0x04
+#define MACON2_MATXRST	0x02
+#define MACON2_TFUNRST	0x01
+/* ENC28J60 MACON3 Register Bit Definitions */
+#define MACON3_PADCFG2	0x80
+#define MACON3_PADCFG1	0x40
+#define MACON3_PADCFG0	0x20
+#define MACON3_TXCRCEN	0x10
+#define MACON3_PHDRLEN	0x08
+#define MACON3_HFRMLEN	0x04
+#define MACON3_FRMLNEN	0x02
+#define MACON3_FULDPX	0x01
+/* ENC28J60 MICMD Register Bit Definitions */
+#define MICMD_MIISCAN	0x02
+#define MICMD_MIIRD	0x01
+/* ENC28J60 MISTAT Register Bit Definitions */
+#define MISTAT_NVALID	0x04
+#define MISTAT_SCAN	0x02
+#define MISTAT_BUSY	0x01
+/* ENC28J60 ERXFCON Register Bit Definitions */
+#define ERXFCON_UCEN	0x80
+#define ERXFCON_ANDOR	0x40
+#define ERXFCON_CRCEN	0x20
+#define ERXFCON_PMEN	0x10
+#define ERXFCON_MPEN	0x08
+#define ERXFCON_HTEN	0x04
+#define ERXFCON_MCEN	0x02
+#define ERXFCON_BCEN	0x01
+
+/* ENC28J60 PHY PHCON1 Register Bit Definitions */
+#define PHCON1_PRST	0x8000
+#define PHCON1_PLOOPBK	0x4000
+#define PHCON1_PPWRSV	0x0800
+#define PHCON1_PDPXMD	0x0100
+/* ENC28J60 PHY PHSTAT1 Register Bit Definitions */
+#define PHSTAT1_PFDPX	0x1000
+#define PHSTAT1_PHDPX	0x0800
+#define PHSTAT1_LLSTAT	0x0004
+#define PHSTAT1_JBSTAT	0x0002
+/* ENC28J60 PHY PHSTAT2 Register Bit Definitions */
+#define PHSTAT2_TXSTAT	(1 << 13)
+#define PHSTAT2_RXSTAT	(1 << 12)
+#define PHSTAT2_COLSTAT	(1 << 11)
+#define PHSTAT2_LSTAT	(1 << 10)
+#define PHSTAT2_DPXSTAT	(1 << 9)
+#define PHSTAT2_PLRITY	(1 << 5)
+/* ENC28J60 PHY PHCON2 Register Bit Definitions */
+#define PHCON2_FRCLINK	0x4000
+#define PHCON2_TXDIS	0x2000
+#define PHCON2_JABBER	0x0400
+#define PHCON2_HDLDIS	0x0100
+/* ENC28J60 PHY PHIE Register Bit Definitions */
+#define PHIE_PLNKIE	(1 << 4)
+#define PHIE_PGEIE	(1 << 1)
+/* ENC28J60 PHY PHIR Register Bit Definitions */
+#define PHIR_PLNKIF	(1 << 4)
+#define PHIR_PGEIF	(1 << 1)
+
+/* ENC28J60 Packet Control Byte Bit Definitions */
+#define PKTCTRL_PHUGEEN		0x08
+#define PKTCTRL_PPADEN		0x04
+#define PKTCTRL_PCRCEN		0x02
+#define PKTCTRL_POVERRIDE	0x01
+
+/* ENC28J60 Transmit Status Vector */
+#define TSV_TXBYTECNT		0
+#define TSV_TXCOLLISIONCNT	16
+#define TSV_TXCRCERROR		20
+#define TSV_TXLENCHKERROR	21
+#define TSV_TXLENOUTOFRANGE	22
+#define TSV_TXDONE		23
+#define TSV_TXMULTICAST		24
+#define TSV_TXBROADCAST		25
+#define TSV_TXPACKETDEFER	26
+#define TSV_TXEXDEFER		27
+#define TSV_TXEXCOLLISION	28
+#define TSV_TXLATECOLLISION	29
+#define TSV_TXGIANT		30
+#define TSV_TXUNDERRUN		31
+#define TSV_TOTBYTETXONWIRE	32
+#define TSV_TXCONTROLFRAME	48
+#define TSV_TXPAUSEFRAME	49
+#define TSV_BACKPRESSUREAPP	50
+#define TSV_TXVLANTAGFRAME	51
+
+#define TSV_SIZE		7
+#define TSV_BYTEOF(x)		((x) / 8)
+#define TSV_BITMASK(x)		(1 << ((x) % 8))
+#define TSV_GETBIT(x, y)	(((x)[TSV_BYTEOF(y)] & TSV_BITMASK(y)) ? 1 : 0)
+
+/* ENC28J60 Receive Status Vector */
+#define RSV_RXLONGEVDROPEV	16
+#define RSV_CARRIEREV		18
+#define RSV_CRCERROR		20
+#define RSV_LENCHECKERR		21
+#define RSV_LENOUTOFRANGE	22
+#define RSV_RXOK		23
+#define RSV_RXMULTICAST		24
+#define RSV_RXBROADCAST		25
+#define RSV_DRIBBLENIBBLE	26
+#define RSV_RXCONTROLFRAME	27
+#define RSV_RXPAUSEFRAME	28
+#define RSV_RXUNKNOWNOPCODE	29
+#define RSV_RXTYPEVLAN		30
+
+#define RSV_SIZE		6
+#define RSV_BITMASK(x)		(1 << ((x) - 16))
+#define RSV_GETBIT(x, y)	(((x) & RSV_BITMASK(y)) ? 1 : 0)
+
+
+/* SPI operation codes */
+#define ENC28J60_READ_CTRL_REG	0x00
+#define ENC28J60_READ_BUF_MEM	0x3A
+#define ENC28J60_WRITE_CTRL_REG 0x40
+#define ENC28J60_WRITE_BUF_MEM	0x7A
+#define ENC28J60_BIT_FIELD_SET	0x80
+#define ENC28J60_BIT_FIELD_CLR	0xA0
+#define ENC28J60_SOFT_RESET	0xFF
+
+
+/* buffer boundaries applied to internal 8K ram
+ * entire available packet buffer space is allocated.
+ * Give TX buffer space for one full ethernet frame (~1500 bytes)
+ * receive buffer gets the rest */
+#define TXSTART_INIT		0x1A00
+#define TXEND_INIT		0x1FFF
+
+/* Put RX buffer at 0 as suggested by the Errata datasheet */
+#define RXSTART_INIT		0x0000
+#define RXEND_INIT		0x19FF
+
+/* maximum ethernet frame length */
+#define MAX_FRAMELEN		1518
+
+/* Preferred half duplex: LEDA: Link status LEDB: Rx/Tx activity */
+#define ENC28J60_LAMPS_MODE	0x3476
+
+#endif
diff -Naur a/net/rtnet/drivers/microchip/Kconfig b/net/rtnet/drivers/microchip/Kconfig
--- a/net/rtnet/drivers/microchip/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/microchip/Kconfig	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,31 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Microchip network device configuration
+#
+
+config RTNET_DRV_ENC28J60
+	bool "ENC28J60 support"
+	depends on SPI
+	select CRC32
+	help
+	  Support for the Microchip EN28J60 ethernet chip.
+
+	  To compile this driver as a module, choose M here. The module will be
+	  called enc28j60.
+
+config RTNET_DRV_ENC28J60_WRITEVERIFY
+	bool "Enable write verify"
+	depends on RTNET_ENC28J60
+	help
+	  Enable the verify after the buffer write useful for debugging purpose.
+	  If unsure, say N.
+
+#config RTNET_DRV_ENCX24J600
+#    bool "ENCX24J600 support"
+#    depends on SPI
+#    help
+#      Support for the Microchip ENC424J600/624J600 ethernet chip.
+#
+#      To compile this driver as a module, choose M here. The module will be
+#      called encx24j600.
+
diff -Naur a/net/rtnet/drivers/microchip/Makefile b/net/rtnet/drivers/microchip/Makefile
--- a/net/rtnet/drivers/microchip/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/microchip/Makefile	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,9 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Makefile for the Microchip network device drivers.
+#
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_DRV_ENC28J60) += enc28j60.o
+#obj-$(CONFIG_RTNET_DRV_ENCX24J600) += encx24j600.o encx24j600-regmap.o
+
diff -Naur a/net/rtnet/drivers/microchip/modules.builtin b/net/rtnet/drivers/microchip/modules.builtin
--- a/net/rtnet/drivers/microchip/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/microchip/modules.builtin	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1 @@
+net/rtnet/drivers/microchip/enc28j60.ko
diff -Naur a/net/rtnet/drivers/modules.builtin b/net/rtnet/drivers/modules.builtin
--- a/net/rtnet/drivers/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/modules.builtin	2021-07-14 15:39:13.246125417 +0300
@@ -0,0 +1,2 @@
+net/rtnet/drivers/rt_loopback.ko
+net/rtnet/drivers/microchip/enc28j60.ko
diff -Naur a/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.c b/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.c
--- a/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.c	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,265 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright Altera Corporation (C) 2016. All rights reserved.
+ *
+ * Author: Tien Hock Loh <thloh@altera.com>
+ */
+
+#include <linux/mfd/syscon.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/phy.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/stmmac.h>
+
+#include "stmmac.h"
+#include "stmmac_platform.h"
+#include "altr_tse_pcs.h"
+
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_GMII_MII	0
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_RGMII		BIT(1)
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_RMII		BIT(2)
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_WIDTH		2
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_MASK			GENMASK(1, 0)
+
+#define TSE_PCS_CONTROL_AN_EN_MASK			BIT(12)
+#define TSE_PCS_CONTROL_REG				0x00
+#define TSE_PCS_CONTROL_RESTART_AN_MASK			BIT(9)
+#define TSE_PCS_CTRL_AUTONEG_SGMII			0x1140
+#define TSE_PCS_IF_MODE_REG				0x28
+#define TSE_PCS_LINK_TIMER_0_REG			0x24
+#define TSE_PCS_LINK_TIMER_1_REG			0x26
+#define TSE_PCS_SIZE					0x40
+#define TSE_PCS_STATUS_AN_COMPLETED_MASK		BIT(5)
+#define TSE_PCS_STATUS_LINK_MASK			0x0004
+#define TSE_PCS_STATUS_REG				0x02
+#define TSE_PCS_SGMII_SPEED_1000			BIT(3)
+#define TSE_PCS_SGMII_SPEED_100				BIT(2)
+#define TSE_PCS_SGMII_SPEED_10				0x0
+#define TSE_PCS_SW_RST_MASK				0x8000
+#define TSE_PCS_PARTNER_ABILITY_REG			0x0A
+#define TSE_PCS_PARTNER_DUPLEX_FULL			0x1000
+#define TSE_PCS_PARTNER_DUPLEX_HALF			0x0000
+#define TSE_PCS_PARTNER_DUPLEX_MASK			0x1000
+#define TSE_PCS_PARTNER_SPEED_MASK			GENMASK(11, 10)
+#define TSE_PCS_PARTNER_SPEED_1000			BIT(11)
+#define TSE_PCS_PARTNER_SPEED_100			BIT(10)
+#define TSE_PCS_PARTNER_SPEED_10			0x0000
+#define TSE_PCS_PARTNER_SPEED_1000			BIT(11)
+#define TSE_PCS_PARTNER_SPEED_100			BIT(10)
+#define TSE_PCS_PARTNER_SPEED_10			0x0000
+#define TSE_PCS_SGMII_SPEED_MASK			GENMASK(3, 2)
+#define TSE_PCS_SGMII_LINK_TIMER_0			0x0D40
+#define TSE_PCS_SGMII_LINK_TIMER_1			0x0003
+#define TSE_PCS_SW_RESET_TIMEOUT			100
+#define TSE_PCS_USE_SGMII_AN_MASK			BIT(1)
+#define TSE_PCS_USE_SGMII_ENA				BIT(0)
+#define TSE_PCS_IF_USE_SGMII				0x03
+
+#define SGMII_ADAPTER_CTRL_REG				0x00
+#define SGMII_ADAPTER_DISABLE				0x0001
+#define SGMII_ADAPTER_ENABLE				0x0000
+
+#define AUTONEGO_LINK_TIMER				20
+
+static int tse_pcs_reset(void __iomem *base, struct tse_pcs *pcs)
+{
+	int counter = 0;
+	u16 val;
+
+	val = readw(base + TSE_PCS_CONTROL_REG);
+	val |= TSE_PCS_SW_RST_MASK;
+	writew(val, base + TSE_PCS_CONTROL_REG);
+
+	while (counter < TSE_PCS_SW_RESET_TIMEOUT) {
+		val = readw(base + TSE_PCS_CONTROL_REG);
+		val &= TSE_PCS_SW_RST_MASK;
+		if (val == 0)
+			break;
+		counter++;
+		udelay(1);
+	}
+	if (counter >= TSE_PCS_SW_RESET_TIMEOUT) {
+		dev_err(pcs->dev, "PCS could not get out of sw reset\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+int tse_pcs_init(void __iomem *base, struct tse_pcs *pcs)
+{
+	int ret = 0;
+
+	writew(TSE_PCS_IF_USE_SGMII, base + TSE_PCS_IF_MODE_REG);
+
+	writew(TSE_PCS_CTRL_AUTONEG_SGMII, base + TSE_PCS_CONTROL_REG);
+
+	writew(TSE_PCS_SGMII_LINK_TIMER_0, base + TSE_PCS_LINK_TIMER_0_REG);
+	writew(TSE_PCS_SGMII_LINK_TIMER_1, base + TSE_PCS_LINK_TIMER_1_REG);
+
+	ret = tse_pcs_reset(base, pcs);
+	if (ret == 0)
+		writew(SGMII_ADAPTER_ENABLE,
+		       pcs->sgmii_adapter_base + SGMII_ADAPTER_CTRL_REG);
+
+	return ret;
+}
+
+static void pcs_link_timer_callback(struct tse_pcs *pcs)
+{
+	u16 val = 0;
+	void __iomem *tse_pcs_base = pcs->tse_pcs_base;
+	void __iomem *sgmii_adapter_base = pcs->sgmii_adapter_base;
+
+	val = readw(tse_pcs_base + TSE_PCS_STATUS_REG);
+	val &= TSE_PCS_STATUS_LINK_MASK;
+
+	if (val != 0) {
+		dev_dbg(pcs->dev, "Adapter: Link is established\n");
+		writew(SGMII_ADAPTER_ENABLE,
+		       sgmii_adapter_base + SGMII_ADAPTER_CTRL_REG);
+	} else {
+		mod_timer(&pcs->aneg_link_timer, jiffies +
+			  msecs_to_jiffies(AUTONEGO_LINK_TIMER));
+	}
+}
+
+static void auto_nego_timer_callback(struct tse_pcs *pcs)
+{
+	u16 val = 0;
+	u16 speed = 0;
+	u16 duplex = 0;
+	void __iomem *tse_pcs_base = pcs->tse_pcs_base;
+	void __iomem *sgmii_adapter_base = pcs->sgmii_adapter_base;
+
+	val = readw(tse_pcs_base + TSE_PCS_STATUS_REG);
+	val &= TSE_PCS_STATUS_AN_COMPLETED_MASK;
+
+	if (val != 0) {
+		dev_dbg(pcs->dev, "Adapter: Auto Negotiation is completed\n");
+		val = readw(tse_pcs_base + TSE_PCS_PARTNER_ABILITY_REG);
+		speed = val & TSE_PCS_PARTNER_SPEED_MASK;
+		duplex = val & TSE_PCS_PARTNER_DUPLEX_MASK;
+
+		if (speed == TSE_PCS_PARTNER_SPEED_10 &&
+		    duplex == TSE_PCS_PARTNER_DUPLEX_FULL)
+			dev_dbg(pcs->dev,
+				"Adapter: Link Partner is Up - 10/Full\n");
+		else if (speed == TSE_PCS_PARTNER_SPEED_100 &&
+			 duplex == TSE_PCS_PARTNER_DUPLEX_FULL)
+			dev_dbg(pcs->dev,
+				"Adapter: Link Partner is Up - 100/Full\n");
+		else if (speed == TSE_PCS_PARTNER_SPEED_1000 &&
+			 duplex == TSE_PCS_PARTNER_DUPLEX_FULL)
+			dev_dbg(pcs->dev,
+				"Adapter: Link Partner is Up - 1000/Full\n");
+		else if (speed == TSE_PCS_PARTNER_SPEED_10 &&
+			 duplex == TSE_PCS_PARTNER_DUPLEX_HALF)
+			dev_err(pcs->dev,
+				"Adapter does not support Half Duplex\n");
+		else if (speed == TSE_PCS_PARTNER_SPEED_100 &&
+			 duplex == TSE_PCS_PARTNER_DUPLEX_HALF)
+			dev_err(pcs->dev,
+				"Adapter does not support Half Duplex\n");
+		else if (speed == TSE_PCS_PARTNER_SPEED_1000 &&
+			 duplex == TSE_PCS_PARTNER_DUPLEX_HALF)
+			dev_err(pcs->dev,
+				"Adapter does not support Half Duplex\n");
+		else
+			dev_err(pcs->dev,
+				"Adapter: Invalid Partner Speed and Duplex\n");
+
+		if (duplex == TSE_PCS_PARTNER_DUPLEX_FULL &&
+		    (speed == TSE_PCS_PARTNER_SPEED_10 ||
+		     speed == TSE_PCS_PARTNER_SPEED_100 ||
+		     speed == TSE_PCS_PARTNER_SPEED_1000))
+			writew(SGMII_ADAPTER_ENABLE,
+			       sgmii_adapter_base + SGMII_ADAPTER_CTRL_REG);
+	} else {
+		val = readw(tse_pcs_base + TSE_PCS_CONTROL_REG);
+		val |= TSE_PCS_CONTROL_RESTART_AN_MASK;
+		writew(val, tse_pcs_base + TSE_PCS_CONTROL_REG);
+
+		tse_pcs_reset(tse_pcs_base, pcs);
+		mod_timer(&pcs->aneg_link_timer, jiffies +
+			  msecs_to_jiffies(AUTONEGO_LINK_TIMER));
+	}
+}
+
+static void aneg_link_timer_callback(struct timer_list *t)
+{
+	struct tse_pcs *pcs = from_timer(pcs, t, aneg_link_timer);
+
+	if (pcs->autoneg == AUTONEG_ENABLE)
+		auto_nego_timer_callback(pcs);
+	else if (pcs->autoneg == AUTONEG_DISABLE)
+		pcs_link_timer_callback(pcs);
+}
+
+void tse_pcs_fix_mac_speed(struct tse_pcs *pcs, struct phy_device *phy_dev,
+			   unsigned int speed)
+{
+	void __iomem *tse_pcs_base = pcs->tse_pcs_base;
+	void __iomem *sgmii_adapter_base = pcs->sgmii_adapter_base;
+	u32 val;
+
+	writew(SGMII_ADAPTER_ENABLE,
+	       sgmii_adapter_base + SGMII_ADAPTER_CTRL_REG);
+
+	pcs->autoneg = phy_dev->autoneg;
+
+	if (phy_dev->autoneg == AUTONEG_ENABLE) {
+		val = readw(tse_pcs_base + TSE_PCS_CONTROL_REG);
+		val |= TSE_PCS_CONTROL_AN_EN_MASK;
+		writew(val, tse_pcs_base + TSE_PCS_CONTROL_REG);
+
+		val = readw(tse_pcs_base + TSE_PCS_IF_MODE_REG);
+		val |= TSE_PCS_USE_SGMII_AN_MASK;
+		writew(val, tse_pcs_base + TSE_PCS_IF_MODE_REG);
+
+		val = readw(tse_pcs_base + TSE_PCS_CONTROL_REG);
+		val |= TSE_PCS_CONTROL_RESTART_AN_MASK;
+
+		tse_pcs_reset(tse_pcs_base, pcs);
+
+		timer_setup(&pcs->aneg_link_timer, aneg_link_timer_callback,
+			    0);
+		mod_timer(&pcs->aneg_link_timer, jiffies +
+			  msecs_to_jiffies(AUTONEGO_LINK_TIMER));
+	} else if (phy_dev->autoneg == AUTONEG_DISABLE) {
+		val = readw(tse_pcs_base + TSE_PCS_CONTROL_REG);
+		val &= ~TSE_PCS_CONTROL_AN_EN_MASK;
+		writew(val, tse_pcs_base + TSE_PCS_CONTROL_REG);
+
+		val = readw(tse_pcs_base + TSE_PCS_IF_MODE_REG);
+		val &= ~TSE_PCS_USE_SGMII_AN_MASK;
+		writew(val, tse_pcs_base + TSE_PCS_IF_MODE_REG);
+
+		val = readw(tse_pcs_base + TSE_PCS_IF_MODE_REG);
+		val &= ~TSE_PCS_SGMII_SPEED_MASK;
+
+		switch (speed) {
+		case 1000:
+			val |= TSE_PCS_SGMII_SPEED_1000;
+			break;
+		case 100:
+			val |= TSE_PCS_SGMII_SPEED_100;
+			break;
+		case 10:
+			val |= TSE_PCS_SGMII_SPEED_10;
+			break;
+		default:
+			return;
+		}
+		writew(val, tse_pcs_base + TSE_PCS_IF_MODE_REG);
+
+		tse_pcs_reset(tse_pcs_base, pcs);
+
+		timer_setup(&pcs->aneg_link_timer, aneg_link_timer_callback,
+			    0);
+		mod_timer(&pcs->aneg_link_timer, jiffies +
+			  msecs_to_jiffies(AUTONEGO_LINK_TIMER));
+	}
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.h b/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.h
--- a/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/altr_tse_pcs.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/* Copyright Altera Corporation (C) 2016. All rights reserved.
+ *
+ * Author: Tien Hock Loh <thloh@altera.com>
+ */
+
+#ifndef __TSE_PCS_H__
+#define __TSE_PCS_H__
+
+#include <linux/phy.h>
+#include <linux/timer.h>
+
+struct tse_pcs {
+	struct device *dev;
+	void __iomem *tse_pcs_base;
+	void __iomem *sgmii_adapter_base;
+	struct timer_list aneg_link_timer;
+	int autoneg;
+};
+
+int tse_pcs_init(void __iomem *base, struct tse_pcs *pcs);
+void tse_pcs_fix_mac_speed(struct tse_pcs *pcs, struct phy_device *phy_dev,
+			   unsigned int speed);
+
+#endif /* __TSE_PCS_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/chain_mode.c b/net/rtnet/drivers/orange-pi-one/chain_mode.c
--- a/net/rtnet/drivers/orange-pi-one/chain_mode.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/chain_mode.c	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,167 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  Specialised functions for managing Chained mode
+
+  Copyright(C) 2011  STMicroelectronics Ltd
+
+  It defines all the functions used to handle the normal/enhanced
+  descriptors in case of the DMA is configured to work in chained or
+  in ring mode.
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include "stmmac.h"
+
+static int jumbo_frm(void *p, struct sk_buff *skb, int csum)
+{
+	struct stmmac_tx_queue *tx_q = (struct stmmac_tx_queue *)p;
+	unsigned int nopaged_len = skb_headlen(skb);
+	struct stmmac_priv *priv = tx_q->priv_data;
+	unsigned int entry = tx_q->cur_tx;
+	unsigned int bmax, des2;
+	unsigned int i = 1, len;
+	struct dma_desc *desc;
+
+	desc = tx_q->dma_tx + entry;
+
+	if (priv->plat->enh_desc)
+		bmax = BUF_SIZE_8KiB;
+	else
+		bmax = BUF_SIZE_2KiB;
+
+	len = nopaged_len - bmax;
+
+	des2 = dma_map_single(priv->device, skb->data,
+			      bmax, DMA_TO_DEVICE);
+	desc->des2 = cpu_to_le32(des2);
+	if (dma_mapping_error(priv->device, des2))
+		return -1;
+	tx_q->tx_skbuff_dma[entry].buf = des2;
+	tx_q->tx_skbuff_dma[entry].len = bmax;
+	/* do not close the descriptor and do not set own bit */
+	stmmac_prepare_tx_desc(priv, desc, 1, bmax, csum, STMMAC_CHAIN_MODE,
+			0, false, skb->len);
+
+	while (len != 0) {
+		tx_q->tx_skbuff[entry] = NULL;
+		entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
+		desc = tx_q->dma_tx + entry;
+
+		if (len > bmax) {
+			des2 = dma_map_single(priv->device,
+					      (skb->data + bmax * i),
+					      bmax, DMA_TO_DEVICE);
+			desc->des2 = cpu_to_le32(des2);
+			if (dma_mapping_error(priv->device, des2))
+				return -1;
+			tx_q->tx_skbuff_dma[entry].buf = des2;
+			tx_q->tx_skbuff_dma[entry].len = bmax;
+			stmmac_prepare_tx_desc(priv, desc, 0, bmax, csum,
+					STMMAC_CHAIN_MODE, 1, false, skb->len);
+			len -= bmax;
+			i++;
+		} else {
+			des2 = dma_map_single(priv->device,
+					      (skb->data + bmax * i), len,
+					      DMA_TO_DEVICE);
+			desc->des2 = cpu_to_le32(des2);
+			if (dma_mapping_error(priv->device, des2))
+				return -1;
+			tx_q->tx_skbuff_dma[entry].buf = des2;
+			tx_q->tx_skbuff_dma[entry].len = len;
+			/* last descriptor can be set now */
+			stmmac_prepare_tx_desc(priv, desc, 0, len, csum,
+					STMMAC_CHAIN_MODE, 1, true, skb->len);
+			len = 0;
+		}
+	}
+
+	tx_q->cur_tx = entry;
+
+	return entry;
+}
+
+static unsigned int is_jumbo_frm(int len, int enh_desc)
+{
+	unsigned int ret = 0;
+
+	if ((enh_desc && (len > BUF_SIZE_8KiB)) ||
+	    (!enh_desc && (len > BUF_SIZE_2KiB))) {
+		ret = 1;
+	}
+
+	return ret;
+}
+
+static void init_dma_chain(void *des, dma_addr_t phy_addr,
+				  unsigned int size, unsigned int extend_desc)
+{
+	/*
+	 * In chained mode the des3 points to the next element in the ring.
+	 * The latest element has to point to the head.
+	 */
+	int i;
+	dma_addr_t dma_phy = phy_addr;
+
+	if (extend_desc) {
+		struct dma_extended_desc *p = (struct dma_extended_desc *)des;
+		for (i = 0; i < (size - 1); i++) {
+			dma_phy += sizeof(struct dma_extended_desc);
+			p->basic.des3 = cpu_to_le32((unsigned int)dma_phy);
+			p++;
+		}
+		p->basic.des3 = cpu_to_le32((unsigned int)phy_addr);
+
+	} else {
+		struct dma_desc *p = (struct dma_desc *)des;
+		for (i = 0; i < (size - 1); i++) {
+			dma_phy += sizeof(struct dma_desc);
+			p->des3 = cpu_to_le32((unsigned int)dma_phy);
+			p++;
+		}
+		p->des3 = cpu_to_le32((unsigned int)phy_addr);
+	}
+}
+
+static void refill_desc3(void *priv_ptr, struct dma_desc *p)
+{
+	struct stmmac_rx_queue *rx_q = (struct stmmac_rx_queue *)priv_ptr;
+	struct stmmac_priv *priv = rx_q->priv_data;
+
+	if (priv->hwts_rx_en && !priv->extend_desc)
+		/* NOTE: Device will overwrite des3 with timestamp value if
+		 * 1588-2002 time stamping is enabled, hence reinitialize it
+		 * to keep explicit chaining in the descriptor.
+		 */
+		p->des3 = cpu_to_le32((unsigned int)(rx_q->dma_rx_phy +
+				      (((rx_q->dirty_rx) + 1) %
+				       DMA_RX_SIZE) *
+				      sizeof(struct dma_desc)));
+}
+
+static void clean_desc3(void *priv_ptr, struct dma_desc *p)
+{
+	struct stmmac_tx_queue *tx_q = (struct stmmac_tx_queue *)priv_ptr;
+	struct stmmac_priv *priv = tx_q->priv_data;
+	unsigned int entry = tx_q->dirty_tx;
+
+	if (tx_q->tx_skbuff_dma[entry].last_segment && !priv->extend_desc &&
+	    priv->hwts_tx_en)
+		/* NOTE: Device will overwrite des3 with timestamp value if
+		 * 1588-2002 time stamping is enabled, hence reinitialize it
+		 * to keep explicit chaining in the descriptor.
+		 */
+		p->des3 = cpu_to_le32((unsigned int)((tx_q->dma_tx_phy +
+				      ((tx_q->dirty_tx + 1) % DMA_TX_SIZE))
+				      * sizeof(struct dma_desc)));
+}
+
+const struct stmmac_mode_ops chain_mode_ops = {
+	.init = init_dma_chain,
+	.is_jumbo_frm = is_jumbo_frm,
+	.jumbo_frm = jumbo_frm,
+	.refill_desc3 = refill_desc3,
+	.clean_desc3 = clean_desc3,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/common.h b/net/rtnet/drivers/orange-pi-one/common.h
--- a/net/rtnet/drivers/orange-pi-one/common.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/common.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,508 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  STMMAC Common Header File
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __COMMON_H__
+#define __COMMON_H__
+
+#include <linux/etherdevice.h>
+#include <linux/netdevice.h>
+#include <linux/stmmac.h>
+#include <linux/phy.h>
+#include <linux/mdio-xpcs.h>
+#include <linux/module.h>
+#if IS_ENABLED(CONFIG_VLAN_8021Q)
+#define STMMAC_VLAN_TAG_USED
+#include <linux/if_vlan.h>
+#endif
+
+#include "descs.h"
+#include "hwif.h"
+#include "mmc.h"
+
+/* Synopsys Core versions */
+#define	DWMAC_CORE_3_40		0x34
+#define	DWMAC_CORE_3_50		0x35
+#define	DWMAC_CORE_4_00		0x40
+#define DWMAC_CORE_4_10		0x41
+#define DWMAC_CORE_5_00		0x50
+#define DWMAC_CORE_5_10		0x51
+#define DWXGMAC_CORE_2_10	0x21
+#define DWXLGMAC_CORE_2_00	0x20
+
+/* Device ID */
+#define DWXGMAC_ID		0x76
+#define DWXLGMAC_ID		0x27
+
+#define STMMAC_CHAN0	0	/* Always supported and default for all chips */
+
+/* These need to be power of two, and >= 4 */
+#define DMA_TX_SIZE 512
+#define DMA_RX_SIZE 512
+#define STMMAC_GET_ENTRY(x, size)	((x + 1) & (size - 1))
+
+#undef FRAME_FILTER_DEBUG
+/* #define FRAME_FILTER_DEBUG */
+
+/* Extra statistic and debug information exposed by ethtool */
+struct stmmac_extra_stats {
+	/* Transmit errors */
+	unsigned long tx_underflow ____cacheline_aligned;
+	unsigned long tx_carrier;
+	unsigned long tx_losscarrier;
+	unsigned long vlan_tag;
+	unsigned long tx_deferred;
+	unsigned long tx_vlan;
+	unsigned long tx_jabber;
+	unsigned long tx_frame_flushed;
+	unsigned long tx_payload_error;
+	unsigned long tx_ip_header_error;
+	/* Receive errors */
+	unsigned long rx_desc;
+	unsigned long sa_filter_fail;
+	unsigned long overflow_error;
+	unsigned long ipc_csum_error;
+	unsigned long rx_collision;
+	unsigned long rx_crc_errors;
+	unsigned long dribbling_bit;
+	unsigned long rx_length;
+	unsigned long rx_mii;
+	unsigned long rx_multicast;
+	unsigned long rx_gmac_overflow;
+	unsigned long rx_watchdog;
+	unsigned long da_rx_filter_fail;
+	unsigned long sa_rx_filter_fail;
+	unsigned long rx_missed_cntr;
+	unsigned long rx_overflow_cntr;
+	unsigned long rx_vlan;
+	unsigned long rx_split_hdr_pkt_n;
+	/* Tx/Rx IRQ error info */
+	unsigned long tx_undeflow_irq;
+	unsigned long tx_process_stopped_irq;
+	unsigned long tx_jabber_irq;
+	unsigned long rx_overflow_irq;
+	unsigned long rx_buf_unav_irq;
+	unsigned long rx_process_stopped_irq;
+	unsigned long rx_watchdog_irq;
+	unsigned long tx_early_irq;
+	unsigned long fatal_bus_error_irq;
+	/* Tx/Rx IRQ Events */
+	unsigned long rx_early_irq;
+	unsigned long threshold;
+	unsigned long tx_pkt_n;
+	unsigned long rx_pkt_n;
+	unsigned long normal_irq_n;
+	unsigned long rx_normal_irq_n;
+	unsigned long napi_poll;
+	unsigned long tx_normal_irq_n;
+	unsigned long tx_clean;
+	unsigned long tx_set_ic_bit;
+	unsigned long irq_receive_pmt_irq_n;
+	/* MMC info */
+	unsigned long mmc_tx_irq_n;
+	unsigned long mmc_rx_irq_n;
+	unsigned long mmc_rx_csum_offload_irq_n;
+	/* EEE */
+	unsigned long irq_tx_path_in_lpi_mode_n;
+	unsigned long irq_tx_path_exit_lpi_mode_n;
+	unsigned long irq_rx_path_in_lpi_mode_n;
+	unsigned long irq_rx_path_exit_lpi_mode_n;
+	unsigned long phy_eee_wakeup_error_n;
+	/* Extended RDES status */
+	unsigned long ip_hdr_err;
+	unsigned long ip_payload_err;
+	unsigned long ip_csum_bypassed;
+	unsigned long ipv4_pkt_rcvd;
+	unsigned long ipv6_pkt_rcvd;
+	unsigned long no_ptp_rx_msg_type_ext;
+	unsigned long ptp_rx_msg_type_sync;
+	unsigned long ptp_rx_msg_type_follow_up;
+	unsigned long ptp_rx_msg_type_delay_req;
+	unsigned long ptp_rx_msg_type_delay_resp;
+	unsigned long ptp_rx_msg_type_pdelay_req;
+	unsigned long ptp_rx_msg_type_pdelay_resp;
+	unsigned long ptp_rx_msg_type_pdelay_follow_up;
+	unsigned long ptp_rx_msg_type_announce;
+	unsigned long ptp_rx_msg_type_management;
+	unsigned long ptp_rx_msg_pkt_reserved_type;
+	unsigned long ptp_frame_type;
+	unsigned long ptp_ver;
+	unsigned long timestamp_dropped;
+	unsigned long av_pkt_rcvd;
+	unsigned long av_tagged_pkt_rcvd;
+	unsigned long vlan_tag_priority_val;
+	unsigned long l3_filter_match;
+	unsigned long l4_filter_match;
+	unsigned long l3_l4_filter_no_match;
+	/* PCS */
+	unsigned long irq_pcs_ane_n;
+	unsigned long irq_pcs_link_n;
+	unsigned long irq_rgmii_n;
+	unsigned long pcs_link;
+	unsigned long pcs_duplex;
+	unsigned long pcs_speed;
+	/* debug register */
+	unsigned long mtl_tx_status_fifo_full;
+	unsigned long mtl_tx_fifo_not_empty;
+	unsigned long mmtl_fifo_ctrl;
+	unsigned long mtl_tx_fifo_read_ctrl_write;
+	unsigned long mtl_tx_fifo_read_ctrl_wait;
+	unsigned long mtl_tx_fifo_read_ctrl_read;
+	unsigned long mtl_tx_fifo_read_ctrl_idle;
+	unsigned long mac_tx_in_pause;
+	unsigned long mac_tx_frame_ctrl_xfer;
+	unsigned long mac_tx_frame_ctrl_idle;
+	unsigned long mac_tx_frame_ctrl_wait;
+	unsigned long mac_tx_frame_ctrl_pause;
+	unsigned long mac_gmii_tx_proto_engine;
+	unsigned long mtl_rx_fifo_fill_level_full;
+	unsigned long mtl_rx_fifo_fill_above_thresh;
+	unsigned long mtl_rx_fifo_fill_below_thresh;
+	unsigned long mtl_rx_fifo_fill_level_empty;
+	unsigned long mtl_rx_fifo_read_ctrl_flush;
+	unsigned long mtl_rx_fifo_read_ctrl_read_data;
+	unsigned long mtl_rx_fifo_read_ctrl_status;
+	unsigned long mtl_rx_fifo_read_ctrl_idle;
+	unsigned long mtl_rx_fifo_ctrl_active;
+	unsigned long mac_rx_frame_ctrl_fifo;
+	unsigned long mac_gmii_rx_proto_engine;
+	/* TSO */
+	unsigned long tx_tso_frames;
+	unsigned long tx_tso_nfrags;
+};
+
+/* Safety Feature statistics exposed by ethtool */
+struct stmmac_safety_stats {
+	unsigned long mac_errors[32];
+	unsigned long mtl_errors[32];
+	unsigned long dma_errors[32];
+};
+
+/* Number of fields in Safety Stats */
+#define STMMAC_SAFETY_FEAT_SIZE	\
+	(sizeof(struct stmmac_safety_stats) / sizeof(unsigned long))
+
+/* CSR Frequency Access Defines*/
+#define CSR_F_35M	35000000
+#define CSR_F_60M	60000000
+#define CSR_F_100M	100000000
+#define CSR_F_150M	150000000
+#define CSR_F_250M	250000000
+#define CSR_F_300M	300000000
+
+#define	MAC_CSR_H_FRQ_MASK	0x20
+
+#define HASH_TABLE_SIZE 64
+#define PAUSE_TIME 0xffff
+
+/* Flow Control defines */
+#define FLOW_OFF	0
+#define FLOW_RX		1
+#define FLOW_TX		2
+#define FLOW_AUTO	(FLOW_TX | FLOW_RX)
+
+/* PCS defines */
+#define STMMAC_PCS_RGMII	(1 << 0)
+#define STMMAC_PCS_SGMII	(1 << 1)
+#define STMMAC_PCS_TBI		(1 << 2)
+#define STMMAC_PCS_RTBI		(1 << 3)
+
+#define SF_DMA_MODE 1		/* DMA STORE-AND-FORWARD Operation Mode */
+
+/* DAM HW feature register fields */
+#define DMA_HW_FEAT_MIISEL	0x00000001	/* 10/100 Mbps Support */
+#define DMA_HW_FEAT_GMIISEL	0x00000002	/* 1000 Mbps Support */
+#define DMA_HW_FEAT_HDSEL	0x00000004	/* Half-Duplex Support */
+#define DMA_HW_FEAT_EXTHASHEN	0x00000008	/* Expanded DA Hash Filter */
+#define DMA_HW_FEAT_HASHSEL	0x00000010	/* HASH Filter */
+#define DMA_HW_FEAT_ADDMAC	0x00000020	/* Multiple MAC Addr Reg */
+#define DMA_HW_FEAT_PCSSEL	0x00000040	/* PCS registers */
+#define DMA_HW_FEAT_L3L4FLTREN	0x00000080	/* Layer 3 & Layer 4 Feature */
+#define DMA_HW_FEAT_SMASEL	0x00000100	/* SMA(MDIO) Interface */
+#define DMA_HW_FEAT_RWKSEL	0x00000200	/* PMT Remote Wakeup */
+#define DMA_HW_FEAT_MGKSEL	0x00000400	/* PMT Magic Packet */
+#define DMA_HW_FEAT_MMCSEL	0x00000800	/* RMON Module */
+#define DMA_HW_FEAT_TSVER1SEL	0x00001000	/* Only IEEE 1588-2002 */
+#define DMA_HW_FEAT_TSVER2SEL	0x00002000	/* IEEE 1588-2008 PTPv2 */
+#define DMA_HW_FEAT_EEESEL	0x00004000	/* Energy Efficient Ethernet */
+#define DMA_HW_FEAT_AVSEL	0x00008000	/* AV Feature */
+#define DMA_HW_FEAT_TXCOESEL	0x00010000	/* Checksum Offload in Tx */
+#define DMA_HW_FEAT_RXTYP1COE	0x00020000	/* IP COE (Type 1) in Rx */
+#define DMA_HW_FEAT_RXTYP2COE	0x00040000	/* IP COE (Type 2) in Rx */
+#define DMA_HW_FEAT_RXFIFOSIZE	0x00080000	/* Rx FIFO > 2048 Bytes */
+#define DMA_HW_FEAT_RXCHCNT	0x00300000	/* No. additional Rx Channels */
+#define DMA_HW_FEAT_TXCHCNT	0x00c00000	/* No. additional Tx Channels */
+#define DMA_HW_FEAT_ENHDESSEL	0x01000000	/* Alternate Descriptor */
+/* Timestamping with Internal System Time */
+#define DMA_HW_FEAT_INTTSEN	0x02000000
+#define DMA_HW_FEAT_FLEXIPPSEN	0x04000000	/* Flexible PPS Output */
+#define DMA_HW_FEAT_SAVLANINS	0x08000000	/* Source Addr or VLAN */
+#define DMA_HW_FEAT_ACTPHYIF	0x70000000	/* Active/selected PHY iface */
+#define DEFAULT_DMA_PBL		8
+
+/* PCS status and mask defines */
+#define	PCS_ANE_IRQ		BIT(2)	/* PCS Auto-Negotiation */
+#define	PCS_LINK_IRQ		BIT(1)	/* PCS Link */
+#define	PCS_RGSMIIIS_IRQ	BIT(0)	/* RGMII or SMII Interrupt */
+
+/* Max/Min RI Watchdog Timer count value */
+#define MAX_DMA_RIWT		0xff
+#define MIN_DMA_RIWT		0x10
+#define DEF_DMA_RIWT		0xa0
+/* Tx coalesce parameters */
+#define STMMAC_COAL_TX_TIMER	1000
+#define STMMAC_MAX_COAL_TX_TICK	100000
+#define STMMAC_TX_MAX_FRAMES	256
+#define STMMAC_TX_FRAMES	25
+#define STMMAC_RX_FRAMES	0
+
+/* Packets types */
+enum packets_types {
+	PACKET_AVCPQ = 0x1, /* AV Untagged Control packets */
+	PACKET_PTPQ = 0x2, /* PTP Packets */
+	PACKET_DCBCPQ = 0x3, /* DCB Control Packets */
+	PACKET_UPQ = 0x4, /* Untagged Packets */
+	PACKET_MCBCQ = 0x5, /* Multicast & Broadcast Packets */
+};
+
+/* Rx IPC status */
+enum rx_frame_status {
+	good_frame = 0x0,
+	discard_frame = 0x1,
+	csum_none = 0x2,
+	llc_snap = 0x4,
+	dma_own = 0x8,
+	rx_not_ls = 0x10,
+};
+
+/* Tx status */
+enum tx_frame_status {
+	tx_done = 0x0,
+	tx_not_ls = 0x1,
+	tx_err = 0x2,
+	tx_dma_own = 0x4,
+};
+
+enum dma_irq_status {
+	tx_hard_error = 0x1,
+	tx_hard_error_bump_tc = 0x2,
+	handle_rx = 0x4,
+	handle_tx = 0x8,
+};
+
+/* EEE and LPI defines */
+#define	CORE_IRQ_TX_PATH_IN_LPI_MODE	(1 << 0)
+#define	CORE_IRQ_TX_PATH_EXIT_LPI_MODE	(1 << 1)
+#define	CORE_IRQ_RX_PATH_IN_LPI_MODE	(1 << 2)
+#define	CORE_IRQ_RX_PATH_EXIT_LPI_MODE	(1 << 3)
+
+#define CORE_IRQ_MTL_RX_OVERFLOW	BIT(8)
+
+/* Physical Coding Sublayer */
+struct rgmii_adv {
+	unsigned int pause;
+	unsigned int duplex;
+	unsigned int lp_pause;
+	unsigned int lp_duplex;
+};
+
+#define STMMAC_PCS_PAUSE	1
+#define STMMAC_PCS_ASYM_PAUSE	2
+
+/* DMA HW capabilities */
+struct dma_features {
+	unsigned int mbps_10_100;
+	unsigned int mbps_1000;
+	unsigned int half_duplex;
+	unsigned int hash_filter;
+	unsigned int multi_addr;
+	unsigned int pcs;
+	unsigned int sma_mdio;
+	unsigned int pmt_remote_wake_up;
+	unsigned int pmt_magic_frame;
+	unsigned int rmon;
+	/* IEEE 1588-2002 */
+	unsigned int time_stamp;
+	/* IEEE 1588-2008 */
+	unsigned int atime_stamp;
+	/* 802.3az - Energy-Efficient Ethernet (EEE) */
+	unsigned int eee;
+	unsigned int av;
+	unsigned int hash_tb_sz;
+	unsigned int tsoen;
+	/* TX and RX csum */
+	unsigned int tx_coe;
+	unsigned int rx_coe;
+	unsigned int rx_coe_type1;
+	unsigned int rx_coe_type2;
+	unsigned int rxfifo_over_2048;
+	/* TX and RX number of channels */
+	unsigned int number_rx_channel;
+	unsigned int number_tx_channel;
+	/* TX and RX number of queues */
+	unsigned int number_rx_queues;
+	unsigned int number_tx_queues;
+	/* PPS output */
+	unsigned int pps_out_num;
+	/* Alternate (enhanced) DESC mode */
+	unsigned int enh_desc;
+	/* TX and RX FIFO sizes */
+	unsigned int tx_fifo_size;
+	unsigned int rx_fifo_size;
+	/* Automotive Safety Package */
+	unsigned int asp;
+	/* RX Parser */
+	unsigned int frpsel;
+	unsigned int frpbs;
+	unsigned int frpes;
+	unsigned int addr64;
+	unsigned int rssen;
+	unsigned int vlhash;
+	unsigned int sphen;
+	unsigned int vlins;
+	unsigned int dvlan;
+	unsigned int l3l4fnum;
+	unsigned int arpoffsel;
+	/* TSN Features */
+	unsigned int estwid;
+	unsigned int estdep;
+	unsigned int estsel;
+	unsigned int fpesel;
+	unsigned int tbssel;
+};
+
+/* RX Buffer size must be multiple of 4/8/16 bytes */
+#define BUF_SIZE_16KiB 16368
+#define BUF_SIZE_8KiB 8188
+#define BUF_SIZE_4KiB 4096
+#define BUF_SIZE_2KiB 2048
+
+/* Power Down and WOL */
+#define PMT_NOT_SUPPORTED 0
+#define PMT_SUPPORTED 1
+
+/* Common MAC defines */
+#define MAC_CTRL_REG		0x00000000	/* MAC Control */
+#define MAC_ENABLE_TX		0x00000008	/* Transmitter Enable */
+#define MAC_ENABLE_RX		0x00000004	/* Receiver Enable */
+
+/* Default LPI timers */
+#define STMMAC_DEFAULT_LIT_LS	0x3E8
+#define STMMAC_DEFAULT_TWT_LS	0x1E
+
+#define STMMAC_CHAIN_MODE	0x1
+#define STMMAC_RING_MODE	0x2
+
+#define JUMBO_LEN		9000
+
+/* Receive Side Scaling */
+#define STMMAC_RSS_HASH_KEY_SIZE	40
+#define STMMAC_RSS_MAX_TABLE_SIZE	256
+
+/* VLAN */
+#define STMMAC_VLAN_NONE	0x0
+#define STMMAC_VLAN_REMOVE	0x1
+#define STMMAC_VLAN_INSERT	0x2
+#define STMMAC_VLAN_REPLACE	0x3
+
+extern const struct stmmac_desc_ops enh_desc_ops;
+extern const struct stmmac_desc_ops ndesc_ops;
+
+struct mac_device_info;
+
+extern const struct stmmac_hwtimestamp stmmac_ptp;
+extern const struct stmmac_mode_ops dwmac4_ring_mode_ops;
+
+struct mac_link {
+	u32 speed_mask;
+	u32 speed10;
+	u32 speed100;
+	u32 speed1000;
+	u32 speed2500;
+	u32 duplex;
+	struct {
+		u32 speed2500;
+		u32 speed5000;
+		u32 speed10000;
+	} xgmii;
+	struct {
+		u32 speed25000;
+		u32 speed40000;
+		u32 speed50000;
+		u32 speed100000;
+	} xlgmii;
+};
+
+struct mii_regs {
+	unsigned int addr;	/* MII Address */
+	unsigned int data;	/* MII Data */
+	unsigned int addr_shift;	/* MII address shift */
+	unsigned int reg_shift;		/* MII reg shift */
+	unsigned int addr_mask;		/* MII address mask */
+	unsigned int reg_mask;		/* MII reg mask */
+	unsigned int clk_csr_shift;
+	unsigned int clk_csr_mask;
+};
+
+struct mac_device_info {
+	const struct stmmac_ops *mac;
+	const struct stmmac_desc_ops *desc;
+	const struct stmmac_dma_ops *dma;
+	const struct stmmac_mode_ops *mode;
+	const struct stmmac_hwtimestamp *ptp;
+	const struct stmmac_tc_ops *tc;
+	const struct stmmac_mmc_ops *mmc;
+	const struct mdio_xpcs_ops *xpcs;
+	struct mdio_xpcs_args xpcs_args;
+	struct mii_regs mii;	/* MII register Addresses */
+	struct mac_link link;
+	void __iomem *pcsr;     /* vpointer to device CSRs */
+	unsigned int multicast_filter_bins;
+	unsigned int unicast_filter_entries;
+	unsigned int mcast_bits_log2;
+	unsigned int rx_csum;
+	unsigned int pcs;
+	unsigned int pmt;
+	unsigned int ps;
+	unsigned int xlgmac;
+	unsigned int num_vlan;
+	u32 vlan_filter[32];
+	unsigned int promisc;
+};
+
+struct stmmac_rx_routing {
+	u32 reg_mask;
+	u32 reg_shift;
+};
+
+int dwmac100_setup(struct stmmac_priv *priv);
+int dwmac1000_setup(struct stmmac_priv *priv);
+int dwmac4_setup(struct stmmac_priv *priv);
+int dwxgmac2_setup(struct stmmac_priv *priv);
+int dwxlgmac2_setup(struct stmmac_priv *priv);
+
+void stmmac_set_mac_addr(void __iomem *ioaddr, u8 addr[6],
+			 unsigned int high, unsigned int low);
+void stmmac_get_mac_addr(void __iomem *ioaddr, unsigned char *addr,
+			 unsigned int high, unsigned int low);
+void stmmac_set_mac(void __iomem *ioaddr, bool enable);
+
+void stmmac_dwmac4_set_mac_addr(void __iomem *ioaddr, u8 addr[6],
+				unsigned int high, unsigned int low);
+void stmmac_dwmac4_get_mac_addr(void __iomem *ioaddr, unsigned char *addr,
+				unsigned int high, unsigned int low);
+void stmmac_dwmac4_set_mac(void __iomem *ioaddr, bool enable);
+
+void dwmac_dma_flush_tx_fifo(void __iomem *ioaddr);
+
+extern const struct stmmac_mode_ops ring_mode_ops;
+extern const struct stmmac_mode_ops chain_mode_ops;
+extern const struct stmmac_desc_ops dwmac4_desc_ops;
+
+#endif /* __COMMON_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/descs_com.h b/net/rtnet/drivers/orange-pi-one/descs_com.h
--- a/net/rtnet/drivers/orange-pi-one/descs_com.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/descs_com.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,121 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  Header File to describe Normal/enhanced descriptor functions used for RING
+  and CHAINED modes.
+
+  Copyright(C) 2011  STMicroelectronics Ltd
+
+  It defines all the functions used to handle the normal/enhanced
+  descriptors in case of the DMA is configured to work in chained or
+  in ring mode.
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __DESC_COM_H__
+#define __DESC_COM_H__
+
+/* Specific functions used for Ring mode */
+
+/* Enhanced descriptors */
+static inline void ehn_desc_rx_set_on_ring(struct dma_desc *p, int end,
+					   int bfsize)
+{
+	if (bfsize == BUF_SIZE_16KiB)
+		p->des1 |= cpu_to_le32((BUF_SIZE_8KiB
+				<< ERDES1_BUFFER2_SIZE_SHIFT)
+			   & ERDES1_BUFFER2_SIZE_MASK);
+
+	if (end)
+		p->des1 |= cpu_to_le32(ERDES1_END_RING);
+}
+
+static inline void enh_desc_end_tx_desc_on_ring(struct dma_desc *p, int end)
+{
+	if (end)
+		p->des0 |= cpu_to_le32(ETDES0_END_RING);
+	else
+		p->des0 &= cpu_to_le32(~ETDES0_END_RING);
+}
+
+static inline void enh_set_tx_desc_len_on_ring(struct dma_desc *p, int len)
+{
+	if (unlikely(len > BUF_SIZE_4KiB)) {
+		p->des1 |= cpu_to_le32((((len - BUF_SIZE_4KiB)
+					<< ETDES1_BUFFER2_SIZE_SHIFT)
+			    & ETDES1_BUFFER2_SIZE_MASK) | (BUF_SIZE_4KiB
+			    & ETDES1_BUFFER1_SIZE_MASK));
+	} else
+		p->des1 |= cpu_to_le32((len & ETDES1_BUFFER1_SIZE_MASK));
+}
+
+/* Normal descriptors */
+static inline void ndesc_rx_set_on_ring(struct dma_desc *p, int end, int bfsize)
+{
+	if (bfsize >= BUF_SIZE_2KiB) {
+		int bfsize2;
+
+		bfsize2 = min(bfsize - BUF_SIZE_2KiB + 1, BUF_SIZE_2KiB - 1);
+		p->des1 |= cpu_to_le32((bfsize2 << RDES1_BUFFER2_SIZE_SHIFT)
+			    & RDES1_BUFFER2_SIZE_MASK);
+	}
+
+	if (end)
+		p->des1 |= cpu_to_le32(RDES1_END_RING);
+}
+
+static inline void ndesc_end_tx_desc_on_ring(struct dma_desc *p, int end)
+{
+	if (end)
+		p->des1 |= cpu_to_le32(TDES1_END_RING);
+	else
+		p->des1 &= cpu_to_le32(~TDES1_END_RING);
+}
+
+static inline void norm_set_tx_desc_len_on_ring(struct dma_desc *p, int len)
+{
+	if (unlikely(len > BUF_SIZE_2KiB)) {
+		unsigned int buffer1 = (BUF_SIZE_2KiB - 1)
+					& TDES1_BUFFER1_SIZE_MASK;
+		p->des1 |= cpu_to_le32((((len - buffer1)
+					<< TDES1_BUFFER2_SIZE_SHIFT)
+				& TDES1_BUFFER2_SIZE_MASK) | buffer1);
+	} else
+		p->des1 |= cpu_to_le32((len & TDES1_BUFFER1_SIZE_MASK));
+}
+
+/* Specific functions used for Chain mode */
+
+/* Enhanced descriptors */
+static inline void ehn_desc_rx_set_on_chain(struct dma_desc *p)
+{
+	p->des1 |= cpu_to_le32(ERDES1_SECOND_ADDRESS_CHAINED);
+}
+
+static inline void enh_desc_end_tx_desc_on_chain(struct dma_desc *p)
+{
+	p->des0 |= cpu_to_le32(ETDES0_SECOND_ADDRESS_CHAINED);
+}
+
+static inline void enh_set_tx_desc_len_on_chain(struct dma_desc *p, int len)
+{
+	p->des1 |= cpu_to_le32(len & ETDES1_BUFFER1_SIZE_MASK);
+}
+
+/* Normal descriptors */
+static inline void ndesc_rx_set_on_chain(struct dma_desc *p, int end)
+{
+	p->des1 |= cpu_to_le32(RDES1_SECOND_ADDRESS_CHAINED);
+}
+
+static inline void ndesc_tx_set_on_chain(struct dma_desc *p)
+{
+	p->des1 |= cpu_to_le32(TDES1_SECOND_ADDRESS_CHAINED);
+}
+
+static inline void norm_set_tx_desc_len_on_chain(struct dma_desc *p, int len)
+{
+	p->des1 |= cpu_to_le32(len & TDES1_BUFFER1_SIZE_MASK);
+}
+#endif /* __DESC_COM_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/descs.h b/net/rtnet/drivers/orange-pi-one/descs.h
--- a/net/rtnet/drivers/orange-pi-one/descs.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/descs.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,186 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  Header File to describe the DMA descriptors and related definitions.
+  This is for DWMAC100 and 1000 cores.
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __DESCS_H__
+#define __DESCS_H__
+
+#include <linux/bitops.h>
+
+/* Normal receive descriptor defines */
+
+/* RDES0 */
+#define	RDES0_PAYLOAD_CSUM_ERR	BIT(0)
+#define	RDES0_CRC_ERROR		BIT(1)
+#define	RDES0_DRIBBLING		BIT(2)
+#define	RDES0_MII_ERROR		BIT(3)
+#define	RDES0_RECEIVE_WATCHDOG	BIT(4)
+#define	RDES0_FRAME_TYPE	BIT(5)
+#define	RDES0_COLLISION		BIT(6)
+#define	RDES0_IPC_CSUM_ERROR	BIT(7)
+#define	RDES0_LAST_DESCRIPTOR	BIT(8)
+#define	RDES0_FIRST_DESCRIPTOR	BIT(9)
+#define	RDES0_VLAN_TAG		BIT(10)
+#define	RDES0_OVERFLOW_ERROR	BIT(11)
+#define	RDES0_LENGTH_ERROR	BIT(12)
+#define	RDES0_SA_FILTER_FAIL	BIT(13)
+#define	RDES0_DESCRIPTOR_ERROR	BIT(14)
+#define	RDES0_ERROR_SUMMARY	BIT(15)
+#define	RDES0_FRAME_LEN_MASK	GENMASK(29, 16)
+#define RDES0_FRAME_LEN_SHIFT	16
+#define	RDES0_DA_FILTER_FAIL	BIT(30)
+#define	RDES0_OWN		BIT(31)
+			/* RDES1 */
+#define	RDES1_BUFFER1_SIZE_MASK		GENMASK(10, 0)
+#define	RDES1_BUFFER2_SIZE_MASK		GENMASK(21, 11)
+#define	RDES1_BUFFER2_SIZE_SHIFT	11
+#define	RDES1_SECOND_ADDRESS_CHAINED	BIT(24)
+#define	RDES1_END_RING			BIT(25)
+#define	RDES1_DISABLE_IC		BIT(31)
+
+/* Enhanced receive descriptor defines */
+
+/* RDES0 (similar to normal RDES) */
+#define	 ERDES0_RX_MAC_ADDR	BIT(0)
+
+/* RDES1: completely differ from normal desc definitions */
+#define	ERDES1_BUFFER1_SIZE_MASK	GENMASK(12, 0)
+#define	ERDES1_SECOND_ADDRESS_CHAINED	BIT(14)
+#define	ERDES1_END_RING			BIT(15)
+#define	ERDES1_BUFFER2_SIZE_MASK	GENMASK(28, 16)
+#define ERDES1_BUFFER2_SIZE_SHIFT	16
+#define	ERDES1_DISABLE_IC		BIT(31)
+
+/* Normal transmit descriptor defines */
+/* TDES0 */
+#define	TDES0_DEFERRED			BIT(0)
+#define	TDES0_UNDERFLOW_ERROR		BIT(1)
+#define	TDES0_EXCESSIVE_DEFERRAL	BIT(2)
+#define	TDES0_COLLISION_COUNT_MASK	GENMASK(6, 3)
+#define	TDES0_VLAN_FRAME		BIT(7)
+#define	TDES0_EXCESSIVE_COLLISIONS	BIT(8)
+#define	TDES0_LATE_COLLISION		BIT(9)
+#define	TDES0_NO_CARRIER		BIT(10)
+#define	TDES0_LOSS_CARRIER		BIT(11)
+#define	TDES0_PAYLOAD_ERROR		BIT(12)
+#define	TDES0_FRAME_FLUSHED		BIT(13)
+#define	TDES0_JABBER_TIMEOUT		BIT(14)
+#define	TDES0_ERROR_SUMMARY		BIT(15)
+#define	TDES0_IP_HEADER_ERROR		BIT(16)
+#define	TDES0_TIME_STAMP_STATUS		BIT(17)
+#define	TDES0_OWN			((u32)BIT(31))	/* silence sparse */
+/* TDES1 */
+#define	TDES1_BUFFER1_SIZE_MASK		GENMASK(10, 0)
+#define	TDES1_BUFFER2_SIZE_MASK		GENMASK(21, 11)
+#define	TDES1_BUFFER2_SIZE_SHIFT	11
+#define	TDES1_TIME_STAMP_ENABLE		BIT(22)
+#define	TDES1_DISABLE_PADDING		BIT(23)
+#define	TDES1_SECOND_ADDRESS_CHAINED	BIT(24)
+#define	TDES1_END_RING			BIT(25)
+#define	TDES1_CRC_DISABLE		BIT(26)
+#define	TDES1_CHECKSUM_INSERTION_MASK	GENMASK(28, 27)
+#define	TDES1_CHECKSUM_INSERTION_SHIFT	27
+#define	TDES1_FIRST_SEGMENT		BIT(29)
+#define	TDES1_LAST_SEGMENT		BIT(30)
+#define	TDES1_INTERRUPT			BIT(31)
+
+/* Enhanced transmit descriptor defines */
+/* TDES0 */
+#define	ETDES0_DEFERRED			BIT(0)
+#define	ETDES0_UNDERFLOW_ERROR		BIT(1)
+#define	ETDES0_EXCESSIVE_DEFERRAL	BIT(2)
+#define	ETDES0_COLLISION_COUNT_MASK	GENMASK(6, 3)
+#define	ETDES0_VLAN_FRAME		BIT(7)
+#define	ETDES0_EXCESSIVE_COLLISIONS	BIT(8)
+#define	ETDES0_LATE_COLLISION		BIT(9)
+#define	ETDES0_NO_CARRIER		BIT(10)
+#define	ETDES0_LOSS_CARRIER		BIT(11)
+#define	ETDES0_PAYLOAD_ERROR		BIT(12)
+#define	ETDES0_FRAME_FLUSHED		BIT(13)
+#define	ETDES0_JABBER_TIMEOUT		BIT(14)
+#define	ETDES0_ERROR_SUMMARY		BIT(15)
+#define	ETDES0_IP_HEADER_ERROR		BIT(16)
+#define	ETDES0_TIME_STAMP_STATUS	BIT(17)
+#define	ETDES0_SECOND_ADDRESS_CHAINED	BIT(20)
+#define	ETDES0_END_RING			BIT(21)
+#define	ETDES0_CHECKSUM_INSERTION_MASK	GENMASK(23, 22)
+#define	ETDES0_CHECKSUM_INSERTION_SHIFT	22
+#define	ETDES0_TIME_STAMP_ENABLE	BIT(25)
+#define	ETDES0_DISABLE_PADDING		BIT(26)
+#define	ETDES0_CRC_DISABLE		BIT(27)
+#define	ETDES0_FIRST_SEGMENT		BIT(28)
+#define	ETDES0_LAST_SEGMENT		BIT(29)
+#define	ETDES0_INTERRUPT		BIT(30)
+#define	ETDES0_OWN			((u32)BIT(31))	/* silence sparse */
+/* TDES1 */
+#define	ETDES1_BUFFER1_SIZE_MASK	GENMASK(12, 0)
+#define	ETDES1_BUFFER2_SIZE_MASK	GENMASK(28, 16)
+#define	ETDES1_BUFFER2_SIZE_SHIFT	16
+
+/* Extended Receive descriptor definitions */
+#define	ERDES4_IP_PAYLOAD_TYPE_MASK	GENMASK(6, 2)
+#define	ERDES4_IP_HDR_ERR		BIT(3)
+#define	ERDES4_IP_PAYLOAD_ERR		BIT(4)
+#define	ERDES4_IP_CSUM_BYPASSED		BIT(5)
+#define	ERDES4_IPV4_PKT_RCVD		BIT(6)
+#define	ERDES4_IPV6_PKT_RCVD		BIT(7)
+#define	ERDES4_MSG_TYPE_MASK		GENMASK(11, 8)
+#define	ERDES4_PTP_FRAME_TYPE		BIT(12)
+#define	ERDES4_PTP_VER			BIT(13)
+#define	ERDES4_TIMESTAMP_DROPPED	BIT(14)
+#define	ERDES4_AV_PKT_RCVD		BIT(16)
+#define	ERDES4_AV_TAGGED_PKT_RCVD	BIT(17)
+#define	ERDES4_VLAN_TAG_PRI_VAL_MASK	GENMASK(20, 18)
+#define	ERDES4_L3_FILTER_MATCH		BIT(24)
+#define	ERDES4_L4_FILTER_MATCH		BIT(25)
+#define	ERDES4_L3_L4_FILT_NO_MATCH_MASK	GENMASK(27, 26)
+
+/* Extended RDES4 message type definitions */
+#define RDES_EXT_NO_PTP			0x0
+#define RDES_EXT_SYNC			0x1
+#define RDES_EXT_FOLLOW_UP		0x2
+#define RDES_EXT_DELAY_REQ		0x3
+#define RDES_EXT_DELAY_RESP		0x4
+#define RDES_EXT_PDELAY_REQ		0x5
+#define RDES_EXT_PDELAY_RESP		0x6
+#define RDES_EXT_PDELAY_FOLLOW_UP	0x7
+#define RDES_PTP_ANNOUNCE		0x8
+#define RDES_PTP_MANAGEMENT		0x9
+#define RDES_PTP_SIGNALING		0xa
+#define RDES_PTP_PKT_RESERVED_TYPE	0xf
+
+/* Basic descriptor structure for normal and alternate descriptors */
+struct dma_desc {
+	__le32 des0;
+	__le32 des1;
+	__le32 des2;
+	__le32 des3;
+};
+
+/* Extended descriptor structure (e.g. >= databook 3.50a) */
+struct dma_extended_desc {
+	struct dma_desc basic;	/* Basic descriptors */
+	__le32 des4;	/* Extended Status */
+	__le32 des5;	/* Reserved */
+	__le32 des6;	/* Tx/Rx Timestamp Low */
+	__le32 des7;	/* Tx/Rx Timestamp High */
+};
+
+/* Enhanced descriptor for TBS */
+struct dma_edesc {
+	__le32 des4;
+	__le32 des5;
+	__le32 des6;
+	__le32 des7;
+	struct dma_desc basic;
+};
+
+/* Transmit checksum insertion control */
+#define	TX_CIC_FULL	3	/* Include IP header and pseudoheader */
+
+#endif /* __DESCS_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac1000.h b/net/rtnet/drivers/orange-pi-one/dwmac1000.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac1000.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac1000.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,333 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+#ifndef __DWMAC1000_H__
+#define __DWMAC1000_H__
+
+#include <linux/phy.h>
+#include "common.h"
+
+#define GMAC_CONTROL		0x00000000	/* Configuration */
+#define GMAC_FRAME_FILTER	0x00000004	/* Frame Filter */
+#define GMAC_HASH_HIGH		0x00000008	/* Multicast Hash Table High */
+#define GMAC_HASH_LOW		0x0000000c	/* Multicast Hash Table Low */
+#define GMAC_MII_ADDR		0x00000010	/* MII Address */
+#define GMAC_MII_DATA		0x00000014	/* MII Data */
+#define GMAC_FLOW_CTRL		0x00000018	/* Flow Control */
+#define GMAC_VLAN_TAG		0x0000001c	/* VLAN Tag */
+#define GMAC_DEBUG		0x00000024	/* GMAC debug register */
+#define GMAC_WAKEUP_FILTER	0x00000028	/* Wake-up Frame Filter */
+
+#define GMAC_INT_STATUS		0x00000038	/* interrupt status register */
+#define GMAC_INT_STATUS_PMT	BIT(3)
+#define GMAC_INT_STATUS_MMCIS	BIT(4)
+#define GMAC_INT_STATUS_MMCRIS	BIT(5)
+#define GMAC_INT_STATUS_MMCTIS	BIT(6)
+#define GMAC_INT_STATUS_MMCCSUM	BIT(7)
+#define GMAC_INT_STATUS_TSTAMP	BIT(9)
+#define GMAC_INT_STATUS_LPIIS	BIT(10)
+
+/* interrupt mask register */
+#define	GMAC_INT_MASK		0x0000003c
+#define	GMAC_INT_DISABLE_RGMII		BIT(0)
+#define	GMAC_INT_DISABLE_PCSLINK	BIT(1)
+#define	GMAC_INT_DISABLE_PCSAN		BIT(2)
+#define	GMAC_INT_DISABLE_PMT		BIT(3)
+#define	GMAC_INT_DISABLE_TIMESTAMP	BIT(9)
+#define	GMAC_INT_DISABLE_PCS	(GMAC_INT_DISABLE_RGMII | \
+				 GMAC_INT_DISABLE_PCSLINK | \
+				 GMAC_INT_DISABLE_PCSAN)
+#define	GMAC_INT_DEFAULT_MASK	(GMAC_INT_DISABLE_TIMESTAMP | \
+				 GMAC_INT_DISABLE_PCS)
+
+/* PMT Control and Status */
+#define GMAC_PMT		0x0000002c
+enum power_event {
+	pointer_reset = 0x80000000,
+	global_unicast = 0x00000200,
+	wake_up_rx_frame = 0x00000040,
+	magic_frame = 0x00000020,
+	wake_up_frame_en = 0x00000004,
+	magic_pkt_en = 0x00000002,
+	power_down = 0x00000001,
+};
+
+/* Energy Efficient Ethernet (EEE)
+ *
+ * LPI status, timer and control register offset
+ */
+#define LPI_CTRL_STATUS	0x0030
+#define LPI_TIMER_CTRL	0x0034
+
+/* LPI control and status defines */
+#define LPI_CTRL_STATUS_LPITXA	0x00080000	/* Enable LPI TX Automate */
+#define LPI_CTRL_STATUS_PLSEN	0x00040000	/* Enable PHY Link Status */
+#define LPI_CTRL_STATUS_PLS	0x00020000	/* PHY Link Status */
+#define LPI_CTRL_STATUS_LPIEN	0x00010000	/* LPI Enable */
+#define LPI_CTRL_STATUS_RLPIST	0x00000200	/* Receive LPI state */
+#define LPI_CTRL_STATUS_TLPIST	0x00000100	/* Transmit LPI state */
+#define LPI_CTRL_STATUS_RLPIEX	0x00000008	/* Receive LPI Exit */
+#define LPI_CTRL_STATUS_RLPIEN	0x00000004	/* Receive LPI Entry */
+#define LPI_CTRL_STATUS_TLPIEX	0x00000002	/* Transmit LPI Exit */
+#define LPI_CTRL_STATUS_TLPIEN	0x00000001	/* Transmit LPI Entry */
+
+/* GMAC HW ADDR regs */
+#define GMAC_ADDR_HIGH(reg)	(((reg > 15) ? 0x00000800 : 0x00000040) + \
+				(reg * 8))
+#define GMAC_ADDR_LOW(reg)	(((reg > 15) ? 0x00000804 : 0x00000044) + \
+				(reg * 8))
+#define GMAC_MAX_PERFECT_ADDRESSES	1
+
+#define GMAC_PCS_BASE		0x000000c0	/* PCS register base */
+#define GMAC_RGSMIIIS		0x000000d8	/* RGMII/SMII status */
+
+/* SGMII/RGMII status register */
+#define GMAC_RGSMIIIS_LNKMODE		BIT(0)
+#define GMAC_RGSMIIIS_SPEED		GENMASK(2, 1)
+#define GMAC_RGSMIIIS_SPEED_SHIFT	1
+#define GMAC_RGSMIIIS_LNKSTS		BIT(3)
+#define GMAC_RGSMIIIS_JABTO		BIT(4)
+#define GMAC_RGSMIIIS_FALSECARDET	BIT(5)
+#define GMAC_RGSMIIIS_SMIDRXS		BIT(16)
+/* LNKMOD */
+#define GMAC_RGSMIIIS_LNKMOD_MASK	0x1
+/* LNKSPEED */
+#define GMAC_RGSMIIIS_SPEED_125		0x2
+#define GMAC_RGSMIIIS_SPEED_25		0x1
+#define GMAC_RGSMIIIS_SPEED_2_5		0x0
+
+/* GMAC Configuration defines */
+#define GMAC_CONTROL_2K 0x08000000	/* IEEE 802.3as 2K packets */
+#define GMAC_CONTROL_TC	0x01000000	/* Transmit Conf. in RGMII/SGMII */
+#define GMAC_CONTROL_WD	0x00800000	/* Disable Watchdog on receive */
+#define GMAC_CONTROL_JD	0x00400000	/* Jabber disable */
+#define GMAC_CONTROL_BE	0x00200000	/* Frame Burst Enable */
+#define GMAC_CONTROL_JE	0x00100000	/* Jumbo frame */
+enum inter_frame_gap {
+	GMAC_CONTROL_IFG_88 = 0x00040000,
+	GMAC_CONTROL_IFG_80 = 0x00020000,
+	GMAC_CONTROL_IFG_40 = 0x000e0000,
+};
+#define GMAC_CONTROL_DCRS	0x00010000	/* Disable carrier sense */
+#define GMAC_CONTROL_PS		0x00008000	/* Port Select 0:GMI 1:MII */
+#define GMAC_CONTROL_FES	0x00004000	/* Speed 0:10 1:100 */
+#define GMAC_CONTROL_DO		0x00002000	/* Disable Rx Own */
+#define GMAC_CONTROL_LM		0x00001000	/* Loop-back mode */
+#define GMAC_CONTROL_DM		0x00000800	/* Duplex Mode */
+#define GMAC_CONTROL_IPC	0x00000400	/* Checksum Offload */
+#define GMAC_CONTROL_DR		0x00000200	/* Disable Retry */
+#define GMAC_CONTROL_LUD	0x00000100	/* Link up/down */
+#define GMAC_CONTROL_ACS	0x00000080	/* Auto Pad/FCS Stripping */
+#define GMAC_CONTROL_DC		0x00000010	/* Deferral Check */
+#define GMAC_CONTROL_TE		0x00000008	/* Transmitter Enable */
+#define GMAC_CONTROL_RE		0x00000004	/* Receiver Enable */
+
+#define GMAC_CORE_INIT (GMAC_CONTROL_JD | GMAC_CONTROL_PS | GMAC_CONTROL_ACS | \
+			GMAC_CONTROL_BE | GMAC_CONTROL_DCRS)
+
+/* GMAC Frame Filter defines */
+#define GMAC_FRAME_FILTER_PR	0x00000001	/* Promiscuous Mode */
+#define GMAC_FRAME_FILTER_HUC	0x00000002	/* Hash Unicast */
+#define GMAC_FRAME_FILTER_HMC	0x00000004	/* Hash Multicast */
+#define GMAC_FRAME_FILTER_DAIF	0x00000008	/* DA Inverse Filtering */
+#define GMAC_FRAME_FILTER_PM	0x00000010	/* Pass all multicast */
+#define GMAC_FRAME_FILTER_DBF	0x00000020	/* Disable Broadcast frames */
+#define GMAC_FRAME_FILTER_PCF	0x00000080	/* Pass Control frames */
+#define GMAC_FRAME_FILTER_SAIF	0x00000100	/* Inverse Filtering */
+#define GMAC_FRAME_FILTER_SAF	0x00000200	/* Source Address Filter */
+#define GMAC_FRAME_FILTER_HPF	0x00000400	/* Hash or perfect Filter */
+#define GMAC_FRAME_FILTER_RA	0x80000000	/* Receive all mode */
+/* GMII ADDR  defines */
+#define GMAC_MII_ADDR_WRITE	0x00000002	/* MII Write */
+#define GMAC_MII_ADDR_BUSY	0x00000001	/* MII Busy */
+/* GMAC FLOW CTRL defines */
+#define GMAC_FLOW_CTRL_PT_MASK	0xffff0000	/* Pause Time Mask */
+#define GMAC_FLOW_CTRL_PT_SHIFT	16
+#define GMAC_FLOW_CTRL_UP	0x00000008	/* Unicast pause frame enable */
+#define GMAC_FLOW_CTRL_RFE	0x00000004	/* Rx Flow Control Enable */
+#define GMAC_FLOW_CTRL_TFE	0x00000002	/* Tx Flow Control Enable */
+#define GMAC_FLOW_CTRL_FCB_BPA	0x00000001	/* Flow Control Busy ... */
+
+/* DEBUG Register defines */
+/* MTL TxStatus FIFO */
+#define GMAC_DEBUG_TXSTSFSTS	BIT(25)	/* MTL TxStatus FIFO Full Status */
+#define GMAC_DEBUG_TXFSTS	BIT(24) /* MTL Tx FIFO Not Empty Status */
+#define GMAC_DEBUG_TWCSTS	BIT(22) /* MTL Tx FIFO Write Controller */
+/* MTL Tx FIFO Read Controller Status */
+#define GMAC_DEBUG_TRCSTS_MASK	GENMASK(21, 20)
+#define GMAC_DEBUG_TRCSTS_SHIFT	20
+#define GMAC_DEBUG_TRCSTS_IDLE	0
+#define GMAC_DEBUG_TRCSTS_READ	1
+#define GMAC_DEBUG_TRCSTS_TXW	2
+#define GMAC_DEBUG_TRCSTS_WRITE	3
+#define GMAC_DEBUG_TXPAUSED	BIT(19) /* MAC Transmitter in PAUSE */
+/* MAC Transmit Frame Controller Status */
+#define GMAC_DEBUG_TFCSTS_MASK	GENMASK(18, 17)
+#define GMAC_DEBUG_TFCSTS_SHIFT	17
+#define GMAC_DEBUG_TFCSTS_IDLE	0
+#define GMAC_DEBUG_TFCSTS_WAIT	1
+#define GMAC_DEBUG_TFCSTS_GEN_PAUSE	2
+#define GMAC_DEBUG_TFCSTS_XFER	3
+/* MAC GMII or MII Transmit Protocol Engine Status */
+#define GMAC_DEBUG_TPESTS	BIT(16)
+#define GMAC_DEBUG_RXFSTS_MASK	GENMASK(9, 8) /* MTL Rx FIFO Fill-level */
+#define GMAC_DEBUG_RXFSTS_SHIFT	8
+#define GMAC_DEBUG_RXFSTS_EMPTY	0
+#define GMAC_DEBUG_RXFSTS_BT	1
+#define GMAC_DEBUG_RXFSTS_AT	2
+#define GMAC_DEBUG_RXFSTS_FULL	3
+#define GMAC_DEBUG_RRCSTS_MASK	GENMASK(6, 5) /* MTL Rx FIFO Read Controller */
+#define GMAC_DEBUG_RRCSTS_SHIFT	5
+#define GMAC_DEBUG_RRCSTS_IDLE	0
+#define GMAC_DEBUG_RRCSTS_RDATA	1
+#define GMAC_DEBUG_RRCSTS_RSTAT	2
+#define GMAC_DEBUG_RRCSTS_FLUSH	3
+#define GMAC_DEBUG_RWCSTS	BIT(4) /* MTL Rx FIFO Write Controller Active */
+/* MAC Receive Frame Controller FIFO Status */
+#define GMAC_DEBUG_RFCFCSTS_MASK	GENMASK(2, 1)
+#define GMAC_DEBUG_RFCFCSTS_SHIFT	1
+/* MAC GMII or MII Receive Protocol Engine Status */
+#define GMAC_DEBUG_RPESTS	BIT(0)
+
+/*--- DMA BLOCK defines ---*/
+/* DMA Bus Mode register defines */
+#define DMA_BUS_MODE_DA		0x00000002	/* Arbitration scheme */
+#define DMA_BUS_MODE_DSL_MASK	0x0000007c	/* Descriptor Skip Length */
+#define DMA_BUS_MODE_DSL_SHIFT	2		/*   (in DWORDS)      */
+/* Programmable burst length (passed thorugh platform)*/
+#define DMA_BUS_MODE_PBL_MASK	0x00003f00	/* Programmable Burst Len */
+#define DMA_BUS_MODE_PBL_SHIFT	8
+#define DMA_BUS_MODE_ATDS	0x00000080	/* Alternate Descriptor Size */
+
+enum rx_tx_priority_ratio {
+	double_ratio = 0x00004000,	/* 2:1 */
+	triple_ratio = 0x00008000,	/* 3:1 */
+	quadruple_ratio = 0x0000c000,	/* 4:1 */
+};
+
+#define DMA_BUS_MODE_FB		0x00010000	/* Fixed burst */
+#define DMA_BUS_MODE_MB		0x04000000	/* Mixed burst */
+#define DMA_BUS_MODE_RPBL_MASK	0x007e0000	/* Rx-Programmable Burst Len */
+#define DMA_BUS_MODE_RPBL_SHIFT	17
+#define DMA_BUS_MODE_USP	0x00800000
+#define DMA_BUS_MODE_MAXPBL	0x01000000
+#define DMA_BUS_MODE_AAL	0x02000000
+
+/* DMA CRS Control and Status Register Mapping */
+#define DMA_HOST_TX_DESC	  0x00001048	/* Current Host Tx descriptor */
+#define DMA_HOST_RX_DESC	  0x0000104c	/* Current Host Rx descriptor */
+/*  DMA Bus Mode register defines */
+#define DMA_BUS_PR_RATIO_MASK	  0x0000c000	/* Rx/Tx priority ratio */
+#define DMA_BUS_PR_RATIO_SHIFT	  14
+#define DMA_BUS_FB	  	  0x00010000	/* Fixed Burst */
+
+/* DMA operation mode defines (start/stop tx/rx are placed in common header)*/
+/* Disable Drop TCP/IP csum error */
+#define DMA_CONTROL_DT		0x04000000
+#define DMA_CONTROL_RSF		0x02000000	/* Receive Store and Forward */
+#define DMA_CONTROL_DFF		0x01000000	/* Disaable flushing */
+/* Threshold for Activating the FC */
+enum rfa {
+	act_full_minus_1 = 0x00800000,
+	act_full_minus_2 = 0x00800200,
+	act_full_minus_3 = 0x00800400,
+	act_full_minus_4 = 0x00800600,
+};
+/* Threshold for Deactivating the FC */
+enum rfd {
+	deac_full_minus_1 = 0x00400000,
+	deac_full_minus_2 = 0x00400800,
+	deac_full_minus_3 = 0x00401000,
+	deac_full_minus_4 = 0x00401800,
+};
+#define DMA_CONTROL_TSF	0x00200000	/* Transmit  Store and Forward */
+
+enum ttc_control {
+	DMA_CONTROL_TTC_64 = 0x00000000,
+	DMA_CONTROL_TTC_128 = 0x00004000,
+	DMA_CONTROL_TTC_192 = 0x00008000,
+	DMA_CONTROL_TTC_256 = 0x0000c000,
+	DMA_CONTROL_TTC_40 = 0x00010000,
+	DMA_CONTROL_TTC_32 = 0x00014000,
+	DMA_CONTROL_TTC_24 = 0x00018000,
+	DMA_CONTROL_TTC_16 = 0x0001c000,
+};
+#define DMA_CONTROL_TC_TX_MASK	0xfffe3fff
+
+#define DMA_CONTROL_EFC		0x00000100
+#define DMA_CONTROL_FEF		0x00000080
+#define DMA_CONTROL_FUF		0x00000040
+
+/* Receive flow control activation field
+ * RFA field in DMA control register, bits 23,10:9
+ */
+#define DMA_CONTROL_RFA_MASK	0x00800600
+
+/* Receive flow control deactivation field
+ * RFD field in DMA control register, bits 22,12:11
+ */
+#define DMA_CONTROL_RFD_MASK	0x00401800
+
+/* RFD and RFA fields are encoded as follows
+ *
+ *   Bit Field
+ *   0,00 - Full minus 1KB (only valid when rxfifo >= 4KB and EFC enabled)
+ *   0,01 - Full minus 2KB (only valid when rxfifo >= 4KB and EFC enabled)
+ *   0,10 - Full minus 3KB (only valid when rxfifo >= 4KB and EFC enabled)
+ *   0,11 - Full minus 4KB (only valid when rxfifo > 4KB and EFC enabled)
+ *   1,00 - Full minus 5KB (only valid when rxfifo > 8KB and EFC enabled)
+ *   1,01 - Full minus 6KB (only valid when rxfifo > 8KB and EFC enabled)
+ *   1,10 - Full minus 7KB (only valid when rxfifo > 8KB and EFC enabled)
+ *   1,11 - Reserved
+ *
+ * RFD should always be > RFA for a given FIFO size. RFD == RFA may work,
+ * but packet throughput performance may not be as expected.
+ *
+ * Be sure that bit 3 in GMAC Register 6 is set for Unicast Pause frame
+ * detection (IEEE Specification Requirement, Annex 31B, 31B.1, Pause
+ * Description).
+ *
+ * Be sure that DZPA (bit 7 in Flow Control Register, GMAC Register 6),
+ * is set to 0. This allows pause frames with a quanta of 0 to be sent
+ * as an XOFF message to the link peer.
+ */
+
+#define RFA_FULL_MINUS_1K	0x00000000
+#define RFA_FULL_MINUS_2K	0x00000200
+#define RFA_FULL_MINUS_3K	0x00000400
+#define RFA_FULL_MINUS_4K	0x00000600
+#define RFA_FULL_MINUS_5K	0x00800000
+#define RFA_FULL_MINUS_6K	0x00800200
+#define RFA_FULL_MINUS_7K	0x00800400
+
+#define RFD_FULL_MINUS_1K	0x00000000
+#define RFD_FULL_MINUS_2K	0x00000800
+#define RFD_FULL_MINUS_3K	0x00001000
+#define RFD_FULL_MINUS_4K	0x00001800
+#define RFD_FULL_MINUS_5K	0x00400000
+#define RFD_FULL_MINUS_6K	0x00400800
+#define RFD_FULL_MINUS_7K	0x00401000
+
+enum rtc_control {
+	DMA_CONTROL_RTC_64 = 0x00000000,
+	DMA_CONTROL_RTC_32 = 0x00000008,
+	DMA_CONTROL_RTC_96 = 0x00000010,
+	DMA_CONTROL_RTC_128 = 0x00000018,
+};
+#define DMA_CONTROL_TC_RX_MASK	0xffffffe7
+
+#define DMA_CONTROL_OSF	0x00000004	/* Operate on second frame */
+
+/* MMC registers offset */
+#define GMAC_MMC_CTRL      0x100
+#define GMAC_MMC_RX_INTR   0x104
+#define GMAC_MMC_TX_INTR   0x108
+#define GMAC_MMC_RX_CSUM_OFFLOAD   0x208
+#define GMAC_EXTHASH_BASE  0x500
+
+extern const struct stmmac_dma_ops dwmac1000_dma_ops;
+#endif /* __DWMAC1000_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac100_core.c b/net/rtnet/drivers/orange-pi-one/dwmac100_core.c
--- a/net/rtnet/drivers/orange-pi-one/dwmac100_core.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac100_core.c	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,201 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This is the driver for the MAC 10/100 on-chip Ethernet controller
+  currently tested on all the ST boards based on STb7109 and stx7200 SoCs.
+
+  DWC Ether MAC 10/100 Universal version 4.0 has been used for developing
+  this code.
+
+  This only implements the mac core functions for this chip.
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/crc32.h>
+#include <net/dsa.h>
+#include <asm/io.h>
+#include "stmmac.h"
+#include "dwmac100.h"
+
+static void dwmac100_core_init(struct mac_device_info *hw,
+			       struct net_device *dev)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	u32 value = readl(ioaddr + MAC_CONTROL);
+
+	value |= MAC_CORE_INIT;
+
+	/* Clear ASTP bit because Ethernet switch tagging formats such as
+	 * Broadcom tags can look like invalid LLC/SNAP packets and cause the
+	 * hardware to truncate packets on reception.
+	 */
+	if (netdev_uses_dsa(dev))
+		value &= ~MAC_CONTROL_ASTP;
+
+	writel(value, ioaddr + MAC_CONTROL);
+
+#ifdef STMMAC_VLAN_TAG_USED
+	writel(ETH_P_8021Q, ioaddr + MAC_VLAN1);
+#endif
+}
+
+static void dwmac100_dump_mac_regs(struct mac_device_info *hw, u32 *reg_space)
+{
+	void __iomem *ioaddr = hw->pcsr;
+
+	reg_space[MAC_CONTROL / 4] = readl(ioaddr + MAC_CONTROL);
+	reg_space[MAC_ADDR_HIGH / 4] = readl(ioaddr + MAC_ADDR_HIGH);
+	reg_space[MAC_ADDR_LOW / 4] = readl(ioaddr + MAC_ADDR_LOW);
+	reg_space[MAC_HASH_HIGH / 4] = readl(ioaddr + MAC_HASH_HIGH);
+	reg_space[MAC_HASH_LOW / 4] = readl(ioaddr + MAC_HASH_LOW);
+	reg_space[MAC_FLOW_CTRL / 4] = readl(ioaddr + MAC_FLOW_CTRL);
+	reg_space[MAC_VLAN1 / 4] = readl(ioaddr + MAC_VLAN1);
+	reg_space[MAC_VLAN2 / 4] = readl(ioaddr + MAC_VLAN2);
+}
+
+static int dwmac100_rx_ipc_enable(struct mac_device_info *hw)
+{
+	return 0;
+}
+
+static int dwmac100_irq_status(struct mac_device_info *hw,
+			       struct stmmac_extra_stats *x)
+{
+	return 0;
+}
+
+static void dwmac100_set_umac_addr(struct mac_device_info *hw,
+				   unsigned char *addr,
+				   unsigned int reg_n)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	stmmac_set_mac_addr(ioaddr, addr, MAC_ADDR_HIGH, MAC_ADDR_LOW);
+}
+
+static void dwmac100_get_umac_addr(struct mac_device_info *hw,
+				   unsigned char *addr,
+				   unsigned int reg_n)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	stmmac_get_mac_addr(ioaddr, addr, MAC_ADDR_HIGH, MAC_ADDR_LOW);
+}
+
+static void dwmac100_set_filter(struct mac_device_info *hw,
+				struct net_device *dev)
+{
+	void __iomem *ioaddr = (void __iomem *)dev->base_addr;
+	u32 value = readl(ioaddr + MAC_CONTROL);
+
+	if (dev->flags & IFF_PROMISC) {
+		value |= MAC_CONTROL_PR;
+		value &= ~(MAC_CONTROL_PM | MAC_CONTROL_IF | MAC_CONTROL_HO |
+			   MAC_CONTROL_HP);
+	} else if ((netdev_mc_count(dev) > HASH_TABLE_SIZE)
+		   || (dev->flags & IFF_ALLMULTI)) {
+		value |= MAC_CONTROL_PM;
+		value &= ~(MAC_CONTROL_PR | MAC_CONTROL_IF | MAC_CONTROL_HO);
+		writel(0xffffffff, ioaddr + MAC_HASH_HIGH);
+		writel(0xffffffff, ioaddr + MAC_HASH_LOW);
+	} else if (netdev_mc_empty(dev)) {	/* no multicast */
+		value &= ~(MAC_CONTROL_PM | MAC_CONTROL_PR | MAC_CONTROL_IF |
+			   MAC_CONTROL_HO | MAC_CONTROL_HP);
+	} else {
+		u32 mc_filter[2];
+		struct netdev_hw_addr *ha;
+
+		/* Perfect filter mode for physical address and Hash
+		 * filter for multicast
+		 */
+		value |= MAC_CONTROL_HP;
+		value &= ~(MAC_CONTROL_PM | MAC_CONTROL_PR |
+			   MAC_CONTROL_IF | MAC_CONTROL_HO);
+
+		memset(mc_filter, 0, sizeof(mc_filter));
+		netdev_for_each_mc_addr(ha, dev) {
+			/* The upper 6 bits of the calculated CRC are used to
+			 * index the contens of the hash table
+			 */
+			int bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;
+			/* The most significant bit determines the register to
+			 * use (H/L) while the other 5 bits determine the bit
+			 * within the register.
+			 */
+			mc_filter[bit_nr >> 5] |= 1 << (bit_nr & 31);
+		}
+		writel(mc_filter[0], ioaddr + MAC_HASH_LOW);
+		writel(mc_filter[1], ioaddr + MAC_HASH_HIGH);
+	}
+
+	writel(value, ioaddr + MAC_CONTROL);
+}
+
+static void dwmac100_flow_ctrl(struct mac_device_info *hw, unsigned int duplex,
+			       unsigned int fc, unsigned int pause_time,
+			       u32 tx_cnt)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	unsigned int flow = MAC_FLOW_CTRL_ENABLE;
+
+	if (duplex)
+		flow |= (pause_time << MAC_FLOW_CTRL_PT_SHIFT);
+	writel(flow, ioaddr + MAC_FLOW_CTRL);
+}
+
+/* No PMT module supported on ST boards with this Eth chip. */
+static void dwmac100_pmt(struct mac_device_info *hw, unsigned long mode)
+{
+	return;
+}
+
+static void dwmac100_set_mac_loopback(void __iomem *ioaddr, bool enable)
+{
+	u32 value = readl(ioaddr + MAC_CONTROL);
+
+	if (enable)
+		value |= MAC_CONTROL_OM;
+	else
+		value &= ~MAC_CONTROL_OM;
+
+	writel(value, ioaddr + MAC_CONTROL);
+}
+
+const struct stmmac_ops dwmac100_ops = {
+	.core_init = dwmac100_core_init,
+	.set_mac = stmmac_set_mac,
+	.rx_ipc = dwmac100_rx_ipc_enable,
+	.dump_regs = dwmac100_dump_mac_regs,
+	.host_irq_status = dwmac100_irq_status,
+	.set_filter = dwmac100_set_filter,
+	.flow_ctrl = dwmac100_flow_ctrl,
+	.pmt = dwmac100_pmt,
+	.set_umac_addr = dwmac100_set_umac_addr,
+	.get_umac_addr = dwmac100_get_umac_addr,
+	.set_mac_loopback = dwmac100_set_mac_loopback,
+};
+
+int dwmac100_setup(struct stmmac_priv *priv)
+{
+	struct mac_device_info *mac = priv->hw;
+
+	dev_info(priv->device, "\tDWMAC100\n");
+
+	mac->pcsr = priv->ioaddr;
+	mac->link.duplex = MAC_CONTROL_F;
+	mac->link.speed10 = 0;
+	mac->link.speed100 = 0;
+	mac->link.speed1000 = 0;
+	mac->link.speed_mask = MAC_CONTROL_PS;
+	mac->mii.addr = MAC_MII_ADDR;
+	mac->mii.data = MAC_MII_DATA;
+	mac->mii.addr_shift = 11;
+	mac->mii.addr_mask = 0x0000F800;
+	mac->mii.reg_shift = 6;
+	mac->mii.reg_mask = 0x000007C0;
+	mac->mii.clk_csr_shift = 2;
+	mac->mii.clk_csr_mask = GENMASK(5, 2);
+
+	return 0;
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac100_dma.c b/net/rtnet/drivers/orange-pi-one/dwmac100_dma.c
--- a/net/rtnet/drivers/orange-pi-one/dwmac100_dma.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac100_dma.c	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,127 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This is the driver for the MAC 10/100 on-chip Ethernet controller
+  currently tested on all the ST boards based on STb7109 and stx7200 SoCs.
+
+  DWC Ether MAC 10/100 Universal version 4.0 has been used for developing
+  this code.
+
+  This contains the functions to handle the dma.
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <asm/io.h>
+#include "dwmac100.h"
+#include "dwmac_dma.h"
+
+static void dwmac100_dma_init(void __iomem *ioaddr,
+			      struct stmmac_dma_cfg *dma_cfg, int atds)
+{
+	/* Enable Application Access by writing to DMA CSR0 */
+	writel(DMA_BUS_MODE_DEFAULT | (dma_cfg->pbl << DMA_BUS_MODE_PBL_SHIFT),
+	       ioaddr + DMA_BUS_MODE);
+
+	/* Mask interrupts by writing to CSR7 */
+	writel(DMA_INTR_DEFAULT_MASK, ioaddr + DMA_INTR_ENA);
+}
+
+static void dwmac100_dma_init_rx(void __iomem *ioaddr,
+				 struct stmmac_dma_cfg *dma_cfg,
+				 dma_addr_t dma_rx_phy, u32 chan)
+{
+	/* RX descriptor base addr lists must be written into DMA CSR3 */
+	writel(lower_32_bits(dma_rx_phy), ioaddr + DMA_RCV_BASE_ADDR);
+}
+
+static void dwmac100_dma_init_tx(void __iomem *ioaddr,
+				 struct stmmac_dma_cfg *dma_cfg,
+				 dma_addr_t dma_tx_phy, u32 chan)
+{
+	/* TX descriptor base addr lists must be written into DMA CSR4 */
+	writel(lower_32_bits(dma_tx_phy), ioaddr + DMA_TX_BASE_ADDR);
+}
+
+/* Store and Forward capability is not used at all.
+ *
+ * The transmit threshold can be programmed by setting the TTC bits in the DMA
+ * control register.
+ */
+static void dwmac100_dma_operation_mode_tx(void __iomem *ioaddr, int mode,
+					   u32 channel, int fifosz, u8 qmode)
+{
+	u32 csr6 = readl(ioaddr + DMA_CONTROL);
+
+	if (mode <= 32)
+		csr6 |= DMA_CONTROL_TTC_32;
+	else if (mode <= 64)
+		csr6 |= DMA_CONTROL_TTC_64;
+	else
+		csr6 |= DMA_CONTROL_TTC_128;
+
+	writel(csr6, ioaddr + DMA_CONTROL);
+}
+
+static void dwmac100_dump_dma_regs(void __iomem *ioaddr, u32 *reg_space)
+{
+	int i;
+
+	for (i = 0; i < NUM_DWMAC100_DMA_REGS; i++)
+		reg_space[DMA_BUS_MODE / 4 + i] =
+			readl(ioaddr + DMA_BUS_MODE + i * 4);
+
+	reg_space[DMA_CUR_TX_BUF_ADDR / 4] =
+		readl(ioaddr + DMA_CUR_TX_BUF_ADDR);
+	reg_space[DMA_CUR_RX_BUF_ADDR / 4] =
+		readl(ioaddr + DMA_CUR_RX_BUF_ADDR);
+}
+
+/* DMA controller has two counters to track the number of the missed frames. */
+static void dwmac100_dma_diagnostic_fr(void *data, struct stmmac_extra_stats *x,
+				       void __iomem *ioaddr)
+{
+	struct net_device_stats *stats = (struct net_device_stats *)data;
+	u32 csr8 = readl(ioaddr + DMA_MISSED_FRAME_CTR);
+
+	if (unlikely(csr8)) {
+		if (csr8 & DMA_MISSED_FRAME_OVE) {
+			stats->rx_over_errors += 0x800;
+			x->rx_overflow_cntr += 0x800;
+		} else {
+			unsigned int ove_cntr;
+			ove_cntr = ((csr8 & DMA_MISSED_FRAME_OVE_CNTR) >> 17);
+			stats->rx_over_errors += ove_cntr;
+			x->rx_overflow_cntr += ove_cntr;
+		}
+
+		if (csr8 & DMA_MISSED_FRAME_OVE_M) {
+			stats->rx_missed_errors += 0xffff;
+			x->rx_missed_cntr += 0xffff;
+		} else {
+			unsigned int miss_f = (csr8 & DMA_MISSED_FRAME_M_CNTR);
+			stats->rx_missed_errors += miss_f;
+			x->rx_missed_cntr += miss_f;
+		}
+	}
+}
+
+const struct stmmac_dma_ops dwmac100_dma_ops = {
+	.reset = dwmac_dma_reset,
+	.init = dwmac100_dma_init,
+	.init_rx_chan = dwmac100_dma_init_rx,
+	.init_tx_chan = dwmac100_dma_init_tx,
+	.dump_regs = dwmac100_dump_dma_regs,
+	.dma_tx_mode = dwmac100_dma_operation_mode_tx,
+	.dma_diagnostic_fr = dwmac100_dma_diagnostic_fr,
+	.enable_dma_transmission = dwmac_enable_dma_transmission,
+	.enable_dma_irq = dwmac_enable_dma_irq,
+	.disable_dma_irq = dwmac_disable_dma_irq,
+	.start_tx = dwmac_dma_start_tx,
+	.stop_tx = dwmac_dma_stop_tx,
+	.start_rx = dwmac_dma_start_rx,
+	.stop_rx = dwmac_dma_stop_rx,
+	.dma_interrupt = dwmac_dma_interrupt,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac100.h b/net/rtnet/drivers/orange-pi-one/dwmac100.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac100.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac100.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,111 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  MAC 10/100 Header File
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __DWMAC100_H__
+#define __DWMAC100_H__
+
+#include <linux/phy.h>
+#include "common.h"
+
+/*----------------------------------------------------------------------------
+ *	 			MAC BLOCK defines
+ *---------------------------------------------------------------------------*/
+/* MAC CSR offset */
+#define MAC_CONTROL	0x00000000	/* MAC Control */
+#define MAC_ADDR_HIGH	0x00000004	/* MAC Address High */
+#define MAC_ADDR_LOW	0x00000008	/* MAC Address Low */
+#define MAC_HASH_HIGH	0x0000000c	/* Multicast Hash Table High */
+#define MAC_HASH_LOW	0x00000010	/* Multicast Hash Table Low */
+#define MAC_MII_ADDR	0x00000014	/* MII Address */
+#define MAC_MII_DATA	0x00000018	/* MII Data */
+#define MAC_FLOW_CTRL	0x0000001c	/* Flow Control */
+#define MAC_VLAN1	0x00000020	/* VLAN1 Tag */
+#define MAC_VLAN2	0x00000024	/* VLAN2 Tag */
+
+/* MAC CTRL defines */
+#define MAC_CONTROL_RA	0x80000000	/* Receive All Mode */
+#define MAC_CONTROL_BLE	0x40000000	/* Endian Mode */
+#define MAC_CONTROL_HBD	0x10000000	/* Heartbeat Disable */
+#define MAC_CONTROL_PS	0x08000000	/* Port Select */
+#define MAC_CONTROL_DRO	0x00800000	/* Disable Receive Own */
+#define MAC_CONTROL_EXT_LOOPBACK 0x00400000	/* Reserved (ext loopback?) */
+#define MAC_CONTROL_OM	0x00200000	/* Loopback Operating Mode */
+#define MAC_CONTROL_F	0x00100000	/* Full Duplex Mode */
+#define MAC_CONTROL_PM	0x00080000	/* Pass All Multicast */
+#define MAC_CONTROL_PR	0x00040000	/* Promiscuous Mode */
+#define MAC_CONTROL_IF	0x00020000	/* Inverse Filtering */
+#define MAC_CONTROL_PB	0x00010000	/* Pass Bad Frames */
+#define MAC_CONTROL_HO	0x00008000	/* Hash Only Filtering Mode */
+#define MAC_CONTROL_HP	0x00002000	/* Hash/Perfect Filtering Mode */
+#define MAC_CONTROL_LCC	0x00001000	/* Late Collision Control */
+#define MAC_CONTROL_DBF	0x00000800	/* Disable Broadcast Frames */
+#define MAC_CONTROL_DRTY	0x00000400	/* Disable Retry */
+#define MAC_CONTROL_ASTP	0x00000100	/* Automatic Pad Stripping */
+#define MAC_CONTROL_BOLMT_10	0x00000000	/* Back Off Limit 10 */
+#define MAC_CONTROL_BOLMT_8	0x00000040	/* Back Off Limit 8 */
+#define MAC_CONTROL_BOLMT_4	0x00000080	/* Back Off Limit 4 */
+#define MAC_CONTROL_BOLMT_1	0x000000c0	/* Back Off Limit 1 */
+#define MAC_CONTROL_DC		0x00000020	/* Deferral Check */
+#define MAC_CONTROL_TE		0x00000008	/* Transmitter Enable */
+#define MAC_CONTROL_RE		0x00000004	/* Receiver Enable */
+
+#define MAC_CORE_INIT (MAC_CONTROL_HBD | MAC_CONTROL_ASTP)
+
+/* MAC FLOW CTRL defines */
+#define MAC_FLOW_CTRL_PT_MASK	0xffff0000	/* Pause Time Mask */
+#define MAC_FLOW_CTRL_PT_SHIFT	16
+#define MAC_FLOW_CTRL_PASS	0x00000004	/* Pass Control Frames */
+#define MAC_FLOW_CTRL_ENABLE	0x00000002	/* Flow Control Enable */
+#define MAC_FLOW_CTRL_PAUSE	0x00000001	/* Flow Control Busy ... */
+
+/* MII ADDR  defines */
+#define MAC_MII_ADDR_WRITE	0x00000002	/* MII Write */
+#define MAC_MII_ADDR_BUSY	0x00000001	/* MII Busy */
+
+/*----------------------------------------------------------------------------
+ * 				DMA BLOCK defines
+ *---------------------------------------------------------------------------*/
+
+/* DMA Bus Mode register defines */
+#define DMA_BUS_MODE_DBO	0x00100000	/* Descriptor Byte Ordering */
+#define DMA_BUS_MODE_BLE	0x00000080	/* Big Endian/Little Endian */
+#define DMA_BUS_MODE_PBL_MASK	0x00003f00	/* Programmable Burst Len */
+#define DMA_BUS_MODE_PBL_SHIFT	8
+#define DMA_BUS_MODE_DSL_MASK	0x0000007c	/* Descriptor Skip Length */
+#define DMA_BUS_MODE_DSL_SHIFT	2	/*   (in DWORDS)      */
+#define DMA_BUS_MODE_BAR_BUS	0x00000002	/* Bar-Bus Arbitration */
+#define DMA_BUS_MODE_DEFAULT	0x00000000
+
+/* DMA Control register defines */
+#define DMA_CONTROL_SF		0x00200000	/* Store And Forward */
+
+/* Transmit Threshold Control */
+enum ttc_control {
+	DMA_CONTROL_TTC_DEFAULT = 0x00000000,	/* Threshold is 32 DWORDS */
+	DMA_CONTROL_TTC_64 = 0x00004000,	/* Threshold is 64 DWORDS */
+	DMA_CONTROL_TTC_128 = 0x00008000,	/* Threshold is 128 DWORDS */
+	DMA_CONTROL_TTC_256 = 0x0000c000,	/* Threshold is 256 DWORDS */
+	DMA_CONTROL_TTC_18 = 0x00400000,	/* Threshold is 18 DWORDS */
+	DMA_CONTROL_TTC_24 = 0x00404000,	/* Threshold is 24 DWORDS */
+	DMA_CONTROL_TTC_32 = 0x00408000,	/* Threshold is 32 DWORDS */
+	DMA_CONTROL_TTC_40 = 0x0040c000,	/* Threshold is 40 DWORDS */
+	DMA_CONTROL_SE = 0x00000008,	/* Stop On Empty */
+	DMA_CONTROL_OSF = 0x00000004,	/* Operate On 2nd Frame */
+};
+
+/* STMAC110 DMA Missed Frame Counter register defines */
+#define DMA_MISSED_FRAME_OVE	0x10000000	/* FIFO Overflow Overflow */
+#define DMA_MISSED_FRAME_OVE_CNTR 0x0ffe0000	/* Overflow Frame Counter */
+#define DMA_MISSED_FRAME_OVE_M	0x00010000	/* Missed Frame Overflow */
+#define DMA_MISSED_FRAME_M_CNTR	0x0000ffff	/* Missed Frame Couinter */
+
+extern const struct stmmac_dma_ops dwmac100_dma_ops;
+
+#endif /* __DWMAC100_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac4_descs.h b/net/rtnet/drivers/orange-pi-one/dwmac4_descs.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac4_descs.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac4_descs.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,147 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Header File to describe the DMA descriptors and related definitions specific
+ * for DesignWare databook 4.xx.
+ *
+ * Copyright (C) 2015  STMicroelectronics Ltd
+ *
+ * Author: Alexandre Torgue <alexandre.torgue@st.com>
+ */
+
+#ifndef __DWMAC4_DESCS_H__
+#define __DWMAC4_DESCS_H__
+
+#include <linux/bitops.h>
+
+/* Normal transmit descriptor defines (without split feature) */
+
+/* TDES2 (read format) */
+#define TDES2_BUFFER1_SIZE_MASK		GENMASK(13, 0)
+#define TDES2_VLAN_TAG_MASK		GENMASK(15, 14)
+#define TDES2_VLAN_TAG_SHIFT		14
+#define TDES2_BUFFER2_SIZE_MASK		GENMASK(29, 16)
+#define TDES2_BUFFER2_SIZE_MASK_SHIFT	16
+#define TDES3_IVTIR_MASK		GENMASK(19, 18)
+#define TDES3_IVTIR_SHIFT		18
+#define TDES3_IVLTV			BIT(17)
+#define TDES2_TIMESTAMP_ENABLE		BIT(30)
+#define TDES2_IVT_MASK			GENMASK(31, 16)
+#define TDES2_IVT_SHIFT			16
+#define TDES2_INTERRUPT_ON_COMPLETION	BIT(31)
+
+/* TDES3 (read format) */
+#define TDES3_PACKET_SIZE_MASK		GENMASK(14, 0)
+#define TDES3_VLAN_TAG			GENMASK(15, 0)
+#define TDES3_VLTV			BIT(16)
+#define TDES3_CHECKSUM_INSERTION_MASK	GENMASK(17, 16)
+#define TDES3_CHECKSUM_INSERTION_SHIFT	16
+#define TDES3_TCP_PKT_PAYLOAD_MASK	GENMASK(17, 0)
+#define TDES3_TCP_SEGMENTATION_ENABLE	BIT(18)
+#define TDES3_HDR_LEN_SHIFT		19
+#define TDES3_SLOT_NUMBER_MASK		GENMASK(22, 19)
+#define TDES3_SA_INSERT_CTRL_MASK	GENMASK(25, 23)
+#define TDES3_SA_INSERT_CTRL_SHIFT	23
+#define TDES3_CRC_PAD_CTRL_MASK		GENMASK(27, 26)
+
+/* TDES3 (write back format) */
+#define TDES3_IP_HDR_ERROR		BIT(0)
+#define TDES3_DEFERRED			BIT(1)
+#define TDES3_UNDERFLOW_ERROR		BIT(2)
+#define TDES3_EXCESSIVE_DEFERRAL	BIT(3)
+#define TDES3_COLLISION_COUNT_MASK	GENMASK(7, 4)
+#define TDES3_COLLISION_COUNT_SHIFT	4
+#define TDES3_EXCESSIVE_COLLISION	BIT(8)
+#define TDES3_LATE_COLLISION		BIT(9)
+#define TDES3_NO_CARRIER		BIT(10)
+#define TDES3_LOSS_CARRIER		BIT(11)
+#define TDES3_PAYLOAD_ERROR		BIT(12)
+#define TDES3_PACKET_FLUSHED		BIT(13)
+#define TDES3_JABBER_TIMEOUT		BIT(14)
+#define TDES3_ERROR_SUMMARY		BIT(15)
+#define TDES3_TIMESTAMP_STATUS		BIT(17)
+#define TDES3_TIMESTAMP_STATUS_SHIFT	17
+
+/* TDES3 context */
+#define TDES3_CTXT_TCMSSV		BIT(26)
+
+/* TDES3 Common */
+#define	TDES3_RS1V			BIT(26)
+#define	TDES3_RS1V_SHIFT		26
+#define TDES3_LAST_DESCRIPTOR		BIT(28)
+#define TDES3_LAST_DESCRIPTOR_SHIFT	28
+#define TDES3_FIRST_DESCRIPTOR		BIT(29)
+#define TDES3_CONTEXT_TYPE		BIT(30)
+#define	TDES3_CONTEXT_TYPE_SHIFT	30
+
+/* TDES4 */
+#define TDES4_LTV			BIT(31)
+#define TDES4_LT			GENMASK(7, 0)
+
+/* TDES5 */
+#define TDES5_LT			GENMASK(31, 8)
+
+/* TDS3 use for both format (read and write back) */
+#define TDES3_OWN			BIT(31)
+#define TDES3_OWN_SHIFT			31
+
+/* Normal receive descriptor defines (without split feature) */
+
+/* RDES0 (write back format) */
+#define RDES0_VLAN_TAG_MASK		GENMASK(15, 0)
+
+/* RDES1 (write back format) */
+#define RDES1_IP_PAYLOAD_TYPE_MASK	GENMASK(2, 0)
+#define RDES1_IP_HDR_ERROR		BIT(3)
+#define RDES1_IPV4_HEADER		BIT(4)
+#define RDES1_IPV6_HEADER		BIT(5)
+#define RDES1_IP_CSUM_BYPASSED		BIT(6)
+#define RDES1_IP_CSUM_ERROR		BIT(7)
+#define RDES1_PTP_MSG_TYPE_MASK		GENMASK(11, 8)
+#define RDES1_PTP_PACKET_TYPE		BIT(12)
+#define RDES1_PTP_VER			BIT(13)
+#define RDES1_TIMESTAMP_AVAILABLE	BIT(14)
+#define RDES1_TIMESTAMP_AVAILABLE_SHIFT	14
+#define RDES1_TIMESTAMP_DROPPED		BIT(15)
+#define RDES1_IP_TYPE1_CSUM_MASK	GENMASK(31, 16)
+
+/* RDES2 (write back format) */
+#define RDES2_L3_L4_HEADER_SIZE_MASK	GENMASK(9, 0)
+#define RDES2_VLAN_FILTER_STATUS	BIT(15)
+#define RDES2_SA_FILTER_FAIL		BIT(16)
+#define RDES2_DA_FILTER_FAIL		BIT(17)
+#define RDES2_HASH_FILTER_STATUS	BIT(18)
+#define RDES2_MAC_ADDR_MATCH_MASK	GENMASK(26, 19)
+#define RDES2_HASH_VALUE_MATCH_MASK	GENMASK(26, 19)
+#define RDES2_L3_FILTER_MATCH		BIT(27)
+#define RDES2_L4_FILTER_MATCH		BIT(28)
+#define RDES2_L3_L4_FILT_NB_MATCH_MASK	GENMASK(27, 26)
+#define RDES2_L3_L4_FILT_NB_MATCH_SHIFT	26
+#define RDES2_HL			GENMASK(9, 0)
+
+/* RDES3 (write back format) */
+#define RDES3_PACKET_SIZE_MASK		GENMASK(14, 0)
+#define RDES3_ERROR_SUMMARY		BIT(15)
+#define RDES3_PACKET_LEN_TYPE_MASK	GENMASK(18, 16)
+#define RDES3_DRIBBLE_ERROR		BIT(19)
+#define RDES3_RECEIVE_ERROR		BIT(20)
+#define RDES3_OVERFLOW_ERROR		BIT(21)
+#define RDES3_RECEIVE_WATCHDOG		BIT(22)
+#define RDES3_GIANT_PACKET		BIT(23)
+#define RDES3_CRC_ERROR			BIT(24)
+#define RDES3_RDES0_VALID		BIT(25)
+#define RDES3_RDES1_VALID		BIT(26)
+#define RDES3_RDES2_VALID		BIT(27)
+#define RDES3_LAST_DESCRIPTOR		BIT(28)
+#define RDES3_FIRST_DESCRIPTOR		BIT(29)
+#define RDES3_CONTEXT_DESCRIPTOR	BIT(30)
+#define RDES3_CONTEXT_DESCRIPTOR_SHIFT	30
+
+/* RDES3 (read format) */
+#define RDES3_BUFFER1_VALID_ADDR	BIT(24)
+#define RDES3_BUFFER2_VALID_ADDR	BIT(25)
+#define RDES3_INT_ON_COMPLETION_EN	BIT(30)
+
+/* TDS3 use for both format (read and write back) */
+#define RDES3_OWN			BIT(31)
+
+#endif /* __DWMAC4_DESCS_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac4_dma.h b/net/rtnet/drivers/orange-pi-one/dwmac4_dma.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac4_dma.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac4_dma.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,215 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * DWMAC4 DMA Header file.
+ *
+ * Copyright (C) 2007-2015  STMicroelectronics Ltd
+ *
+ * Author: Alexandre Torgue <alexandre.torgue@st.com>
+ */
+
+#ifndef __DWMAC4_DMA_H__
+#define __DWMAC4_DMA_H__
+
+/* Define the max channel number used for tx (also rx).
+ * dwmac4 accepts up to 8 channels for TX (and also 8 channels for RX
+ */
+#define DMA_CHANNEL_NB_MAX		1
+
+#define DMA_BUS_MODE			0x00001000
+#define DMA_SYS_BUS_MODE		0x00001004
+#define DMA_STATUS			0x00001008
+#define DMA_DEBUG_STATUS_0		0x0000100c
+#define DMA_DEBUG_STATUS_1		0x00001010
+#define DMA_DEBUG_STATUS_2		0x00001014
+#define DMA_AXI_BUS_MODE		0x00001028
+#define DMA_TBS_CTRL			0x00001050
+
+/* DMA Bus Mode bitmap */
+#define DMA_BUS_MODE_SFT_RESET		BIT(0)
+
+/* DMA SYS Bus Mode bitmap */
+#define DMA_BUS_MODE_SPH		BIT(24)
+#define DMA_BUS_MODE_PBL		BIT(16)
+#define DMA_BUS_MODE_PBL_SHIFT		16
+#define DMA_BUS_MODE_RPBL_SHIFT		16
+#define DMA_BUS_MODE_MB			BIT(14)
+#define DMA_BUS_MODE_FB			BIT(0)
+
+/* DMA Interrupt top status */
+#define DMA_STATUS_MAC			BIT(17)
+#define DMA_STATUS_MTL			BIT(16)
+#define DMA_STATUS_CHAN7		BIT(7)
+#define DMA_STATUS_CHAN6		BIT(6)
+#define DMA_STATUS_CHAN5		BIT(5)
+#define DMA_STATUS_CHAN4		BIT(4)
+#define DMA_STATUS_CHAN3		BIT(3)
+#define DMA_STATUS_CHAN2		BIT(2)
+#define DMA_STATUS_CHAN1		BIT(1)
+#define DMA_STATUS_CHAN0		BIT(0)
+
+/* DMA debug status bitmap */
+#define DMA_DEBUG_STATUS_TS_MASK	0xf
+#define DMA_DEBUG_STATUS_RS_MASK	0xf
+
+/* DMA AXI bitmap */
+#define DMA_AXI_EN_LPI			BIT(31)
+#define DMA_AXI_LPI_XIT_FRM		BIT(30)
+#define DMA_AXI_WR_OSR_LMT		GENMASK(27, 24)
+#define DMA_AXI_WR_OSR_LMT_SHIFT	24
+#define DMA_AXI_RD_OSR_LMT		GENMASK(19, 16)
+#define DMA_AXI_RD_OSR_LMT_SHIFT	16
+
+#define DMA_AXI_OSR_MAX			0xf
+#define DMA_AXI_MAX_OSR_LIMIT ((DMA_AXI_OSR_MAX << DMA_AXI_WR_OSR_LMT_SHIFT) | \
+				(DMA_AXI_OSR_MAX << DMA_AXI_RD_OSR_LMT_SHIFT))
+
+#define DMA_SYS_BUS_MB			BIT(14)
+#define DMA_AXI_1KBBE			BIT(13)
+#define DMA_SYS_BUS_AAL			BIT(12)
+#define DMA_SYS_BUS_EAME		BIT(11)
+#define DMA_AXI_BLEN256			BIT(7)
+#define DMA_AXI_BLEN128			BIT(6)
+#define DMA_AXI_BLEN64			BIT(5)
+#define DMA_AXI_BLEN32			BIT(4)
+#define DMA_AXI_BLEN16			BIT(3)
+#define DMA_AXI_BLEN8			BIT(2)
+#define DMA_AXI_BLEN4			BIT(1)
+#define DMA_SYS_BUS_FB			BIT(0)
+
+#define DMA_BURST_LEN_DEFAULT		(DMA_AXI_BLEN256 | DMA_AXI_BLEN128 | \
+					DMA_AXI_BLEN64 | DMA_AXI_BLEN32 | \
+					DMA_AXI_BLEN16 | DMA_AXI_BLEN8 | \
+					DMA_AXI_BLEN4)
+
+#define DMA_AXI_BURST_LEN_MASK		0x000000FE
+
+/* DMA TBS Control */
+#define DMA_TBS_FTOS			GENMASK(31, 8)
+#define DMA_TBS_FTOV			BIT(0)
+#define DMA_TBS_DEF_FTOS		(DMA_TBS_FTOS | DMA_TBS_FTOV)
+
+/* Following DMA defines are chanels oriented */
+#define DMA_CHAN_BASE_ADDR		0x00001100
+#define DMA_CHAN_BASE_OFFSET		0x80
+#define DMA_CHANX_BASE_ADDR(x)		(DMA_CHAN_BASE_ADDR + \
+					(x * DMA_CHAN_BASE_OFFSET))
+#define DMA_CHAN_REG_NUMBER		17
+
+#define DMA_CHAN_CONTROL(x)		DMA_CHANX_BASE_ADDR(x)
+#define DMA_CHAN_TX_CONTROL(x)		(DMA_CHANX_BASE_ADDR(x) + 0x4)
+#define DMA_CHAN_RX_CONTROL(x)		(DMA_CHANX_BASE_ADDR(x) + 0x8)
+#define DMA_CHAN_TX_BASE_ADDR_HI(x)	(DMA_CHANX_BASE_ADDR(x) + 0x10)
+#define DMA_CHAN_TX_BASE_ADDR(x)	(DMA_CHANX_BASE_ADDR(x) + 0x14)
+#define DMA_CHAN_RX_BASE_ADDR_HI(x)	(DMA_CHANX_BASE_ADDR(x) + 0x18)
+#define DMA_CHAN_RX_BASE_ADDR(x)	(DMA_CHANX_BASE_ADDR(x) + 0x1c)
+#define DMA_CHAN_TX_END_ADDR(x)		(DMA_CHANX_BASE_ADDR(x) + 0x20)
+#define DMA_CHAN_RX_END_ADDR(x)		(DMA_CHANX_BASE_ADDR(x) + 0x28)
+#define DMA_CHAN_TX_RING_LEN(x)		(DMA_CHANX_BASE_ADDR(x) + 0x2c)
+#define DMA_CHAN_RX_RING_LEN(x)		(DMA_CHANX_BASE_ADDR(x) + 0x30)
+#define DMA_CHAN_INTR_ENA(x)		(DMA_CHANX_BASE_ADDR(x) + 0x34)
+#define DMA_CHAN_RX_WATCHDOG(x)		(DMA_CHANX_BASE_ADDR(x) + 0x38)
+#define DMA_CHAN_SLOT_CTRL_STATUS(x)	(DMA_CHANX_BASE_ADDR(x) + 0x3c)
+#define DMA_CHAN_CUR_TX_DESC(x)		(DMA_CHANX_BASE_ADDR(x) + 0x44)
+#define DMA_CHAN_CUR_RX_DESC(x)		(DMA_CHANX_BASE_ADDR(x) + 0x4c)
+#define DMA_CHAN_CUR_TX_BUF_ADDR(x)	(DMA_CHANX_BASE_ADDR(x) + 0x54)
+#define DMA_CHAN_CUR_RX_BUF_ADDR(x)	(DMA_CHANX_BASE_ADDR(x) + 0x5c)
+#define DMA_CHAN_STATUS(x)		(DMA_CHANX_BASE_ADDR(x) + 0x60)
+
+/* DMA Control X */
+#define DMA_CONTROL_SPH			BIT(24)
+#define DMA_CONTROL_MSS_MASK		GENMASK(13, 0)
+
+/* DMA Tx Channel X Control register defines */
+#define DMA_CONTROL_EDSE		BIT(28)
+#define DMA_CONTROL_TSE			BIT(12)
+#define DMA_CONTROL_OSP			BIT(4)
+#define DMA_CONTROL_ST			BIT(0)
+
+/* DMA Rx Channel X Control register defines */
+#define DMA_CONTROL_SR			BIT(0)
+#define DMA_RBSZ_MASK			GENMASK(14, 1)
+#define DMA_RBSZ_SHIFT			1
+
+/* Interrupt status per channel */
+#define DMA_CHAN_STATUS_REB		GENMASK(21, 19)
+#define DMA_CHAN_STATUS_REB_SHIFT	19
+#define DMA_CHAN_STATUS_TEB		GENMASK(18, 16)
+#define DMA_CHAN_STATUS_TEB_SHIFT	16
+#define DMA_CHAN_STATUS_NIS		BIT(15)
+#define DMA_CHAN_STATUS_AIS		BIT(14)
+#define DMA_CHAN_STATUS_CDE		BIT(13)
+#define DMA_CHAN_STATUS_FBE		BIT(12)
+#define DMA_CHAN_STATUS_ERI		BIT(11)
+#define DMA_CHAN_STATUS_ETI		BIT(10)
+#define DMA_CHAN_STATUS_RWT		BIT(9)
+#define DMA_CHAN_STATUS_RPS		BIT(8)
+#define DMA_CHAN_STATUS_RBU		BIT(7)
+#define DMA_CHAN_STATUS_RI		BIT(6)
+#define DMA_CHAN_STATUS_TBU		BIT(2)
+#define DMA_CHAN_STATUS_TPS		BIT(1)
+#define DMA_CHAN_STATUS_TI		BIT(0)
+
+/* Interrupt enable bits per channel */
+#define DMA_CHAN_INTR_ENA_NIE		BIT(16)
+#define DMA_CHAN_INTR_ENA_AIE		BIT(15)
+#define DMA_CHAN_INTR_ENA_NIE_4_10	BIT(15)
+#define DMA_CHAN_INTR_ENA_AIE_4_10	BIT(14)
+#define DMA_CHAN_INTR_ENA_CDE		BIT(13)
+#define DMA_CHAN_INTR_ENA_FBE		BIT(12)
+#define DMA_CHAN_INTR_ENA_ERE		BIT(11)
+#define DMA_CHAN_INTR_ENA_ETE		BIT(10)
+#define DMA_CHAN_INTR_ENA_RWE		BIT(9)
+#define DMA_CHAN_INTR_ENA_RSE		BIT(8)
+#define DMA_CHAN_INTR_ENA_RBUE		BIT(7)
+#define DMA_CHAN_INTR_ENA_RIE		BIT(6)
+#define DMA_CHAN_INTR_ENA_TBUE		BIT(2)
+#define DMA_CHAN_INTR_ENA_TSE		BIT(1)
+#define DMA_CHAN_INTR_ENA_TIE		BIT(0)
+
+#define DMA_CHAN_INTR_NORMAL		(DMA_CHAN_INTR_ENA_NIE | \
+					 DMA_CHAN_INTR_ENA_RIE | \
+					 DMA_CHAN_INTR_ENA_TIE)
+
+#define DMA_CHAN_INTR_ABNORMAL		(DMA_CHAN_INTR_ENA_AIE | \
+					 DMA_CHAN_INTR_ENA_FBE)
+/* DMA default interrupt mask for 4.00 */
+#define DMA_CHAN_INTR_DEFAULT_MASK	(DMA_CHAN_INTR_NORMAL | \
+					 DMA_CHAN_INTR_ABNORMAL)
+#define DMA_CHAN_INTR_DEFAULT_RX	(DMA_CHAN_INTR_ENA_RIE)
+#define DMA_CHAN_INTR_DEFAULT_TX	(DMA_CHAN_INTR_ENA_TIE)
+
+#define DMA_CHAN_INTR_NORMAL_4_10	(DMA_CHAN_INTR_ENA_NIE_4_10 | \
+					 DMA_CHAN_INTR_ENA_RIE | \
+					 DMA_CHAN_INTR_ENA_TIE)
+
+#define DMA_CHAN_INTR_ABNORMAL_4_10	(DMA_CHAN_INTR_ENA_AIE_4_10 | \
+					 DMA_CHAN_INTR_ENA_FBE)
+/* DMA default interrupt mask for 4.10a */
+#define DMA_CHAN_INTR_DEFAULT_MASK_4_10	(DMA_CHAN_INTR_NORMAL_4_10 | \
+					 DMA_CHAN_INTR_ABNORMAL_4_10)
+#define DMA_CHAN_INTR_DEFAULT_RX_4_10	(DMA_CHAN_INTR_ENA_RIE)
+#define DMA_CHAN_INTR_DEFAULT_TX_4_10	(DMA_CHAN_INTR_ENA_TIE)
+
+/* channel 0 specific fields */
+#define DMA_CHAN0_DBG_STAT_TPS		GENMASK(15, 12)
+#define DMA_CHAN0_DBG_STAT_TPS_SHIFT	12
+#define DMA_CHAN0_DBG_STAT_RPS		GENMASK(11, 8)
+#define DMA_CHAN0_DBG_STAT_RPS_SHIFT	8
+
+int dwmac4_dma_reset(void __iomem *ioaddr);
+void dwmac4_enable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx);
+void dwmac410_enable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx);
+void dwmac4_disable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx);
+void dwmac410_disable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx);
+void dwmac4_dma_start_tx(void __iomem *ioaddr, u32 chan);
+void dwmac4_dma_stop_tx(void __iomem *ioaddr, u32 chan);
+void dwmac4_dma_start_rx(void __iomem *ioaddr, u32 chan);
+void dwmac4_dma_stop_rx(void __iomem *ioaddr, u32 chan);
+int dwmac4_dma_interrupt(void __iomem *ioaddr,
+			 struct stmmac_extra_stats *x, u32 chan);
+void dwmac4_set_rx_ring_len(void __iomem *ioaddr, u32 len, u32 chan);
+void dwmac4_set_tx_ring_len(void __iomem *ioaddr, u32 len, u32 chan);
+void dwmac4_set_rx_tail_ptr(void __iomem *ioaddr, u32 tail_ptr, u32 chan);
+void dwmac4_set_tx_tail_ptr(void __iomem *ioaddr, u32 tail_ptr, u32 chan);
+
+#endif /* __DWMAC4_DMA_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac4.h b/net/rtnet/drivers/orange-pi-one/dwmac4.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac4.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac4.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,500 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * DWMAC4 Header file.
+ *
+ * Copyright (C) 2015  STMicroelectronics Ltd
+ *
+ * Author: Alexandre Torgue <alexandre.torgue@st.com>
+ */
+
+#ifndef __DWMAC4_H__
+#define __DWMAC4_H__
+
+#include "common.h"
+
+/*  MAC registers */
+#define GMAC_CONFIG			0x00000000
+#define GMAC_EXT_CONFIG			0x00000004
+#define GMAC_PACKET_FILTER		0x00000008
+#define GMAC_HASH_TAB(x)		(0x10 + (x) * 4)
+#define GMAC_VLAN_TAG			0x00000050
+#define GMAC_VLAN_TAG_DATA		0x00000054
+#define GMAC_VLAN_HASH_TABLE		0x00000058
+#define GMAC_RX_FLOW_CTRL		0x00000090
+#define GMAC_VLAN_INCL			0x00000060
+#define GMAC_QX_TX_FLOW_CTRL(x)		(0x70 + x * 4)
+#define GMAC_TXQ_PRTY_MAP0		0x98
+#define GMAC_TXQ_PRTY_MAP1		0x9C
+#define GMAC_RXQ_CTRL0			0x000000a0
+#define GMAC_RXQ_CTRL1			0x000000a4
+#define GMAC_RXQ_CTRL2			0x000000a8
+#define GMAC_RXQ_CTRL3			0x000000ac
+#define GMAC_INT_STATUS			0x000000b0
+#define GMAC_INT_EN			0x000000b4
+#define GMAC_1US_TIC_COUNTER		0x000000dc
+#define GMAC_PCS_BASE			0x000000e0
+#define GMAC_PHYIF_CONTROL_STATUS	0x000000f8
+#define GMAC_PMT			0x000000c0
+#define GMAC_DEBUG			0x00000114
+#define GMAC_HW_FEATURE0		0x0000011c
+#define GMAC_HW_FEATURE1		0x00000120
+#define GMAC_HW_FEATURE2		0x00000124
+#define GMAC_HW_FEATURE3		0x00000128
+#define GMAC_MDIO_ADDR			0x00000200
+#define GMAC_MDIO_DATA			0x00000204
+#define GMAC_ARP_ADDR			0x00000210
+#define GMAC_ADDR_HIGH(reg)		(0x300 + reg * 8)
+#define GMAC_ADDR_LOW(reg)		(0x304 + reg * 8)
+#define GMAC_L3L4_CTRL(reg)		(0x900 + (reg) * 0x30)
+#define GMAC_L4_ADDR(reg)		(0x904 + (reg) * 0x30)
+#define GMAC_L3_ADDR0(reg)		(0x910 + (reg) * 0x30)
+#define GMAC_L3_ADDR1(reg)		(0x914 + (reg) * 0x30)
+
+/* RX Queues Routing */
+#define GMAC_RXQCTRL_AVCPQ_MASK		GENMASK(2, 0)
+#define GMAC_RXQCTRL_AVCPQ_SHIFT	0
+#define GMAC_RXQCTRL_PTPQ_MASK		GENMASK(6, 4)
+#define GMAC_RXQCTRL_PTPQ_SHIFT		4
+#define GMAC_RXQCTRL_DCBCPQ_MASK	GENMASK(10, 8)
+#define GMAC_RXQCTRL_DCBCPQ_SHIFT	8
+#define GMAC_RXQCTRL_UPQ_MASK		GENMASK(14, 12)
+#define GMAC_RXQCTRL_UPQ_SHIFT		12
+#define GMAC_RXQCTRL_MCBCQ_MASK		GENMASK(18, 16)
+#define GMAC_RXQCTRL_MCBCQ_SHIFT	16
+#define GMAC_RXQCTRL_MCBCQEN		BIT(20)
+#define GMAC_RXQCTRL_MCBCQEN_SHIFT	20
+#define GMAC_RXQCTRL_TACPQE		BIT(21)
+#define GMAC_RXQCTRL_TACPQE_SHIFT	21
+#define GMAC_RXQCTRL_FPRQ		GENMASK(26, 24)
+#define GMAC_RXQCTRL_FPRQ_SHIFT		24
+
+/* MAC Packet Filtering */
+#define GMAC_PACKET_FILTER_PR		BIT(0)
+#define GMAC_PACKET_FILTER_HMC		BIT(2)
+#define GMAC_PACKET_FILTER_PM		BIT(4)
+#define GMAC_PACKET_FILTER_PCF		BIT(7)
+#define GMAC_PACKET_FILTER_HPF		BIT(10)
+#define GMAC_PACKET_FILTER_VTFE		BIT(16)
+#define GMAC_PACKET_FILTER_IPFE		BIT(20)
+
+#define GMAC_MAX_PERFECT_ADDRESSES	128
+
+/* MAC VLAN */
+#define GMAC_VLAN_EDVLP			BIT(26)
+#define GMAC_VLAN_VTHM			BIT(25)
+#define GMAC_VLAN_DOVLTC		BIT(20)
+#define GMAC_VLAN_ESVL			BIT(18)
+#define GMAC_VLAN_ETV			BIT(16)
+#define GMAC_VLAN_VID			GENMASK(15, 0)
+#define GMAC_VLAN_VLTI			BIT(20)
+#define GMAC_VLAN_CSVL			BIT(19)
+#define GMAC_VLAN_VLC			GENMASK(17, 16)
+#define GMAC_VLAN_VLC_SHIFT		16
+#define GMAC_VLAN_VLHT			GENMASK(15, 0)
+
+/* MAC VLAN Tag */
+#define GMAC_VLAN_TAG_VID		GENMASK(15, 0)
+#define GMAC_VLAN_TAG_ETV		BIT(16)
+
+/* MAC VLAN Tag Control */
+#define GMAC_VLAN_TAG_CTRL_OB		BIT(0)
+#define GMAC_VLAN_TAG_CTRL_CT		BIT(1)
+#define GMAC_VLAN_TAG_CTRL_OFS_MASK	GENMASK(6, 2)
+#define GMAC_VLAN_TAG_CTRL_OFS_SHIFT	2
+#define GMAC_VLAN_TAG_CTRL_EVLS_MASK	GENMASK(22, 21)
+#define GMAC_VLAN_TAG_CTRL_EVLS_SHIFT	21
+#define GMAC_VLAN_TAG_CTRL_EVLRXS	BIT(24)
+
+#define GMAC_VLAN_TAG_STRIP_NONE	(0x0 << GMAC_VLAN_TAG_CTRL_EVLS_SHIFT)
+#define GMAC_VLAN_TAG_STRIP_PASS	(0x1 << GMAC_VLAN_TAG_CTRL_EVLS_SHIFT)
+#define GMAC_VLAN_TAG_STRIP_FAIL	(0x2 << GMAC_VLAN_TAG_CTRL_EVLS_SHIFT)
+#define GMAC_VLAN_TAG_STRIP_ALL		(0x3 << GMAC_VLAN_TAG_CTRL_EVLS_SHIFT)
+
+/* MAC VLAN Tag Data/Filter */
+#define GMAC_VLAN_TAG_DATA_VID		GENMASK(15, 0)
+#define GMAC_VLAN_TAG_DATA_VEN		BIT(16)
+#define GMAC_VLAN_TAG_DATA_ETV		BIT(17)
+
+/* MAC RX Queue Enable */
+#define GMAC_RX_QUEUE_CLEAR(queue)	~(GENMASK(1, 0) << ((queue) * 2))
+#define GMAC_RX_AV_QUEUE_ENABLE(queue)	BIT((queue) * 2)
+#define GMAC_RX_DCB_QUEUE_ENABLE(queue)	BIT(((queue) * 2) + 1)
+
+/* MAC Flow Control RX */
+#define GMAC_RX_FLOW_CTRL_RFE		BIT(0)
+
+/* RX Queues Priorities */
+#define GMAC_RXQCTRL_PSRQX_MASK(x)	GENMASK(7 + ((x) * 8), 0 + ((x) * 8))
+#define GMAC_RXQCTRL_PSRQX_SHIFT(x)	((x) * 8)
+
+/* TX Queues Priorities */
+#define GMAC_TXQCTRL_PSTQX_MASK(x)	GENMASK(7 + ((x) * 8), 0 + ((x) * 8))
+#define GMAC_TXQCTRL_PSTQX_SHIFT(x)	((x) * 8)
+
+/* MAC Flow Control TX */
+#define GMAC_TX_FLOW_CTRL_TFE		BIT(1)
+#define GMAC_TX_FLOW_CTRL_PT_SHIFT	16
+
+/*  MAC Interrupt bitmap*/
+#define GMAC_INT_RGSMIIS		BIT(0)
+#define GMAC_INT_PCS_LINK		BIT(1)
+#define GMAC_INT_PCS_ANE		BIT(2)
+#define GMAC_INT_PCS_PHYIS		BIT(3)
+#define GMAC_INT_PMT_EN			BIT(4)
+#define GMAC_INT_LPI_EN			BIT(5)
+
+#define	GMAC_PCS_IRQ_DEFAULT	(GMAC_INT_RGSMIIS | GMAC_INT_PCS_LINK |	\
+				 GMAC_INT_PCS_ANE)
+
+#define	GMAC_INT_DEFAULT_ENABLE	(GMAC_INT_PMT_EN | GMAC_INT_LPI_EN)
+
+enum dwmac4_irq_status {
+	time_stamp_irq = 0x00001000,
+	mmc_rx_csum_offload_irq = 0x00000800,
+	mmc_tx_irq = 0x00000400,
+	mmc_rx_irq = 0x00000200,
+	mmc_irq = 0x00000100,
+	lpi_irq = 0x00000020,
+	pmt_irq = 0x00000010,
+};
+
+/* MAC PMT bitmap */
+enum power_event {
+	pointer_reset =	0x80000000,
+	global_unicast = 0x00000200,
+	wake_up_rx_frame = 0x00000040,
+	magic_frame = 0x00000020,
+	wake_up_frame_en = 0x00000004,
+	magic_pkt_en = 0x00000002,
+	power_down = 0x00000001,
+};
+
+/* Energy Efficient Ethernet (EEE) for GMAC4
+ *
+ * LPI status, timer and control register offset
+ */
+#define GMAC4_LPI_CTRL_STATUS	0xd0
+#define GMAC4_LPI_TIMER_CTRL	0xd4
+
+/* LPI control and status defines */
+#define GMAC4_LPI_CTRL_STATUS_LPITCSE	BIT(21)	/* LPI Tx Clock Stop Enable */
+#define GMAC4_LPI_CTRL_STATUS_LPITXA	BIT(19)	/* Enable LPI TX Automate */
+#define GMAC4_LPI_CTRL_STATUS_PLS	BIT(17) /* PHY Link Status */
+#define GMAC4_LPI_CTRL_STATUS_LPIEN	BIT(16)	/* LPI Enable */
+#define GMAC4_LPI_CTRL_STATUS_RLPIEX	BIT(3) /* Receive LPI Exit */
+#define GMAC4_LPI_CTRL_STATUS_RLPIEN	BIT(2) /* Receive LPI Entry */
+#define GMAC4_LPI_CTRL_STATUS_TLPIEX	BIT(1) /* Transmit LPI Exit */
+#define GMAC4_LPI_CTRL_STATUS_TLPIEN	BIT(0) /* Transmit LPI Entry */
+
+/* MAC Debug bitmap */
+#define GMAC_DEBUG_TFCSTS_MASK		GENMASK(18, 17)
+#define GMAC_DEBUG_TFCSTS_SHIFT		17
+#define GMAC_DEBUG_TFCSTS_IDLE		0
+#define GMAC_DEBUG_TFCSTS_WAIT		1
+#define GMAC_DEBUG_TFCSTS_GEN_PAUSE	2
+#define GMAC_DEBUG_TFCSTS_XFER		3
+#define GMAC_DEBUG_TPESTS		BIT(16)
+#define GMAC_DEBUG_RFCFCSTS_MASK	GENMASK(2, 1)
+#define GMAC_DEBUG_RFCFCSTS_SHIFT	1
+#define GMAC_DEBUG_RPESTS		BIT(0)
+
+/* MAC config */
+#define GMAC_CONFIG_ARPEN		BIT(31)
+#define GMAC_CONFIG_SARC		GENMASK(30, 28)
+#define GMAC_CONFIG_SARC_SHIFT		28
+#define GMAC_CONFIG_IPC			BIT(27)
+#define GMAC_CONFIG_IPG			GENMASK(26, 24)
+#define GMAC_CONFIG_IPG_SHIFT		24
+#define GMAC_CONFIG_2K			BIT(22)
+#define GMAC_CONFIG_ACS			BIT(20)
+#define GMAC_CONFIG_BE			BIT(18)
+#define GMAC_CONFIG_JD			BIT(17)
+#define GMAC_CONFIG_JE			BIT(16)
+#define GMAC_CONFIG_PS			BIT(15)
+#define GMAC_CONFIG_FES			BIT(14)
+#define GMAC_CONFIG_FES_SHIFT		14
+#define GMAC_CONFIG_DM			BIT(13)
+#define GMAC_CONFIG_LM			BIT(12)
+#define GMAC_CONFIG_DCRS		BIT(9)
+#define GMAC_CONFIG_TE			BIT(1)
+#define GMAC_CONFIG_RE			BIT(0)
+
+/* MAC extended config */
+#define GMAC_CONFIG_EIPG		GENMASK(29, 25)
+#define GMAC_CONFIG_EIPG_SHIFT		25
+#define GMAC_CONFIG_EIPG_EN		BIT(24)
+#define GMAC_CONFIG_HDSMS		GENMASK(22, 20)
+#define GMAC_CONFIG_HDSMS_SHIFT		20
+#define GMAC_CONFIG_HDSMS_256		(0x2 << GMAC_CONFIG_HDSMS_SHIFT)
+
+/* MAC HW features0 bitmap */
+#define GMAC_HW_FEAT_SAVLANINS		BIT(27)
+#define GMAC_HW_FEAT_ADDMAC		BIT(18)
+#define GMAC_HW_FEAT_RXCOESEL		BIT(16)
+#define GMAC_HW_FEAT_TXCOSEL		BIT(14)
+#define GMAC_HW_FEAT_EEESEL		BIT(13)
+#define GMAC_HW_FEAT_TSSEL		BIT(12)
+#define GMAC_HW_FEAT_ARPOFFSEL		BIT(9)
+#define GMAC_HW_FEAT_MMCSEL		BIT(8)
+#define GMAC_HW_FEAT_MGKSEL		BIT(7)
+#define GMAC_HW_FEAT_RWKSEL		BIT(6)
+#define GMAC_HW_FEAT_SMASEL		BIT(5)
+#define GMAC_HW_FEAT_VLHASH		BIT(4)
+#define GMAC_HW_FEAT_PCSSEL		BIT(3)
+#define GMAC_HW_FEAT_HDSEL		BIT(2)
+#define GMAC_HW_FEAT_GMIISEL		BIT(1)
+#define GMAC_HW_FEAT_MIISEL		BIT(0)
+
+/* MAC HW features1 bitmap */
+#define GMAC_HW_FEAT_L3L4FNUM		GENMASK(30, 27)
+#define GMAC_HW_HASH_TB_SZ		GENMASK(25, 24)
+#define GMAC_HW_FEAT_AVSEL		BIT(20)
+#define GMAC_HW_TSOEN			BIT(18)
+#define GMAC_HW_FEAT_SPHEN		BIT(17)
+#define GMAC_HW_ADDR64			GENMASK(15, 14)
+#define GMAC_HW_TXFIFOSIZE		GENMASK(10, 6)
+#define GMAC_HW_RXFIFOSIZE		GENMASK(4, 0)
+
+/* MAC HW features2 bitmap */
+#define GMAC_HW_FEAT_PPSOUTNUM		GENMASK(26, 24)
+#define GMAC_HW_FEAT_TXCHCNT		GENMASK(21, 18)
+#define GMAC_HW_FEAT_RXCHCNT		GENMASK(15, 12)
+#define GMAC_HW_FEAT_TXQCNT		GENMASK(9, 6)
+#define GMAC_HW_FEAT_RXQCNT		GENMASK(3, 0)
+
+/* MAC HW features3 bitmap */
+#define GMAC_HW_FEAT_ASP		GENMASK(29, 28)
+#define GMAC_HW_FEAT_TBSSEL		BIT(27)
+#define GMAC_HW_FEAT_FPESEL		BIT(26)
+#define GMAC_HW_FEAT_ESTWID		GENMASK(21, 20)
+#define GMAC_HW_FEAT_ESTDEP		GENMASK(19, 17)
+#define GMAC_HW_FEAT_ESTSEL		BIT(16)
+#define GMAC_HW_FEAT_FRPES		GENMASK(14, 13)
+#define GMAC_HW_FEAT_FRPBS		GENMASK(12, 11)
+#define GMAC_HW_FEAT_FRPSEL		BIT(10)
+#define GMAC_HW_FEAT_DVLAN		BIT(5)
+#define GMAC_HW_FEAT_NRVF		GENMASK(2, 0)
+
+/* MAC HW ADDR regs */
+#define GMAC_HI_DCS			GENMASK(18, 16)
+#define GMAC_HI_DCS_SHIFT		16
+#define GMAC_HI_REG_AE			BIT(31)
+
+/* L3/L4 Filters regs */
+#define GMAC_L4DPIM0			BIT(21)
+#define GMAC_L4DPM0			BIT(20)
+#define GMAC_L4SPIM0			BIT(19)
+#define GMAC_L4SPM0			BIT(18)
+#define GMAC_L4PEN0			BIT(16)
+#define GMAC_L3DAIM0			BIT(5)
+#define GMAC_L3DAM0			BIT(4)
+#define GMAC_L3SAIM0			BIT(3)
+#define GMAC_L3SAM0			BIT(2)
+#define GMAC_L3PEN0			BIT(0)
+#define GMAC_L4DP0			GENMASK(31, 16)
+#define GMAC_L4DP0_SHIFT		16
+#define GMAC_L4SP0			GENMASK(15, 0)
+
+/*  MTL registers */
+#define MTL_OPERATION_MODE		0x00000c00
+#define MTL_FRPE			BIT(15)
+#define MTL_OPERATION_SCHALG_MASK	GENMASK(6, 5)
+#define MTL_OPERATION_SCHALG_WRR	(0x0 << 5)
+#define MTL_OPERATION_SCHALG_WFQ	(0x1 << 5)
+#define MTL_OPERATION_SCHALG_DWRR	(0x2 << 5)
+#define MTL_OPERATION_SCHALG_SP		(0x3 << 5)
+#define MTL_OPERATION_RAA		BIT(2)
+#define MTL_OPERATION_RAA_SP		(0x0 << 2)
+#define MTL_OPERATION_RAA_WSP		(0x1 << 2)
+
+#define MTL_INT_STATUS			0x00000c20
+#define MTL_INT_QX(x)			BIT(x)
+
+#define MTL_RXQ_DMA_MAP0		0x00000c30 /* queue 0 to 3 */
+#define MTL_RXQ_DMA_MAP1		0x00000c34 /* queue 4 to 7 */
+#define MTL_RXQ_DMA_Q04MDMACH_MASK	GENMASK(3, 0)
+#define MTL_RXQ_DMA_Q04MDMACH(x)	((x) << 0)
+#define MTL_RXQ_DMA_QXMDMACH_MASK(x)	GENMASK(11 + (8 * ((x) - 1)), 8 * (x))
+#define MTL_RXQ_DMA_QXMDMACH(chan, q)	((chan) << (8 * (q)))
+
+#define MTL_CHAN_BASE_ADDR		0x00000d00
+#define MTL_CHAN_BASE_OFFSET		0x40
+#define MTL_CHANX_BASE_ADDR(x)		(MTL_CHAN_BASE_ADDR + \
+					(x * MTL_CHAN_BASE_OFFSET))
+
+#define MTL_CHAN_TX_OP_MODE(x)		MTL_CHANX_BASE_ADDR(x)
+#define MTL_CHAN_TX_DEBUG(x)		(MTL_CHANX_BASE_ADDR(x) + 0x8)
+#define MTL_CHAN_INT_CTRL(x)		(MTL_CHANX_BASE_ADDR(x) + 0x2c)
+#define MTL_CHAN_RX_OP_MODE(x)		(MTL_CHANX_BASE_ADDR(x) + 0x30)
+#define MTL_CHAN_RX_DEBUG(x)		(MTL_CHANX_BASE_ADDR(x) + 0x38)
+
+#define MTL_OP_MODE_RSF			BIT(5)
+#define MTL_OP_MODE_TXQEN_MASK		GENMASK(3, 2)
+#define MTL_OP_MODE_TXQEN_AV		BIT(2)
+#define MTL_OP_MODE_TXQEN		BIT(3)
+#define MTL_OP_MODE_TSF			BIT(1)
+
+#define MTL_OP_MODE_TQS_MASK		GENMASK(24, 16)
+#define MTL_OP_MODE_TQS_SHIFT		16
+
+#define MTL_OP_MODE_TTC_MASK		0x70
+#define MTL_OP_MODE_TTC_SHIFT		4
+
+#define MTL_OP_MODE_TTC_32		0
+#define MTL_OP_MODE_TTC_64		(1 << MTL_OP_MODE_TTC_SHIFT)
+#define MTL_OP_MODE_TTC_96		(2 << MTL_OP_MODE_TTC_SHIFT)
+#define MTL_OP_MODE_TTC_128		(3 << MTL_OP_MODE_TTC_SHIFT)
+#define MTL_OP_MODE_TTC_192		(4 << MTL_OP_MODE_TTC_SHIFT)
+#define MTL_OP_MODE_TTC_256		(5 << MTL_OP_MODE_TTC_SHIFT)
+#define MTL_OP_MODE_TTC_384		(6 << MTL_OP_MODE_TTC_SHIFT)
+#define MTL_OP_MODE_TTC_512		(7 << MTL_OP_MODE_TTC_SHIFT)
+
+#define MTL_OP_MODE_RQS_MASK		GENMASK(29, 20)
+#define MTL_OP_MODE_RQS_SHIFT		20
+
+#define MTL_OP_MODE_RFD_MASK		GENMASK(19, 14)
+#define MTL_OP_MODE_RFD_SHIFT		14
+
+#define MTL_OP_MODE_RFA_MASK		GENMASK(13, 8)
+#define MTL_OP_MODE_RFA_SHIFT		8
+
+#define MTL_OP_MODE_EHFC		BIT(7)
+
+#define MTL_OP_MODE_RTC_MASK		0x18
+#define MTL_OP_MODE_RTC_SHIFT		3
+
+#define MTL_OP_MODE_RTC_32		(1 << MTL_OP_MODE_RTC_SHIFT)
+#define MTL_OP_MODE_RTC_64		0
+#define MTL_OP_MODE_RTC_96		(2 << MTL_OP_MODE_RTC_SHIFT)
+#define MTL_OP_MODE_RTC_128		(3 << MTL_OP_MODE_RTC_SHIFT)
+
+/* MTL ETS Control register */
+#define MTL_ETS_CTRL_BASE_ADDR		0x00000d10
+#define MTL_ETS_CTRL_BASE_OFFSET	0x40
+#define MTL_ETSX_CTRL_BASE_ADDR(x)	(MTL_ETS_CTRL_BASE_ADDR + \
+					((x) * MTL_ETS_CTRL_BASE_OFFSET))
+
+#define MTL_ETS_CTRL_CC			BIT(3)
+#define MTL_ETS_CTRL_AVALG		BIT(2)
+
+/* MTL Queue Quantum Weight */
+#define MTL_TXQ_WEIGHT_BASE_ADDR	0x00000d18
+#define MTL_TXQ_WEIGHT_BASE_OFFSET	0x40
+#define MTL_TXQX_WEIGHT_BASE_ADDR(x)	(MTL_TXQ_WEIGHT_BASE_ADDR + \
+					((x) * MTL_TXQ_WEIGHT_BASE_OFFSET))
+#define MTL_TXQ_WEIGHT_ISCQW_MASK	GENMASK(20, 0)
+
+/* MTL sendSlopeCredit register */
+#define MTL_SEND_SLP_CRED_BASE_ADDR	0x00000d1c
+#define MTL_SEND_SLP_CRED_OFFSET	0x40
+#define MTL_SEND_SLP_CREDX_BASE_ADDR(x)	(MTL_SEND_SLP_CRED_BASE_ADDR + \
+					((x) * MTL_SEND_SLP_CRED_OFFSET))
+
+#define MTL_SEND_SLP_CRED_SSC_MASK	GENMASK(13, 0)
+
+/* MTL hiCredit register */
+#define MTL_HIGH_CRED_BASE_ADDR		0x00000d20
+#define MTL_HIGH_CRED_OFFSET		0x40
+#define MTL_HIGH_CREDX_BASE_ADDR(x)	(MTL_HIGH_CRED_BASE_ADDR + \
+					((x) * MTL_HIGH_CRED_OFFSET))
+
+#define MTL_HIGH_CRED_HC_MASK		GENMASK(28, 0)
+
+/* MTL loCredit register */
+#define MTL_LOW_CRED_BASE_ADDR		0x00000d24
+#define MTL_LOW_CRED_OFFSET		0x40
+#define MTL_LOW_CREDX_BASE_ADDR(x)	(MTL_LOW_CRED_BASE_ADDR + \
+					((x) * MTL_LOW_CRED_OFFSET))
+
+#define MTL_HIGH_CRED_LC_MASK		GENMASK(28, 0)
+
+/*  MTL debug */
+#define MTL_DEBUG_TXSTSFSTS		BIT(5)
+#define MTL_DEBUG_TXFSTS		BIT(4)
+#define MTL_DEBUG_TWCSTS		BIT(3)
+
+/* MTL debug: Tx FIFO Read Controller Status */
+#define MTL_DEBUG_TRCSTS_MASK		GENMASK(2, 1)
+#define MTL_DEBUG_TRCSTS_SHIFT		1
+#define MTL_DEBUG_TRCSTS_IDLE		0
+#define MTL_DEBUG_TRCSTS_READ		1
+#define MTL_DEBUG_TRCSTS_TXW		2
+#define MTL_DEBUG_TRCSTS_WRITE		3
+#define MTL_DEBUG_TXPAUSED		BIT(0)
+
+/* MAC debug: GMII or MII Transmit Protocol Engine Status */
+#define MTL_DEBUG_RXFSTS_MASK		GENMASK(5, 4)
+#define MTL_DEBUG_RXFSTS_SHIFT		4
+#define MTL_DEBUG_RXFSTS_EMPTY		0
+#define MTL_DEBUG_RXFSTS_BT		1
+#define MTL_DEBUG_RXFSTS_AT		2
+#define MTL_DEBUG_RXFSTS_FULL		3
+#define MTL_DEBUG_RRCSTS_MASK		GENMASK(2, 1)
+#define MTL_DEBUG_RRCSTS_SHIFT		1
+#define MTL_DEBUG_RRCSTS_IDLE		0
+#define MTL_DEBUG_RRCSTS_RDATA		1
+#define MTL_DEBUG_RRCSTS_RSTAT		2
+#define MTL_DEBUG_RRCSTS_FLUSH		3
+#define MTL_DEBUG_RWCSTS		BIT(0)
+
+/*  MTL interrupt */
+#define MTL_RX_OVERFLOW_INT_EN		BIT(24)
+#define MTL_RX_OVERFLOW_INT		BIT(16)
+
+/* Default operating mode of the MAC */
+#define GMAC_CORE_INIT (GMAC_CONFIG_JD | GMAC_CONFIG_PS | \
+			GMAC_CONFIG_BE | GMAC_CONFIG_DCRS | \
+			GMAC_CONFIG_JE)
+
+/* To dump the core regs excluding  the Address Registers */
+#define	GMAC_REG_NUM	132
+
+/*  MTL debug */
+#define MTL_DEBUG_TXSTSFSTS		BIT(5)
+#define MTL_DEBUG_TXFSTS		BIT(4)
+#define MTL_DEBUG_TWCSTS		BIT(3)
+
+/* MTL debug: Tx FIFO Read Controller Status */
+#define MTL_DEBUG_TRCSTS_MASK		GENMASK(2, 1)
+#define MTL_DEBUG_TRCSTS_SHIFT		1
+#define MTL_DEBUG_TRCSTS_IDLE		0
+#define MTL_DEBUG_TRCSTS_READ		1
+#define MTL_DEBUG_TRCSTS_TXW		2
+#define MTL_DEBUG_TRCSTS_WRITE		3
+#define MTL_DEBUG_TXPAUSED		BIT(0)
+
+/* MAC debug: GMII or MII Transmit Protocol Engine Status */
+#define MTL_DEBUG_RXFSTS_MASK		GENMASK(5, 4)
+#define MTL_DEBUG_RXFSTS_SHIFT		4
+#define MTL_DEBUG_RXFSTS_EMPTY		0
+#define MTL_DEBUG_RXFSTS_BT		1
+#define MTL_DEBUG_RXFSTS_AT		2
+#define MTL_DEBUG_RXFSTS_FULL		3
+#define MTL_DEBUG_RRCSTS_MASK		GENMASK(2, 1)
+#define MTL_DEBUG_RRCSTS_SHIFT		1
+#define MTL_DEBUG_RRCSTS_IDLE		0
+#define MTL_DEBUG_RRCSTS_RDATA		1
+#define MTL_DEBUG_RRCSTS_RSTAT		2
+#define MTL_DEBUG_RRCSTS_FLUSH		3
+#define MTL_DEBUG_RWCSTS		BIT(0)
+
+/* SGMII/RGMII status register */
+#define GMAC_PHYIF_CTRLSTATUS_TC		BIT(0)
+#define GMAC_PHYIF_CTRLSTATUS_LUD		BIT(1)
+#define GMAC_PHYIF_CTRLSTATUS_SMIDRXS		BIT(4)
+#define GMAC_PHYIF_CTRLSTATUS_LNKMOD		BIT(16)
+#define GMAC_PHYIF_CTRLSTATUS_SPEED		GENMASK(18, 17)
+#define GMAC_PHYIF_CTRLSTATUS_SPEED_SHIFT	17
+#define GMAC_PHYIF_CTRLSTATUS_LNKSTS		BIT(19)
+#define GMAC_PHYIF_CTRLSTATUS_JABTO		BIT(20)
+#define GMAC_PHYIF_CTRLSTATUS_FALSECARDET	BIT(21)
+/* LNKMOD */
+#define GMAC_PHYIF_CTRLSTATUS_LNKMOD_MASK	0x1
+/* LNKSPEED */
+#define GMAC_PHYIF_CTRLSTATUS_SPEED_125		0x2
+#define GMAC_PHYIF_CTRLSTATUS_SPEED_25		0x1
+#define GMAC_PHYIF_CTRLSTATUS_SPEED_2_5		0x0
+
+extern const struct stmmac_dma_ops dwmac4_dma_ops;
+extern const struct stmmac_dma_ops dwmac410_dma_ops;
+#endif /* __DWMAC4_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac5.h b/net/rtnet/drivers/orange-pi-one/dwmac5.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac5.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac5.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,111 @@
+/* SPDX-License-Identifier: (GPL-2.0 OR MIT) */
+// Copyright (c) 2017 Synopsys, Inc. and/or its affiliates.
+// stmmac Support for 5.xx Ethernet QoS cores
+
+#ifndef __DWMAC5_H__
+#define __DWMAC5_H__
+
+#define MAC_DPP_FSM_INT_STATUS		0x00000140
+#define MAC_AXI_SLV_DPE_ADDR_STATUS	0x00000144
+#define MAC_FSM_CONTROL			0x00000148
+#define PRTYEN				BIT(1)
+#define TMOUTEN				BIT(0)
+
+#define MAC_FPE_CTRL_STS		0x00000234
+#define EFPE				BIT(0)
+
+#define MAC_PPS_CONTROL			0x00000b70
+#define PPS_MAXIDX(x)			((((x) + 1) * 8) - 1)
+#define PPS_MINIDX(x)			((x) * 8)
+#define PPSx_MASK(x)			GENMASK(PPS_MAXIDX(x), PPS_MINIDX(x))
+#define MCGRENx(x)			BIT(PPS_MAXIDX(x))
+#define TRGTMODSELx(x, val)		\
+	GENMASK(PPS_MAXIDX(x) - 1, PPS_MAXIDX(x) - 2) & \
+	((val) << (PPS_MAXIDX(x) - 2))
+#define PPSCMDx(x, val)			\
+	GENMASK(PPS_MINIDX(x) + 3, PPS_MINIDX(x)) & \
+	((val) << PPS_MINIDX(x))
+#define PPSEN0				BIT(4)
+#define MAC_PPSx_TARGET_TIME_SEC(x)	(0x00000b80 + ((x) * 0x10))
+#define MAC_PPSx_TARGET_TIME_NSEC(x)	(0x00000b84 + ((x) * 0x10))
+#define TRGTBUSY0			BIT(31)
+#define TTSL0				GENMASK(30, 0)
+#define MAC_PPSx_INTERVAL(x)		(0x00000b88 + ((x) * 0x10))
+#define MAC_PPSx_WIDTH(x)		(0x00000b8c + ((x) * 0x10))
+
+#define MTL_EST_CONTROL			0x00000c50
+#define PTOV				GENMASK(31, 24)
+#define PTOV_SHIFT			24
+#define SSWL				BIT(1)
+#define EEST				BIT(0)
+#define MTL_EST_GCL_CONTROL		0x00000c80
+#define BTR_LOW				0x0
+#define BTR_HIGH			0x1
+#define CTR_LOW				0x2
+#define CTR_HIGH			0x3
+#define TER				0x4
+#define LLR				0x5
+#define ADDR_SHIFT			8
+#define GCRR				BIT(2)
+#define SRWO				BIT(0)
+#define MTL_EST_GCL_DATA		0x00000c84
+
+#define MTL_RXP_CONTROL_STATUS		0x00000ca0
+#define RXPI				BIT(31)
+#define NPE				GENMASK(23, 16)
+#define NVE				GENMASK(7, 0)
+#define MTL_RXP_IACC_CTRL_STATUS	0x00000cb0
+#define STARTBUSY			BIT(31)
+#define RXPEIEC				GENMASK(22, 21)
+#define RXPEIEE				BIT(20)
+#define WRRDN				BIT(16)
+#define ADDR				GENMASK(15, 0)
+#define MTL_RXP_IACC_DATA		0x00000cb4
+#define MTL_ECC_CONTROL			0x00000cc0
+#define TSOEE				BIT(4)
+#define MRXPEE				BIT(3)
+#define MESTEE				BIT(2)
+#define MRXEE				BIT(1)
+#define MTXEE				BIT(0)
+
+#define MTL_SAFETY_INT_STATUS		0x00000cc4
+#define MCSIS				BIT(31)
+#define MEUIS				BIT(1)
+#define MECIS				BIT(0)
+#define MTL_ECC_INT_ENABLE		0x00000cc8
+#define RPCEIE				BIT(12)
+#define ECEIE				BIT(8)
+#define RXCEIE				BIT(4)
+#define TXCEIE				BIT(0)
+#define MTL_ECC_INT_STATUS		0x00000ccc
+#define MTL_DPP_CONTROL			0x00000ce0
+#define EPSI				BIT(2)
+#define OPE				BIT(1)
+#define EDPP				BIT(0)
+
+#define DMA_SAFETY_INT_STATUS		0x00001080
+#define MSUIS				BIT(29)
+#define MSCIS				BIT(28)
+#define DEUIS				BIT(1)
+#define DECIS				BIT(0)
+#define DMA_ECC_INT_ENABLE		0x00001084
+#define TCEIE				BIT(0)
+#define DMA_ECC_INT_STATUS		0x00001088
+
+int dwmac5_safety_feat_config(void __iomem *ioaddr, unsigned int asp);
+int dwmac5_safety_feat_irq_status(struct net_device *ndev,
+		void __iomem *ioaddr, unsigned int asp,
+		struct stmmac_safety_stats *stats);
+int dwmac5_safety_feat_dump(struct stmmac_safety_stats *stats,
+			int index, unsigned long *count, const char **desc);
+int dwmac5_rxp_config(void __iomem *ioaddr, struct stmmac_tc_entry *entries,
+		      unsigned int count);
+int dwmac5_flex_pps_config(void __iomem *ioaddr, int index,
+			   struct stmmac_pps_cfg *cfg, bool enable,
+			   u32 sub_second_inc, u32 systime_flags);
+int dwmac5_est_configure(void __iomem *ioaddr, struct stmmac_est *cfg,
+			 unsigned int ptp_rate);
+void dwmac5_fpe_configure(void __iomem *ioaddr, u32 num_txq, u32 num_rxq,
+			  bool enable);
+
+#endif /* __DWMAC5_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac_dma.h b/net/rtnet/drivers/orange-pi-one/dwmac_dma.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac_dma.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac_dma.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,145 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  DWMAC DMA Header file.
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __DWMAC_DMA_H__
+#define __DWMAC_DMA_H__
+
+/* DMA CRS Control and Status Register Mapping */
+#define DMA_BUS_MODE		0x00001000	/* Bus Mode */
+#define DMA_XMT_POLL_DEMAND	0x00001004	/* Transmit Poll Demand */
+#define DMA_RCV_POLL_DEMAND	0x00001008	/* Received Poll Demand */
+#define DMA_RCV_BASE_ADDR	0x0000100c	/* Receive List Base */
+#define DMA_TX_BASE_ADDR	0x00001010	/* Transmit List Base */
+#define DMA_STATUS		0x00001014	/* Status Register */
+#define DMA_CONTROL		0x00001018	/* Ctrl (Operational Mode) */
+#define DMA_INTR_ENA		0x0000101c	/* Interrupt Enable */
+#define DMA_MISSED_FRAME_CTR	0x00001020	/* Missed Frame Counter */
+
+/* SW Reset */
+#define DMA_BUS_MODE_SFT_RESET	0x00000001	/* Software Reset */
+
+/* Rx watchdog register */
+#define DMA_RX_WATCHDOG		0x00001024
+
+/* AXI Master Bus Mode */
+#define DMA_AXI_BUS_MODE	0x00001028
+
+#define DMA_AXI_EN_LPI		BIT(31)
+#define DMA_AXI_LPI_XIT_FRM	BIT(30)
+#define DMA_AXI_WR_OSR_LMT	GENMASK(23, 20)
+#define DMA_AXI_WR_OSR_LMT_SHIFT	20
+#define DMA_AXI_WR_OSR_LMT_MASK	0xf
+#define DMA_AXI_RD_OSR_LMT	GENMASK(19, 16)
+#define DMA_AXI_RD_OSR_LMT_SHIFT	16
+#define DMA_AXI_RD_OSR_LMT_MASK	0xf
+
+#define DMA_AXI_OSR_MAX		0xf
+#define DMA_AXI_MAX_OSR_LIMIT ((DMA_AXI_OSR_MAX << DMA_AXI_WR_OSR_LMT_SHIFT) | \
+			       (DMA_AXI_OSR_MAX << DMA_AXI_RD_OSR_LMT_SHIFT))
+#define	DMA_AXI_1KBBE		BIT(13)
+#define DMA_AXI_AAL		BIT(12)
+#define DMA_AXI_BLEN256		BIT(7)
+#define DMA_AXI_BLEN128		BIT(6)
+#define DMA_AXI_BLEN64		BIT(5)
+#define DMA_AXI_BLEN32		BIT(4)
+#define DMA_AXI_BLEN16		BIT(3)
+#define DMA_AXI_BLEN8		BIT(2)
+#define DMA_AXI_BLEN4		BIT(1)
+#define DMA_BURST_LEN_DEFAULT	(DMA_AXI_BLEN256 | DMA_AXI_BLEN128 | \
+				 DMA_AXI_BLEN64 | DMA_AXI_BLEN32 | \
+				 DMA_AXI_BLEN16 | DMA_AXI_BLEN8 | \
+				 DMA_AXI_BLEN4)
+
+#define DMA_AXI_UNDEF		BIT(0)
+
+#define DMA_AXI_BURST_LEN_MASK	0x000000FE
+
+#define DMA_CUR_TX_BUF_ADDR	0x00001050	/* Current Host Tx Buffer */
+#define DMA_CUR_RX_BUF_ADDR	0x00001054	/* Current Host Rx Buffer */
+#define DMA_HW_FEATURE		0x00001058	/* HW Feature Register */
+
+/* DMA Control register defines */
+#define DMA_CONTROL_ST		0x00002000	/* Start/Stop Transmission */
+#define DMA_CONTROL_SR		0x00000002	/* Start/Stop Receive */
+
+/* DMA Normal interrupt */
+#define DMA_INTR_ENA_NIE 0x00010000	/* Normal Summary */
+#define DMA_INTR_ENA_TIE 0x00000001	/* Transmit Interrupt */
+#define DMA_INTR_ENA_TUE 0x00000004	/* Transmit Buffer Unavailable */
+#define DMA_INTR_ENA_RIE 0x00000040	/* Receive Interrupt */
+#define DMA_INTR_ENA_ERE 0x00004000	/* Early Receive */
+
+#define DMA_INTR_NORMAL	(DMA_INTR_ENA_NIE | DMA_INTR_ENA_RIE | \
+			DMA_INTR_ENA_TIE)
+
+/* DMA Abnormal interrupt */
+#define DMA_INTR_ENA_AIE 0x00008000	/* Abnormal Summary */
+#define DMA_INTR_ENA_FBE 0x00002000	/* Fatal Bus Error */
+#define DMA_INTR_ENA_ETE 0x00000400	/* Early Transmit */
+#define DMA_INTR_ENA_RWE 0x00000200	/* Receive Watchdog */
+#define DMA_INTR_ENA_RSE 0x00000100	/* Receive Stopped */
+#define DMA_INTR_ENA_RUE 0x00000080	/* Receive Buffer Unavailable */
+#define DMA_INTR_ENA_UNE 0x00000020	/* Tx Underflow */
+#define DMA_INTR_ENA_OVE 0x00000010	/* Receive Overflow */
+#define DMA_INTR_ENA_TJE 0x00000008	/* Transmit Jabber */
+#define DMA_INTR_ENA_TSE 0x00000002	/* Transmit Stopped */
+
+#define DMA_INTR_ABNORMAL	(DMA_INTR_ENA_AIE | DMA_INTR_ENA_FBE | \
+				DMA_INTR_ENA_UNE)
+
+/* DMA default interrupt mask */
+#define DMA_INTR_DEFAULT_MASK	(DMA_INTR_NORMAL | DMA_INTR_ABNORMAL)
+#define DMA_INTR_DEFAULT_RX	(DMA_INTR_ENA_RIE)
+#define DMA_INTR_DEFAULT_TX	(DMA_INTR_ENA_TIE)
+
+/* DMA Status register defines */
+#define DMA_STATUS_GLPII	0x40000000	/* GMAC LPI interrupt */
+#define DMA_STATUS_GPI		0x10000000	/* PMT interrupt */
+#define DMA_STATUS_GMI		0x08000000	/* MMC interrupt */
+#define DMA_STATUS_GLI		0x04000000	/* GMAC Line interface int */
+#define DMA_STATUS_EB_MASK	0x00380000	/* Error Bits Mask */
+#define DMA_STATUS_EB_TX_ABORT	0x00080000	/* Error Bits - TX Abort */
+#define DMA_STATUS_EB_RX_ABORT	0x00100000	/* Error Bits - RX Abort */
+#define DMA_STATUS_TS_MASK	0x00700000	/* Transmit Process State */
+#define DMA_STATUS_TS_SHIFT	20
+#define DMA_STATUS_RS_MASK	0x000e0000	/* Receive Process State */
+#define DMA_STATUS_RS_SHIFT	17
+#define DMA_STATUS_NIS	0x00010000	/* Normal Interrupt Summary */
+#define DMA_STATUS_AIS	0x00008000	/* Abnormal Interrupt Summary */
+#define DMA_STATUS_ERI	0x00004000	/* Early Receive Interrupt */
+#define DMA_STATUS_FBI	0x00002000	/* Fatal Bus Error Interrupt */
+#define DMA_STATUS_ETI	0x00000400	/* Early Transmit Interrupt */
+#define DMA_STATUS_RWT	0x00000200	/* Receive Watchdog Timeout */
+#define DMA_STATUS_RPS	0x00000100	/* Receive Process Stopped */
+#define DMA_STATUS_RU	0x00000080	/* Receive Buffer Unavailable */
+#define DMA_STATUS_RI	0x00000040	/* Receive Interrupt */
+#define DMA_STATUS_UNF	0x00000020	/* Transmit Underflow */
+#define DMA_STATUS_OVF	0x00000010	/* Receive Overflow */
+#define DMA_STATUS_TJT	0x00000008	/* Transmit Jabber Timeout */
+#define DMA_STATUS_TU	0x00000004	/* Transmit Buffer Unavailable */
+#define DMA_STATUS_TPS	0x00000002	/* Transmit Process Stopped */
+#define DMA_STATUS_TI	0x00000001	/* Transmit Interrupt */
+#define DMA_CONTROL_FTF		0x00100000	/* Flush transmit FIFO */
+
+#define NUM_DWMAC100_DMA_REGS	9
+#define NUM_DWMAC1000_DMA_REGS	23
+
+void dwmac_enable_dma_transmission(void __iomem *ioaddr);
+void dwmac_enable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx);
+void dwmac_disable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx);
+void dwmac_dma_start_tx(void __iomem *ioaddr, u32 chan);
+void dwmac_dma_stop_tx(void __iomem *ioaddr, u32 chan);
+void dwmac_dma_start_rx(void __iomem *ioaddr, u32 chan);
+void dwmac_dma_stop_rx(void __iomem *ioaddr, u32 chan);
+int dwmac_dma_interrupt(void __iomem *ioaddr, struct stmmac_extra_stats *x,
+			u32 chan);
+int dwmac_dma_reset(void __iomem *ioaddr);
+
+#endif /* __DWMAC_DMA_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac-intel.c b/net/rtnet/drivers/orange-pi-one/dwmac-intel.c
--- a/net/rtnet/drivers/orange-pi-one/dwmac-intel.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac-intel.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,747 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (c) 2020, Intel Corporation
+ */
+
+#include <linux/clk-provider.h>
+#include <linux/pci.h>
+#include <linux/dmi.h>
+#include "dwmac-intel.h"
+#include "stmmac.h"
+
+struct intel_priv_data {
+	int mdio_adhoc_addr;	/* mdio address for serdes & etc */
+};
+
+/* This struct is used to associate PCI Function of MAC controller on a board,
+ * discovered via DMI, with the address of PHY connected to the MAC. The
+ * negative value of the address means that MAC controller is not connected
+ * with PHY.
+ */
+struct stmmac_pci_func_data {
+	unsigned int func;
+	int phy_addr;
+};
+
+struct stmmac_pci_dmi_data {
+	const struct stmmac_pci_func_data *func;
+	size_t nfuncs;
+};
+
+struct stmmac_pci_info {
+	int (*setup)(struct pci_dev *pdev, struct plat_stmmacenet_data *plat);
+};
+
+static int stmmac_pci_find_phy_addr(struct pci_dev *pdev,
+				    const struct dmi_system_id *dmi_list)
+{
+	const struct stmmac_pci_func_data *func_data;
+	const struct stmmac_pci_dmi_data *dmi_data;
+	const struct dmi_system_id *dmi_id;
+	int func = PCI_FUNC(pdev->devfn);
+	size_t n;
+
+	dmi_id = dmi_first_match(dmi_list);
+	if (!dmi_id)
+		return -ENODEV;
+
+	dmi_data = dmi_id->driver_data;
+	func_data = dmi_data->func;
+
+	for (n = 0; n < dmi_data->nfuncs; n++, func_data++)
+		if (func_data->func == func)
+			return func_data->phy_addr;
+
+	return -ENODEV;
+}
+
+static int serdes_status_poll(struct stmmac_priv *priv, int phyaddr,
+			      int phyreg, u32 mask, u32 val)
+{
+	unsigned int retries = 10;
+	int val_rd;
+
+	do {
+		val_rd = mdiobus_read(priv->mii, phyaddr, phyreg);
+		if ((val_rd & mask) == (val & mask))
+			return 0;
+		udelay(POLL_DELAY_US);
+	} while (--retries);
+
+	return -ETIMEDOUT;
+}
+
+static int intel_serdes_powerup(struct net_device *ndev, void *priv_data)
+{
+	struct intel_priv_data *intel_priv = priv_data;
+	struct stmmac_priv *priv = netdev_priv(ndev);
+	int serdes_phy_addr = 0;
+	u32 data = 0;
+
+	if (!intel_priv->mdio_adhoc_addr)
+		return 0;
+
+	serdes_phy_addr = intel_priv->mdio_adhoc_addr;
+
+	/* assert clk_req */
+	data = mdiobus_read(priv->mii, serdes_phy_addr, SERDES_GCR0);
+	data |= SERDES_PLL_CLK;
+	mdiobus_write(priv->mii, serdes_phy_addr, SERDES_GCR0, data);
+
+	/* check for clk_ack assertion */
+	data = serdes_status_poll(priv, serdes_phy_addr,
+				  SERDES_GSR0,
+				  SERDES_PLL_CLK,
+				  SERDES_PLL_CLK);
+
+	if (data) {
+		dev_err(priv->device, "Serdes PLL clk request timeout\n");
+		return data;
+	}
+
+	/* assert lane reset */
+	data = mdiobus_read(priv->mii, serdes_phy_addr, SERDES_GCR0);
+	data |= SERDES_RST;
+	mdiobus_write(priv->mii, serdes_phy_addr, SERDES_GCR0, data);
+
+	/* check for assert lane reset reflection */
+	data = serdes_status_poll(priv, serdes_phy_addr,
+				  SERDES_GSR0,
+				  SERDES_RST,
+				  SERDES_RST);
+
+	if (data) {
+		dev_err(priv->device, "Serdes assert lane reset timeout\n");
+		return data;
+	}
+
+	/*  move power state to P0 */
+	data = mdiobus_read(priv->mii, serdes_phy_addr, SERDES_GCR0);
+
+	data &= ~SERDES_PWR_ST_MASK;
+	data |= SERDES_PWR_ST_P0 << SERDES_PWR_ST_SHIFT;
+
+	mdiobus_write(priv->mii, serdes_phy_addr, SERDES_GCR0, data);
+
+	/* Check for P0 state */
+	data = serdes_status_poll(priv, serdes_phy_addr,
+				  SERDES_GSR0,
+				  SERDES_PWR_ST_MASK,
+				  SERDES_PWR_ST_P0 << SERDES_PWR_ST_SHIFT);
+
+	if (data) {
+		dev_err(priv->device, "Serdes power state P0 timeout.\n");
+		return data;
+	}
+
+	return 0;
+}
+
+static void intel_serdes_powerdown(struct net_device *ndev, void *intel_data)
+{
+	struct intel_priv_data *intel_priv = intel_data;
+	struct stmmac_priv *priv = netdev_priv(ndev);
+	int serdes_phy_addr = 0;
+	u32 data = 0;
+
+	if (!intel_priv->mdio_adhoc_addr)
+		return;
+
+	serdes_phy_addr = intel_priv->mdio_adhoc_addr;
+
+	/*  move power state to P3 */
+	data = mdiobus_read(priv->mii, serdes_phy_addr, SERDES_GCR0);
+
+	data &= ~SERDES_PWR_ST_MASK;
+	data |= SERDES_PWR_ST_P3 << SERDES_PWR_ST_SHIFT;
+
+	mdiobus_write(priv->mii, serdes_phy_addr, SERDES_GCR0, data);
+
+	/* Check for P3 state */
+	data = serdes_status_poll(priv, serdes_phy_addr,
+				  SERDES_GSR0,
+				  SERDES_PWR_ST_MASK,
+				  SERDES_PWR_ST_P3 << SERDES_PWR_ST_SHIFT);
+
+	if (data) {
+		dev_err(priv->device, "Serdes power state P3 timeout\n");
+		return;
+	}
+
+	/* de-assert clk_req */
+	data = mdiobus_read(priv->mii, serdes_phy_addr, SERDES_GCR0);
+	data &= ~SERDES_PLL_CLK;
+	mdiobus_write(priv->mii, serdes_phy_addr, SERDES_GCR0, data);
+
+	/* check for clk_ack de-assert */
+	data = serdes_status_poll(priv, serdes_phy_addr,
+				  SERDES_GSR0,
+				  SERDES_PLL_CLK,
+				  (u32)~SERDES_PLL_CLK);
+
+	if (data) {
+		dev_err(priv->device, "Serdes PLL clk de-assert timeout\n");
+		return;
+	}
+
+	/* de-assert lane reset */
+	data = mdiobus_read(priv->mii, serdes_phy_addr, SERDES_GCR0);
+	data &= ~SERDES_RST;
+	mdiobus_write(priv->mii, serdes_phy_addr, SERDES_GCR0, data);
+
+	/* check for de-assert lane reset reflection */
+	data = serdes_status_poll(priv, serdes_phy_addr,
+				  SERDES_GSR0,
+				  SERDES_RST,
+				  (u32)~SERDES_RST);
+
+	if (data) {
+		dev_err(priv->device, "Serdes de-assert lane reset timeout\n");
+		return;
+	}
+}
+
+static void common_default_data(struct plat_stmmacenet_data *plat)
+{
+	plat->clk_csr = 2;	/* clk_csr_i = 20-35MHz & MDC = clk_csr_i/16 */
+	plat->has_gmac = 1;
+	plat->force_sf_dma_mode = 1;
+
+	plat->mdio_bus_data->needs_reset = true;
+
+	/* Set default value for multicast hash bins */
+	plat->multicast_filter_bins = HASH_TABLE_SIZE;
+
+	/* Set default value for unicast filter entries */
+	plat->unicast_filter_entries = 1;
+
+	/* Set the maxmtu to a default of JUMBO_LEN */
+	plat->maxmtu = JUMBO_LEN;
+
+	/* Set default number of RX and TX queues to use */
+	plat->tx_queues_to_use = 1;
+	plat->rx_queues_to_use = 1;
+
+	/* Disable Priority config by default */
+	plat->tx_queues_cfg[0].use_prio = false;
+	plat->rx_queues_cfg[0].use_prio = false;
+
+	/* Disable RX queues routing by default */
+	plat->rx_queues_cfg[0].pkt_route = 0x0;
+}
+
+static int intel_mgbe_common_data(struct pci_dev *pdev,
+				  struct plat_stmmacenet_data *plat)
+{
+	int ret;
+	int i;
+
+	plat->clk_csr = 5;
+	plat->has_gmac = 0;
+	plat->has_gmac4 = 1;
+	plat->force_sf_dma_mode = 0;
+	plat->tso_en = 1;
+
+	plat->rx_sched_algorithm = MTL_RX_ALGORITHM_SP;
+
+	for (i = 0; i < plat->rx_queues_to_use; i++) {
+		plat->rx_queues_cfg[i].mode_to_use = MTL_QUEUE_DCB;
+		plat->rx_queues_cfg[i].chan = i;
+
+		/* Disable Priority config by default */
+		plat->rx_queues_cfg[i].use_prio = false;
+
+		/* Disable RX queues routing by default */
+		plat->rx_queues_cfg[i].pkt_route = 0x0;
+	}
+
+	for (i = 0; i < plat->tx_queues_to_use; i++) {
+		plat->tx_queues_cfg[i].mode_to_use = MTL_QUEUE_DCB;
+
+		/* Disable Priority config by default */
+		plat->tx_queues_cfg[i].use_prio = false;
+	}
+
+	/* FIFO size is 4096 bytes for 1 tx/rx queue */
+	plat->tx_fifo_size = plat->tx_queues_to_use * 4096;
+	plat->rx_fifo_size = plat->rx_queues_to_use * 4096;
+
+	plat->tx_sched_algorithm = MTL_TX_ALGORITHM_WRR;
+	plat->tx_queues_cfg[0].weight = 0x09;
+	plat->tx_queues_cfg[1].weight = 0x0A;
+	plat->tx_queues_cfg[2].weight = 0x0B;
+	plat->tx_queues_cfg[3].weight = 0x0C;
+	plat->tx_queues_cfg[4].weight = 0x0D;
+	plat->tx_queues_cfg[5].weight = 0x0E;
+	plat->tx_queues_cfg[6].weight = 0x0F;
+	plat->tx_queues_cfg[7].weight = 0x10;
+
+	plat->dma_cfg->pbl = 32;
+	plat->dma_cfg->pblx8 = true;
+	plat->dma_cfg->fixed_burst = 0;
+	plat->dma_cfg->mixed_burst = 0;
+	plat->dma_cfg->aal = 0;
+
+	plat->axi = devm_kzalloc(&pdev->dev, sizeof(*plat->axi),
+				 GFP_KERNEL);
+	if (!plat->axi)
+		return -ENOMEM;
+
+	plat->axi->axi_lpi_en = 0;
+	plat->axi->axi_xit_frm = 0;
+	plat->axi->axi_wr_osr_lmt = 1;
+	plat->axi->axi_rd_osr_lmt = 1;
+	plat->axi->axi_blen[0] = 4;
+	plat->axi->axi_blen[1] = 8;
+	plat->axi->axi_blen[2] = 16;
+
+	plat->ptp_max_adj = plat->clk_ptp_rate;
+
+	/* Set system clock */
+	plat->stmmac_clk = clk_register_fixed_rate(&pdev->dev,
+						   "stmmac-clk", NULL, 0,
+						   plat->clk_ptp_rate);
+
+	if (IS_ERR(plat->stmmac_clk)) {
+		dev_warn(&pdev->dev, "Fail to register stmmac-clk\n");
+		plat->stmmac_clk = NULL;
+	}
+
+	ret = clk_prepare_enable(plat->stmmac_clk);
+	if (ret) {
+		clk_unregister_fixed_rate(plat->stmmac_clk);
+		return ret;
+	}
+
+	/* Set default value for multicast hash bins */
+	plat->multicast_filter_bins = HASH_TABLE_SIZE;
+
+	/* Set default value for unicast filter entries */
+	plat->unicast_filter_entries = 1;
+
+	/* Set the maxmtu to a default of JUMBO_LEN */
+	plat->maxmtu = JUMBO_LEN;
+
+	return 0;
+}
+
+static int ehl_common_data(struct pci_dev *pdev,
+			   struct plat_stmmacenet_data *plat)
+{
+	plat->rx_queues_to_use = 8;
+	plat->tx_queues_to_use = 8;
+	plat->clk_ptp_rate = 200000000;
+
+	return intel_mgbe_common_data(pdev, plat);
+}
+
+static int ehl_sgmii_data(struct pci_dev *pdev,
+			  struct plat_stmmacenet_data *plat)
+{
+	plat->bus_id = 1;
+	plat->phy_addr = 0;
+	plat->phy_interface = PHY_INTERFACE_MODE_SGMII;
+
+	plat->serdes_powerup = intel_serdes_powerup;
+	plat->serdes_powerdown = intel_serdes_powerdown;
+
+	return ehl_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info ehl_sgmii1g_info = {
+	.setup = ehl_sgmii_data,
+};
+
+static int ehl_rgmii_data(struct pci_dev *pdev,
+			  struct plat_stmmacenet_data *plat)
+{
+	plat->bus_id = 1;
+	plat->phy_addr = 0;
+	plat->phy_interface = PHY_INTERFACE_MODE_RGMII;
+
+	return ehl_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info ehl_rgmii1g_info = {
+	.setup = ehl_rgmii_data,
+};
+
+static int ehl_pse0_common_data(struct pci_dev *pdev,
+				struct plat_stmmacenet_data *plat)
+{
+	plat->bus_id = 2;
+	plat->phy_addr = 1;
+	return ehl_common_data(pdev, plat);
+}
+
+static int ehl_pse0_rgmii1g_data(struct pci_dev *pdev,
+				 struct plat_stmmacenet_data *plat)
+{
+	plat->phy_interface = PHY_INTERFACE_MODE_RGMII_ID;
+	return ehl_pse0_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info ehl_pse0_rgmii1g_info = {
+	.setup = ehl_pse0_rgmii1g_data,
+};
+
+static int ehl_pse0_sgmii1g_data(struct pci_dev *pdev,
+				 struct plat_stmmacenet_data *plat)
+{
+	plat->phy_interface = PHY_INTERFACE_MODE_SGMII;
+	plat->serdes_powerup = intel_serdes_powerup;
+	plat->serdes_powerdown = intel_serdes_powerdown;
+	return ehl_pse0_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info ehl_pse0_sgmii1g_info = {
+	.setup = ehl_pse0_sgmii1g_data,
+};
+
+static int ehl_pse1_common_data(struct pci_dev *pdev,
+				struct plat_stmmacenet_data *plat)
+{
+	plat->bus_id = 3;
+	plat->phy_addr = 1;
+	return ehl_common_data(pdev, plat);
+}
+
+static int ehl_pse1_rgmii1g_data(struct pci_dev *pdev,
+				 struct plat_stmmacenet_data *plat)
+{
+	plat->phy_interface = PHY_INTERFACE_MODE_RGMII_ID;
+	return ehl_pse1_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info ehl_pse1_rgmii1g_info = {
+	.setup = ehl_pse1_rgmii1g_data,
+};
+
+static int ehl_pse1_sgmii1g_data(struct pci_dev *pdev,
+				 struct plat_stmmacenet_data *plat)
+{
+	plat->phy_interface = PHY_INTERFACE_MODE_SGMII;
+	plat->serdes_powerup = intel_serdes_powerup;
+	plat->serdes_powerdown = intel_serdes_powerdown;
+	return ehl_pse1_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info ehl_pse1_sgmii1g_info = {
+	.setup = ehl_pse1_sgmii1g_data,
+};
+
+static int tgl_common_data(struct pci_dev *pdev,
+			   struct plat_stmmacenet_data *plat)
+{
+	plat->rx_queues_to_use = 6;
+	plat->tx_queues_to_use = 4;
+	plat->clk_ptp_rate = 200000000;
+
+	return intel_mgbe_common_data(pdev, plat);
+}
+
+static int tgl_sgmii_data(struct pci_dev *pdev,
+			  struct plat_stmmacenet_data *plat)
+{
+	plat->bus_id = 1;
+	plat->phy_addr = 0;
+	plat->phy_interface = PHY_INTERFACE_MODE_SGMII;
+	plat->serdes_powerup = intel_serdes_powerup;
+	plat->serdes_powerdown = intel_serdes_powerdown;
+	return tgl_common_data(pdev, plat);
+}
+
+static struct stmmac_pci_info tgl_sgmii1g_info = {
+	.setup = tgl_sgmii_data,
+};
+
+static const struct stmmac_pci_func_data galileo_stmmac_func_data[] = {
+	{
+		.func = 6,
+		.phy_addr = 1,
+	},
+};
+
+static const struct stmmac_pci_dmi_data galileo_stmmac_dmi_data = {
+	.func = galileo_stmmac_func_data,
+	.nfuncs = ARRAY_SIZE(galileo_stmmac_func_data),
+};
+
+static const struct stmmac_pci_func_data iot2040_stmmac_func_data[] = {
+	{
+		.func = 6,
+		.phy_addr = 1,
+	},
+	{
+		.func = 7,
+		.phy_addr = 1,
+	},
+};
+
+static const struct stmmac_pci_dmi_data iot2040_stmmac_dmi_data = {
+	.func = iot2040_stmmac_func_data,
+	.nfuncs = ARRAY_SIZE(iot2040_stmmac_func_data),
+};
+
+static const struct dmi_system_id quark_pci_dmi[] = {
+	{
+		.matches = {
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "Galileo"),
+		},
+		.driver_data = (void *)&galileo_stmmac_dmi_data,
+	},
+	{
+		.matches = {
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "GalileoGen2"),
+		},
+		.driver_data = (void *)&galileo_stmmac_dmi_data,
+	},
+	/* There are 2 types of SIMATIC IOT2000: IOT2020 and IOT2040.
+	 * The asset tag "6ES7647-0AA00-0YA2" is only for IOT2020 which
+	 * has only one pci network device while other asset tags are
+	 * for IOT2040 which has two.
+	 */
+	{
+		.matches = {
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "SIMATIC IOT2000"),
+			DMI_EXACT_MATCH(DMI_BOARD_ASSET_TAG,
+					"6ES7647-0AA00-0YA2"),
+		},
+		.driver_data = (void *)&galileo_stmmac_dmi_data,
+	},
+	{
+		.matches = {
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "SIMATIC IOT2000"),
+		},
+		.driver_data = (void *)&iot2040_stmmac_dmi_data,
+	},
+	{}
+};
+
+static int quark_default_data(struct pci_dev *pdev,
+			      struct plat_stmmacenet_data *plat)
+{
+	int ret;
+
+	/* Set common default data first */
+	common_default_data(plat);
+
+	/* Refuse to load the driver and register net device if MAC controller
+	 * does not connect to any PHY interface.
+	 */
+	ret = stmmac_pci_find_phy_addr(pdev, quark_pci_dmi);
+	if (ret < 0) {
+		/* Return error to the caller on DMI enabled boards. */
+		if (dmi_get_system_info(DMI_BOARD_NAME))
+			return ret;
+
+		/* Galileo boards with old firmware don't support DMI. We always
+		 * use 1 here as PHY address, so at least the first found MAC
+		 * controller would be probed.
+		 */
+		ret = 1;
+	}
+
+	plat->bus_id = pci_dev_id(pdev);
+	plat->phy_addr = ret;
+	plat->phy_interface = PHY_INTERFACE_MODE_RMII;
+
+	plat->dma_cfg->pbl = 16;
+	plat->dma_cfg->pblx8 = true;
+	plat->dma_cfg->fixed_burst = 1;
+	/* AXI (TODO) */
+
+	return 0;
+}
+
+static const struct stmmac_pci_info quark_info = {
+	.setup = quark_default_data,
+};
+
+/**
+ * intel_eth_pci_probe
+ *
+ * @pdev: pci device pointer
+ * @id: pointer to table of device id/id's.
+ *
+ * Description: This probing function gets called for all PCI devices which
+ * match the ID table and are not "owned" by other driver yet. This function
+ * gets passed a "struct pci_dev *" for each device whose entry in the ID table
+ * matches the device. The probe functions returns zero when the driver choose
+ * to take "ownership" of the device or an error code(-ve no) otherwise.
+ */
+static int intel_eth_pci_probe(struct pci_dev *pdev,
+			       const struct pci_device_id *id)
+{
+	struct stmmac_pci_info *info = (struct stmmac_pci_info *)id->driver_data;
+	struct intel_priv_data *intel_priv;
+	struct plat_stmmacenet_data *plat;
+	struct stmmac_resources res;
+	int ret;
+
+	intel_priv = devm_kzalloc(&pdev->dev, sizeof(*intel_priv), GFP_KERNEL);
+	if (!intel_priv)
+		return -ENOMEM;
+
+	plat = devm_kzalloc(&pdev->dev, sizeof(*plat), GFP_KERNEL);
+	if (!plat)
+		return -ENOMEM;
+
+	plat->mdio_bus_data = devm_kzalloc(&pdev->dev,
+					   sizeof(*plat->mdio_bus_data),
+					   GFP_KERNEL);
+	if (!plat->mdio_bus_data)
+		return -ENOMEM;
+
+	plat->dma_cfg = devm_kzalloc(&pdev->dev, sizeof(*plat->dma_cfg),
+				     GFP_KERNEL);
+	if (!plat->dma_cfg)
+		return -ENOMEM;
+
+	/* Enable pci device */
+	ret = pci_enable_device(pdev);
+	if (ret) {
+		dev_err(&pdev->dev, "%s: ERROR: failed to enable device\n",
+			__func__);
+		return ret;
+	}
+
+	ret = pcim_iomap_regions(pdev, BIT(0), pci_name(pdev));
+	if (ret)
+		return ret;
+
+	pci_set_master(pdev);
+
+	plat->bsp_priv = intel_priv;
+	intel_priv->mdio_adhoc_addr = 0x15;
+
+	ret = info->setup(pdev, plat);
+	if (ret)
+		return ret;
+
+	ret = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);
+	if (ret < 0)
+		return ret;
+
+	memset(&res, 0, sizeof(res));
+	res.addr = pcim_iomap_table(pdev)[0];
+	res.wol_irq = pci_irq_vector(pdev, 0);
+	res.irq = pci_irq_vector(pdev, 0);
+
+	ret = stmmac_dvr_probe(&pdev->dev, plat, &res);
+	if (ret) {
+		pci_free_irq_vectors(pdev);
+		clk_disable_unprepare(plat->stmmac_clk);
+		clk_unregister_fixed_rate(plat->stmmac_clk);
+	}
+
+	return ret;
+}
+
+/**
+ * intel_eth_pci_remove
+ *
+ * @pdev: platform device pointer
+ * Description: this function calls the main to free the net resources
+ * and releases the PCI resources.
+ */
+static void intel_eth_pci_remove(struct pci_dev *pdev)
+{
+	struct net_device *ndev = dev_get_drvdata(&pdev->dev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+
+	stmmac_dvr_remove(&pdev->dev);
+
+	pci_free_irq_vectors(pdev);
+
+	clk_disable_unprepare(priv->plat->stmmac_clk);
+	clk_unregister_fixed_rate(priv->plat->stmmac_clk);
+
+	pcim_iounmap_regions(pdev, BIT(0));
+
+	pci_disable_device(pdev);
+}
+
+static int __maybe_unused intel_eth_pci_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int ret;
+
+	ret = stmmac_suspend(dev);
+	if (ret)
+		return ret;
+
+	ret = pci_save_state(pdev);
+	if (ret)
+		return ret;
+
+	pci_disable_device(pdev);
+	pci_wake_from_d3(pdev, true);
+	return 0;
+}
+
+static int __maybe_unused intel_eth_pci_resume(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int ret;
+
+	pci_restore_state(pdev);
+	pci_set_power_state(pdev, PCI_D0);
+
+	ret = pci_enable_device(pdev);
+	if (ret)
+		return ret;
+
+	pci_set_master(pdev);
+
+	return stmmac_resume(dev);
+}
+
+static SIMPLE_DEV_PM_OPS(intel_eth_pm_ops, intel_eth_pci_suspend,
+			 intel_eth_pci_resume);
+
+#define PCI_DEVICE_ID_INTEL_QUARK_ID			0x0937
+#define PCI_DEVICE_ID_INTEL_EHL_RGMII1G_ID		0x4b30
+#define PCI_DEVICE_ID_INTEL_EHL_SGMII1G_ID		0x4b31
+#define PCI_DEVICE_ID_INTEL_EHL_SGMII2G5_ID		0x4b32
+/* Intel(R) Programmable Services Engine (Intel(R) PSE) consist of 2 MAC
+ * which are named PSE0 and PSE1
+ */
+#define PCI_DEVICE_ID_INTEL_EHL_PSE0_RGMII1G_ID		0x4ba0
+#define PCI_DEVICE_ID_INTEL_EHL_PSE0_SGMII1G_ID		0x4ba1
+#define PCI_DEVICE_ID_INTEL_EHL_PSE0_SGMII2G5_ID	0x4ba2
+#define PCI_DEVICE_ID_INTEL_EHL_PSE1_RGMII1G_ID		0x4bb0
+#define PCI_DEVICE_ID_INTEL_EHL_PSE1_SGMII1G_ID		0x4bb1
+#define PCI_DEVICE_ID_INTEL_EHL_PSE1_SGMII2G5_ID	0x4bb2
+#define PCI_DEVICE_ID_INTEL_TGL_SGMII1G_ID		0xa0ac
+
+static const struct pci_device_id intel_eth_pci_id_table[] = {
+	{ PCI_DEVICE_DATA(INTEL, QUARK_ID, &quark_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_RGMII1G_ID, &ehl_rgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_SGMII1G_ID, &ehl_sgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_SGMII2G5_ID, &ehl_sgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_PSE0_RGMII1G_ID, &ehl_pse0_rgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_PSE0_SGMII1G_ID, &ehl_pse0_sgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_PSE0_SGMII2G5_ID, &ehl_pse0_sgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_PSE1_RGMII1G_ID, &ehl_pse1_rgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_PSE1_SGMII1G_ID, &ehl_pse1_sgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, EHL_PSE1_SGMII2G5_ID, &ehl_pse1_sgmii1g_info) },
+	{ PCI_DEVICE_DATA(INTEL, TGL_SGMII1G_ID, &tgl_sgmii1g_info) },
+	{}
+};
+MODULE_DEVICE_TABLE(pci, intel_eth_pci_id_table);
+
+static struct pci_driver intel_eth_pci_driver = {
+	.name = "intel-eth-pci",
+	.id_table = intel_eth_pci_id_table,
+	.probe = intel_eth_pci_probe,
+	.remove = intel_eth_pci_remove,
+	.driver         = {
+		.pm     = &intel_eth_pm_ops,
+	},
+};
+
+module_pci_driver(intel_eth_pci_driver);
+
+MODULE_DESCRIPTION("INTEL 10/100/1000 Ethernet PCI driver");
+MODULE_AUTHOR("Voon Weifeng <weifeng.voon@intel.com>");
+MODULE_LICENSE("GPL v2");
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac-intel.h b/net/rtnet/drivers/orange-pi-one/dwmac-intel.h
--- a/net/rtnet/drivers/orange-pi-one/dwmac-intel.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac-intel.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2020, Intel Corporation
+ * DWMAC Intel header file
+ */
+
+#ifndef __DWMAC_INTEL_H__
+#define __DWMAC_INTEL_H__
+
+#define POLL_DELAY_US 8
+
+/* SERDES Register */
+#define SERDES_GSR0	0x5	/* Global Status Reg0 */
+#define SERDES_GCR0	0xb	/* Global Configuration Reg0 */
+
+/* SERDES defines */
+#define SERDES_PLL_CLK		BIT(0)		/* PLL clk valid signal */
+#define SERDES_RST		BIT(2)		/* Serdes Reset */
+#define SERDES_PWR_ST_MASK	GENMASK(6, 4)	/* Serdes Power state*/
+#define SERDES_PWR_ST_SHIFT	4
+#define SERDES_PWR_ST_P0	0x0
+#define SERDES_PWR_ST_P3	0x3
+
+#endif /* __DWMAC_INTEL_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac_lib.c b/net/rtnet/drivers/orange-pi-one/dwmac_lib.c
--- a/net/rtnet/drivers/orange-pi-one/dwmac_lib.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac_lib.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,282 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include "common.h"
+#include "dwmac_dma.h"
+
+#define GMAC_HI_REG_AE		0x80000000
+
+int dwmac_dma_reset(void __iomem *ioaddr)
+{
+	u32 value = readl(ioaddr + DMA_BUS_MODE);
+
+	/* DMA SW reset */
+	value |= DMA_BUS_MODE_SFT_RESET;
+	writel(value, ioaddr + DMA_BUS_MODE);
+
+	return readl_poll_timeout(ioaddr + DMA_BUS_MODE, value,
+				 !(value & DMA_BUS_MODE_SFT_RESET),
+				 10000, 100000);
+}
+
+/* CSR1 enables the transmit DMA to check for new descriptor */
+void dwmac_enable_dma_transmission(void __iomem *ioaddr)
+{
+	writel(1, ioaddr + DMA_XMT_POLL_DEMAND);
+}
+
+void dwmac_enable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx)
+{
+	u32 value = readl(ioaddr + DMA_INTR_ENA);
+
+	if (rx)
+		value |= DMA_INTR_DEFAULT_RX;
+	if (tx)
+		value |= DMA_INTR_DEFAULT_TX;
+
+	writel(value, ioaddr + DMA_INTR_ENA);
+}
+
+void dwmac_disable_dma_irq(void __iomem *ioaddr, u32 chan, bool rx, bool tx)
+{
+	u32 value = readl(ioaddr + DMA_INTR_ENA);
+
+	if (rx)
+		value &= ~DMA_INTR_DEFAULT_RX;
+	if (tx)
+		value &= ~DMA_INTR_DEFAULT_TX;
+
+	writel(value, ioaddr + DMA_INTR_ENA);
+}
+
+void dwmac_dma_start_tx(void __iomem *ioaddr, u32 chan)
+{
+	u32 value = readl(ioaddr + DMA_CONTROL);
+	value |= DMA_CONTROL_ST;
+	writel(value, ioaddr + DMA_CONTROL);
+}
+
+void dwmac_dma_stop_tx(void __iomem *ioaddr, u32 chan)
+{
+	u32 value = readl(ioaddr + DMA_CONTROL);
+	value &= ~DMA_CONTROL_ST;
+	writel(value, ioaddr + DMA_CONTROL);
+}
+
+void dwmac_dma_start_rx(void __iomem *ioaddr, u32 chan)
+{
+	u32 value = readl(ioaddr + DMA_CONTROL);
+	value |= DMA_CONTROL_SR;
+	writel(value, ioaddr + DMA_CONTROL);
+}
+
+void dwmac_dma_stop_rx(void __iomem *ioaddr, u32 chan)
+{
+	u32 value = readl(ioaddr + DMA_CONTROL);
+	value &= ~DMA_CONTROL_SR;
+	writel(value, ioaddr + DMA_CONTROL);
+}
+
+#ifdef DWMAC_DMA_DEBUG
+static void show_tx_process_state(unsigned int status)
+{
+	unsigned int state;
+	state = (status & DMA_STATUS_TS_MASK) >> DMA_STATUS_TS_SHIFT;
+
+	switch (state) {
+	case 0:
+		pr_debug("- TX (Stopped): Reset or Stop command\n");
+		break;
+	case 1:
+		pr_debug("- TX (Running): Fetching the Tx desc\n");
+		break;
+	case 2:
+		pr_debug("- TX (Running): Waiting for end of tx\n");
+		break;
+	case 3:
+		pr_debug("- TX (Running): Reading the data "
+		       "and queuing the data into the Tx buf\n");
+		break;
+	case 6:
+		pr_debug("- TX (Suspended): Tx Buff Underflow "
+		       "or an unavailable Transmit descriptor\n");
+		break;
+	case 7:
+		pr_debug("- TX (Running): Closing Tx descriptor\n");
+		break;
+	default:
+		break;
+	}
+}
+
+static void show_rx_process_state(unsigned int status)
+{
+	unsigned int state;
+	state = (status & DMA_STATUS_RS_MASK) >> DMA_STATUS_RS_SHIFT;
+
+	switch (state) {
+	case 0:
+		pr_debug("- RX (Stopped): Reset or Stop command\n");
+		break;
+	case 1:
+		pr_debug("- RX (Running): Fetching the Rx desc\n");
+		break;
+	case 2:
+		pr_debug("- RX (Running): Checking for end of pkt\n");
+		break;
+	case 3:
+		pr_debug("- RX (Running): Waiting for Rx pkt\n");
+		break;
+	case 4:
+		pr_debug("- RX (Suspended): Unavailable Rx buf\n");
+		break;
+	case 5:
+		pr_debug("- RX (Running): Closing Rx descriptor\n");
+		break;
+	case 6:
+		pr_debug("- RX(Running): Flushing the current frame"
+		       " from the Rx buf\n");
+		break;
+	case 7:
+		pr_debug("- RX (Running): Queuing the Rx frame"
+		       " from the Rx buf into memory\n");
+		break;
+	default:
+		break;
+	}
+}
+#endif
+
+int dwmac_dma_interrupt(void __iomem *ioaddr,
+			struct stmmac_extra_stats *x, u32 chan)
+{
+	int ret = 0;
+	/* read the status register (CSR5) */
+	u32 intr_status = readl(ioaddr + DMA_STATUS);
+
+#ifdef DWMAC_DMA_DEBUG
+	/* Enable it to monitor DMA rx/tx status in case of critical problems */
+	pr_debug("%s: [CSR5: 0x%08x]\n", __func__, intr_status);
+	show_tx_process_state(intr_status);
+	show_rx_process_state(intr_status);
+#endif
+	/* ABNORMAL interrupts */
+	if (unlikely(intr_status & DMA_STATUS_AIS)) {
+		if (unlikely(intr_status & DMA_STATUS_UNF)) {
+			ret = tx_hard_error_bump_tc;
+			x->tx_undeflow_irq++;
+		}
+		if (unlikely(intr_status & DMA_STATUS_TJT))
+			x->tx_jabber_irq++;
+
+		if (unlikely(intr_status & DMA_STATUS_OVF))
+			x->rx_overflow_irq++;
+
+		if (unlikely(intr_status & DMA_STATUS_RU))
+			x->rx_buf_unav_irq++;
+		if (unlikely(intr_status & DMA_STATUS_RPS))
+			x->rx_process_stopped_irq++;
+		if (unlikely(intr_status & DMA_STATUS_RWT))
+			x->rx_watchdog_irq++;
+		if (unlikely(intr_status & DMA_STATUS_ETI))
+			x->tx_early_irq++;
+		if (unlikely(intr_status & DMA_STATUS_TPS)) {
+			x->tx_process_stopped_irq++;
+			ret = tx_hard_error;
+		}
+		if (unlikely(intr_status & DMA_STATUS_FBI)) {
+			x->fatal_bus_error_irq++;
+			ret = tx_hard_error;
+		}
+	}
+	/* TX/RX NORMAL interrupts */
+	if (likely(intr_status & DMA_STATUS_NIS)) {
+		x->normal_irq_n++;
+		if (likely(intr_status & DMA_STATUS_RI)) {
+			u32 value = readl(ioaddr + DMA_INTR_ENA);
+			/* to schedule NAPI on real RIE event. */
+			if (likely(value & DMA_INTR_ENA_RIE)) {
+				x->rx_normal_irq_n++;
+				ret |= handle_rx;
+			}
+		}
+		if (likely(intr_status & DMA_STATUS_TI)) {
+			x->tx_normal_irq_n++;
+			ret |= handle_tx;
+		}
+		if (unlikely(intr_status & DMA_STATUS_ERI))
+			x->rx_early_irq++;
+	}
+	/* Optional hardware blocks, interrupts should be disabled */
+	if (unlikely(intr_status &
+		     (DMA_STATUS_GPI | DMA_STATUS_GMI | DMA_STATUS_GLI)))
+		pr_warn("%s: unexpected status %08x\n", __func__, intr_status);
+
+	/* Clear the interrupt by writing a logic 1 to the CSR5[15-0] */
+	writel((intr_status & 0x1ffff), ioaddr + DMA_STATUS);
+
+	return ret;
+}
+
+void dwmac_dma_flush_tx_fifo(void __iomem *ioaddr)
+{
+	u32 csr6 = readl(ioaddr + DMA_CONTROL);
+	writel((csr6 | DMA_CONTROL_FTF), ioaddr + DMA_CONTROL);
+
+	do {} while ((readl(ioaddr + DMA_CONTROL) & DMA_CONTROL_FTF));
+}
+
+void stmmac_set_mac_addr(void __iomem *ioaddr, u8 addr[6],
+			 unsigned int high, unsigned int low)
+{
+	unsigned long data;
+
+	data = (addr[5] << 8) | addr[4];
+	/* For MAC Addr registers we have to set the Address Enable (AE)
+	 * bit that has no effect on the High Reg 0 where the bit 31 (MO)
+	 * is RO.
+	 */
+	writel(data | GMAC_HI_REG_AE, ioaddr + high);
+	data = (addr[3] << 24) | (addr[2] << 16) | (addr[1] << 8) | addr[0];
+	writel(data, ioaddr + low);
+}
+EXPORT_SYMBOL_GPL(stmmac_set_mac_addr);
+
+/* Enable disable MAC RX/TX */
+void stmmac_set_mac(void __iomem *ioaddr, bool enable)
+{
+	u32 value = readl(ioaddr + MAC_CTRL_REG);
+
+	if (enable)
+		value |= MAC_ENABLE_RX | MAC_ENABLE_TX;
+	else
+		value &= ~(MAC_ENABLE_TX | MAC_ENABLE_RX);
+
+	writel(value, ioaddr + MAC_CTRL_REG);
+}
+
+void stmmac_get_mac_addr(void __iomem *ioaddr, unsigned char *addr,
+			 unsigned int high, unsigned int low)
+{
+	unsigned int hi_addr, lo_addr;
+
+	/* Read the MAC address from the hardware */
+	hi_addr = readl(ioaddr + high);
+	lo_addr = readl(ioaddr + low);
+
+	/* Extract the MAC address from the high and low words */
+	addr[0] = lo_addr & 0xff;
+	addr[1] = (lo_addr >> 8) & 0xff;
+	addr[2] = (lo_addr >> 16) & 0xff;
+	addr[3] = (lo_addr >> 24) & 0xff;
+	addr[4] = hi_addr & 0xff;
+	addr[5] = (hi_addr >> 8) & 0xff;
+}
+EXPORT_SYMBOL_GPL(stmmac_get_mac_addr);
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac-socfpga.c b/net/rtnet/drivers/orange-pi-one/dwmac-socfpga.c
--- a/net/rtnet/drivers/orange-pi-one/dwmac-socfpga.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac-socfpga.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,517 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright Altera Corporation (C) 2014. All rights reserved.
+ *
+ * Adopted from dwmac-sti.c
+ */
+
+#include <linux/mfd/altera-sysmgr.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/phy.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/stmmac.h>
+
+#include "stmmac.h"
+#include "stmmac_platform.h"
+
+#include "altr_tse_pcs.h"
+
+#define SGMII_ADAPTER_CTRL_REG                          0x00
+#define SGMII_ADAPTER_DISABLE                           0x0001
+
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_GMII_MII 0x0
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_RGMII 0x1
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_RMII 0x2
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_WIDTH 2
+#define SYSMGR_EMACGRP_CTRL_PHYSEL_MASK 0x00000003
+#define SYSMGR_EMACGRP_CTRL_PTP_REF_CLK_MASK 0x00000010
+#define SYSMGR_GEN10_EMACGRP_CTRL_PTP_REF_CLK_MASK 0x00000100
+
+#define SYSMGR_FPGAGRP_MODULE_REG  0x00000028
+#define SYSMGR_FPGAGRP_MODULE_EMAC 0x00000004
+#define SYSMGR_FPGAINTF_EMAC_REG	0x00000070
+#define SYSMGR_FPGAINTF_EMAC_BIT	0x1
+
+#define EMAC_SPLITTER_CTRL_REG			0x0
+#define EMAC_SPLITTER_CTRL_SPEED_MASK		0x3
+#define EMAC_SPLITTER_CTRL_SPEED_10		0x2
+#define EMAC_SPLITTER_CTRL_SPEED_100		0x3
+#define EMAC_SPLITTER_CTRL_SPEED_1000		0x0
+
+struct socfpga_dwmac;
+struct socfpga_dwmac_ops {
+	int (*set_phy_mode)(struct socfpga_dwmac *dwmac_priv);
+};
+
+struct socfpga_dwmac {
+	u32	reg_offset;
+	u32	reg_shift;
+	struct	device *dev;
+	struct regmap *sys_mgr_base_addr;
+	struct reset_control *stmmac_rst;
+	struct reset_control *stmmac_ocp_rst;
+	void __iomem *splitter_base;
+	bool f2h_ptp_ref_clk;
+	struct tse_pcs pcs;
+	const struct socfpga_dwmac_ops *ops;
+};
+
+static void socfpga_dwmac_fix_mac_speed(void *priv, unsigned int speed)
+{
+	struct socfpga_dwmac *dwmac = (struct socfpga_dwmac *)priv;
+	void __iomem *splitter_base = dwmac->splitter_base;
+	void __iomem *tse_pcs_base = dwmac->pcs.tse_pcs_base;
+	void __iomem *sgmii_adapter_base = dwmac->pcs.sgmii_adapter_base;
+	struct device *dev = dwmac->dev;
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct phy_device *phy_dev = ndev->phydev;
+	u32 val;
+
+	if ((tse_pcs_base) && (sgmii_adapter_base))
+		writew(SGMII_ADAPTER_DISABLE,
+		       sgmii_adapter_base + SGMII_ADAPTER_CTRL_REG);
+
+	if (splitter_base) {
+		val = readl(splitter_base + EMAC_SPLITTER_CTRL_REG);
+		val &= ~EMAC_SPLITTER_CTRL_SPEED_MASK;
+
+		switch (speed) {
+		case 1000:
+			val |= EMAC_SPLITTER_CTRL_SPEED_1000;
+			break;
+		case 100:
+			val |= EMAC_SPLITTER_CTRL_SPEED_100;
+			break;
+		case 10:
+			val |= EMAC_SPLITTER_CTRL_SPEED_10;
+			break;
+		default:
+			return;
+		}
+		writel(val, splitter_base + EMAC_SPLITTER_CTRL_REG);
+	}
+
+	if (tse_pcs_base && sgmii_adapter_base)
+		tse_pcs_fix_mac_speed(&dwmac->pcs, phy_dev, speed);
+}
+
+static int socfpga_dwmac_parse_data(struct socfpga_dwmac *dwmac, struct device *dev)
+{
+	struct device_node *np = dev->of_node;
+	struct regmap *sys_mgr_base_addr;
+	u32 reg_offset, reg_shift;
+	int ret, index;
+	struct device_node *np_splitter = NULL;
+	struct device_node *np_sgmii_adapter = NULL;
+	struct resource res_splitter;
+	struct resource res_tse_pcs;
+	struct resource res_sgmii_adapter;
+
+	sys_mgr_base_addr =
+		altr_sysmgr_regmap_lookup_by_phandle(np, "altr,sysmgr-syscon");
+	if (IS_ERR(sys_mgr_base_addr)) {
+		dev_info(dev, "No sysmgr-syscon node found\n");
+		return PTR_ERR(sys_mgr_base_addr);
+	}
+
+	ret = of_property_read_u32_index(np, "altr,sysmgr-syscon", 1, &reg_offset);
+	if (ret) {
+		dev_info(dev, "Could not read reg_offset from sysmgr-syscon!\n");
+		return -EINVAL;
+	}
+
+	ret = of_property_read_u32_index(np, "altr,sysmgr-syscon", 2, &reg_shift);
+	if (ret) {
+		dev_info(dev, "Could not read reg_shift from sysmgr-syscon!\n");
+		return -EINVAL;
+	}
+
+	dwmac->f2h_ptp_ref_clk = of_property_read_bool(np, "altr,f2h_ptp_ref_clk");
+
+	np_splitter = of_parse_phandle(np, "altr,emac-splitter", 0);
+	if (np_splitter) {
+		ret = of_address_to_resource(np_splitter, 0, &res_splitter);
+		of_node_put(np_splitter);
+		if (ret) {
+			dev_info(dev, "Missing emac splitter address\n");
+			return -EINVAL;
+		}
+
+		dwmac->splitter_base = devm_ioremap_resource(dev, &res_splitter);
+		if (IS_ERR(dwmac->splitter_base)) {
+			dev_info(dev, "Failed to mapping emac splitter\n");
+			return PTR_ERR(dwmac->splitter_base);
+		}
+	}
+
+	np_sgmii_adapter = of_parse_phandle(np,
+					    "altr,gmii-to-sgmii-converter", 0);
+	if (np_sgmii_adapter) {
+		index = of_property_match_string(np_sgmii_adapter, "reg-names",
+						 "hps_emac_interface_splitter_avalon_slave");
+
+		if (index >= 0) {
+			if (of_address_to_resource(np_sgmii_adapter, index,
+						   &res_splitter)) {
+				dev_err(dev,
+					"%s: ERROR: missing emac splitter address\n",
+					__func__);
+				ret = -EINVAL;
+				goto err_node_put;
+			}
+
+			dwmac->splitter_base =
+			    devm_ioremap_resource(dev, &res_splitter);
+
+			if (IS_ERR(dwmac->splitter_base)) {
+				ret = PTR_ERR(dwmac->splitter_base);
+				goto err_node_put;
+			}
+		}
+
+		index = of_property_match_string(np_sgmii_adapter, "reg-names",
+						 "gmii_to_sgmii_adapter_avalon_slave");
+
+		if (index >= 0) {
+			if (of_address_to_resource(np_sgmii_adapter, index,
+						   &res_sgmii_adapter)) {
+				dev_err(dev,
+					"%s: ERROR: failed mapping adapter\n",
+					__func__);
+				ret = -EINVAL;
+				goto err_node_put;
+			}
+
+			dwmac->pcs.sgmii_adapter_base =
+			    devm_ioremap_resource(dev, &res_sgmii_adapter);
+
+			if (IS_ERR(dwmac->pcs.sgmii_adapter_base)) {
+				ret = PTR_ERR(dwmac->pcs.sgmii_adapter_base);
+				goto err_node_put;
+			}
+		}
+
+		index = of_property_match_string(np_sgmii_adapter, "reg-names",
+						 "eth_tse_control_port");
+
+		if (index >= 0) {
+			if (of_address_to_resource(np_sgmii_adapter, index,
+						   &res_tse_pcs)) {
+				dev_err(dev,
+					"%s: ERROR: failed mapping tse control port\n",
+					__func__);
+				ret = -EINVAL;
+				goto err_node_put;
+			}
+
+			dwmac->pcs.tse_pcs_base =
+			    devm_ioremap_resource(dev, &res_tse_pcs);
+
+			if (IS_ERR(dwmac->pcs.tse_pcs_base)) {
+				ret = PTR_ERR(dwmac->pcs.tse_pcs_base);
+				goto err_node_put;
+			}
+		}
+	}
+	dwmac->reg_offset = reg_offset;
+	dwmac->reg_shift = reg_shift;
+	dwmac->sys_mgr_base_addr = sys_mgr_base_addr;
+	dwmac->dev = dev;
+	of_node_put(np_sgmii_adapter);
+
+	return 0;
+
+err_node_put:
+	of_node_put(np_sgmii_adapter);
+	return ret;
+}
+
+static int socfpga_get_plat_phymode(struct socfpga_dwmac *dwmac)
+{
+	struct net_device *ndev = dev_get_drvdata(dwmac->dev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+
+	return priv->plat->interface;
+}
+
+static int socfpga_set_phy_mode_common(int phymode, u32 *val)
+{
+	switch (phymode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_RGMII_ID:
+	case PHY_INTERFACE_MODE_RGMII_RXID:
+	case PHY_INTERFACE_MODE_RGMII_TXID:
+		*val = SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_RGMII;
+		break;
+	case PHY_INTERFACE_MODE_MII:
+	case PHY_INTERFACE_MODE_GMII:
+	case PHY_INTERFACE_MODE_SGMII:
+		*val = SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_GMII_MII;
+		break;
+	case PHY_INTERFACE_MODE_RMII:
+		*val = SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_RMII;
+		break;
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int socfpga_gen5_set_phy_mode(struct socfpga_dwmac *dwmac)
+{
+	struct regmap *sys_mgr_base_addr = dwmac->sys_mgr_base_addr;
+	int phymode = socfpga_get_plat_phymode(dwmac);
+	u32 reg_offset = dwmac->reg_offset;
+	u32 reg_shift = dwmac->reg_shift;
+	u32 ctrl, val, module;
+
+	if (socfpga_set_phy_mode_common(phymode, &val)) {
+		dev_err(dwmac->dev, "bad phy mode %d\n", phymode);
+		return -EINVAL;
+	}
+
+	/* Overwrite val to GMII if splitter core is enabled. The phymode here
+	 * is the actual phy mode on phy hardware, but phy interface from
+	 * EMAC core is GMII.
+	 */
+	if (dwmac->splitter_base)
+		val = SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_GMII_MII;
+
+	/* Assert reset to the enet controller before changing the phy mode */
+	reset_control_assert(dwmac->stmmac_ocp_rst);
+	reset_control_assert(dwmac->stmmac_rst);
+
+	regmap_read(sys_mgr_base_addr, reg_offset, &ctrl);
+	ctrl &= ~(SYSMGR_EMACGRP_CTRL_PHYSEL_MASK << reg_shift);
+	ctrl |= val << reg_shift;
+
+	if (dwmac->f2h_ptp_ref_clk ||
+	    phymode == PHY_INTERFACE_MODE_MII ||
+	    phymode == PHY_INTERFACE_MODE_GMII ||
+	    phymode == PHY_INTERFACE_MODE_SGMII) {
+		regmap_read(sys_mgr_base_addr, SYSMGR_FPGAGRP_MODULE_REG,
+			    &module);
+		module |= (SYSMGR_FPGAGRP_MODULE_EMAC << (reg_shift / 2));
+		regmap_write(sys_mgr_base_addr, SYSMGR_FPGAGRP_MODULE_REG,
+			     module);
+	}
+
+	if (dwmac->f2h_ptp_ref_clk)
+		ctrl |= SYSMGR_EMACGRP_CTRL_PTP_REF_CLK_MASK << (reg_shift / 2);
+	else
+		ctrl &= ~(SYSMGR_EMACGRP_CTRL_PTP_REF_CLK_MASK <<
+			  (reg_shift / 2));
+
+	regmap_write(sys_mgr_base_addr, reg_offset, ctrl);
+
+	/* Deassert reset for the phy configuration to be sampled by
+	 * the enet controller, and operation to start in requested mode
+	 */
+	reset_control_deassert(dwmac->stmmac_ocp_rst);
+	reset_control_deassert(dwmac->stmmac_rst);
+	if (phymode == PHY_INTERFACE_MODE_SGMII) {
+		if (tse_pcs_init(dwmac->pcs.tse_pcs_base, &dwmac->pcs) != 0) {
+			dev_err(dwmac->dev, "Unable to initialize TSE PCS");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int socfpga_gen10_set_phy_mode(struct socfpga_dwmac *dwmac)
+{
+	struct regmap *sys_mgr_base_addr = dwmac->sys_mgr_base_addr;
+	int phymode = socfpga_get_plat_phymode(dwmac);
+	u32 reg_offset = dwmac->reg_offset;
+	u32 reg_shift = dwmac->reg_shift;
+	u32 ctrl, val, module;
+
+	if (socfpga_set_phy_mode_common(phymode, &val))
+		return -EINVAL;
+
+	/* Overwrite val to GMII if splitter core is enabled. The phymode here
+	 * is the actual phy mode on phy hardware, but phy interface from
+	 * EMAC core is GMII.
+	 */
+	if (dwmac->splitter_base)
+		val = SYSMGR_EMACGRP_CTRL_PHYSEL_ENUM_GMII_MII;
+
+	/* Assert reset to the enet controller before changing the phy mode */
+	reset_control_assert(dwmac->stmmac_ocp_rst);
+	reset_control_assert(dwmac->stmmac_rst);
+
+	regmap_read(sys_mgr_base_addr, reg_offset, &ctrl);
+	ctrl &= ~(SYSMGR_EMACGRP_CTRL_PHYSEL_MASK);
+	ctrl |= val;
+
+	if (dwmac->f2h_ptp_ref_clk ||
+	    phymode == PHY_INTERFACE_MODE_MII ||
+	    phymode == PHY_INTERFACE_MODE_GMII ||
+	    phymode == PHY_INTERFACE_MODE_SGMII) {
+		ctrl |= SYSMGR_GEN10_EMACGRP_CTRL_PTP_REF_CLK_MASK;
+		regmap_read(sys_mgr_base_addr, SYSMGR_FPGAINTF_EMAC_REG,
+			    &module);
+		module |= (SYSMGR_FPGAINTF_EMAC_BIT << reg_shift);
+		regmap_write(sys_mgr_base_addr, SYSMGR_FPGAINTF_EMAC_REG,
+			     module);
+	} else {
+		ctrl &= ~SYSMGR_GEN10_EMACGRP_CTRL_PTP_REF_CLK_MASK;
+	}
+
+	regmap_write(sys_mgr_base_addr, reg_offset, ctrl);
+
+	/* Deassert reset for the phy configuration to be sampled by
+	 * the enet controller, and operation to start in requested mode
+	 */
+	reset_control_deassert(dwmac->stmmac_ocp_rst);
+	reset_control_deassert(dwmac->stmmac_rst);
+	if (phymode == PHY_INTERFACE_MODE_SGMII) {
+		if (tse_pcs_init(dwmac->pcs.tse_pcs_base, &dwmac->pcs) != 0) {
+			dev_err(dwmac->dev, "Unable to initialize TSE PCS");
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+static int socfpga_dwmac_probe(struct platform_device *pdev)
+{
+	struct plat_stmmacenet_data *plat_dat;
+	struct stmmac_resources stmmac_res;
+	struct device		*dev = &pdev->dev;
+	int			ret;
+	struct socfpga_dwmac	*dwmac;
+	struct net_device	*ndev;
+	struct stmmac_priv	*stpriv;
+	const struct socfpga_dwmac_ops *ops;
+
+	ops = device_get_match_data(&pdev->dev);
+	if (!ops) {
+		dev_err(&pdev->dev, "no of match data provided\n");
+		return -EINVAL;
+	}
+
+	ret = stmmac_get_platform_resources(pdev, &stmmac_res);
+	if (ret)
+		return ret;
+
+	plat_dat = stmmac_probe_config_dt(pdev, &stmmac_res.mac);
+	if (IS_ERR(plat_dat))
+		return PTR_ERR(plat_dat);
+
+	dwmac = devm_kzalloc(dev, sizeof(*dwmac), GFP_KERNEL);
+	if (!dwmac) {
+		ret = -ENOMEM;
+		goto err_remove_config_dt;
+	}
+
+	dwmac->stmmac_ocp_rst = devm_reset_control_get_optional(dev, "stmmaceth-ocp");
+	if (IS_ERR(dwmac->stmmac_ocp_rst)) {
+		ret = PTR_ERR(dwmac->stmmac_ocp_rst);
+		dev_err(dev, "error getting reset control of ocp %d\n", ret);
+		goto err_remove_config_dt;
+	}
+
+	reset_control_deassert(dwmac->stmmac_ocp_rst);
+
+	ret = socfpga_dwmac_parse_data(dwmac, dev);
+	if (ret) {
+		dev_err(dev, "Unable to parse OF data\n");
+		goto err_remove_config_dt;
+	}
+
+	dwmac->ops = ops;
+	plat_dat->bsp_priv = dwmac;
+	plat_dat->fix_mac_speed = socfpga_dwmac_fix_mac_speed;
+
+	ret = stmmac_dvr_probe(&pdev->dev, plat_dat, &stmmac_res);
+	if (ret)
+		goto err_remove_config_dt;
+
+	ndev = platform_get_drvdata(pdev);
+	stpriv = netdev_priv(ndev);
+
+	/* The socfpga driver needs to control the stmmac reset to set the phy
+	 * mode. Create a copy of the core reset handle so it can be used by
+	 * the driver later.
+	 */
+	dwmac->stmmac_rst = stpriv->plat->stmmac_rst;
+
+	ret = ops->set_phy_mode(dwmac);
+	if (ret)
+		goto err_dvr_remove;
+
+	return 0;
+
+err_dvr_remove:
+	stmmac_dvr_remove(&pdev->dev);
+err_remove_config_dt:
+	stmmac_remove_config_dt(pdev, plat_dat);
+
+	return ret;
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int socfpga_dwmac_resume(struct device *dev)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+	struct socfpga_dwmac *dwmac_priv = get_stmmac_bsp_priv(dev);
+
+	dwmac_priv->ops->set_phy_mode(priv->plat->bsp_priv);
+
+	/* Before the enet controller is suspended, the phy is suspended.
+	 * This causes the phy clock to be gated. The enet controller is
+	 * resumed before the phy, so the clock is still gated "off" when
+	 * the enet controller is resumed. This code makes sure the phy
+	 * is "resumed" before reinitializing the enet controller since
+	 * the enet controller depends on an active phy clock to complete
+	 * a DMA reset. A DMA reset will "time out" if executed
+	 * with no phy clock input on the Synopsys enet controller.
+	 * Verified through Synopsys Case #8000711656.
+	 *
+	 * Note that the phy clock is also gated when the phy is isolated.
+	 * Phy "suspend" and "isolate" controls are located in phy basic
+	 * control register 0, and can be modified by the phy driver
+	 * framework.
+	 */
+	if (ndev->phydev)
+		phy_resume(ndev->phydev);
+
+	return stmmac_resume(dev);
+}
+#endif /* CONFIG_PM_SLEEP */
+
+static SIMPLE_DEV_PM_OPS(socfpga_dwmac_pm_ops, stmmac_suspend,
+					       socfpga_dwmac_resume);
+
+static const struct socfpga_dwmac_ops socfpga_gen5_ops = {
+	.set_phy_mode = socfpga_gen5_set_phy_mode,
+};
+
+static const struct socfpga_dwmac_ops socfpga_gen10_ops = {
+	.set_phy_mode = socfpga_gen10_set_phy_mode,
+};
+
+static const struct of_device_id socfpga_dwmac_match[] = {
+	{ .compatible = "altr,socfpga-stmmac", .data = &socfpga_gen5_ops },
+	{ .compatible = "altr,socfpga-stmmac-a10-s10", .data = &socfpga_gen10_ops },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, socfpga_dwmac_match);
+
+static struct platform_driver socfpga_dwmac_driver = {
+	.probe  = socfpga_dwmac_probe,
+	.remove = stmmac_pltfr_remove,
+	.driver = {
+		.name           = "socfpga-dwmac",
+		.pm		= &socfpga_dwmac_pm_ops,
+		.of_match_table = socfpga_dwmac_match,
+	},
+};
+module_platform_driver(socfpga_dwmac_driver);
+
+MODULE_LICENSE("GPL v2");
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwmac-sun8i.c b/net/rtnet/drivers/orange-pi-one/dwmac-sun8i.c
--- a/net/rtnet/drivers/orange-pi-one/dwmac-sun8i.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwmac-sun8i.c	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,1290 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * dwmac-sun8i.c - Allwinner sun8i DWMAC specific glue layer
+ *
+ * Copyright (C) 2017 Corentin Labbe <clabbe.montjoie@gmail.com>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ */
+
+#include <linux/clk.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/mdio-mux.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <linux/regulator/consumer.h>
+#include <linux/regmap.h>
+#include <linux/stmmac.h>
+
+#include "stmmac.h"
+#include "stmmac_platform.h"
+
+/* General notes on dwmac-sun8i:
+ * Locking: no locking is necessary in this file because all necessary locking
+ *		is done in the "stmmac files"
+ */
+
+/* struct emac_variant - Describe dwmac-sun8i hardware variant
+ * @default_syscon_value:	The default value of the EMAC register in syscon
+ *				This value is used for disabling properly EMAC
+ *				and used as a good starting value in case of the
+ *				boot process(uboot) leave some stuff.
+ * @syscon_field		reg_field for the syscon's gmac register
+ * @soc_has_internal_phy:	Does the MAC embed an internal PHY
+ * @support_mii:		Does the MAC handle MII
+ * @support_rmii:		Does the MAC handle RMII
+ * @support_rgmii:		Does the MAC handle RGMII
+ *
+ * @rx_delay_max:		Maximum raw value for RX delay chain
+ * @tx_delay_max:		Maximum raw value for TX delay chain
+ *				These two also indicate the bitmask for
+ *				the RX and TX delay chain registers. A
+ *				value of zero indicates this is not supported.
+ */
+struct emac_variant {
+	u32 default_syscon_value;
+	const struct reg_field *syscon_field;
+	bool soc_has_internal_phy;
+	bool support_mii;
+	bool support_rmii;
+	bool support_rgmii;
+	u8 rx_delay_max;
+	u8 tx_delay_max;
+};
+
+/* struct sunxi_priv_data - hold all sunxi private data
+ * @tx_clk:	reference to MAC TX clock
+ * @ephy_clk:	reference to the optional EPHY clock for the internal PHY
+ * @regulator:	reference to the optional regulator
+ * @rst_ephy:	reference to the optional EPHY reset for the internal PHY
+ * @variant:	reference to the current board variant
+ * @regmap:	regmap for using the syscon
+ * @internal_phy_powered: Does the internal PHY is enabled
+ * @mux_handle:	Internal pointer used by mdio-mux lib
+ */
+struct sunxi_priv_data {
+	struct clk *tx_clk;
+	struct clk *ephy_clk;
+	struct regulator *regulator;
+	struct reset_control *rst_ephy;
+	const struct emac_variant *variant;
+	struct regmap_field *regmap_field;
+	bool internal_phy_powered;
+	void *mux_handle;
+};
+
+/* EMAC clock register @ 0x30 in the "system control" address range */
+static const struct reg_field sun8i_syscon_reg_field = {
+	.reg = 0x30,
+	.lsb = 0,
+	.msb = 31,
+};
+
+/* EMAC clock register @ 0x164 in the CCU address range */
+static const struct reg_field sun8i_ccu_reg_field = {
+	.reg = 0x164,
+	.lsb = 0,
+	.msb = 31,
+};
+
+static const struct emac_variant emac_variant_h3 = {
+	.default_syscon_value = 0x58000,
+	.syscon_field = &sun8i_syscon_reg_field,
+	.soc_has_internal_phy = true,
+	.support_mii = true,
+	.support_rmii = true,
+	.support_rgmii = true,
+	.rx_delay_max = 31,
+	.tx_delay_max = 7,
+};
+
+static const struct emac_variant emac_variant_v3s = {
+	.default_syscon_value = 0x38000,
+	.syscon_field = &sun8i_syscon_reg_field,
+	.soc_has_internal_phy = true,
+	.support_mii = true
+};
+
+static const struct emac_variant emac_variant_a83t = {
+	.default_syscon_value = 0,
+	.syscon_field = &sun8i_syscon_reg_field,
+	.soc_has_internal_phy = false,
+	.support_mii = true,
+	.support_rgmii = true,
+	.rx_delay_max = 31,
+	.tx_delay_max = 7,
+};
+
+static const struct emac_variant emac_variant_r40 = {
+	.default_syscon_value = 0,
+	.syscon_field = &sun8i_ccu_reg_field,
+	.support_mii = true,
+	.support_rgmii = true,
+	.rx_delay_max = 7,
+};
+
+static const struct emac_variant emac_variant_a64 = {
+	.default_syscon_value = 0,
+	.syscon_field = &sun8i_syscon_reg_field,
+	.soc_has_internal_phy = false,
+	.support_mii = true,
+	.support_rmii = true,
+	.support_rgmii = true,
+	.rx_delay_max = 31,
+	.tx_delay_max = 7,
+};
+
+static const struct emac_variant emac_variant_h6 = {
+	.default_syscon_value = 0x50000,
+	.syscon_field = &sun8i_syscon_reg_field,
+	/* The "Internal PHY" of H6 is not on the die. It's on the
+	 * co-packaged AC200 chip instead.
+	 */
+	.soc_has_internal_phy = false,
+	.support_mii = true,
+	.support_rmii = true,
+	.support_rgmii = true,
+	.rx_delay_max = 31,
+	.tx_delay_max = 7,
+};
+
+#define EMAC_BASIC_CTL0 0x00
+#define EMAC_BASIC_CTL1 0x04
+#define EMAC_INT_STA    0x08
+#define EMAC_INT_EN     0x0C
+#define EMAC_TX_CTL0    0x10
+#define EMAC_TX_CTL1    0x14
+#define EMAC_TX_FLOW_CTL        0x1C
+#define EMAC_TX_DESC_LIST 0x20
+#define EMAC_RX_CTL0    0x24
+#define EMAC_RX_CTL1    0x28
+#define EMAC_RX_DESC_LIST 0x34
+#define EMAC_RX_FRM_FLT 0x38
+#define EMAC_MDIO_CMD   0x48
+#define EMAC_MDIO_DATA  0x4C
+#define EMAC_MACADDR_HI(reg) (0x50 + (reg) * 8)
+#define EMAC_MACADDR_LO(reg) (0x54 + (reg) * 8)
+#define EMAC_TX_DMA_STA 0xB0
+#define EMAC_TX_CUR_DESC        0xB4
+#define EMAC_TX_CUR_BUF 0xB8
+#define EMAC_RX_DMA_STA 0xC0
+#define EMAC_RX_CUR_DESC        0xC4
+#define EMAC_RX_CUR_BUF 0xC8
+
+/* Use in EMAC_BASIC_CTL0 */
+#define EMAC_DUPLEX_FULL	BIT(0)
+#define EMAC_LOOPBACK		BIT(1)
+#define EMAC_SPEED_1000 0
+#define EMAC_SPEED_100 (0x03 << 2)
+#define EMAC_SPEED_10 (0x02 << 2)
+
+/* Use in EMAC_BASIC_CTL1 */
+#define EMAC_BURSTLEN_SHIFT		24
+
+/* Used in EMAC_RX_FRM_FLT */
+#define EMAC_FRM_FLT_RXALL              BIT(0)
+#define EMAC_FRM_FLT_CTL                BIT(13)
+#define EMAC_FRM_FLT_MULTICAST          BIT(16)
+
+/* Used in RX_CTL1*/
+#define EMAC_RX_MD              BIT(1)
+#define EMAC_RX_TH_MASK		GENMASK(5, 4)
+#define EMAC_RX_TH_32		0
+#define EMAC_RX_TH_64		(0x1 << 4)
+#define EMAC_RX_TH_96		(0x2 << 4)
+#define EMAC_RX_TH_128		(0x3 << 4)
+#define EMAC_RX_DMA_EN  BIT(30)
+#define EMAC_RX_DMA_START       BIT(31)
+
+/* Used in TX_CTL1*/
+#define EMAC_TX_MD              BIT(1)
+#define EMAC_TX_NEXT_FRM        BIT(2)
+#define EMAC_TX_TH_MASK		GENMASK(10, 8)
+#define EMAC_TX_TH_64		0
+#define EMAC_TX_TH_128		(0x1 << 8)
+#define EMAC_TX_TH_192		(0x2 << 8)
+#define EMAC_TX_TH_256		(0x3 << 8)
+#define EMAC_TX_DMA_EN  BIT(30)
+#define EMAC_TX_DMA_START       BIT(31)
+
+/* Used in RX_CTL0 */
+#define EMAC_RX_RECEIVER_EN             BIT(31)
+#define EMAC_RX_DO_CRC BIT(27)
+#define EMAC_RX_FLOW_CTL_EN             BIT(16)
+
+/* Used in TX_CTL0 */
+#define EMAC_TX_TRANSMITTER_EN  BIT(31)
+
+/* Used in EMAC_TX_FLOW_CTL */
+#define EMAC_TX_FLOW_CTL_EN             BIT(0)
+
+/* Used in EMAC_INT_STA */
+#define EMAC_TX_INT             BIT(0)
+#define EMAC_TX_DMA_STOP_INT    BIT(1)
+#define EMAC_TX_BUF_UA_INT      BIT(2)
+#define EMAC_TX_TIMEOUT_INT     BIT(3)
+#define EMAC_TX_UNDERFLOW_INT   BIT(4)
+#define EMAC_TX_EARLY_INT       BIT(5)
+#define EMAC_RX_INT             BIT(8)
+#define EMAC_RX_BUF_UA_INT      BIT(9)
+#define EMAC_RX_DMA_STOP_INT    BIT(10)
+#define EMAC_RX_TIMEOUT_INT     BIT(11)
+#define EMAC_RX_OVERFLOW_INT    BIT(12)
+#define EMAC_RX_EARLY_INT       BIT(13)
+#define EMAC_RGMII_STA_INT      BIT(16)
+
+#define MAC_ADDR_TYPE_DST BIT(31)
+
+/* H3 specific bits for EPHY */
+#define H3_EPHY_ADDR_SHIFT	20
+#define H3_EPHY_CLK_SEL		BIT(18) /* 1: 24MHz, 0: 25MHz */
+#define H3_EPHY_LED_POL		BIT(17) /* 1: active low, 0: active high */
+#define H3_EPHY_SHUTDOWN	BIT(16) /* 1: shutdown, 0: power up */
+#define H3_EPHY_SELECT		BIT(15) /* 1: internal PHY, 0: external PHY */
+#define H3_EPHY_MUX_MASK	(H3_EPHY_SHUTDOWN | H3_EPHY_SELECT)
+#define DWMAC_SUN8I_MDIO_MUX_INTERNAL_ID	1
+#define DWMAC_SUN8I_MDIO_MUX_EXTERNAL_ID	2
+
+/* H3/A64 specific bits */
+#define SYSCON_RMII_EN		BIT(13) /* 1: enable RMII (overrides EPIT) */
+
+/* Generic system control EMAC_CLK bits */
+#define SYSCON_ETXDC_SHIFT		10
+#define SYSCON_ERXDC_SHIFT		5
+/* EMAC PHY Interface Type */
+#define SYSCON_EPIT			BIT(2) /* 1: RGMII, 0: MII */
+#define SYSCON_ETCS_MASK		GENMASK(1, 0)
+#define SYSCON_ETCS_MII		0x0
+#define SYSCON_ETCS_EXT_GMII	0x1
+#define SYSCON_ETCS_INT_GMII	0x2
+
+/* sun8i_dwmac_dma_reset() - reset the EMAC
+ * Called from stmmac via stmmac_dma_ops->reset
+ */
+static int sun8i_dwmac_dma_reset(void __iomem *ioaddr)
+{
+	writel(0, ioaddr + EMAC_RX_CTL1);
+	writel(0, ioaddr + EMAC_TX_CTL1);
+	writel(0, ioaddr + EMAC_RX_FRM_FLT);
+	writel(0, ioaddr + EMAC_RX_DESC_LIST);
+	writel(0, ioaddr + EMAC_TX_DESC_LIST);
+	writel(0, ioaddr + EMAC_INT_EN);
+	writel(0x1FFFFFF, ioaddr + EMAC_INT_STA);
+	return 0;
+}
+
+/* sun8i_dwmac_dma_init() - initialize the EMAC
+ * Called from stmmac via stmmac_dma_ops->init
+ */
+static void sun8i_dwmac_dma_init(void __iomem *ioaddr,
+				 struct stmmac_dma_cfg *dma_cfg, int atds)
+{
+	writel(EMAC_RX_INT | EMAC_TX_INT, ioaddr + EMAC_INT_EN);
+	writel(0x1FFFFFF, ioaddr + EMAC_INT_STA);
+}
+
+static void sun8i_dwmac_dma_init_rx(void __iomem *ioaddr,
+				    struct stmmac_dma_cfg *dma_cfg,
+				    dma_addr_t dma_rx_phy, u32 chan)
+{
+	/* Write RX descriptors address */
+	writel(lower_32_bits(dma_rx_phy), ioaddr + EMAC_RX_DESC_LIST);
+}
+
+static void sun8i_dwmac_dma_init_tx(void __iomem *ioaddr,
+				    struct stmmac_dma_cfg *dma_cfg,
+				    dma_addr_t dma_tx_phy, u32 chan)
+{
+	/* Write TX descriptors address */
+	writel(lower_32_bits(dma_tx_phy), ioaddr + EMAC_TX_DESC_LIST);
+}
+
+/* sun8i_dwmac_dump_regs() - Dump EMAC address space
+ * Called from stmmac_dma_ops->dump_regs
+ * Used for ethtool
+ */
+static void sun8i_dwmac_dump_regs(void __iomem *ioaddr, u32 *reg_space)
+{
+	int i;
+
+	for (i = 0; i < 0xC8; i += 4) {
+		if (i == 0x32 || i == 0x3C)
+			continue;
+		reg_space[i / 4] = readl(ioaddr + i);
+	}
+}
+
+/* sun8i_dwmac_dump_mac_regs() - Dump EMAC address space
+ * Called from stmmac_ops->dump_regs
+ * Used for ethtool
+ */
+static void sun8i_dwmac_dump_mac_regs(struct mac_device_info *hw,
+				      u32 *reg_space)
+{
+	int i;
+	void __iomem *ioaddr = hw->pcsr;
+
+	for (i = 0; i < 0xC8; i += 4) {
+		if (i == 0x32 || i == 0x3C)
+			continue;
+		reg_space[i / 4] = readl(ioaddr + i);
+	}
+}
+
+static void sun8i_dwmac_enable_dma_irq(void __iomem *ioaddr, u32 chan,
+				       bool rx, bool tx)
+{
+	u32 value = readl(ioaddr + EMAC_INT_EN);
+
+	if (rx)
+		value |= EMAC_RX_INT;
+	if (tx)
+		value |= EMAC_TX_INT;
+
+	writel(value, ioaddr + EMAC_INT_EN);
+}
+
+static void sun8i_dwmac_disable_dma_irq(void __iomem *ioaddr, u32 chan,
+					bool rx, bool tx)
+{
+	u32 value = readl(ioaddr + EMAC_INT_EN);
+
+	if (rx)
+		value &= ~EMAC_RX_INT;
+	if (tx)
+		value &= ~EMAC_TX_INT;
+
+	writel(value, ioaddr + EMAC_INT_EN);
+}
+
+static void sun8i_dwmac_dma_start_tx(void __iomem *ioaddr, u32 chan)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_TX_CTL1);
+	v |= EMAC_TX_DMA_START;
+	v |= EMAC_TX_DMA_EN;
+	writel(v, ioaddr + EMAC_TX_CTL1);
+}
+
+static void sun8i_dwmac_enable_dma_transmission(void __iomem *ioaddr)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_TX_CTL1);
+	v |= EMAC_TX_DMA_START;
+	v |= EMAC_TX_DMA_EN;
+	writel(v, ioaddr + EMAC_TX_CTL1);
+}
+
+static void sun8i_dwmac_dma_stop_tx(void __iomem *ioaddr, u32 chan)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_TX_CTL1);
+	v &= ~EMAC_TX_DMA_EN;
+	writel(v, ioaddr + EMAC_TX_CTL1);
+}
+
+static void sun8i_dwmac_dma_start_rx(void __iomem *ioaddr, u32 chan)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_RX_CTL1);
+	v |= EMAC_RX_DMA_START;
+	v |= EMAC_RX_DMA_EN;
+	writel(v, ioaddr + EMAC_RX_CTL1);
+}
+
+static void sun8i_dwmac_dma_stop_rx(void __iomem *ioaddr, u32 chan)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_RX_CTL1);
+	v &= ~EMAC_RX_DMA_EN;
+	writel(v, ioaddr + EMAC_RX_CTL1);
+}
+
+static int sun8i_dwmac_dma_interrupt(void __iomem *ioaddr,
+				     struct stmmac_extra_stats *x, u32 chan)
+{
+	u32 v;
+	int ret = 0;
+
+	v = readl(ioaddr + EMAC_INT_STA);
+
+	if (v & EMAC_TX_INT) {
+		ret |= handle_tx;
+		x->tx_normal_irq_n++;
+	}
+
+	if (v & EMAC_TX_DMA_STOP_INT)
+		x->tx_process_stopped_irq++;
+
+	if (v & EMAC_TX_BUF_UA_INT)
+		x->tx_process_stopped_irq++;
+
+	if (v & EMAC_TX_TIMEOUT_INT)
+		ret |= tx_hard_error;
+
+	if (v & EMAC_TX_UNDERFLOW_INT) {
+		ret |= tx_hard_error;
+		x->tx_undeflow_irq++;
+	}
+
+	if (v & EMAC_TX_EARLY_INT)
+		x->tx_early_irq++;
+
+	if (v & EMAC_RX_INT) {
+		ret |= handle_rx;
+		x->rx_normal_irq_n++;
+	}
+
+	if (v & EMAC_RX_BUF_UA_INT)
+		x->rx_buf_unav_irq++;
+
+	if (v & EMAC_RX_DMA_STOP_INT)
+		x->rx_process_stopped_irq++;
+
+	if (v & EMAC_RX_TIMEOUT_INT)
+		ret |= tx_hard_error;
+
+	if (v & EMAC_RX_OVERFLOW_INT) {
+		ret |= tx_hard_error;
+		x->rx_overflow_irq++;
+	}
+
+	if (v & EMAC_RX_EARLY_INT)
+		x->rx_early_irq++;
+
+	if (v & EMAC_RGMII_STA_INT)
+		x->irq_rgmii_n++;
+
+	writel(v, ioaddr + EMAC_INT_STA);
+
+	return ret;
+}
+
+static void sun8i_dwmac_dma_operation_mode_rx(void __iomem *ioaddr, int mode,
+					      u32 channel, int fifosz, u8 qmode)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_RX_CTL1);
+	if (mode == SF_DMA_MODE) {
+		v |= EMAC_RX_MD;
+	} else {
+		v &= ~EMAC_RX_MD;
+		v &= ~EMAC_RX_TH_MASK;
+		if (mode < 32)
+			v |= EMAC_RX_TH_32;
+		else if (mode < 64)
+			v |= EMAC_RX_TH_64;
+		else if (mode < 96)
+			v |= EMAC_RX_TH_96;
+		else if (mode < 128)
+			v |= EMAC_RX_TH_128;
+	}
+	writel(v, ioaddr + EMAC_RX_CTL1);
+}
+
+static void sun8i_dwmac_dma_operation_mode_tx(void __iomem *ioaddr, int mode,
+					      u32 channel, int fifosz, u8 qmode)
+{
+	u32 v;
+
+	v = readl(ioaddr + EMAC_TX_CTL1);
+	if (mode == SF_DMA_MODE) {
+		v |= EMAC_TX_MD;
+		/* Undocumented bit (called TX_NEXT_FRM in BSP), the original
+		 * comment is
+		 * "Operating on second frame increase the performance
+		 * especially when transmit store-and-forward is used."
+		 */
+		v |= EMAC_TX_NEXT_FRM;
+	} else {
+		v &= ~EMAC_TX_MD;
+		v &= ~EMAC_TX_TH_MASK;
+		if (mode < 64)
+			v |= EMAC_TX_TH_64;
+		else if (mode < 128)
+			v |= EMAC_TX_TH_128;
+		else if (mode < 192)
+			v |= EMAC_TX_TH_192;
+		else if (mode < 256)
+			v |= EMAC_TX_TH_256;
+	}
+	writel(v, ioaddr + EMAC_TX_CTL1);
+}
+
+static const struct stmmac_dma_ops sun8i_dwmac_dma_ops = {
+	.reset = sun8i_dwmac_dma_reset,
+	.init = sun8i_dwmac_dma_init,
+	.init_rx_chan = sun8i_dwmac_dma_init_rx,
+	.init_tx_chan = sun8i_dwmac_dma_init_tx,
+	.dump_regs = sun8i_dwmac_dump_regs,
+	.dma_rx_mode = sun8i_dwmac_dma_operation_mode_rx,
+	.dma_tx_mode = sun8i_dwmac_dma_operation_mode_tx,
+	.enable_dma_transmission = sun8i_dwmac_enable_dma_transmission,
+	.enable_dma_irq = sun8i_dwmac_enable_dma_irq,
+	.disable_dma_irq = sun8i_dwmac_disable_dma_irq,
+	.start_tx = sun8i_dwmac_dma_start_tx,
+	.stop_tx = sun8i_dwmac_dma_stop_tx,
+	.start_rx = sun8i_dwmac_dma_start_rx,
+	.stop_rx = sun8i_dwmac_dma_stop_rx,
+	.dma_interrupt = sun8i_dwmac_dma_interrupt,
+};
+
+static int sun8i_dwmac_init(struct platform_device *pdev, void *priv)
+{
+	struct sunxi_priv_data *gmac = priv;
+	int ret;
+
+	if (gmac->regulator) {
+		ret = regulator_enable(gmac->regulator);
+		if (ret) {
+			dev_err(&pdev->dev, "Fail to enable regulator\n");
+			return ret;
+		}
+	}
+
+	ret = clk_prepare_enable(gmac->tx_clk);
+	if (ret) {
+		if (gmac->regulator)
+			regulator_disable(gmac->regulator);
+		dev_err(&pdev->dev, "Could not enable AHB clock\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static void sun8i_dwmac_core_init(struct mac_device_info *hw,
+				  struct net_device *dev)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	u32 v;
+
+	v = (8 << EMAC_BURSTLEN_SHIFT); /* burst len */
+	writel(v, ioaddr + EMAC_BASIC_CTL1);
+}
+
+static void sun8i_dwmac_set_mac(void __iomem *ioaddr, bool enable)
+{
+	u32 t, r;
+
+	t = readl(ioaddr + EMAC_TX_CTL0);
+	r = readl(ioaddr + EMAC_RX_CTL0);
+	if (enable) {
+		t |= EMAC_TX_TRANSMITTER_EN;
+		r |= EMAC_RX_RECEIVER_EN;
+	} else {
+		t &= ~EMAC_TX_TRANSMITTER_EN;
+		r &= ~EMAC_RX_RECEIVER_EN;
+	}
+	writel(t, ioaddr + EMAC_TX_CTL0);
+	writel(r, ioaddr + EMAC_RX_CTL0);
+}
+
+/* Set MAC address at slot reg_n
+ * All slot > 0 need to be enabled with MAC_ADDR_TYPE_DST
+ * If addr is NULL, clear the slot
+ */
+static void sun8i_dwmac_set_umac_addr(struct mac_device_info *hw,
+				      unsigned char *addr,
+				      unsigned int reg_n)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	u32 v;
+
+	if (!addr) {
+		writel(0, ioaddr + EMAC_MACADDR_HI(reg_n));
+		return;
+	}
+
+	stmmac_set_mac_addr(ioaddr, addr, EMAC_MACADDR_HI(reg_n),
+			    EMAC_MACADDR_LO(reg_n));
+	if (reg_n > 0) {
+		v = readl(ioaddr + EMAC_MACADDR_HI(reg_n));
+		v |= MAC_ADDR_TYPE_DST;
+		writel(v, ioaddr + EMAC_MACADDR_HI(reg_n));
+	}
+}
+
+static void sun8i_dwmac_get_umac_addr(struct mac_device_info *hw,
+				      unsigned char *addr,
+				      unsigned int reg_n)
+{
+	void __iomem *ioaddr = hw->pcsr;
+
+	stmmac_get_mac_addr(ioaddr, addr, EMAC_MACADDR_HI(reg_n),
+			    EMAC_MACADDR_LO(reg_n));
+}
+
+/* caution this function must return non 0 to work */
+static int sun8i_dwmac_rx_ipc_enable(struct mac_device_info *hw)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	u32 v;
+
+	v = readl(ioaddr + EMAC_RX_CTL0);
+	v |= EMAC_RX_DO_CRC;
+	writel(v, ioaddr + EMAC_RX_CTL0);
+
+	return 1;
+}
+
+static void sun8i_dwmac_set_filter(struct mac_device_info *hw,
+				   struct net_device *dev)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	u32 v;
+	int i = 1;
+	struct netdev_hw_addr *ha;
+	int macaddrs = netdev_uc_count(dev) + netdev_mc_count(dev) + 1;
+
+	v = EMAC_FRM_FLT_CTL;
+
+	if (dev->flags & IFF_PROMISC) {
+		v = EMAC_FRM_FLT_RXALL;
+	} else if (dev->flags & IFF_ALLMULTI) {
+		v |= EMAC_FRM_FLT_MULTICAST;
+	} else if (macaddrs <= hw->unicast_filter_entries) {
+		if (!netdev_mc_empty(dev)) {
+			netdev_for_each_mc_addr(ha, dev) {
+				sun8i_dwmac_set_umac_addr(hw, ha->addr, i);
+				i++;
+			}
+		}
+		if (!netdev_uc_empty(dev)) {
+			netdev_for_each_uc_addr(ha, dev) {
+				sun8i_dwmac_set_umac_addr(hw, ha->addr, i);
+				i++;
+			}
+		}
+	} else {
+		if (!(readl(ioaddr + EMAC_RX_FRM_FLT) & EMAC_FRM_FLT_RXALL))
+			netdev_info(dev, "Too many address, switching to promiscuous\n");
+		v = EMAC_FRM_FLT_RXALL;
+	}
+
+	/* Disable unused address filter slots */
+	while (i < hw->unicast_filter_entries)
+		sun8i_dwmac_set_umac_addr(hw, NULL, i++);
+
+	writel(v, ioaddr + EMAC_RX_FRM_FLT);
+}
+
+static void sun8i_dwmac_flow_ctrl(struct mac_device_info *hw,
+				  unsigned int duplex, unsigned int fc,
+				  unsigned int pause_time, u32 tx_cnt)
+{
+	void __iomem *ioaddr = hw->pcsr;
+	u32 v;
+
+	v = readl(ioaddr + EMAC_RX_CTL0);
+	if (fc == FLOW_AUTO)
+		v |= EMAC_RX_FLOW_CTL_EN;
+	else
+		v &= ~EMAC_RX_FLOW_CTL_EN;
+	writel(v, ioaddr + EMAC_RX_CTL0);
+
+	v = readl(ioaddr + EMAC_TX_FLOW_CTL);
+	if (fc == FLOW_AUTO)
+		v |= EMAC_TX_FLOW_CTL_EN;
+	else
+		v &= ~EMAC_TX_FLOW_CTL_EN;
+	writel(v, ioaddr + EMAC_TX_FLOW_CTL);
+}
+
+static int sun8i_dwmac_reset(struct stmmac_priv *priv)
+{
+	u32 v;
+	int err;
+
+	v = readl(priv->ioaddr + EMAC_BASIC_CTL1);
+	writel(v | 0x01, priv->ioaddr + EMAC_BASIC_CTL1);
+
+	/* The timeout was previoulsy set to 10ms, but some board (OrangePI0)
+	 * need more if no cable plugged. 100ms seems OK
+	 */
+	err = readl_poll_timeout(priv->ioaddr + EMAC_BASIC_CTL1, v,
+				 !(v & 0x01), 100, 100000);
+
+	if (err) {
+		dev_err(priv->device, "EMAC reset timeout\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+/* Search in mdio-mux node for internal PHY node and get its clk/reset */
+static int get_ephy_nodes(struct stmmac_priv *priv)
+{
+	struct sunxi_priv_data *gmac = priv->plat->bsp_priv;
+	struct device_node *mdio_mux, *iphynode;
+	struct device_node *mdio_internal;
+	int ret;
+
+	mdio_mux = of_get_child_by_name(priv->device->of_node, "mdio-mux");
+	if (!mdio_mux) {
+		dev_err(priv->device, "Cannot get mdio-mux node\n");
+		return -ENODEV;
+	}
+
+	mdio_internal = of_get_compatible_child(mdio_mux,
+						"allwinner,sun8i-h3-mdio-internal");
+	of_node_put(mdio_mux);
+	if (!mdio_internal) {
+		dev_err(priv->device, "Cannot get internal_mdio node\n");
+		return -ENODEV;
+	}
+
+	/* Seek for internal PHY */
+	for_each_child_of_node(mdio_internal, iphynode) {
+		gmac->ephy_clk = of_clk_get(iphynode, 0);
+		if (IS_ERR(gmac->ephy_clk))
+			continue;
+		gmac->rst_ephy = of_reset_control_get_exclusive(iphynode, NULL);
+		if (IS_ERR(gmac->rst_ephy)) {
+			ret = PTR_ERR(gmac->rst_ephy);
+			if (ret == -EPROBE_DEFER) {
+				of_node_put(iphynode);
+				of_node_put(mdio_internal);
+				return ret;
+			}
+			continue;
+		}
+		dev_info(priv->device, "Found internal PHY node\n");
+		of_node_put(iphynode);
+		of_node_put(mdio_internal);
+		return 0;
+	}
+
+	of_node_put(mdio_internal);
+	return -ENODEV;
+}
+
+static int sun8i_dwmac_power_internal_phy(struct stmmac_priv *priv)
+{
+	struct sunxi_priv_data *gmac = priv->plat->bsp_priv;
+	int ret;
+
+	if (gmac->internal_phy_powered) {
+		dev_warn(priv->device, "Internal PHY already powered\n");
+		return 0;
+	}
+
+	dev_info(priv->device, "Powering internal PHY\n");
+	ret = clk_prepare_enable(gmac->ephy_clk);
+	if (ret) {
+		dev_err(priv->device, "Cannot enable internal PHY\n");
+		return ret;
+	}
+
+	/* Make sure the EPHY is properly reseted, as U-Boot may leave
+	 * it at deasserted state, and thus it may fail to reset EMAC.
+	 */
+	reset_control_assert(gmac->rst_ephy);
+
+	ret = reset_control_deassert(gmac->rst_ephy);
+	if (ret) {
+		dev_err(priv->device, "Cannot deassert internal phy\n");
+		clk_disable_unprepare(gmac->ephy_clk);
+		return ret;
+	}
+
+	gmac->internal_phy_powered = true;
+
+	return 0;
+}
+
+static int sun8i_dwmac_unpower_internal_phy(struct sunxi_priv_data *gmac)
+{
+	if (!gmac->internal_phy_powered)
+		return 0;
+
+	clk_disable_unprepare(gmac->ephy_clk);
+	reset_control_assert(gmac->rst_ephy);
+	gmac->internal_phy_powered = false;
+	return 0;
+}
+
+/* MDIO multiplexing switch function
+ * This function is called by the mdio-mux layer when it thinks the mdio bus
+ * multiplexer needs to switch.
+ * 'current_child' is the current value of the mux register
+ * 'desired_child' is the value of the 'reg' property of the target child MDIO
+ * node.
+ * The first time this function is called, current_child == -1.
+ * If current_child == desired_child, then the mux is already set to the
+ * correct bus.
+ */
+static int mdio_mux_syscon_switch_fn(int current_child, int desired_child,
+				     void *data)
+{
+	struct stmmac_priv *priv = data;
+	struct sunxi_priv_data *gmac = priv->plat->bsp_priv;
+	u32 reg, val;
+	int ret = 0;
+	bool need_power_ephy = false;
+
+	if (current_child ^ desired_child) {
+		regmap_field_read(gmac->regmap_field, &reg);
+		switch (desired_child) {
+		case DWMAC_SUN8I_MDIO_MUX_INTERNAL_ID:
+			dev_info(priv->device, "Switch mux to internal PHY");
+			val = (reg & ~H3_EPHY_MUX_MASK) | H3_EPHY_SELECT;
+
+			need_power_ephy = true;
+			break;
+		case DWMAC_SUN8I_MDIO_MUX_EXTERNAL_ID:
+			dev_info(priv->device, "Switch mux to external PHY");
+			val = (reg & ~H3_EPHY_MUX_MASK) | H3_EPHY_SHUTDOWN;
+			need_power_ephy = false;
+			break;
+		default:
+			dev_err(priv->device, "Invalid child ID %x\n",
+				desired_child);
+			return -EINVAL;
+		}
+		regmap_field_write(gmac->regmap_field, val);
+		if (need_power_ephy) {
+			ret = sun8i_dwmac_power_internal_phy(priv);
+			if (ret)
+				return ret;
+		} else {
+			sun8i_dwmac_unpower_internal_phy(gmac);
+		}
+		/* After changing syscon value, the MAC need reset or it will
+		 * use the last value (and so the last PHY set).
+		 */
+		ret = sun8i_dwmac_reset(priv);
+	}
+	return ret;
+}
+
+static int sun8i_dwmac_register_mdio_mux(struct stmmac_priv *priv)
+{
+	int ret;
+	struct device_node *mdio_mux;
+	struct sunxi_priv_data *gmac = priv->plat->bsp_priv;
+
+	mdio_mux = of_get_child_by_name(priv->device->of_node, "mdio-mux");
+	if (!mdio_mux)
+		return -ENODEV;
+
+	ret = mdio_mux_init(priv->device, mdio_mux, mdio_mux_syscon_switch_fn,
+			    &gmac->mux_handle, priv, priv->mii);
+	return ret;
+}
+
+static int sun8i_dwmac_set_syscon(struct stmmac_priv *priv)
+{
+	struct sunxi_priv_data *gmac = priv->plat->bsp_priv;
+	struct device_node *node = priv->device->of_node;
+	int ret;
+	u32 reg, val;
+
+	ret = regmap_field_read(gmac->regmap_field, &val);
+	if (ret) {
+		dev_err(priv->device, "Fail to read from regmap field.\n");
+		return ret;
+	}
+
+	reg = gmac->variant->default_syscon_value;
+	if (reg != val)
+		dev_warn(priv->device,
+			 "Current syscon value is not the default %x (expect %x)\n",
+			 val, reg);
+
+	if (gmac->variant->soc_has_internal_phy) {
+		if (of_property_read_bool(node, "allwinner,leds-active-low"))
+			reg |= H3_EPHY_LED_POL;
+		else
+			reg &= ~H3_EPHY_LED_POL;
+
+		/* Force EPHY xtal frequency to 24MHz. */
+		reg |= H3_EPHY_CLK_SEL;
+
+		ret = of_mdio_parse_addr(priv->device, priv->plat->phy_node);
+		if (ret < 0) {
+			dev_err(priv->device, "Could not parse MDIO addr\n");
+			return ret;
+		}
+		/* of_mdio_parse_addr returns a valid (0 ~ 31) PHY
+		 * address. No need to mask it again.
+		 */
+		reg |= 1 << H3_EPHY_ADDR_SHIFT;
+	} else {
+		/* For SoCs without internal PHY the PHY selection bit should be
+		 * set to 0 (external PHY).
+		 */
+		reg &= ~H3_EPHY_SELECT;
+	}
+
+	if (!of_property_read_u32(node, "allwinner,tx-delay-ps", &val)) {
+		if (val % 100) {
+			dev_err(priv->device, "tx-delay must be a multiple of 100\n");
+			return -EINVAL;
+		}
+		val /= 100;
+		dev_dbg(priv->device, "set tx-delay to %x\n", val);
+		if (val <= gmac->variant->tx_delay_max) {
+			reg &= ~(gmac->variant->tx_delay_max <<
+				 SYSCON_ETXDC_SHIFT);
+			reg |= (val << SYSCON_ETXDC_SHIFT);
+		} else {
+			dev_err(priv->device, "Invalid TX clock delay: %d\n",
+				val);
+			return -EINVAL;
+		}
+	}
+
+	if (!of_property_read_u32(node, "allwinner,rx-delay-ps", &val)) {
+		if (val % 100) {
+			dev_err(priv->device, "rx-delay must be a multiple of 100\n");
+			return -EINVAL;
+		}
+		val /= 100;
+		dev_dbg(priv->device, "set rx-delay to %x\n", val);
+		if (val <= gmac->variant->rx_delay_max) {
+			reg &= ~(gmac->variant->rx_delay_max <<
+				 SYSCON_ERXDC_SHIFT);
+			reg |= (val << SYSCON_ERXDC_SHIFT);
+		} else {
+			dev_err(priv->device, "Invalid RX clock delay: %d\n",
+				val);
+			return -EINVAL;
+		}
+	}
+
+	/* Clear interface mode bits */
+	reg &= ~(SYSCON_ETCS_MASK | SYSCON_EPIT);
+	if (gmac->variant->support_rmii)
+		reg &= ~SYSCON_RMII_EN;
+
+	switch (priv->plat->interface) {
+	case PHY_INTERFACE_MODE_MII:
+		/* default */
+		break;
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_RGMII_ID:
+	case PHY_INTERFACE_MODE_RGMII_RXID:
+	case PHY_INTERFACE_MODE_RGMII_TXID:
+		reg |= SYSCON_EPIT | SYSCON_ETCS_INT_GMII;
+		break;
+	case PHY_INTERFACE_MODE_RMII:
+		reg |= SYSCON_RMII_EN | SYSCON_ETCS_EXT_GMII;
+		break;
+	default:
+		dev_err(priv->device, "Unsupported interface mode: %s",
+			phy_modes(priv->plat->interface));
+		return -EINVAL;
+	}
+
+	regmap_field_write(gmac->regmap_field, reg);
+
+	return 0;
+}
+
+static void sun8i_dwmac_unset_syscon(struct sunxi_priv_data *gmac)
+{
+	u32 reg = gmac->variant->default_syscon_value;
+
+	regmap_field_write(gmac->regmap_field, reg);
+}
+
+static void sun8i_dwmac_exit(struct platform_device *pdev, void *priv)
+{
+	struct sunxi_priv_data *gmac = priv;
+
+	if (gmac->variant->soc_has_internal_phy) {
+		/* sun8i_dwmac_exit could be called with mdiomux uninit */
+		if (gmac->mux_handle)
+			mdio_mux_uninit(gmac->mux_handle);
+		if (gmac->internal_phy_powered)
+			sun8i_dwmac_unpower_internal_phy(gmac);
+	}
+
+	sun8i_dwmac_unset_syscon(gmac);
+
+	reset_control_put(gmac->rst_ephy);
+
+	clk_disable_unprepare(gmac->tx_clk);
+
+	if (gmac->regulator)
+		regulator_disable(gmac->regulator);
+}
+
+static void sun8i_dwmac_set_mac_loopback(void __iomem *ioaddr, bool enable)
+{
+	u32 value = readl(ioaddr + EMAC_BASIC_CTL0);
+
+	if (enable)
+		value |= EMAC_LOOPBACK;
+	else
+		value &= ~EMAC_LOOPBACK;
+
+	writel(value, ioaddr + EMAC_BASIC_CTL0);
+}
+
+static const struct stmmac_ops sun8i_dwmac_ops = {
+	.core_init = sun8i_dwmac_core_init,
+	.set_mac = sun8i_dwmac_set_mac,
+	.dump_regs = sun8i_dwmac_dump_mac_regs,
+	.rx_ipc = sun8i_dwmac_rx_ipc_enable,
+	.set_filter = sun8i_dwmac_set_filter,
+	.flow_ctrl = sun8i_dwmac_flow_ctrl,
+	.set_umac_addr = sun8i_dwmac_set_umac_addr,
+	.get_umac_addr = sun8i_dwmac_get_umac_addr,
+	.set_mac_loopback = sun8i_dwmac_set_mac_loopback,
+};
+
+static struct mac_device_info *sun8i_dwmac_setup(void *ppriv)
+{
+	struct mac_device_info *mac;
+	struct stmmac_priv *priv = ppriv;
+	int ret;
+
+	mac = devm_kzalloc(priv->device, sizeof(*mac), GFP_KERNEL);
+	if (!mac)
+		return NULL;
+
+	ret = sun8i_dwmac_set_syscon(priv);
+	if (ret)
+		return NULL;
+
+	mac->pcsr = priv->ioaddr;
+	mac->mac = &sun8i_dwmac_ops;
+	mac->dma = &sun8i_dwmac_dma_ops;
+
+	priv->dev->priv_flags |= IFF_UNICAST_FLT;
+
+	/* The loopback bit seems to be re-set when link change
+	 * Simply mask it each time
+	 * Speed 10/100/1000 are set in BIT(2)/BIT(3)
+	 */
+	mac->link.speed_mask = GENMASK(3, 2) | EMAC_LOOPBACK;
+	mac->link.speed10 = EMAC_SPEED_10;
+	mac->link.speed100 = EMAC_SPEED_100;
+	mac->link.speed1000 = EMAC_SPEED_1000;
+	mac->link.duplex = EMAC_DUPLEX_FULL;
+	mac->mii.addr = EMAC_MDIO_CMD;
+	mac->mii.data = EMAC_MDIO_DATA;
+	mac->mii.reg_shift = 4;
+	mac->mii.reg_mask = GENMASK(8, 4);
+	mac->mii.addr_shift = 12;
+	mac->mii.addr_mask = GENMASK(16, 12);
+	mac->mii.clk_csr_shift = 20;
+	mac->mii.clk_csr_mask = GENMASK(22, 20);
+	mac->unicast_filter_entries = 8;
+
+	/* Synopsys Id is not available */
+	priv->synopsys_id = 0;
+
+	return mac;
+}
+
+static struct regmap *sun8i_dwmac_get_syscon_from_dev(struct device_node *node)
+{
+	struct device_node *syscon_node;
+	struct platform_device *syscon_pdev;
+	struct regmap *regmap = NULL;
+
+	syscon_node = of_parse_phandle(node, "syscon", 0);
+	if (!syscon_node)
+		return ERR_PTR(-ENODEV);
+
+	syscon_pdev = of_find_device_by_node(syscon_node);
+	if (!syscon_pdev) {
+		/* platform device might not be probed yet */
+		regmap = ERR_PTR(-EPROBE_DEFER);
+		goto out_put_node;
+	}
+
+	/* If no regmap is found then the other device driver is at fault */
+	regmap = dev_get_regmap(&syscon_pdev->dev, NULL);
+	if (!regmap)
+		regmap = ERR_PTR(-EINVAL);
+
+	platform_device_put(syscon_pdev);
+out_put_node:
+	of_node_put(syscon_node);
+	return regmap;
+}
+
+static int sun8i_dwmac_probe(struct platform_device *pdev)
+{
+	struct plat_stmmacenet_data *plat_dat;
+	struct stmmac_resources stmmac_res;
+	struct sunxi_priv_data *gmac;
+	struct device *dev = &pdev->dev;
+	phy_interface_t interface;
+	int ret;
+	struct stmmac_priv *priv;
+	struct rtnet_device *ndev;
+	struct regmap *regmap;
+
+	ret = stmmac_get_platform_resources(pdev, &stmmac_res);
+	if (ret)
+		return ret;
+
+	plat_dat = stmmac_probe_config_dt(pdev, &stmmac_res.mac);
+	if (IS_ERR(plat_dat))
+		return PTR_ERR(plat_dat);
+
+	gmac = devm_kzalloc(dev, sizeof(*gmac), GFP_KERNEL);
+	if (!gmac)
+		return -ENOMEM;
+
+	gmac->variant = of_device_get_match_data(&pdev->dev);
+	if (!gmac->variant) {
+		dev_err(&pdev->dev, "Missing dwmac-sun8i variant\n");
+		return -EINVAL;
+	}
+
+	gmac->tx_clk = devm_clk_get(dev, "stmmaceth");
+	if (IS_ERR(gmac->tx_clk)) {
+		dev_err(dev, "Could not get TX clock\n");
+		return PTR_ERR(gmac->tx_clk);
+	}
+
+	/* Optional regulator for PHY */
+	gmac->regulator = devm_regulator_get_optional(dev, "phy");
+	if (IS_ERR(gmac->regulator)) {
+		if (PTR_ERR(gmac->regulator) == -EPROBE_DEFER)
+			return -EPROBE_DEFER;
+		dev_info(dev, "No regulator found\n");
+		gmac->regulator = NULL;
+	}
+
+	/* The "GMAC clock control" register might be located in the
+	 * CCU address range (on the R40), or the system control address
+	 * range (on most other sun8i and later SoCs).
+	 *
+	 * The former controls most if not all clocks in the SoC. The
+	 * latter has an SoC identification register, and on some SoCs,
+	 * controls to map device specific SRAM to either the intended
+	 * peripheral, or the CPU address space.
+	 *
+	 * In either case, there should be a coordinated and restricted
+	 * method of accessing the register needed here. This is done by
+	 * having the device export a custom regmap, instead of a generic
+	 * syscon, which grants all access to all registers.
+	 *
+	 * To support old device trees, we fall back to using the syscon
+	 * interface if possible.
+	 */
+	regmap = sun8i_dwmac_get_syscon_from_dev(pdev->dev.of_node);
+	if (IS_ERR(regmap))
+		regmap = syscon_regmap_lookup_by_phandle(pdev->dev.of_node,
+							 "syscon");
+	if (IS_ERR(regmap)) {
+		ret = PTR_ERR(regmap);
+		dev_err(&pdev->dev, "Unable to map syscon: %d\n", ret);
+		return ret;
+	}
+
+	gmac->regmap_field = devm_regmap_field_alloc(dev, regmap,
+						     *gmac->variant->syscon_field);
+	if (IS_ERR(gmac->regmap_field)) {
+		ret = PTR_ERR(gmac->regmap_field);
+		dev_err(dev, "Unable to map syscon register: %d\n", ret);
+		return ret;
+	}
+
+	ret = of_get_phy_mode(dev->of_node, &interface);
+	if (ret)
+		return -EINVAL;
+	plat_dat->interface = interface;
+
+	/* platform data specifying hardware features and callbacks.
+	 * hardware features were copied from Allwinner drivers.
+	 */
+	plat_dat->rx_coe = STMMAC_RX_COE_TYPE2;
+	plat_dat->tx_coe = 1;
+	plat_dat->has_sun8i = true;
+	plat_dat->bsp_priv = gmac;
+	plat_dat->init = sun8i_dwmac_init;
+	plat_dat->exit = sun8i_dwmac_exit;
+	plat_dat->setup = sun8i_dwmac_setup;
+
+	ret = sun8i_dwmac_init(pdev, plat_dat->bsp_priv);
+	if (ret)
+		return ret;
+
+	ret = stmmac_dvr_probe(&pdev->dev, plat_dat, &stmmac_res);
+	if (ret)
+		goto dwmac_exit;
+
+	ndev = dev_get_drvdata(&pdev->dev);
+	priv = rtnetdev_priv(ndev);
+	/* The mux must be registered after parent MDIO
+	 * so after stmmac_dvr_probe()
+	 */
+	if (gmac->variant->soc_has_internal_phy) {
+		ret = get_ephy_nodes(priv);
+		if (ret)
+			goto dwmac_exit;
+		ret = sun8i_dwmac_register_mdio_mux(priv);
+		if (ret) {
+			dev_err(&pdev->dev, "Failed to register mux\n");
+			goto dwmac_mux;
+		}
+	} else {
+		ret = sun8i_dwmac_reset(priv);
+		if (ret)
+			goto dwmac_exit;
+	}
+
+	return ret;
+dwmac_mux:
+	sun8i_dwmac_unset_syscon(gmac);
+dwmac_exit:
+	stmmac_pltfr_remove(pdev);
+return ret;
+}
+
+static const struct of_device_id sun8i_dwmac_match[] = {
+	{ .compatible = "allwinner,sun8i-h3-emac",
+		.data = &emac_variant_h3 },
+	{ .compatible = "allwinner,sun8i-v3s-emac",
+		.data = &emac_variant_v3s },
+	{ .compatible = "allwinner,sun8i-a83t-emac",
+		.data = &emac_variant_a83t },
+	{ .compatible = "allwinner,sun8i-r40-gmac",
+		.data = &emac_variant_r40 },
+	{ .compatible = "allwinner,sun50i-a64-emac",
+		.data = &emac_variant_a64 },
+	{ .compatible = "allwinner,sun50i-h6-emac",
+		.data = &emac_variant_h6 },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, sun8i_dwmac_match);
+
+static struct platform_driver sun8i_dwmac_driver = {
+	.probe  = sun8i_dwmac_probe,
+	.remove = stmmac_pltfr_remove,
+	.driver = {
+		.name           = "dwmac-sun8i",
+		.pm 	= NULL,
+#if 0
+		.pm		= &stmmac_pltfr_pm_ops,
+#endif
+		.of_match_table = sun8i_dwmac_match,
+	},
+};
+module_platform_driver(sun8i_dwmac_driver);
+
+MODULE_AUTHOR("Corentin Labbe <clabbe.montjoie@gmail.com>");
+MODULE_DESCRIPTION("Allwinner sun8i DWMAC specific glue layer");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwxgmac2.h b/net/rtnet/drivers/orange-pi-one/dwxgmac2.h
--- a/net/rtnet/drivers/orange-pi-one/dwxgmac2.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwxgmac2.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,468 @@
+/* SPDX-License-Identifier: (GPL-2.0 OR MIT) */
+/*
+ * Copyright (c) 2018 Synopsys, Inc. and/or its affiliates.
+ * stmmac XGMAC definitions.
+ */
+
+#ifndef __STMMAC_DWXGMAC2_H__
+#define __STMMAC_DWXGMAC2_H__
+
+#include "common.h"
+
+/* Misc */
+#define XGMAC_JUMBO_LEN			16368
+
+/* MAC Registers */
+#define XGMAC_TX_CONFIG			0x00000000
+#define XGMAC_CONFIG_SS_OFF		29
+#define XGMAC_CONFIG_SS_MASK		GENMASK(31, 29)
+#define XGMAC_CONFIG_SS_10000		(0x0 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SS_2500_GMII	(0x2 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SS_1000_GMII	(0x3 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SS_100_MII		(0x4 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SS_5000		(0x5 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SS_2500		(0x6 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SS_10_MII		(0x7 << XGMAC_CONFIG_SS_OFF)
+#define XGMAC_CONFIG_SARC		GENMASK(22, 20)
+#define XGMAC_CONFIG_SARC_SHIFT		20
+#define XGMAC_CONFIG_JD			BIT(16)
+#define XGMAC_CONFIG_TE			BIT(0)
+#define XGMAC_CORE_INIT_TX		(XGMAC_CONFIG_JD)
+#define XGMAC_RX_CONFIG			0x00000004
+#define XGMAC_CONFIG_ARPEN		BIT(31)
+#define XGMAC_CONFIG_GPSL		GENMASK(29, 16)
+#define XGMAC_CONFIG_GPSL_SHIFT		16
+#define XGMAC_CONFIG_HDSMS		GENMASK(14, 12)
+#define XGMAC_CONFIG_HDSMS_SHIFT	12
+#define XGMAC_CONFIG_HDSMS_256		(0x2 << XGMAC_CONFIG_HDSMS_SHIFT)
+#define XGMAC_CONFIG_S2KP		BIT(11)
+#define XGMAC_CONFIG_LM			BIT(10)
+#define XGMAC_CONFIG_IPC		BIT(9)
+#define XGMAC_CONFIG_JE			BIT(8)
+#define XGMAC_CONFIG_WD			BIT(7)
+#define XGMAC_CONFIG_GPSLCE		BIT(6)
+#define XGMAC_CONFIG_CST		BIT(2)
+#define XGMAC_CONFIG_ACS		BIT(1)
+#define XGMAC_CONFIG_RE			BIT(0)
+#define XGMAC_CORE_INIT_RX		(XGMAC_CONFIG_GPSLCE | XGMAC_CONFIG_WD | \
+					 (XGMAC_JUMBO_LEN << XGMAC_CONFIG_GPSL_SHIFT))
+#define XGMAC_PACKET_FILTER		0x00000008
+#define XGMAC_FILTER_RA			BIT(31)
+#define XGMAC_FILTER_IPFE		BIT(20)
+#define XGMAC_FILTER_VTFE		BIT(16)
+#define XGMAC_FILTER_HPF		BIT(10)
+#define XGMAC_FILTER_PCF		BIT(7)
+#define XGMAC_FILTER_PM			BIT(4)
+#define XGMAC_FILTER_HMC		BIT(2)
+#define XGMAC_FILTER_PR			BIT(0)
+#define XGMAC_HASH_TABLE(x)		(0x00000010 + (x) * 4)
+#define XGMAC_MAX_HASH_TABLE		8
+#define XGMAC_VLAN_TAG			0x00000050
+#define XGMAC_VLAN_EDVLP		BIT(26)
+#define XGMAC_VLAN_VTHM			BIT(25)
+#define XGMAC_VLAN_DOVLTC		BIT(20)
+#define XGMAC_VLAN_ESVL			BIT(18)
+#define XGMAC_VLAN_ETV			BIT(16)
+#define XGMAC_VLAN_VID			GENMASK(15, 0)
+#define XGMAC_VLAN_HASH_TABLE		0x00000058
+#define XGMAC_VLAN_INCL			0x00000060
+#define XGMAC_VLAN_VLTI			BIT(20)
+#define XGMAC_VLAN_CSVL			BIT(19)
+#define XGMAC_VLAN_VLC			GENMASK(17, 16)
+#define XGMAC_VLAN_VLC_SHIFT		16
+#define XGMAC_RXQ_CTRL0			0x000000a0
+#define XGMAC_RXQEN(x)			GENMASK((x) * 2 + 1, (x) * 2)
+#define XGMAC_RXQEN_SHIFT(x)		((x) * 2)
+#define XGMAC_RXQ_CTRL1			0x000000a4
+#define XGMAC_RQ			GENMASK(7, 4)
+#define XGMAC_RQ_SHIFT			4
+#define XGMAC_RXQ_CTRL2			0x000000a8
+#define XGMAC_RXQ_CTRL3			0x000000ac
+#define XGMAC_PSRQ(x)			GENMASK((x) * 8 + 7, (x) * 8)
+#define XGMAC_PSRQ_SHIFT(x)		((x) * 8)
+#define XGMAC_INT_STATUS		0x000000b0
+#define XGMAC_LPIIS			BIT(5)
+#define XGMAC_PMTIS			BIT(4)
+#define XGMAC_INT_EN			0x000000b4
+#define XGMAC_TSIE			BIT(12)
+#define XGMAC_LPIIE			BIT(5)
+#define XGMAC_PMTIE			BIT(4)
+#define XGMAC_INT_DEFAULT_EN		(XGMAC_LPIIE | XGMAC_PMTIE)
+#define XGMAC_Qx_TX_FLOW_CTRL(x)	(0x00000070 + (x) * 4)
+#define XGMAC_PT			GENMASK(31, 16)
+#define XGMAC_PT_SHIFT			16
+#define XGMAC_TFE			BIT(1)
+#define XGMAC_RX_FLOW_CTRL		0x00000090
+#define XGMAC_RFE			BIT(0)
+#define XGMAC_PMT			0x000000c0
+#define XGMAC_GLBLUCAST			BIT(9)
+#define XGMAC_RWKPKTEN			BIT(2)
+#define XGMAC_MGKPKTEN			BIT(1)
+#define XGMAC_PWRDWN			BIT(0)
+#define XGMAC_LPI_CTRL			0x000000d0
+#define XGMAC_TXCGE			BIT(21)
+#define XGMAC_LPITXA			BIT(19)
+#define XGMAC_PLS			BIT(17)
+#define XGMAC_LPITXEN			BIT(16)
+#define XGMAC_RLPIEX			BIT(3)
+#define XGMAC_RLPIEN			BIT(2)
+#define XGMAC_TLPIEX			BIT(1)
+#define XGMAC_TLPIEN			BIT(0)
+#define XGMAC_LPI_TIMER_CTRL		0x000000d4
+#define XGMAC_HW_FEATURE0		0x0000011c
+#define XGMAC_HWFEAT_SAVLANINS		BIT(27)
+#define XGMAC_HWFEAT_RXCOESEL		BIT(16)
+#define XGMAC_HWFEAT_TXCOESEL		BIT(14)
+#define XGMAC_HWFEAT_EEESEL		BIT(13)
+#define XGMAC_HWFEAT_TSSEL		BIT(12)
+#define XGMAC_HWFEAT_AVSEL		BIT(11)
+#define XGMAC_HWFEAT_RAVSEL		BIT(10)
+#define XGMAC_HWFEAT_ARPOFFSEL		BIT(9)
+#define XGMAC_HWFEAT_MMCSEL		BIT(8)
+#define XGMAC_HWFEAT_MGKSEL		BIT(7)
+#define XGMAC_HWFEAT_RWKSEL		BIT(6)
+#define XGMAC_HWFEAT_VLHASH		BIT(4)
+#define XGMAC_HWFEAT_GMIISEL		BIT(1)
+#define XGMAC_HW_FEATURE1		0x00000120
+#define XGMAC_HWFEAT_L3L4FNUM		GENMASK(30, 27)
+#define XGMAC_HWFEAT_HASHTBLSZ		GENMASK(25, 24)
+#define XGMAC_HWFEAT_RSSEN		BIT(20)
+#define XGMAC_HWFEAT_TSOEN		BIT(18)
+#define XGMAC_HWFEAT_SPHEN		BIT(17)
+#define XGMAC_HWFEAT_ADDR64		GENMASK(15, 14)
+#define XGMAC_HWFEAT_TXFIFOSIZE		GENMASK(10, 6)
+#define XGMAC_HWFEAT_RXFIFOSIZE		GENMASK(4, 0)
+#define XGMAC_HW_FEATURE2		0x00000124
+#define XGMAC_HWFEAT_PPSOUTNUM		GENMASK(26, 24)
+#define XGMAC_HWFEAT_TXCHCNT		GENMASK(21, 18)
+#define XGMAC_HWFEAT_RXCHCNT		GENMASK(15, 12)
+#define XGMAC_HWFEAT_TXQCNT		GENMASK(9, 6)
+#define XGMAC_HWFEAT_RXQCNT		GENMASK(3, 0)
+#define XGMAC_HW_FEATURE3		0x00000128
+#define XGMAC_HWFEAT_TBSSEL		BIT(27)
+#define XGMAC_HWFEAT_FPESEL		BIT(26)
+#define XGMAC_HWFEAT_ESTWID		GENMASK(24, 23)
+#define XGMAC_HWFEAT_ESTDEP		GENMASK(22, 20)
+#define XGMAC_HWFEAT_ESTSEL		BIT(19)
+#define XGMAC_HWFEAT_ASP		GENMASK(15, 14)
+#define XGMAC_HWFEAT_DVLAN		BIT(13)
+#define XGMAC_HWFEAT_FRPES		GENMASK(12, 11)
+#define XGMAC_HWFEAT_FRPPB		GENMASK(10, 9)
+#define XGMAC_HWFEAT_FRPSEL		BIT(3)
+#define XGMAC_MAC_DPP_FSM_INT_STATUS	0x00000150
+#define XGMAC_MAC_FSM_CONTROL		0x00000158
+#define XGMAC_PRTYEN			BIT(1)
+#define XGMAC_TMOUTEN			BIT(0)
+#define XGMAC_MDIO_ADDR			0x00000200
+#define XGMAC_MDIO_DATA			0x00000204
+#define XGMAC_MDIO_C22P			0x00000220
+#define XGMAC_FPE_CTRL_STS		0x00000280
+#define XGMAC_EFPE			BIT(0)
+#define XGMAC_ADDRx_HIGH(x)		(0x00000300 + (x) * 0x8)
+#define XGMAC_ADDR_MAX			32
+#define XGMAC_AE			BIT(31)
+#define XGMAC_DCS			GENMASK(19, 16)
+#define XGMAC_DCS_SHIFT			16
+#define XGMAC_ADDRx_LOW(x)		(0x00000304 + (x) * 0x8)
+#define XGMAC_L3L4_ADDR_CTRL		0x00000c00
+#define XGMAC_IDDR			GENMASK(15, 8)
+#define XGMAC_IDDR_SHIFT		8
+#define XGMAC_IDDR_FNUM			4
+#define XGMAC_TT			BIT(1)
+#define XGMAC_XB			BIT(0)
+#define XGMAC_L3L4_DATA			0x00000c04
+#define XGMAC_L3L4_CTRL			0x0
+#define XGMAC_L4DPIM0			BIT(21)
+#define XGMAC_L4DPM0			BIT(20)
+#define XGMAC_L4SPIM0			BIT(19)
+#define XGMAC_L4SPM0			BIT(18)
+#define XGMAC_L4PEN0			BIT(16)
+#define XGMAC_L3HDBM0			GENMASK(15, 11)
+#define XGMAC_L3HSBM0			GENMASK(10, 6)
+#define XGMAC_L3DAIM0			BIT(5)
+#define XGMAC_L3DAM0			BIT(4)
+#define XGMAC_L3SAIM0			BIT(3)
+#define XGMAC_L3SAM0			BIT(2)
+#define XGMAC_L3PEN0			BIT(0)
+#define XGMAC_L4_ADDR			0x1
+#define XGMAC_L4DP0			GENMASK(31, 16)
+#define XGMAC_L4DP0_SHIFT		16
+#define XGMAC_L4SP0			GENMASK(15, 0)
+#define XGMAC_L3_ADDR0			0x4
+#define XGMAC_L3_ADDR1			0x5
+#define XGMAC_L3_ADDR2			0x6
+#define XMGAC_L3_ADDR3			0x7
+#define XGMAC_ARP_ADDR			0x00000c10
+#define XGMAC_RSS_CTRL			0x00000c80
+#define XGMAC_UDP4TE			BIT(3)
+#define XGMAC_TCP4TE			BIT(2)
+#define XGMAC_IP2TE			BIT(1)
+#define XGMAC_RSSE			BIT(0)
+#define XGMAC_RSS_ADDR			0x00000c88
+#define XGMAC_RSSIA_SHIFT		8
+#define XGMAC_ADDRT			BIT(2)
+#define XGMAC_CT			BIT(1)
+#define XGMAC_OB			BIT(0)
+#define XGMAC_RSS_DATA			0x00000c8c
+#define XGMAC_TIMESTAMP_STATUS		0x00000d20
+#define XGMAC_TXTSC			BIT(15)
+#define XGMAC_TXTIMESTAMP_NSEC		0x00000d30
+#define XGMAC_TXTSSTSLO			GENMASK(30, 0)
+#define XGMAC_TXTIMESTAMP_SEC		0x00000d34
+#define XGMAC_PPS_CONTROL		0x00000d70
+#define XGMAC_PPS_MAXIDX(x)		((((x) + 1) * 8) - 1)
+#define XGMAC_PPS_MINIDX(x)		((x) * 8)
+#define XGMAC_PPSx_MASK(x)		\
+	GENMASK(XGMAC_PPS_MAXIDX(x), XGMAC_PPS_MINIDX(x))
+#define XGMAC_TRGTMODSELx(x, val)	\
+	GENMASK(XGMAC_PPS_MAXIDX(x) - 1, XGMAC_PPS_MAXIDX(x) - 2) & \
+	((val) << (XGMAC_PPS_MAXIDX(x) - 2))
+#define XGMAC_PPSCMDx(x, val)		\
+	GENMASK(XGMAC_PPS_MINIDX(x) + 3, XGMAC_PPS_MINIDX(x)) & \
+	((val) << XGMAC_PPS_MINIDX(x))
+#define XGMAC_PPSCMD_START		0x2
+#define XGMAC_PPSCMD_STOP		0x5
+#define XGMAC_PPSEN0			BIT(4)
+#define XGMAC_PPSx_TARGET_TIME_SEC(x)	(0x00000d80 + (x) * 0x10)
+#define XGMAC_PPSx_TARGET_TIME_NSEC(x)	(0x00000d84 + (x) * 0x10)
+#define XGMAC_TRGTBUSY0			BIT(31)
+#define XGMAC_PPSx_INTERVAL(x)		(0x00000d88 + (x) * 0x10)
+#define XGMAC_PPSx_WIDTH(x)		(0x00000d8c + (x) * 0x10)
+
+/* MTL Registers */
+#define XGMAC_MTL_OPMODE		0x00001000
+#define XGMAC_FRPE			BIT(15)
+#define XGMAC_ETSALG			GENMASK(6, 5)
+#define XGMAC_WRR			(0x0 << 5)
+#define XGMAC_WFQ			(0x1 << 5)
+#define XGMAC_DWRR			(0x2 << 5)
+#define XGMAC_RAA			BIT(2)
+#define XGMAC_MTL_INT_STATUS		0x00001020
+#define XGMAC_MTL_RXQ_DMA_MAP0		0x00001030
+#define XGMAC_MTL_RXQ_DMA_MAP1		0x00001034
+#define XGMAC_QxMDMACH(x)		GENMASK((x) * 8 + 7, (x) * 8)
+#define XGMAC_QxMDMACH_SHIFT(x)		((x) * 8)
+#define XGMAC_QDDMACH			BIT(7)
+#define XGMAC_TC_PRTY_MAP0		0x00001040
+#define XGMAC_TC_PRTY_MAP1		0x00001044
+#define XGMAC_PSTC(x)			GENMASK((x) * 8 + 7, (x) * 8)
+#define XGMAC_PSTC_SHIFT(x)		((x) * 8)
+#define XGMAC_MTL_EST_CONTROL		0x00001050
+#define XGMAC_PTOV			GENMASK(31, 23)
+#define XGMAC_PTOV_SHIFT		23
+#define XGMAC_SSWL			BIT(1)
+#define XGMAC_EEST			BIT(0)
+#define XGMAC_MTL_EST_GCL_CONTROL	0x00001080
+#define XGMAC_BTR_LOW			0x0
+#define XGMAC_BTR_HIGH			0x1
+#define XGMAC_CTR_LOW			0x2
+#define XGMAC_CTR_HIGH			0x3
+#define XGMAC_TER			0x4
+#define XGMAC_LLR			0x5
+#define XGMAC_ADDR_SHIFT		8
+#define XGMAC_GCRR			BIT(2)
+#define XGMAC_SRWO			BIT(0)
+#define XGMAC_MTL_EST_GCL_DATA		0x00001084
+#define XGMAC_MTL_RXP_CONTROL_STATUS	0x000010a0
+#define XGMAC_RXPI			BIT(31)
+#define XGMAC_NPE			GENMASK(23, 16)
+#define XGMAC_NVE			GENMASK(7, 0)
+#define XGMAC_MTL_RXP_IACC_CTRL_ST	0x000010b0
+#define XGMAC_STARTBUSY			BIT(31)
+#define XGMAC_WRRDN			BIT(16)
+#define XGMAC_ADDR			GENMASK(9, 0)
+#define XGMAC_MTL_RXP_IACC_DATA		0x000010b4
+#define XGMAC_MTL_ECC_CONTROL		0x000010c0
+#define XGMAC_MTL_SAFETY_INT_STATUS	0x000010c4
+#define XGMAC_MEUIS			BIT(1)
+#define XGMAC_MECIS			BIT(0)
+#define XGMAC_MTL_ECC_INT_ENABLE	0x000010c8
+#define XGMAC_RPCEIE			BIT(12)
+#define XGMAC_ECEIE			BIT(8)
+#define XGMAC_RXCEIE			BIT(4)
+#define XGMAC_TXCEIE			BIT(0)
+#define XGMAC_MTL_ECC_INT_STATUS	0x000010cc
+#define XGMAC_MTL_TXQ_OPMODE(x)		(0x00001100 + (0x80 * (x)))
+#define XGMAC_TQS			GENMASK(25, 16)
+#define XGMAC_TQS_SHIFT			16
+#define XGMAC_Q2TCMAP			GENMASK(10, 8)
+#define XGMAC_Q2TCMAP_SHIFT		8
+#define XGMAC_TTC			GENMASK(6, 4)
+#define XGMAC_TTC_SHIFT			4
+#define XGMAC_TXQEN			GENMASK(3, 2)
+#define XGMAC_TXQEN_SHIFT		2
+#define XGMAC_TSF			BIT(1)
+#define XGMAC_MTL_TCx_ETS_CONTROL(x)	(0x00001110 + (0x80 * (x)))
+#define XGMAC_MTL_TCx_QUANTUM_WEIGHT(x)	(0x00001118 + (0x80 * (x)))
+#define XGMAC_MTL_TCx_SENDSLOPE(x)	(0x0000111c + (0x80 * (x)))
+#define XGMAC_MTL_TCx_HICREDIT(x)	(0x00001120 + (0x80 * (x)))
+#define XGMAC_MTL_TCx_LOCREDIT(x)	(0x00001124 + (0x80 * (x)))
+#define XGMAC_CC			BIT(3)
+#define XGMAC_TSA			GENMASK(1, 0)
+#define XGMAC_SP			(0x0 << 0)
+#define XGMAC_CBS			(0x1 << 0)
+#define XGMAC_ETS			(0x2 << 0)
+#define XGMAC_MTL_RXQ_OPMODE(x)		(0x00001140 + (0x80 * (x)))
+#define XGMAC_RQS			GENMASK(25, 16)
+#define XGMAC_RQS_SHIFT			16
+#define XGMAC_EHFC			BIT(7)
+#define XGMAC_RSF			BIT(5)
+#define XGMAC_RTC			GENMASK(1, 0)
+#define XGMAC_RTC_SHIFT			0
+#define XGMAC_MTL_RXQ_FLOW_CONTROL(x)	(0x00001150 + (0x80 * (x)))
+#define XGMAC_RFD			GENMASK(31, 17)
+#define XGMAC_RFD_SHIFT			17
+#define XGMAC_RFA			GENMASK(15, 1)
+#define XGMAC_RFA_SHIFT			1
+#define XGMAC_MTL_QINTEN(x)		(0x00001170 + (0x80 * (x)))
+#define XGMAC_RXOIE			BIT(16)
+#define XGMAC_MTL_QINT_STATUS(x)	(0x00001174 + (0x80 * (x)))
+#define XGMAC_RXOVFIS			BIT(16)
+#define XGMAC_ABPSIS			BIT(1)
+#define XGMAC_TXUNFIS			BIT(0)
+#define XGMAC_MAC_REGSIZE		(XGMAC_MTL_QINT_STATUS(15) / 4)
+
+/* DMA Registers */
+#define XGMAC_DMA_MODE			0x00003000
+#define XGMAC_SWR			BIT(0)
+#define XGMAC_DMA_SYSBUS_MODE		0x00003004
+#define XGMAC_WR_OSR_LMT		GENMASK(29, 24)
+#define XGMAC_WR_OSR_LMT_SHIFT		24
+#define XGMAC_RD_OSR_LMT		GENMASK(21, 16)
+#define XGMAC_RD_OSR_LMT_SHIFT		16
+#define XGMAC_EN_LPI			BIT(15)
+#define XGMAC_LPI_XIT_PKT		BIT(14)
+#define XGMAC_AAL			BIT(12)
+#define XGMAC_EAME			BIT(11)
+#define XGMAC_BLEN			GENMASK(7, 1)
+#define XGMAC_BLEN256			BIT(7)
+#define XGMAC_BLEN128			BIT(6)
+#define XGMAC_BLEN64			BIT(5)
+#define XGMAC_BLEN32			BIT(4)
+#define XGMAC_BLEN16			BIT(3)
+#define XGMAC_BLEN8			BIT(2)
+#define XGMAC_BLEN4			BIT(1)
+#define XGMAC_UNDEF			BIT(0)
+#define XGMAC_TX_EDMA_CTRL		0x00003040
+#define XGMAC_TDPS			GENMASK(29, 0)
+#define XGMAC_RX_EDMA_CTRL		0x00003044
+#define XGMAC_RDPS			GENMASK(29, 0)
+#define XGMAC_DMA_TBS_CTRL0		0x00003054
+#define XGMAC_DMA_TBS_CTRL1		0x00003058
+#define XGMAC_DMA_TBS_CTRL2		0x0000305c
+#define XGMAC_DMA_TBS_CTRL3		0x00003060
+#define XGMAC_FTOS			GENMASK(31, 8)
+#define XGMAC_FTOV			BIT(0)
+#define XGMAC_DEF_FTOS			(XGMAC_FTOS | XGMAC_FTOV)
+#define XGMAC_DMA_SAFETY_INT_STATUS	0x00003064
+#define XGMAC_MCSIS			BIT(31)
+#define XGMAC_MSUIS			BIT(29)
+#define XGMAC_MSCIS			BIT(28)
+#define XGMAC_DEUIS			BIT(1)
+#define XGMAC_DECIS			BIT(0)
+#define XGMAC_DMA_ECC_INT_ENABLE	0x00003068
+#define XGMAC_DCEIE			BIT(1)
+#define XGMAC_TCEIE			BIT(0)
+#define XGMAC_DMA_ECC_INT_STATUS	0x0000306c
+#define XGMAC_DMA_CH_CONTROL(x)		(0x00003100 + (0x80 * (x)))
+#define XGMAC_SPH			BIT(24)
+#define XGMAC_PBLx8			BIT(16)
+#define XGMAC_DMA_CH_TX_CONTROL(x)	(0x00003104 + (0x80 * (x)))
+#define XGMAC_EDSE			BIT(28)
+#define XGMAC_TxPBL			GENMASK(21, 16)
+#define XGMAC_TxPBL_SHIFT		16
+#define XGMAC_TSE			BIT(12)
+#define XGMAC_OSP			BIT(4)
+#define XGMAC_TXST			BIT(0)
+#define XGMAC_DMA_CH_RX_CONTROL(x)	(0x00003108 + (0x80 * (x)))
+#define XGMAC_RxPBL			GENMASK(21, 16)
+#define XGMAC_RxPBL_SHIFT		16
+#define XGMAC_RBSZ			GENMASK(14, 1)
+#define XGMAC_RBSZ_SHIFT		1
+#define XGMAC_RXST			BIT(0)
+#define XGMAC_DMA_CH_TxDESC_HADDR(x)	(0x00003110 + (0x80 * (x)))
+#define XGMAC_DMA_CH_TxDESC_LADDR(x)	(0x00003114 + (0x80 * (x)))
+#define XGMAC_DMA_CH_RxDESC_HADDR(x)	(0x00003118 + (0x80 * (x)))
+#define XGMAC_DMA_CH_RxDESC_LADDR(x)	(0x0000311c + (0x80 * (x)))
+#define XGMAC_DMA_CH_TxDESC_TAIL_LPTR(x)	(0x00003124 + (0x80 * (x)))
+#define XGMAC_DMA_CH_RxDESC_TAIL_LPTR(x)	(0x0000312c + (0x80 * (x)))
+#define XGMAC_DMA_CH_TxDESC_RING_LEN(x)		(0x00003130 + (0x80 * (x)))
+#define XGMAC_DMA_CH_RxDESC_RING_LEN(x)		(0x00003134 + (0x80 * (x)))
+#define XGMAC_DMA_CH_INT_EN(x)		(0x00003138 + (0x80 * (x)))
+#define XGMAC_NIE			BIT(15)
+#define XGMAC_AIE			BIT(14)
+#define XGMAC_RBUE			BIT(7)
+#define XGMAC_RIE			BIT(6)
+#define XGMAC_TBUE			BIT(2)
+#define XGMAC_TIE			BIT(0)
+#define XGMAC_DMA_INT_DEFAULT_EN	(XGMAC_NIE | XGMAC_AIE | XGMAC_RBUE | \
+					XGMAC_RIE | XGMAC_TIE)
+#define XGMAC_DMA_INT_DEFAULT_RX	(XGMAC_RBUE | XGMAC_RIE)
+#define XGMAC_DMA_INT_DEFAULT_TX	(XGMAC_TIE)
+#define XGMAC_DMA_CH_Rx_WATCHDOG(x)	(0x0000313c + (0x80 * (x)))
+#define XGMAC_RWT			GENMASK(7, 0)
+#define XGMAC_DMA_CH_STATUS(x)		(0x00003160 + (0x80 * (x)))
+#define XGMAC_NIS			BIT(15)
+#define XGMAC_AIS			BIT(14)
+#define XGMAC_FBE			BIT(12)
+#define XGMAC_RBU			BIT(7)
+#define XGMAC_RI			BIT(6)
+#define XGMAC_TBU			BIT(2)
+#define XGMAC_TPS			BIT(1)
+#define XGMAC_TI			BIT(0)
+#define XGMAC_REGSIZE			((0x0000317c + (0x80 * 15)) / 4)
+
+/* Descriptors */
+#define XGMAC_TDES0_LTV			BIT(31)
+#define XGMAC_TDES0_LT			GENMASK(7, 0)
+#define XGMAC_TDES1_LT			GENMASK(31, 8)
+#define XGMAC_TDES2_IVT			GENMASK(31, 16)
+#define XGMAC_TDES2_IVT_SHIFT		16
+#define XGMAC_TDES2_IOC			BIT(31)
+#define XGMAC_TDES2_TTSE		BIT(30)
+#define XGMAC_TDES2_B2L			GENMASK(29, 16)
+#define XGMAC_TDES2_B2L_SHIFT		16
+#define XGMAC_TDES2_VTIR		GENMASK(15, 14)
+#define XGMAC_TDES2_VTIR_SHIFT		14
+#define XGMAC_TDES2_B1L			GENMASK(13, 0)
+#define XGMAC_TDES3_OWN			BIT(31)
+#define XGMAC_TDES3_CTXT		BIT(30)
+#define XGMAC_TDES3_FD			BIT(29)
+#define XGMAC_TDES3_LD			BIT(28)
+#define XGMAC_TDES3_CPC			GENMASK(27, 26)
+#define XGMAC_TDES3_CPC_SHIFT		26
+#define XGMAC_TDES3_TCMSSV		BIT(26)
+#define XGMAC_TDES3_SAIC		GENMASK(25, 23)
+#define XGMAC_TDES3_SAIC_SHIFT		23
+#define XGMAC_TDES3_TBSV		BIT(24)
+#define XGMAC_TDES3_THL			GENMASK(22, 19)
+#define XGMAC_TDES3_THL_SHIFT		19
+#define XGMAC_TDES3_IVTIR		GENMASK(19, 18)
+#define XGMAC_TDES3_IVTIR_SHIFT		18
+#define XGMAC_TDES3_TSE			BIT(18)
+#define XGMAC_TDES3_IVLTV		BIT(17)
+#define XGMAC_TDES3_CIC			GENMASK(17, 16)
+#define XGMAC_TDES3_CIC_SHIFT		16
+#define XGMAC_TDES3_TPL			GENMASK(17, 0)
+#define XGMAC_TDES3_VLTV		BIT(16)
+#define XGMAC_TDES3_VT			GENMASK(15, 0)
+#define XGMAC_TDES3_FL			GENMASK(14, 0)
+#define XGMAC_RDES2_HL			GENMASK(9, 0)
+#define XGMAC_RDES3_OWN			BIT(31)
+#define XGMAC_RDES3_CTXT		BIT(30)
+#define XGMAC_RDES3_IOC			BIT(30)
+#define XGMAC_RDES3_LD			BIT(28)
+#define XGMAC_RDES3_CDA			BIT(27)
+#define XGMAC_RDES3_RSV			BIT(26)
+#define XGMAC_RDES3_L34T		GENMASK(23, 20)
+#define XGMAC_RDES3_L34T_SHIFT		20
+#define XGMAC_L34T_IP4TCP		0x1
+#define XGMAC_L34T_IP4UDP		0x2
+#define XGMAC_L34T_IP6TCP		0x9
+#define XGMAC_L34T_IP6UDP		0xA
+#define XGMAC_RDES3_ES			BIT(15)
+#define XGMAC_RDES3_PL			GENMASK(13, 0)
+#define XGMAC_RDES3_TSD			BIT(6)
+#define XGMAC_RDES3_TSA			BIT(4)
+
+#endif /* __STMMAC_DWXGMAC2_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/dwxlgmac2.h b/net/rtnet/drivers/orange-pi-one/dwxlgmac2.h
--- a/net/rtnet/drivers/orange-pi-one/dwxlgmac2.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/dwxlgmac2.h	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,22 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2020 Synopsys, Inc. and/or its affiliates.
+ * Synopsys DesignWare XLGMAC definitions.
+ */
+
+#ifndef __STMMAC_DWXLGMAC2_H__
+#define __STMMAC_DWXLGMAC2_H__
+
+/* MAC Registers */
+#define XLGMAC_CONFIG_SS		GENMASK(30, 28)
+#define XLGMAC_CONFIG_SS_SHIFT		28
+#define XLGMAC_CONFIG_SS_40G		(0x0 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_CONFIG_SS_25G		(0x1 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_CONFIG_SS_50G		(0x2 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_CONFIG_SS_100G		(0x3 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_CONFIG_SS_10G		(0x4 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_CONFIG_SS_2500		(0x6 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_CONFIG_SS_1000		(0x7 << XLGMAC_CONFIG_SS_SHIFT)
+#define XLGMAC_RXQ_ENABLE_CTRL0		0x00000140
+
+#endif /* __STMMAC_DWXLGMAC2_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/enh_desc.c b/net/rtnet/drivers/orange-pi-one/enh_desc.c
--- a/net/rtnet/drivers/orange-pi-one/enh_desc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/enh_desc.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,478 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This contains the functions to handle the enhanced descriptors.
+
+  Copyright (C) 2007-2014  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/stmmac.h>
+#include "common.h"
+#include "descs_com.h"
+
+static int enh_desc_get_tx_status(void *data, struct stmmac_extra_stats *x,
+				  struct dma_desc *p, void __iomem *ioaddr)
+{
+	struct net_device_stats *stats = (struct net_device_stats *)data;
+	unsigned int tdes0 = le32_to_cpu(p->des0);
+	int ret = tx_done;
+
+	/* Get tx owner first */
+	if (unlikely(tdes0 & ETDES0_OWN))
+		return tx_dma_own;
+
+	/* Verify tx error by looking at the last segment. */
+	if (likely(!(tdes0 & ETDES0_LAST_SEGMENT)))
+		return tx_not_ls;
+
+	if (unlikely(tdes0 & ETDES0_ERROR_SUMMARY)) {
+		if (unlikely(tdes0 & ETDES0_JABBER_TIMEOUT))
+			x->tx_jabber++;
+
+		if (unlikely(tdes0 & ETDES0_FRAME_FLUSHED)) {
+			x->tx_frame_flushed++;
+			dwmac_dma_flush_tx_fifo(ioaddr);
+		}
+
+		if (unlikely(tdes0 & ETDES0_LOSS_CARRIER)) {
+			x->tx_losscarrier++;
+			stats->tx_carrier_errors++;
+		}
+		if (unlikely(tdes0 & ETDES0_NO_CARRIER)) {
+			x->tx_carrier++;
+			stats->tx_carrier_errors++;
+		}
+		if (unlikely((tdes0 & ETDES0_LATE_COLLISION) ||
+			     (tdes0 & ETDES0_EXCESSIVE_COLLISIONS)))
+			stats->collisions +=
+				(tdes0 & ETDES0_COLLISION_COUNT_MASK) >> 3;
+
+		if (unlikely(tdes0 & ETDES0_EXCESSIVE_DEFERRAL))
+			x->tx_deferred++;
+
+		if (unlikely(tdes0 & ETDES0_UNDERFLOW_ERROR)) {
+			dwmac_dma_flush_tx_fifo(ioaddr);
+			x->tx_underflow++;
+		}
+
+		if (unlikely(tdes0 & ETDES0_IP_HEADER_ERROR))
+			x->tx_ip_header_error++;
+
+		if (unlikely(tdes0 & ETDES0_PAYLOAD_ERROR)) {
+			x->tx_payload_error++;
+			dwmac_dma_flush_tx_fifo(ioaddr);
+		}
+
+		ret = tx_err;
+	}
+
+	if (unlikely(tdes0 & ETDES0_DEFERRED))
+		x->tx_deferred++;
+
+#ifdef STMMAC_VLAN_TAG_USED
+	if (tdes0 & ETDES0_VLAN_FRAME)
+		x->tx_vlan++;
+#endif
+
+	return ret;
+}
+
+static int enh_desc_get_tx_len(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des1) & ETDES1_BUFFER1_SIZE_MASK);
+}
+
+static int enh_desc_coe_rdes0(int ipc_err, int type, int payload_err)
+{
+	int ret = good_frame;
+	u32 status = (type << 2 | ipc_err << 1 | payload_err) & 0x7;
+
+	/* bits 5 7 0 | Frame status
+	 * ----------------------------------------------------------
+	 *      0 0 0 | IEEE 802.3 Type frame (length < 1536 octects)
+	 *      1 0 0 | IPv4/6 No CSUM errorS.
+	 *      1 0 1 | IPv4/6 CSUM PAYLOAD error
+	 *      1 1 0 | IPv4/6 CSUM IP HR error
+	 *      1 1 1 | IPv4/6 IP PAYLOAD AND HEADER errorS
+	 *      0 0 1 | IPv4/6 unsupported IP PAYLOAD
+	 *      0 1 1 | COE bypassed.. no IPv4/6 frame
+	 *      0 1 0 | Reserved.
+	 */
+	if (status == 0x0)
+		ret = llc_snap;
+	else if (status == 0x4)
+		ret = good_frame;
+	else if (status == 0x5)
+		ret = csum_none;
+	else if (status == 0x6)
+		ret = csum_none;
+	else if (status == 0x7)
+		ret = csum_none;
+	else if (status == 0x1)
+		ret = discard_frame;
+	else if (status == 0x3)
+		ret = discard_frame;
+	return ret;
+}
+
+static void enh_desc_get_ext_status(void *data, struct stmmac_extra_stats *x,
+				    struct dma_extended_desc *p)
+{
+	unsigned int rdes0 = le32_to_cpu(p->basic.des0);
+	unsigned int rdes4 = le32_to_cpu(p->des4);
+
+	if (unlikely(rdes0 & ERDES0_RX_MAC_ADDR)) {
+		int message_type = (rdes4 & ERDES4_MSG_TYPE_MASK) >> 8;
+
+		if (rdes4 & ERDES4_IP_HDR_ERR)
+			x->ip_hdr_err++;
+		if (rdes4 & ERDES4_IP_PAYLOAD_ERR)
+			x->ip_payload_err++;
+		if (rdes4 & ERDES4_IP_CSUM_BYPASSED)
+			x->ip_csum_bypassed++;
+		if (rdes4 & ERDES4_IPV4_PKT_RCVD)
+			x->ipv4_pkt_rcvd++;
+		if (rdes4 & ERDES4_IPV6_PKT_RCVD)
+			x->ipv6_pkt_rcvd++;
+
+		if (message_type == RDES_EXT_NO_PTP)
+			x->no_ptp_rx_msg_type_ext++;
+		else if (message_type == RDES_EXT_SYNC)
+			x->ptp_rx_msg_type_sync++;
+		else if (message_type == RDES_EXT_FOLLOW_UP)
+			x->ptp_rx_msg_type_follow_up++;
+		else if (message_type == RDES_EXT_DELAY_REQ)
+			x->ptp_rx_msg_type_delay_req++;
+		else if (message_type == RDES_EXT_DELAY_RESP)
+			x->ptp_rx_msg_type_delay_resp++;
+		else if (message_type == RDES_EXT_PDELAY_REQ)
+			x->ptp_rx_msg_type_pdelay_req++;
+		else if (message_type == RDES_EXT_PDELAY_RESP)
+			x->ptp_rx_msg_type_pdelay_resp++;
+		else if (message_type == RDES_EXT_PDELAY_FOLLOW_UP)
+			x->ptp_rx_msg_type_pdelay_follow_up++;
+		else if (message_type == RDES_PTP_ANNOUNCE)
+			x->ptp_rx_msg_type_announce++;
+		else if (message_type == RDES_PTP_MANAGEMENT)
+			x->ptp_rx_msg_type_management++;
+		else if (message_type == RDES_PTP_PKT_RESERVED_TYPE)
+			x->ptp_rx_msg_pkt_reserved_type++;
+
+		if (rdes4 & ERDES4_PTP_FRAME_TYPE)
+			x->ptp_frame_type++;
+		if (rdes4 & ERDES4_PTP_VER)
+			x->ptp_ver++;
+		if (rdes4 & ERDES4_TIMESTAMP_DROPPED)
+			x->timestamp_dropped++;
+		if (rdes4 & ERDES4_AV_PKT_RCVD)
+			x->av_pkt_rcvd++;
+		if (rdes4 & ERDES4_AV_TAGGED_PKT_RCVD)
+			x->av_tagged_pkt_rcvd++;
+		if ((rdes4 & ERDES4_VLAN_TAG_PRI_VAL_MASK) >> 18)
+			x->vlan_tag_priority_val++;
+		if (rdes4 & ERDES4_L3_FILTER_MATCH)
+			x->l3_filter_match++;
+		if (rdes4 & ERDES4_L4_FILTER_MATCH)
+			x->l4_filter_match++;
+		if ((rdes4 & ERDES4_L3_L4_FILT_NO_MATCH_MASK) >> 26)
+			x->l3_l4_filter_no_match++;
+	}
+}
+
+static int enh_desc_get_rx_status(void *data, struct stmmac_extra_stats *x,
+				  struct dma_desc *p)
+{
+	struct net_device_stats *stats = (struct net_device_stats *)data;
+	unsigned int rdes0 = le32_to_cpu(p->des0);
+	int ret = good_frame;
+
+	if (unlikely(rdes0 & RDES0_OWN))
+		return dma_own;
+
+	if (unlikely(!(rdes0 & RDES0_LAST_DESCRIPTOR))) {
+		stats->rx_length_errors++;
+		return discard_frame;
+	}
+
+	if (unlikely(rdes0 & RDES0_ERROR_SUMMARY)) {
+		if (unlikely(rdes0 & RDES0_DESCRIPTOR_ERROR)) {
+			x->rx_desc++;
+			stats->rx_length_errors++;
+		}
+		if (unlikely(rdes0 & RDES0_OVERFLOW_ERROR))
+			x->rx_gmac_overflow++;
+
+		if (unlikely(rdes0 & RDES0_IPC_CSUM_ERROR))
+			pr_err("\tIPC Csum Error/Giant frame\n");
+
+		if (unlikely(rdes0 & RDES0_COLLISION))
+			stats->collisions++;
+		if (unlikely(rdes0 & RDES0_RECEIVE_WATCHDOG))
+			x->rx_watchdog++;
+
+		if (unlikely(rdes0 & RDES0_MII_ERROR))	/* GMII */
+			x->rx_mii++;
+
+		if (unlikely(rdes0 & RDES0_CRC_ERROR)) {
+			x->rx_crc_errors++;
+			stats->rx_crc_errors++;
+		}
+		ret = discard_frame;
+	}
+
+	/* After a payload csum error, the ES bit is set.
+	 * It doesn't match with the information reported into the databook.
+	 * At any rate, we need to understand if the CSUM hw computation is ok
+	 * and report this info to the upper layers. */
+	if (likely(ret == good_frame))
+		ret = enh_desc_coe_rdes0(!!(rdes0 & RDES0_IPC_CSUM_ERROR),
+					 !!(rdes0 & RDES0_FRAME_TYPE),
+					 !!(rdes0 & ERDES0_RX_MAC_ADDR));
+
+	if (unlikely(rdes0 & RDES0_DRIBBLING))
+		x->dribbling_bit++;
+
+	if (unlikely(rdes0 & RDES0_SA_FILTER_FAIL)) {
+		x->sa_rx_filter_fail++;
+		ret = discard_frame;
+	}
+	if (unlikely(rdes0 & RDES0_DA_FILTER_FAIL)) {
+		x->da_rx_filter_fail++;
+		ret = discard_frame;
+	}
+	if (unlikely(rdes0 & RDES0_LENGTH_ERROR)) {
+		x->rx_length++;
+		ret = discard_frame;
+	}
+#ifdef STMMAC_VLAN_TAG_USED
+	if (rdes0 & RDES0_VLAN_TAG)
+		x->rx_vlan++;
+#endif
+
+	return ret;
+}
+
+static void enh_desc_init_rx_desc(struct dma_desc *p, int disable_rx_ic,
+				  int mode, int end, int bfsize)
+{
+	int bfsize1;
+
+	p->des0 |= cpu_to_le32(RDES0_OWN);
+
+	bfsize1 = min(bfsize, BUF_SIZE_8KiB);
+	p->des1 |= cpu_to_le32(bfsize1 & ERDES1_BUFFER1_SIZE_MASK);
+
+	if (mode == STMMAC_CHAIN_MODE)
+		ehn_desc_rx_set_on_chain(p);
+	else
+		ehn_desc_rx_set_on_ring(p, end, bfsize);
+
+	if (disable_rx_ic)
+		p->des1 |= cpu_to_le32(ERDES1_DISABLE_IC);
+}
+
+static void enh_desc_init_tx_desc(struct dma_desc *p, int mode, int end)
+{
+	p->des0 &= cpu_to_le32(~ETDES0_OWN);
+	if (mode == STMMAC_CHAIN_MODE)
+		enh_desc_end_tx_desc_on_chain(p);
+	else
+		enh_desc_end_tx_desc_on_ring(p, end);
+}
+
+static int enh_desc_get_tx_owner(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des0) & ETDES0_OWN) >> 31;
+}
+
+static void enh_desc_set_tx_owner(struct dma_desc *p)
+{
+	p->des0 |= cpu_to_le32(ETDES0_OWN);
+}
+
+static void enh_desc_set_rx_owner(struct dma_desc *p, int disable_rx_ic)
+{
+	p->des0 |= cpu_to_le32(RDES0_OWN);
+}
+
+static int enh_desc_get_tx_ls(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des0) & ETDES0_LAST_SEGMENT) >> 29;
+}
+
+static void enh_desc_release_tx_desc(struct dma_desc *p, int mode)
+{
+	int ter = (le32_to_cpu(p->des0) & ETDES0_END_RING) >> 21;
+
+	memset(p, 0, offsetof(struct dma_desc, des2));
+	if (mode == STMMAC_CHAIN_MODE)
+		enh_desc_end_tx_desc_on_chain(p);
+	else
+		enh_desc_end_tx_desc_on_ring(p, ter);
+}
+
+static void enh_desc_prepare_tx_desc(struct dma_desc *p, int is_fs, int len,
+				     bool csum_flag, int mode, bool tx_own,
+				     bool ls, unsigned int tot_pkt_len)
+{
+	unsigned int tdes0 = le32_to_cpu(p->des0);
+
+	if (mode == STMMAC_CHAIN_MODE)
+		enh_set_tx_desc_len_on_chain(p, len);
+	else
+		enh_set_tx_desc_len_on_ring(p, len);
+
+	if (is_fs)
+		tdes0 |= ETDES0_FIRST_SEGMENT;
+	else
+		tdes0 &= ~ETDES0_FIRST_SEGMENT;
+
+	if (likely(csum_flag))
+		tdes0 |= (TX_CIC_FULL << ETDES0_CHECKSUM_INSERTION_SHIFT);
+	else
+		tdes0 &= ~(TX_CIC_FULL << ETDES0_CHECKSUM_INSERTION_SHIFT);
+
+	if (ls)
+		tdes0 |= ETDES0_LAST_SEGMENT;
+
+	/* Finally set the OWN bit. Later the DMA will start! */
+	if (tx_own)
+		tdes0 |= ETDES0_OWN;
+
+	if (is_fs && tx_own)
+		/* When the own bit, for the first frame, has to be set, all
+		 * descriptors for the same frame has to be set before, to
+		 * avoid race condition.
+		 */
+		dma_wmb();
+
+	p->des0 = cpu_to_le32(tdes0);
+}
+
+static void enh_desc_set_tx_ic(struct dma_desc *p)
+{
+	p->des0 |= cpu_to_le32(ETDES0_INTERRUPT);
+}
+
+static int enh_desc_get_rx_frame_len(struct dma_desc *p, int rx_coe_type)
+{
+	unsigned int csum = 0;
+	/* The type-1 checksum offload engines append the checksum at
+	 * the end of frame and the two bytes of checksum are added in
+	 * the length.
+	 * Adjust for that in the framelen for type-1 checksum offload
+	 * engines.
+	 */
+	if (rx_coe_type == STMMAC_RX_COE_TYPE1)
+		csum = 2;
+
+	return (((le32_to_cpu(p->des0) & RDES0_FRAME_LEN_MASK)
+				>> RDES0_FRAME_LEN_SHIFT) - csum);
+}
+
+static void enh_desc_enable_tx_timestamp(struct dma_desc *p)
+{
+	p->des0 |= cpu_to_le32(ETDES0_TIME_STAMP_ENABLE);
+}
+
+static int enh_desc_get_tx_timestamp_status(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des0) & ETDES0_TIME_STAMP_STATUS) >> 17;
+}
+
+static void enh_desc_get_timestamp(void *desc, u32 ats, u64 *ts)
+{
+	u64 ns;
+
+	if (ats) {
+		struct dma_extended_desc *p = (struct dma_extended_desc *)desc;
+		ns = le32_to_cpu(p->des6);
+		/* convert high/sec time stamp value to nanosecond */
+		ns += le32_to_cpu(p->des7) * 1000000000ULL;
+	} else {
+		struct dma_desc *p = (struct dma_desc *)desc;
+		ns = le32_to_cpu(p->des2);
+		ns += le32_to_cpu(p->des3) * 1000000000ULL;
+	}
+
+	*ts = ns;
+}
+
+static int enh_desc_get_rx_timestamp_status(void *desc, void *next_desc,
+					    u32 ats)
+{
+	if (ats) {
+		struct dma_extended_desc *p = (struct dma_extended_desc *)desc;
+		return (le32_to_cpu(p->basic.des0) & RDES0_IPC_CSUM_ERROR) >> 7;
+	} else {
+		struct dma_desc *p = (struct dma_desc *)desc;
+		if ((le32_to_cpu(p->des2) == 0xffffffff) &&
+		    (le32_to_cpu(p->des3) == 0xffffffff))
+			/* timestamp is corrupted, hence don't store it */
+			return 0;
+		else
+			return 1;
+	}
+}
+
+static void enh_desc_display_ring(void *head, unsigned int size, bool rx)
+{
+	struct dma_extended_desc *ep = (struct dma_extended_desc *)head;
+	int i;
+
+	pr_info("Extended %s descriptor ring:\n", rx ? "RX" : "TX");
+
+	for (i = 0; i < size; i++) {
+		u64 x;
+
+		x = *(u64 *)ep;
+		pr_info("%03d [0x%x]: 0x%x 0x%x 0x%x 0x%x\n",
+			i, (unsigned int)virt_to_phys(ep),
+			(unsigned int)x, (unsigned int)(x >> 32),
+			ep->basic.des2, ep->basic.des3);
+		ep++;
+	}
+	pr_info("\n");
+}
+
+static void enh_desc_get_addr(struct dma_desc *p, unsigned int *addr)
+{
+	*addr = le32_to_cpu(p->des2);
+}
+
+static void enh_desc_set_addr(struct dma_desc *p, dma_addr_t addr)
+{
+	p->des2 = cpu_to_le32(addr);
+}
+
+static void enh_desc_clear(struct dma_desc *p)
+{
+	p->des2 = 0;
+}
+
+const struct stmmac_desc_ops enh_desc_ops = {
+	.tx_status = enh_desc_get_tx_status,
+	.rx_status = enh_desc_get_rx_status,
+	.get_tx_len = enh_desc_get_tx_len,
+	.init_rx_desc = enh_desc_init_rx_desc,
+	.init_tx_desc = enh_desc_init_tx_desc,
+	.get_tx_owner = enh_desc_get_tx_owner,
+	.release_tx_desc = enh_desc_release_tx_desc,
+	.prepare_tx_desc = enh_desc_prepare_tx_desc,
+	.set_tx_ic = enh_desc_set_tx_ic,
+	.get_tx_ls = enh_desc_get_tx_ls,
+	.set_tx_owner = enh_desc_set_tx_owner,
+	.set_rx_owner = enh_desc_set_rx_owner,
+	.get_rx_frame_len = enh_desc_get_rx_frame_len,
+	.rx_extended_status = enh_desc_get_ext_status,
+	.enable_tx_timestamp = enh_desc_enable_tx_timestamp,
+	.get_tx_timestamp_status = enh_desc_get_tx_timestamp_status,
+	.get_timestamp = enh_desc_get_timestamp,
+	.get_rx_timestamp_status = enh_desc_get_rx_timestamp_status,
+	.display_ring = enh_desc_display_ring,
+	.get_addr = enh_desc_get_addr,
+	.set_addr = enh_desc_set_addr,
+	.clear = enh_desc_clear,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/hwif.c b/net/rtnet/drivers/orange-pi-one/hwif.c
--- a/net/rtnet/drivers/orange-pi-one/hwif.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/hwif.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,351 @@
+// SPDX-License-Identifier: (GPL-2.0 OR MIT)
+/*
+ * Copyright (c) 2018 Synopsys, Inc. and/or its affiliates.
+ * stmmac HW Interface Handling
+ */
+
+#include "common.h"
+#include "stmmac.h"
+#include "stmmac_ptp.h"
+
+static u32 stmmac_get_id(struct stmmac_priv *priv, u32 id_reg)
+{
+	u32 reg = readl(priv->ioaddr + id_reg);
+
+	if (!reg) {
+		dev_info(priv->device, "Version ID not available\n");
+		return 0x0;
+	}
+
+	dev_info(priv->device, "User ID: 0x%x, Synopsys ID: 0x%x\n",
+			(unsigned int)(reg & GENMASK(15, 8)) >> 8,
+			(unsigned int)(reg & GENMASK(7, 0)));
+	return reg & GENMASK(7, 0);
+}
+
+static u32 stmmac_get_dev_id(struct stmmac_priv *priv, u32 id_reg)
+{
+	u32 reg = readl(priv->ioaddr + id_reg);
+
+	if (!reg) {
+		dev_info(priv->device, "Version ID not available\n");
+		return 0x0;
+	}
+
+	return (reg & GENMASK(15, 8)) >> 8;
+}
+
+static void stmmac_dwmac_mode_quirk(struct stmmac_priv *priv)
+{
+	struct mac_device_info *mac = priv->hw;
+
+	if (priv->chain_mode) {
+		dev_info(priv->device, "Chain mode enabled\n");
+		priv->mode = STMMAC_CHAIN_MODE;
+		mac->mode = &chain_mode_ops;
+	} else {
+		dev_info(priv->device, "Ring mode enabled\n");
+		priv->mode = STMMAC_RING_MODE;
+		mac->mode = &ring_mode_ops;
+	}
+}
+
+static int stmmac_dwmac1_quirks(struct stmmac_priv *priv)
+{
+	struct mac_device_info *mac = priv->hw;
+
+	if (priv->plat->enh_desc) {
+		dev_info(priv->device, "Enhanced/Alternate descriptors\n");
+
+		/* GMAC older than 3.50 has no extended descriptors */
+		if (priv->synopsys_id >= DWMAC_CORE_3_50) {
+			dev_info(priv->device, "Enabled extended descriptors\n");
+			priv->extend_desc = 1;
+		} else {
+			dev_warn(priv->device, "Extended descriptors not supported\n");
+		}
+
+		mac->desc = &enh_desc_ops;
+	} else {
+		dev_info(priv->device, "Normal descriptors\n");
+		mac->desc = &ndesc_ops;
+	}
+
+	stmmac_dwmac_mode_quirk(priv);
+	return 0;
+}
+
+#if 0
+static int stmmac_dwmac4_quirks(struct stmmac_priv *priv)
+{
+	stmmac_dwmac_mode_quirk(priv);
+	return 0;
+}
+
+static int stmmac_dwxlgmac_quirks(struct stmmac_priv *priv)
+{
+	priv->hw->xlgmac = true;
+	return 0;
+}
+#endif
+
+static const struct stmmac_hwif_entry {
+	bool gmac;
+	bool gmac4;
+	bool xgmac;
+	u32 min_id;
+	u32 dev_id;
+	const struct stmmac_regs_off regs;
+	const void *desc;
+	const void *dma;
+	const void *mac;
+	const void *hwtimestamp;
+	const void *mode;
+	const void *tc;
+	const void *mmc;
+	int (*setup)(struct stmmac_priv *priv);
+	int (*quirks)(struct stmmac_priv *priv);
+} stmmac_hw[] = {
+	/* NOTE: New HW versions shall go to the end of this table */
+	
+	/* orage pi one */
+	{
+		.gmac = false,
+		.gmac4 = false,
+		.xgmac = false,
+		.min_id = 0,
+		.regs = {
+			.ptp_off = PTP_GMAC3_X_OFFSET,
+			.mmc_off = MMC_GMAC3_X_OFFSET,
+		},
+		.desc = NULL,
+		.dma = &dwmac100_dma_ops,
+		.mac = &dwmac100_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = NULL,
+		.tc = NULL,
+		.mmc = &dwmac_mmc_ops,
+		.setup = dwmac100_setup,
+		.quirks = stmmac_dwmac1_quirks,
+	},
+#if 0
+	{
+		.gmac = true,
+		.gmac4 = false,
+		.xgmac = false,
+		.min_id = 0,
+		.regs = {
+			.ptp_off = PTP_GMAC3_X_OFFSET,
+			.mmc_off = MMC_GMAC3_X_OFFSET,
+		},
+		.desc = NULL,
+		.dma = &dwmac1000_dma_ops,
+		.mac = &dwmac1000_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = NULL,
+		.tc = NULL,
+		.mmc = &dwmac_mmc_ops,
+		.setup = dwmac1000_setup,
+		.quirks = stmmac_dwmac1_quirks,
+	}, {
+		.gmac = false,
+		.gmac4 = true,
+		.xgmac = false,
+		.min_id = 0,
+		.regs = {
+			.ptp_off = PTP_GMAC4_OFFSET,
+			.mmc_off = MMC_GMAC4_OFFSET,
+		},
+		.desc = &dwmac4_desc_ops,
+		.dma = &dwmac4_dma_ops,
+		.mac = &dwmac4_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = NULL,
+		.tc = &dwmac510_tc_ops,
+		.mmc = &dwmac_mmc_ops,
+		.setup = dwmac4_setup,
+		.quirks = stmmac_dwmac4_quirks,
+	}, {
+		.gmac = false,
+		.gmac4 = true,
+		.xgmac = false,
+		.min_id = DWMAC_CORE_4_00,
+		.regs = {
+			.ptp_off = PTP_GMAC4_OFFSET,
+			.mmc_off = MMC_GMAC4_OFFSET,
+		},
+		.desc = &dwmac4_desc_ops,
+		.dma = &dwmac4_dma_ops,
+		.mac = &dwmac410_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = &dwmac4_ring_mode_ops,
+		.tc = &dwmac510_tc_ops,
+		.mmc = &dwmac_mmc_ops,
+		.setup = dwmac4_setup,
+		.quirks = NULL,
+	}, {
+		.gmac = false,
+		.gmac4 = true,
+		.xgmac = false,
+		.min_id = DWMAC_CORE_4_10,
+		.regs = {
+			.ptp_off = PTP_GMAC4_OFFSET,
+			.mmc_off = MMC_GMAC4_OFFSET,
+		},
+		.desc = &dwmac4_desc_ops,
+		.dma = &dwmac410_dma_ops,
+		.mac = &dwmac410_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = &dwmac4_ring_mode_ops,
+		.tc = &dwmac510_tc_ops,
+		.mmc = &dwmac_mmc_ops,
+		.setup = dwmac4_setup,
+		.quirks = NULL,
+	}, {
+		.gmac = false,
+		.gmac4 = true,
+		.xgmac = false,
+		.min_id = DWMAC_CORE_5_10,
+		.regs = {
+			.ptp_off = PTP_GMAC4_OFFSET,
+			.mmc_off = MMC_GMAC4_OFFSET,
+		},
+		.desc = &dwmac4_desc_ops,
+		.dma = &dwmac410_dma_ops,
+		.mac = &dwmac510_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = &dwmac4_ring_mode_ops,
+		.tc = &dwmac510_tc_ops,
+		.mmc = &dwmac_mmc_ops,
+		.setup = dwmac4_setup,
+		.quirks = NULL,
+	}, {
+		.gmac = false,
+		.gmac4 = false,
+		.xgmac = true,
+		.min_id = DWXGMAC_CORE_2_10,
+		.dev_id = DWXGMAC_ID,
+		.regs = {
+			.ptp_off = PTP_XGMAC_OFFSET,
+			.mmc_off = MMC_XGMAC_OFFSET,
+		},
+		.desc = &dwxgmac210_desc_ops,
+		.dma = &dwxgmac210_dma_ops,
+		.mac = &dwxgmac210_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = NULL,
+		.tc = &dwmac510_tc_ops,
+		.mmc = &dwxgmac_mmc_ops,
+		.setup = dwxgmac2_setup,
+		.quirks = NULL,
+	}, {
+		.gmac = false,
+		.gmac4 = false,
+		.xgmac = true,
+		.min_id = DWXLGMAC_CORE_2_00,
+		.dev_id = DWXLGMAC_ID,
+		.regs = {
+			.ptp_off = PTP_XGMAC_OFFSET,
+			.mmc_off = MMC_XGMAC_OFFSET,
+		},
+		.desc = &dwxgmac210_desc_ops,
+		.dma = &dwxgmac210_dma_ops,
+		.mac = &dwxlgmac2_ops,
+		.hwtimestamp = &stmmac_ptp,
+		.mode = NULL,
+		.tc = &dwmac510_tc_ops,
+		.mmc = &dwxgmac_mmc_ops,
+		.setup = dwxlgmac2_setup,
+		.quirks = stmmac_dwxlgmac_quirks,
+	},
+#endif
+};
+
+int stmmac_hwif_init(struct stmmac_priv *priv)
+{
+	bool needs_xgmac = priv->plat->has_xgmac;
+	bool needs_gmac4 = priv->plat->has_gmac4;
+	bool needs_gmac = priv->plat->has_gmac;
+	const struct stmmac_hwif_entry *entry;
+	struct mac_device_info *mac;
+	bool needs_setup = true;
+	u32 id, dev_id = 0;
+	int i, ret;
+
+	if (needs_gmac) {
+		id = stmmac_get_id(priv, GMAC_VERSION);
+	} else if (needs_gmac4 || needs_xgmac) {
+		id = stmmac_get_id(priv, GMAC4_VERSION);
+		if (needs_xgmac)
+			dev_id = stmmac_get_dev_id(priv, GMAC4_VERSION);
+	} else {
+		id = 0;
+	}
+
+	/* Save ID for later use */
+	priv->synopsys_id = id;
+
+	/* Lets assume some safe values first */
+	priv->ptpaddr = priv->ioaddr +
+		(needs_gmac4 ? PTP_GMAC4_OFFSET : PTP_GMAC3_X_OFFSET);
+	priv->mmcaddr = priv->ioaddr +
+		(needs_gmac4 ? MMC_GMAC4_OFFSET : MMC_GMAC3_X_OFFSET);
+
+	/* Check for HW specific setup first */
+	if (priv->plat->setup) {
+		mac = priv->plat->setup(priv);
+		needs_setup = false;
+		printk(KERN_INFO "%s needs_setup is false\n", __func__);
+	} else {
+		mac = devm_kzalloc(priv->device, sizeof(*mac), GFP_KERNEL);
+	}
+
+	if (!mac)
+		return -ENOMEM;
+
+	/* Fallback to generic HW */
+	for (i = ARRAY_SIZE(stmmac_hw) - 1; i >= 0; i--) {
+		entry = &stmmac_hw[i];
+
+		if (needs_gmac ^ entry->gmac)
+			continue;
+		if (needs_gmac4 ^ entry->gmac4)
+			continue;
+		if (needs_xgmac ^ entry->xgmac)
+			continue;
+		/* Use synopsys_id var because some setups can override this */
+		if (priv->synopsys_id < entry->min_id)
+			continue;
+		if (needs_xgmac && (dev_id ^ entry->dev_id))
+			continue;
+
+		/* Only use generic HW helpers if needed */
+		mac->desc = mac->desc ? : entry->desc;
+		mac->dma = mac->dma ? : entry->dma;
+		mac->mac = mac->mac ? : entry->mac;
+		mac->ptp = mac->ptp ? : entry->hwtimestamp;
+		mac->mode = mac->mode ? : entry->mode;
+		mac->tc = mac->tc ? : entry->tc;
+		mac->mmc = mac->mmc ? : entry->mmc;
+
+		priv->hw = mac;
+		priv->ptpaddr = priv->ioaddr + entry->regs.ptp_off;
+		priv->mmcaddr = priv->ioaddr + entry->regs.mmc_off;
+
+		/* Entry found */
+		if (needs_setup) {
+			printk(KERN_INFO "%s found entry i=%d", __func__, i);
+			ret = entry->setup(priv);
+			if (ret)
+				return ret;
+		}
+		printk(KERN_INFO "%s save quirks for i=%d", __func__, i);
+		/* Save quirks, if needed for posterior use */
+		priv->hwif_quirks = entry->quirks;
+		return 0;
+	}
+
+	dev_err(priv->device, "Failed to find HW IF (id=0x%x, gmac=%d/%d)\n",
+			id, needs_gmac, needs_gmac4);
+	return -EINVAL;
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/hwif.h b/net/rtnet/drivers/orange-pi-one/hwif.h
--- a/net/rtnet/drivers/orange-pi-one/hwif.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/hwif.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,633 @@
+/* SPDX-License-Identifier: (GPL-2.0 OR MIT) */
+// Copyright (c) 2018 Synopsys, Inc. and/or its affiliates.
+// stmmac HW Interface Callbacks
+
+#ifndef __STMMAC_HWIF_H__
+#define __STMMAC_HWIF_H__
+
+#include <linux/netdevice.h>
+#include <linux/stmmac.h>
+
+#define stmmac_do_void_callback(__priv, __module, __cname,  __arg0, __args...) \
+({ \
+	int __result = -EINVAL; \
+	if ((__priv)->hw->__module && (__priv)->hw->__module->__cname) { \
+		(__priv)->hw->__module->__cname((__arg0), ##__args); \
+		__result = 0; \
+	} \
+	__result; \
+})
+#define stmmac_do_callback(__priv, __module, __cname,  __arg0, __args...) \
+({ \
+	int __result = -EINVAL; \
+	if ((__priv)->hw->__module && (__priv)->hw->__module->__cname) \
+		__result = (__priv)->hw->__module->__cname((__arg0), ##__args); \
+	__result; \
+})
+
+struct stmmac_extra_stats;
+struct stmmac_safety_stats;
+struct dma_desc;
+struct dma_extended_desc;
+struct dma_edesc;
+
+/* Descriptors helpers */
+struct stmmac_desc_ops {
+	/* DMA RX descriptor ring initialization */
+	void (*init_rx_desc)(struct dma_desc *p, int disable_rx_ic, int mode,
+			int end, int bfsize);
+	/* DMA TX descriptor ring initialization */
+	void (*init_tx_desc)(struct dma_desc *p, int mode, int end);
+	/* Invoked by the xmit function to prepare the tx descriptor */
+	void (*prepare_tx_desc)(struct dma_desc *p, int is_fs, int len,
+			bool csum_flag, int mode, bool tx_own, bool ls,
+			unsigned int tot_pkt_len);
+	void (*prepare_tso_tx_desc)(struct dma_desc *p, int is_fs, int len1,
+			int len2, bool tx_own, bool ls, unsigned int tcphdrlen,
+			unsigned int tcppayloadlen);
+	/* Set/get the owner of the descriptor */
+	void (*set_tx_owner)(struct dma_desc *p);
+	int (*get_tx_owner)(struct dma_desc *p);
+	/* Clean the tx descriptor as soon as the tx irq is received */
+	void (*release_tx_desc)(struct dma_desc *p, int mode);
+	/* Clear interrupt on tx frame completion. When this bit is
+	 * set an interrupt happens as soon as the frame is transmitted */
+	void (*set_tx_ic)(struct dma_desc *p);
+	/* Last tx segment reports the transmit status */
+	int (*get_tx_ls)(struct dma_desc *p);
+	/* Return the transmit status looking at the TDES1 */
+	int (*tx_status)(void *data, struct stmmac_extra_stats *x,
+			struct dma_desc *p, void __iomem *ioaddr);
+	/* Get the buffer size from the descriptor */
+	int (*get_tx_len)(struct dma_desc *p);
+	/* Handle extra events on specific interrupts hw dependent */
+	void (*set_rx_owner)(struct dma_desc *p, int disable_rx_ic);
+	/* Get the receive frame size */
+	int (*get_rx_frame_len)(struct dma_desc *p, int rx_coe_type);
+	/* Return the reception status looking at the RDES1 */
+	int (*rx_status)(void *data, struct stmmac_extra_stats *x,
+			struct dma_desc *p);
+	void (*rx_extended_status)(void *data, struct stmmac_extra_stats *x,
+			struct dma_extended_desc *p);
+	/* Set tx timestamp enable bit */
+	void (*enable_tx_timestamp) (struct dma_desc *p);
+	/* get tx timestamp status */
+	int (*get_tx_timestamp_status) (struct dma_desc *p);
+	/* get timestamp value */
+	void (*get_timestamp)(void *desc, u32 ats, u64 *ts);
+	/* get rx timestamp status */
+	int (*get_rx_timestamp_status)(void *desc, void *next_desc, u32 ats);
+	/* Display ring */
+	void (*display_ring)(void *head, unsigned int size, bool rx);
+	/* set MSS via context descriptor */
+	void (*set_mss)(struct dma_desc *p, unsigned int mss);
+	/* get descriptor skbuff address */
+	void (*get_addr)(struct dma_desc *p, unsigned int *addr);
+	/* set descriptor skbuff address */
+	void (*set_addr)(struct dma_desc *p, dma_addr_t addr);
+	/* clear descriptor */
+	void (*clear)(struct dma_desc *p);
+	/* RSS */
+	int (*get_rx_hash)(struct dma_desc *p, u32 *hash,
+			   enum pkt_hash_types *type);
+	int (*get_rx_header_len)(struct dma_desc *p, unsigned int *len);
+	void (*set_sec_addr)(struct dma_desc *p, dma_addr_t addr);
+	void (*set_sarc)(struct dma_desc *p, u32 sarc_type);
+	void (*set_vlan_tag)(struct dma_desc *p, u16 tag, u16 inner_tag,
+			     u32 inner_type);
+	void (*set_vlan)(struct dma_desc *p, u32 type);
+	void (*set_tbs)(struct dma_edesc *p, u32 sec, u32 nsec);
+};
+
+#define stmmac_init_rx_desc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, init_rx_desc, __args)
+#define stmmac_init_tx_desc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, init_tx_desc, __args)
+#define stmmac_prepare_tx_desc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, prepare_tx_desc, __args)
+#define stmmac_prepare_tso_tx_desc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, prepare_tso_tx_desc, __args)
+#define stmmac_set_tx_owner(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_tx_owner, __args)
+#define stmmac_get_tx_owner(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_tx_owner, __args)
+#define stmmac_release_tx_desc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, release_tx_desc, __args)
+#define stmmac_set_tx_ic(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_tx_ic, __args)
+#define stmmac_get_tx_ls(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_tx_ls, __args)
+#define stmmac_tx_status(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, tx_status, __args)
+#define stmmac_get_tx_len(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_tx_len, __args)
+#define stmmac_set_rx_owner(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_rx_owner, __args)
+#define stmmac_get_rx_frame_len(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_rx_frame_len, __args)
+#define stmmac_rx_status(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, rx_status, __args)
+#define stmmac_rx_extended_status(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, rx_extended_status, __args)
+#define stmmac_enable_tx_timestamp(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, enable_tx_timestamp, __args)
+#define stmmac_get_tx_timestamp_status(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_tx_timestamp_status, __args)
+#define stmmac_get_timestamp(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, get_timestamp, __args)
+#define stmmac_get_rx_timestamp_status(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_rx_timestamp_status, __args)
+#define stmmac_display_ring(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, display_ring, __args)
+#define stmmac_set_mss(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_mss, __args)
+#define stmmac_get_desc_addr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, get_addr, __args)
+#define stmmac_set_desc_addr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_addr, __args)
+#define stmmac_clear_desc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, clear, __args)
+#define stmmac_get_rx_hash(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_rx_hash, __args)
+#define stmmac_get_rx_header_len(__priv, __args...) \
+	stmmac_do_callback(__priv, desc, get_rx_header_len, __args)
+#define stmmac_set_desc_sec_addr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_sec_addr, __args)
+#define stmmac_set_desc_sarc(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_sarc, __args)
+#define stmmac_set_desc_vlan_tag(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_vlan_tag, __args)
+#define stmmac_set_desc_vlan(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_vlan, __args)
+#define stmmac_set_desc_tbs(__priv, __args...) \
+	stmmac_do_void_callback(__priv, desc, set_tbs, __args)
+
+struct stmmac_dma_cfg;
+struct dma_features;
+
+/* Specific DMA helpers */
+struct stmmac_dma_ops {
+	/* DMA core initialization */
+	int (*reset)(void __iomem *ioaddr);
+	void (*init)(void __iomem *ioaddr, struct stmmac_dma_cfg *dma_cfg,
+		     int atds);
+	void (*init_chan)(void __iomem *ioaddr,
+			  struct stmmac_dma_cfg *dma_cfg, u32 chan);
+	void (*init_rx_chan)(void __iomem *ioaddr,
+			     struct stmmac_dma_cfg *dma_cfg,
+			     dma_addr_t phy, u32 chan);
+	void (*init_tx_chan)(void __iomem *ioaddr,
+			     struct stmmac_dma_cfg *dma_cfg,
+			     dma_addr_t phy, u32 chan);
+	/* Configure the AXI Bus Mode Register */
+	void (*axi)(void __iomem *ioaddr, struct stmmac_axi *axi);
+	/* Dump DMA registers */
+	void (*dump_regs)(void __iomem *ioaddr, u32 *reg_space);
+	void (*dma_rx_mode)(void __iomem *ioaddr, int mode, u32 channel,
+			    int fifosz, u8 qmode);
+	void (*dma_tx_mode)(void __iomem *ioaddr, int mode, u32 channel,
+			    int fifosz, u8 qmode);
+	/* To track extra statistic (if supported) */
+	void (*dma_diagnostic_fr) (void *data, struct stmmac_extra_stats *x,
+				   void __iomem *ioaddr);
+	void (*enable_dma_transmission) (void __iomem *ioaddr);
+	void (*enable_dma_irq)(void __iomem *ioaddr, u32 chan,
+			       bool rx, bool tx);
+	void (*disable_dma_irq)(void __iomem *ioaddr, u32 chan,
+				bool rx, bool tx);
+	void (*start_tx)(void __iomem *ioaddr, u32 chan);
+	void (*stop_tx)(void __iomem *ioaddr, u32 chan);
+	void (*start_rx)(void __iomem *ioaddr, u32 chan);
+	void (*stop_rx)(void __iomem *ioaddr, u32 chan);
+	int (*dma_interrupt) (void __iomem *ioaddr,
+			      struct stmmac_extra_stats *x, u32 chan);
+	/* If supported then get the optional core features */
+	void (*get_hw_feature)(void __iomem *ioaddr,
+			       struct dma_features *dma_cap);
+	/* Program the HW RX Watchdog */
+	void (*rx_watchdog)(void __iomem *ioaddr, u32 riwt, u32 number_chan);
+	void (*set_tx_ring_len)(void __iomem *ioaddr, u32 len, u32 chan);
+	void (*set_rx_ring_len)(void __iomem *ioaddr, u32 len, u32 chan);
+	void (*set_rx_tail_ptr)(void __iomem *ioaddr, u32 tail_ptr, u32 chan);
+	void (*set_tx_tail_ptr)(void __iomem *ioaddr, u32 tail_ptr, u32 chan);
+	void (*enable_tso)(void __iomem *ioaddr, bool en, u32 chan);
+	void (*qmode)(void __iomem *ioaddr, u32 channel, u8 qmode);
+	void (*set_bfsize)(void __iomem *ioaddr, int bfsize, u32 chan);
+	void (*enable_sph)(void __iomem *ioaddr, bool en, u32 chan);
+	int (*enable_tbs)(void __iomem *ioaddr, bool en, u32 chan);
+};
+
+#define stmmac_reset(__priv, __args...) \
+	stmmac_do_callback(__priv, dma, reset, __args)
+#define stmmac_dma_init(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, init, __args)
+#define stmmac_init_chan(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, init_chan, __args)
+#define stmmac_init_rx_chan(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, init_rx_chan, __args)
+#define stmmac_init_tx_chan(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, init_tx_chan, __args)
+#define stmmac_axi(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, axi, __args)
+#define stmmac_dump_dma_regs(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, dump_regs, __args)
+#define stmmac_dma_rx_mode(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, dma_rx_mode, __args)
+#define stmmac_dma_tx_mode(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, dma_tx_mode, __args)
+#define stmmac_dma_diagnostic_fr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, dma_diagnostic_fr, __args)
+#define stmmac_enable_dma_transmission(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, enable_dma_transmission, __args)
+#define stmmac_enable_dma_irq(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, enable_dma_irq, __args)
+#define stmmac_disable_dma_irq(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, disable_dma_irq, __args)
+#define stmmac_start_tx(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, start_tx, __args)
+#define stmmac_stop_tx(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, stop_tx, __args)
+#define stmmac_start_rx(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, start_rx, __args)
+#define stmmac_stop_rx(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, stop_rx, __args)
+#define stmmac_dma_interrupt_status(__priv, __args...) \
+	stmmac_do_callback(__priv, dma, dma_interrupt, __args)
+#define stmmac_get_hw_feature(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, get_hw_feature, __args)
+#define stmmac_rx_watchdog(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, rx_watchdog, __args)
+#define stmmac_set_tx_ring_len(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, set_tx_ring_len, __args)
+#define stmmac_set_rx_ring_len(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, set_rx_ring_len, __args)
+#define stmmac_set_rx_tail_ptr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, set_rx_tail_ptr, __args)
+#define stmmac_set_tx_tail_ptr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, set_tx_tail_ptr, __args)
+#define stmmac_enable_tso(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, enable_tso, __args)
+#define stmmac_dma_qmode(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, qmode, __args)
+#define stmmac_set_dma_bfsize(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, set_bfsize, __args)
+#define stmmac_enable_sph(__priv, __args...) \
+	stmmac_do_void_callback(__priv, dma, enable_sph, __args)
+#define stmmac_enable_tbs(__priv, __args...) \
+	stmmac_do_callback(__priv, dma, enable_tbs, __args)
+
+struct mac_device_info;
+struct net_device;
+struct rgmii_adv;
+struct stmmac_safety_stats;
+struct stmmac_tc_entry;
+struct stmmac_pps_cfg;
+struct stmmac_rss;
+struct stmmac_est;
+
+/* Helpers to program the MAC core */
+struct stmmac_ops {
+	/* MAC core initialization */
+	void (*core_init)(struct mac_device_info *hw, struct net_device *dev);
+	/* Enable the MAC RX/TX */
+	void (*set_mac)(void __iomem *ioaddr, bool enable);
+	/* Enable and verify that the IPC module is supported */
+	int (*rx_ipc)(struct mac_device_info *hw);
+	/* Enable RX Queues */
+	void (*rx_queue_enable)(struct mac_device_info *hw, u8 mode, u32 queue);
+	/* RX Queues Priority */
+	void (*rx_queue_prio)(struct mac_device_info *hw, u32 prio, u32 queue);
+	/* TX Queues Priority */
+	void (*tx_queue_prio)(struct mac_device_info *hw, u32 prio, u32 queue);
+	/* RX Queues Routing */
+	void (*rx_queue_routing)(struct mac_device_info *hw, u8 packet,
+				 u32 queue);
+	/* Program RX Algorithms */
+	void (*prog_mtl_rx_algorithms)(struct mac_device_info *hw, u32 rx_alg);
+	/* Program TX Algorithms */
+	void (*prog_mtl_tx_algorithms)(struct mac_device_info *hw, u32 tx_alg);
+	/* Set MTL TX queues weight */
+	void (*set_mtl_tx_queue_weight)(struct mac_device_info *hw,
+					u32 weight, u32 queue);
+	/* RX MTL queue to RX dma mapping */
+	void (*map_mtl_to_dma)(struct mac_device_info *hw, u32 queue, u32 chan);
+	/* Configure AV Algorithm */
+	void (*config_cbs)(struct mac_device_info *hw, u32 send_slope,
+			   u32 idle_slope, u32 high_credit, u32 low_credit,
+			   u32 queue);
+	/* Dump MAC registers */
+	void (*dump_regs)(struct mac_device_info *hw, u32 *reg_space);
+	/* Handle extra events on specific interrupts hw dependent */
+	int (*host_irq_status)(struct mac_device_info *hw,
+			       struct stmmac_extra_stats *x);
+	/* Handle MTL interrupts */
+	int (*host_mtl_irq_status)(struct mac_device_info *hw, u32 chan);
+	/* Multicast filter setting */
+	void (*set_filter)(struct mac_device_info *hw, struct net_device *dev);
+	/* Flow control setting */
+	void (*flow_ctrl)(struct mac_device_info *hw, unsigned int duplex,
+			  unsigned int fc, unsigned int pause_time, u32 tx_cnt);
+	/* Set power management mode (e.g. magic frame) */
+	void (*pmt)(struct mac_device_info *hw, unsigned long mode);
+	/* Set/Get Unicast MAC addresses */
+	void (*set_umac_addr)(struct mac_device_info *hw, unsigned char *addr,
+			      unsigned int reg_n);
+	void (*get_umac_addr)(struct mac_device_info *hw, unsigned char *addr,
+			      unsigned int reg_n);
+	void (*set_eee_mode)(struct mac_device_info *hw,
+			     bool en_tx_lpi_clockgating);
+	void (*reset_eee_mode)(struct mac_device_info *hw);
+	void (*set_eee_timer)(struct mac_device_info *hw, int ls, int tw);
+	void (*set_eee_pls)(struct mac_device_info *hw, int link);
+	void (*debug)(void __iomem *ioaddr, struct stmmac_extra_stats *x,
+		      u32 rx_queues, u32 tx_queues);
+	/* PCS calls */
+	void (*pcs_ctrl_ane)(void __iomem *ioaddr, bool ane, bool srgmi_ral,
+			     bool loopback);
+	void (*pcs_rane)(void __iomem *ioaddr, bool restart);
+	void (*pcs_get_adv_lp)(void __iomem *ioaddr, struct rgmii_adv *adv);
+	/* Safety Features */
+	int (*safety_feat_config)(void __iomem *ioaddr, unsigned int asp);
+	int (*safety_feat_irq_status)(struct net_device *ndev,
+			void __iomem *ioaddr, unsigned int asp,
+			struct stmmac_safety_stats *stats);
+	int (*safety_feat_dump)(struct stmmac_safety_stats *stats,
+			int index, unsigned long *count, const char **desc);
+	/* Flexible RX Parser */
+	int (*rxp_config)(void __iomem *ioaddr, struct stmmac_tc_entry *entries,
+			  unsigned int count);
+	/* Flexible PPS */
+	int (*flex_pps_config)(void __iomem *ioaddr, int index,
+			       struct stmmac_pps_cfg *cfg, bool enable,
+			       u32 sub_second_inc, u32 systime_flags);
+	/* Loopback for selftests */
+	void (*set_mac_loopback)(void __iomem *ioaddr, bool enable);
+	/* RSS */
+	int (*rss_configure)(struct mac_device_info *hw,
+			     struct stmmac_rss *cfg, u32 num_rxq);
+	/* VLAN */
+	void (*update_vlan_hash)(struct mac_device_info *hw, u32 hash,
+				 __le16 perfect_match, bool is_double);
+	void (*enable_vlan)(struct mac_device_info *hw, u32 type);
+	int (*add_hw_vlan_rx_fltr)(struct net_device *dev,
+				   struct mac_device_info *hw,
+				   __be16 proto, u16 vid);
+	int (*del_hw_vlan_rx_fltr)(struct net_device *dev,
+				   struct mac_device_info *hw,
+				   __be16 proto, u16 vid);
+	void (*restore_hw_vlan_rx_fltr)(struct net_device *dev,
+					struct mac_device_info *hw);
+	/* TX Timestamp */
+	int (*get_mac_tx_timestamp)(struct mac_device_info *hw, u64 *ts);
+	/* Source Address Insertion / Replacement */
+	void (*sarc_configure)(void __iomem *ioaddr, int val);
+	/* Filtering */
+	int (*config_l3_filter)(struct mac_device_info *hw, u32 filter_no,
+				bool en, bool ipv6, bool sa, bool inv,
+				u32 match);
+	int (*config_l4_filter)(struct mac_device_info *hw, u32 filter_no,
+				bool en, bool udp, bool sa, bool inv,
+				u32 match);
+	void (*set_arp_offload)(struct mac_device_info *hw, bool en, u32 addr);
+	int (*est_configure)(void __iomem *ioaddr, struct stmmac_est *cfg,
+			     unsigned int ptp_rate);
+	void (*fpe_configure)(void __iomem *ioaddr, u32 num_txq, u32 num_rxq,
+			      bool enable);
+};
+
+#define stmmac_core_init(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, core_init, __args)
+#define stmmac_mac_set(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_mac, __args)
+#define stmmac_rx_ipc(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, rx_ipc, __args)
+#define stmmac_rx_queue_enable(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, rx_queue_enable, __args)
+#define stmmac_rx_queue_prio(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, rx_queue_prio, __args)
+#define stmmac_tx_queue_prio(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, tx_queue_prio, __args)
+#define stmmac_rx_queue_routing(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, rx_queue_routing, __args)
+#define stmmac_prog_mtl_rx_algorithms(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, prog_mtl_rx_algorithms, __args)
+#define stmmac_prog_mtl_tx_algorithms(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, prog_mtl_tx_algorithms, __args)
+#define stmmac_set_mtl_tx_queue_weight(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_mtl_tx_queue_weight, __args)
+#define stmmac_map_mtl_to_dma(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, map_mtl_to_dma, __args)
+#define stmmac_config_cbs(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, config_cbs, __args)
+#define stmmac_dump_mac_regs(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, dump_regs, __args)
+#define stmmac_host_irq_status(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, host_irq_status, __args)
+#define stmmac_host_mtl_irq_status(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, host_mtl_irq_status, __args)
+#define stmmac_set_filter(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_filter, __args)
+#define stmmac_flow_ctrl(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, flow_ctrl, __args)
+#define stmmac_pmt(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, pmt, __args)
+#define stmmac_set_umac_addr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_umac_addr, __args)
+#define stmmac_get_umac_addr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, get_umac_addr, __args)
+#define stmmac_set_eee_mode(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_eee_mode, __args)
+#define stmmac_reset_eee_mode(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, reset_eee_mode, __args)
+#define stmmac_set_eee_timer(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_eee_timer, __args)
+#define stmmac_set_eee_pls(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_eee_pls, __args)
+#define stmmac_mac_debug(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, debug, __args)
+#define stmmac_pcs_ctrl_ane(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, pcs_ctrl_ane, __args)
+#define stmmac_pcs_rane(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, pcs_rane, __args)
+#define stmmac_pcs_get_adv_lp(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, pcs_get_adv_lp, __args)
+#define stmmac_safety_feat_config(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, safety_feat_config, __args)
+#define stmmac_safety_feat_irq_status(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, safety_feat_irq_status, __args)
+#define stmmac_safety_feat_dump(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, safety_feat_dump, __args)
+#define stmmac_rxp_config(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, rxp_config, __args)
+#define stmmac_flex_pps_config(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, flex_pps_config, __args)
+#define stmmac_set_mac_loopback(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_mac_loopback, __args)
+#define stmmac_rss_configure(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, rss_configure, __args)
+#define stmmac_update_vlan_hash(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, update_vlan_hash, __args)
+#define stmmac_enable_vlan(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, enable_vlan, __args)
+#define stmmac_add_hw_vlan_rx_fltr(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, add_hw_vlan_rx_fltr, __args)
+#define stmmac_del_hw_vlan_rx_fltr(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, del_hw_vlan_rx_fltr, __args)
+#define stmmac_restore_hw_vlan_rx_fltr(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, restore_hw_vlan_rx_fltr, __args)
+#define stmmac_get_mac_tx_timestamp(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, get_mac_tx_timestamp, __args)
+#define stmmac_sarc_configure(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, sarc_configure, __args)
+#define stmmac_config_l3_filter(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, config_l3_filter, __args)
+#define stmmac_config_l4_filter(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, config_l4_filter, __args)
+#define stmmac_set_arp_offload(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, set_arp_offload, __args)
+#define stmmac_est_configure(__priv, __args...) \
+	stmmac_do_callback(__priv, mac, est_configure, __args)
+#define stmmac_fpe_configure(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mac, fpe_configure, __args)
+
+/* PTP and HW Timer helpers */
+struct stmmac_hwtimestamp {
+	void (*config_hw_tstamping) (void __iomem *ioaddr, u32 data);
+	void (*config_sub_second_increment)(void __iomem *ioaddr, u32 ptp_clock,
+					   int gmac4, u32 *ssinc);
+	int (*init_systime) (void __iomem *ioaddr, u32 sec, u32 nsec);
+	int (*config_addend) (void __iomem *ioaddr, u32 addend);
+	int (*adjust_systime) (void __iomem *ioaddr, u32 sec, u32 nsec,
+			       int add_sub, int gmac4);
+	void (*get_systime) (void __iomem *ioaddr, u64 *systime);
+};
+
+#define stmmac_config_hw_tstamping(__priv, __args...) \
+	stmmac_do_void_callback(__priv, ptp, config_hw_tstamping, __args)
+#define stmmac_config_sub_second_increment(__priv, __args...) \
+	stmmac_do_void_callback(__priv, ptp, config_sub_second_increment, __args)
+#define stmmac_init_systime(__priv, __args...) \
+	stmmac_do_callback(__priv, ptp, init_systime, __args)
+#define stmmac_config_addend(__priv, __args...) \
+	stmmac_do_callback(__priv, ptp, config_addend, __args)
+#define stmmac_adjust_systime(__priv, __args...) \
+	stmmac_do_callback(__priv, ptp, adjust_systime, __args)
+#define stmmac_get_systime(__priv, __args...) \
+	stmmac_do_void_callback(__priv, ptp, get_systime, __args)
+
+/* Helpers to manage the descriptors for chain and ring modes */
+struct stmmac_mode_ops {
+	void (*init) (void *des, dma_addr_t phy_addr, unsigned int size,
+		      unsigned int extend_desc);
+	unsigned int (*is_jumbo_frm) (int len, int ehn_desc);
+	int (*jumbo_frm)(void *priv, struct sk_buff *skb, int csum);
+	int (*set_16kib_bfsize)(int mtu);
+	void (*init_desc3)(struct dma_desc *p);
+	void (*refill_desc3) (void *priv, struct dma_desc *p);
+	void (*clean_desc3) (void *priv, struct dma_desc *p);
+};
+
+#define stmmac_mode_init(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mode, init, __args)
+#define stmmac_is_jumbo_frm(__priv, __args...) \
+	stmmac_do_callback(__priv, mode, is_jumbo_frm, __args)
+#define stmmac_jumbo_frm(__priv, __args...) \
+	stmmac_do_callback(__priv, mode, jumbo_frm, __args)
+#define stmmac_set_16kib_bfsize(__priv, __args...) \
+	stmmac_do_callback(__priv, mode, set_16kib_bfsize, __args)
+#define stmmac_init_desc3(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mode, init_desc3, __args)
+#define stmmac_refill_desc3(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mode, refill_desc3, __args)
+#define stmmac_clean_desc3(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mode, clean_desc3, __args)
+
+struct stmmac_priv;
+struct tc_cls_u32_offload;
+struct tc_cbs_qopt_offload;
+struct flow_cls_offload;
+struct tc_taprio_qopt_offload;
+struct tc_etf_qopt_offload;
+
+struct stmmac_tc_ops {
+	int (*init)(struct stmmac_priv *priv);
+	int (*setup_cls_u32)(struct stmmac_priv *priv,
+			     struct tc_cls_u32_offload *cls);
+	int (*setup_cbs)(struct stmmac_priv *priv,
+			 struct tc_cbs_qopt_offload *qopt);
+	int (*setup_cls)(struct stmmac_priv *priv,
+			 struct flow_cls_offload *cls);
+	int (*setup_taprio)(struct stmmac_priv *priv,
+			    struct tc_taprio_qopt_offload *qopt);
+	int (*setup_etf)(struct stmmac_priv *priv,
+			 struct tc_etf_qopt_offload *qopt);
+};
+
+#define stmmac_tc_init(__priv, __args...) \
+	stmmac_do_callback(__priv, tc, init, __args)
+#define stmmac_tc_setup_cls_u32(__priv, __args...) \
+	stmmac_do_callback(__priv, tc, setup_cls_u32, __args)
+#define stmmac_tc_setup_cbs(__priv, __args...) \
+	stmmac_do_callback(__priv, tc, setup_cbs, __args)
+#define stmmac_tc_setup_cls(__priv, __args...) \
+	stmmac_do_callback(__priv, tc, setup_cls, __args)
+#define stmmac_tc_setup_taprio(__priv, __args...) \
+	stmmac_do_callback(__priv, tc, setup_taprio, __args)
+#define stmmac_tc_setup_etf(__priv, __args...) \
+	stmmac_do_callback(__priv, tc, setup_etf, __args)
+
+struct stmmac_counters;
+
+struct stmmac_mmc_ops {
+	void (*ctrl)(void __iomem *ioaddr, unsigned int mode);
+	void (*intr_all_mask)(void __iomem *ioaddr);
+	void (*read)(void __iomem *ioaddr, struct stmmac_counters *mmc);
+};
+
+#define stmmac_mmc_ctrl(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mmc, ctrl, __args)
+#define stmmac_mmc_intr_all_mask(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mmc, intr_all_mask, __args)
+#define stmmac_mmc_read(__priv, __args...) \
+	stmmac_do_void_callback(__priv, mmc, read, __args)
+
+/* XPCS callbacks */
+#define stmmac_xpcs_validate(__priv, __args...) \
+	stmmac_do_callback(__priv, xpcs, validate, __args)
+#define stmmac_xpcs_config(__priv, __args...) \
+	stmmac_do_callback(__priv, xpcs, config, __args)
+#define stmmac_xpcs_get_state(__priv, __args...) \
+	stmmac_do_callback(__priv, xpcs, get_state, __args)
+#define stmmac_xpcs_link_up(__priv, __args...) \
+	stmmac_do_callback(__priv, xpcs, link_up, __args)
+#define stmmac_xpcs_probe(__priv, __args...) \
+	stmmac_do_callback(__priv, xpcs, probe, __args)
+
+struct stmmac_regs_off {
+	u32 ptp_off;
+	u32 mmc_off;
+};
+
+extern const struct stmmac_ops dwmac100_ops;
+extern const struct stmmac_dma_ops dwmac100_dma_ops;
+extern const struct stmmac_ops dwmac1000_ops;
+extern const struct stmmac_dma_ops dwmac1000_dma_ops;
+extern const struct stmmac_ops dwmac4_ops;
+extern const struct stmmac_dma_ops dwmac4_dma_ops;
+extern const struct stmmac_ops dwmac410_ops;
+extern const struct stmmac_dma_ops dwmac410_dma_ops;
+extern const struct stmmac_ops dwmac510_ops;
+extern const struct stmmac_tc_ops dwmac510_tc_ops;
+extern const struct stmmac_ops dwxgmac210_ops;
+extern const struct stmmac_ops dwxlgmac2_ops;
+extern const struct stmmac_dma_ops dwxgmac210_dma_ops;
+extern const struct stmmac_desc_ops dwxgmac210_desc_ops;
+extern const struct stmmac_mmc_ops dwmac_mmc_ops;
+extern const struct stmmac_mmc_ops dwxgmac_mmc_ops;
+
+#define GMAC_VERSION		0x00000020	/* GMAC CORE Version */
+#define GMAC4_VERSION		0x00000110	/* GMAC4+ CORE Version */
+
+int stmmac_hwif_init(struct stmmac_priv *priv);
+
+#endif /* __STMMAC_HWIF_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/Kconfig b/net/rtnet/drivers/orange-pi-one/Kconfig
--- a/net/rtnet/drivers/orange-pi-one/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/Kconfig	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,63 @@
+# SPDX-License-Identifier: GPL-2.0-only
+config RTNET_STMMAC_ETH
+	bool "STMicroelectronics Multi-Gigabit Ethernet driver"
+	depends on HAS_IOMEM && HAS_DMA
+	select MII
+	select MDIO_XPCS
+	select PAGE_POOL
+	select PHYLINK
+	select CRC32
+	imply PTP_1588_CLOCK
+	select RESET_CONTROLLER
+	help
+	  This is the driver for the Ethernet IPs built around a
+	  Synopsys IP Core.
+
+if RTNET_STMMAC_ETH
+
+config RTNET_STMMAC_PLATFORM
+	bool "STMMAC Platform bus support"
+	depends on RTNET_STMMAC_ETH
+	select MFD_SYSCON
+	default y
+	help
+	  This selects the platform specific bus support for the stmmac driver.
+	  This is the driver used on several SoCs:
+	  STi, Allwinner, Amlogic Meson, Altera SOCFPGA.
+
+	  If you have a controller with this interface, say Y or M here.
+
+	  If unsure, say N.
+
+if RTNET_STMMAC_PLATFORM
+
+#config RTNET_DWMAC_DWC_QOS_ETH
+#	bool "Support for snps,dwc-qos-ethernet.txt DT binding."
+#	select CRC32
+#	select MII
+#	depends on OF && HAS_DMA
+#	help
+#	  Support for chips using the snps,dwc-qos-ethernet.txt DT binding.
+
+#config RTNET_DWMAC_GENERIC
+#	bool "Generic driver for DWMAC"
+#	default RTNET_STMMAC_PLATFORM
+#	help
+#	  Generic DWMAC driver for platforms that don't require any
+#	  platform specific code to function or is using platform
+#	  data for setup.
+
+config RTNET_DWMAC_SUN8I
+	bool "Allwinner sun8i GMAC support"
+	default ARCH_SUNXI
+	depends on OF && (ARCH_SUNXI || COMPILE_TEST)
+	select MDIO_BUS_MUX
+	help
+	  Support for Allwinner H3 A83T A64 EMAC ethernet controllers.
+
+	  This selects Allwinner SoC glue layer support for the
+	  stmmac device driver. This driver is used for H3/A83T/A64
+	  EMAC ethernet controller.
+endif
+endif
+
diff -Naur a/net/rtnet/drivers/orange-pi-one/Makefile b/net/rtnet/drivers/orange-pi-one/Makefile
--- a/net/rtnet/drivers/orange-pi-one/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/Makefile	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,38 @@
+# SPDX-License-Identifier: GPL-2.0
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_STMMAC_ETH) += stmmac.o
+stmmac-objs:= stmmac_main.o stmmac_ethtool.o stmmac_mdio.o ring_mode.o	\
+	      chain_mode.o dwmac_lib.o \
+	      dwmac100_core.o dwmac100_dma.o enh_desc.o norm_desc.o	\
+	      mmc_core.o stmmac_hwtstamp.o stmmac_ptp.o \
+	      hwif.o \
+	      stmmac_tc.o \
+	      $(stmmac-y)
+
+#stmmac-objs += dwmac1000_core.o dwmac1000_dma.o \
+	dwmac4_descs.o dwmac4_dma.o dwmac4_lib.o dwmac4_core.o dwmac5.o \
+	dwxgmac2_core.o dwxgmac2_dma.o dwxgmac2_descs.o
+
+#stmmac-$(CONFIG_STMMAC_SELFTESTS) += stmmac_selftests.o
+
+# Ordering matters. Generic driver must be last.
+obj-$(CONFIG_RTNET_STMMAC_PLATFORM)	+= stmmac-platform.o
+#obj-$(CONFIG_DWMAC_ANARION)	+= dwmac-anarion.o
+#obj-$(CONFIG_DWMAC_IPQ806X)	+= dwmac-ipq806x.o
+#obj-$(CONFIG_DWMAC_LPC18XX)	+= dwmac-lpc18xx.o
+#obj-$(CONFIG_DWMAC_MEDIATEK)	+= dwmac-mediatek.o
+#obj-$(CONFIG_DWMAC_MESON)	+= dwmac-meson.o dwmac-meson8b.o
+#obj-$(CONFIG_DWMAC_OXNAS)	+= dwmac-oxnas.o
+#obj-$(CONFIG_DWMAC_QCOM_ETHQOS)	+= dwmac-qcom-ethqos.o
+#obj-$(CONFIG_DWMAC_ROCKCHIP)	+= dwmac-rk.o
+#obj-$(CONFIG_DWMAC_SOCFPGA)	+= dwmac-altr-socfpga.o
+#obj-$(CONFIG_DWMAC_STI)		+= dwmac-sti.o
+#obj-$(CONFIG_DWMAC_STM32)	+= dwmac-stm32.o
+#obj-$(CONFIG_DWMAC_SUNXI)	+= dwmac-sunxi.o
+obj-$(CONFIG_RTNET_DWMAC_SUN8I)	+= dwmac-sun8i.o
+#obj-$(CONFIG_DWMAC_DWC_QOS_ETH)	+= dwmac-dwc-qos-eth.o
+#obj-$(CONFIG_DWMAC_GENERIC)	+= dwmac-generic.o
+#obj-$(CONFIG_DWMAC_IMX8)	+= dwmac-imx.o
+stmmac-platform-objs:= stmmac_platform.o
+
diff -Naur a/net/rtnet/drivers/orange-pi-one/mmc_core.c b/net/rtnet/drivers/orange-pi-one/mmc_core.c
--- a/net/rtnet/drivers/orange-pi-one/mmc_core.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/mmc_core.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,475 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  DWMAC Management Counters
+
+  Copyright (C) 2011  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/kernel.h>
+#include <linux/io.h>
+#include "hwif.h"
+#include "mmc.h"
+
+/* MAC Management Counters register offset */
+
+#define MMC_CNTRL		0x00	/* MMC Control */
+#define MMC_RX_INTR		0x04	/* MMC RX Interrupt */
+#define MMC_TX_INTR		0x08	/* MMC TX Interrupt */
+#define MMC_RX_INTR_MASK	0x0c	/* MMC Interrupt Mask */
+#define MMC_TX_INTR_MASK	0x10	/* MMC Interrupt Mask */
+#define MMC_DEFAULT_MASK	0xffffffff
+
+/* MMC TX counter registers */
+
+/* Note:
+ * _GB register stands for good and bad frames
+ * _G is for good only.
+ */
+#define MMC_TX_OCTETCOUNT_GB		0x14
+#define MMC_TX_FRAMECOUNT_GB		0x18
+#define MMC_TX_BROADCASTFRAME_G		0x1c
+#define MMC_TX_MULTICASTFRAME_G		0x20
+#define MMC_TX_64_OCTETS_GB		0x24
+#define MMC_TX_65_TO_127_OCTETS_GB	0x28
+#define MMC_TX_128_TO_255_OCTETS_GB	0x2c
+#define MMC_TX_256_TO_511_OCTETS_GB	0x30
+#define MMC_TX_512_TO_1023_OCTETS_GB	0x34
+#define MMC_TX_1024_TO_MAX_OCTETS_GB	0x38
+#define MMC_TX_UNICAST_GB		0x3c
+#define MMC_TX_MULTICAST_GB		0x40
+#define MMC_TX_BROADCAST_GB		0x44
+#define MMC_TX_UNDERFLOW_ERROR		0x48
+#define MMC_TX_SINGLECOL_G		0x4c
+#define MMC_TX_MULTICOL_G		0x50
+#define MMC_TX_DEFERRED			0x54
+#define MMC_TX_LATECOL			0x58
+#define MMC_TX_EXESSCOL			0x5c
+#define MMC_TX_CARRIER_ERROR		0x60
+#define MMC_TX_OCTETCOUNT_G		0x64
+#define MMC_TX_FRAMECOUNT_G		0x68
+#define MMC_TX_EXCESSDEF		0x6c
+#define MMC_TX_PAUSE_FRAME		0x70
+#define MMC_TX_VLAN_FRAME_G		0x74
+
+/* MMC RX counter registers */
+#define MMC_RX_FRAMECOUNT_GB		0x80
+#define MMC_RX_OCTETCOUNT_GB		0x84
+#define MMC_RX_OCTETCOUNT_G		0x88
+#define MMC_RX_BROADCASTFRAME_G		0x8c
+#define MMC_RX_MULTICASTFRAME_G		0x90
+#define MMC_RX_CRC_ERROR		0x94
+#define MMC_RX_ALIGN_ERROR		0x98
+#define MMC_RX_RUN_ERROR		0x9C
+#define MMC_RX_JABBER_ERROR		0xA0
+#define MMC_RX_UNDERSIZE_G		0xA4
+#define MMC_RX_OVERSIZE_G		0xA8
+#define MMC_RX_64_OCTETS_GB		0xAC
+#define MMC_RX_65_TO_127_OCTETS_GB	0xb0
+#define MMC_RX_128_TO_255_OCTETS_GB	0xb4
+#define MMC_RX_256_TO_511_OCTETS_GB	0xb8
+#define MMC_RX_512_TO_1023_OCTETS_GB	0xbc
+#define MMC_RX_1024_TO_MAX_OCTETS_GB	0xc0
+#define MMC_RX_UNICAST_G		0xc4
+#define MMC_RX_LENGTH_ERROR		0xc8
+#define MMC_RX_AUTOFRANGETYPE		0xcc
+#define MMC_RX_PAUSE_FRAMES		0xd0
+#define MMC_RX_FIFO_OVERFLOW		0xd4
+#define MMC_RX_VLAN_FRAMES_GB		0xd8
+#define MMC_RX_WATCHDOG_ERROR		0xdc
+/* IPC*/
+#define MMC_RX_IPC_INTR_MASK		0x100
+#define MMC_RX_IPC_INTR			0x108
+/* IPv4*/
+#define MMC_RX_IPV4_GD			0x110
+#define MMC_RX_IPV4_HDERR		0x114
+#define MMC_RX_IPV4_NOPAY		0x118
+#define MMC_RX_IPV4_FRAG		0x11C
+#define MMC_RX_IPV4_UDSBL		0x120
+
+#define MMC_RX_IPV4_GD_OCTETS		0x150
+#define MMC_RX_IPV4_HDERR_OCTETS	0x154
+#define MMC_RX_IPV4_NOPAY_OCTETS	0x158
+#define MMC_RX_IPV4_FRAG_OCTETS		0x15c
+#define MMC_RX_IPV4_UDSBL_OCTETS	0x160
+
+/* IPV6*/
+#define MMC_RX_IPV6_GD_OCTETS		0x164
+#define MMC_RX_IPV6_HDERR_OCTETS	0x168
+#define MMC_RX_IPV6_NOPAY_OCTETS	0x16c
+
+#define MMC_RX_IPV6_GD			0x124
+#define MMC_RX_IPV6_HDERR		0x128
+#define MMC_RX_IPV6_NOPAY		0x12c
+
+/* Protocols*/
+#define MMC_RX_UDP_GD			0x130
+#define MMC_RX_UDP_ERR			0x134
+#define MMC_RX_TCP_GD			0x138
+#define MMC_RX_TCP_ERR			0x13c
+#define MMC_RX_ICMP_GD			0x140
+#define MMC_RX_ICMP_ERR			0x144
+
+#define MMC_RX_UDP_GD_OCTETS		0x170
+#define MMC_RX_UDP_ERR_OCTETS		0x174
+#define MMC_RX_TCP_GD_OCTETS		0x178
+#define MMC_RX_TCP_ERR_OCTETS		0x17c
+#define MMC_RX_ICMP_GD_OCTETS		0x180
+#define MMC_RX_ICMP_ERR_OCTETS		0x184
+
+#define MMC_TX_FPE_FRAG			0x1a8
+#define MMC_TX_HOLD_REQ			0x1ac
+#define MMC_RX_PKT_ASSEMBLY_ERR		0x1c8
+#define MMC_RX_PKT_SMD_ERR		0x1cc
+#define MMC_RX_PKT_ASSEMBLY_OK		0x1d0
+#define MMC_RX_FPE_FRAG			0x1d4
+
+/* XGMAC MMC Registers */
+#define MMC_XGMAC_TX_OCTET_GB		0x14
+#define MMC_XGMAC_TX_PKT_GB		0x1c
+#define MMC_XGMAC_TX_BROAD_PKT_G	0x24
+#define MMC_XGMAC_TX_MULTI_PKT_G	0x2c
+#define MMC_XGMAC_TX_64OCT_GB		0x34
+#define MMC_XGMAC_TX_65OCT_GB		0x3c
+#define MMC_XGMAC_TX_128OCT_GB		0x44
+#define MMC_XGMAC_TX_256OCT_GB		0x4c
+#define MMC_XGMAC_TX_512OCT_GB		0x54
+#define MMC_XGMAC_TX_1024OCT_GB		0x5c
+#define MMC_XGMAC_TX_UNI_PKT_GB		0x64
+#define MMC_XGMAC_TX_MULTI_PKT_GB	0x6c
+#define MMC_XGMAC_TX_BROAD_PKT_GB	0x74
+#define MMC_XGMAC_TX_UNDER		0x7c
+#define MMC_XGMAC_TX_OCTET_G		0x84
+#define MMC_XGMAC_TX_PKT_G		0x8c
+#define MMC_XGMAC_TX_PAUSE		0x94
+#define MMC_XGMAC_TX_VLAN_PKT_G		0x9c
+#define MMC_XGMAC_TX_LPI_USEC		0xa4
+#define MMC_XGMAC_TX_LPI_TRAN		0xa8
+
+#define MMC_XGMAC_RX_PKT_GB		0x100
+#define MMC_XGMAC_RX_OCTET_GB		0x108
+#define MMC_XGMAC_RX_OCTET_G		0x110
+#define MMC_XGMAC_RX_BROAD_PKT_G	0x118
+#define MMC_XGMAC_RX_MULTI_PKT_G	0x120
+#define MMC_XGMAC_RX_CRC_ERR		0x128
+#define MMC_XGMAC_RX_RUNT_ERR		0x130
+#define MMC_XGMAC_RX_JABBER_ERR		0x134
+#define MMC_XGMAC_RX_UNDER		0x138
+#define MMC_XGMAC_RX_OVER		0x13c
+#define MMC_XGMAC_RX_64OCT_GB		0x140
+#define MMC_XGMAC_RX_65OCT_GB		0x148
+#define MMC_XGMAC_RX_128OCT_GB		0x150
+#define MMC_XGMAC_RX_256OCT_GB		0x158
+#define MMC_XGMAC_RX_512OCT_GB		0x160
+#define MMC_XGMAC_RX_1024OCT_GB		0x168
+#define MMC_XGMAC_RX_UNI_PKT_G		0x170
+#define MMC_XGMAC_RX_LENGTH_ERR		0x178
+#define MMC_XGMAC_RX_RANGE		0x180
+#define MMC_XGMAC_RX_PAUSE		0x188
+#define MMC_XGMAC_RX_FIFOOVER_PKT	0x190
+#define MMC_XGMAC_RX_VLAN_PKT_GB	0x198
+#define MMC_XGMAC_RX_WATCHDOG_ERR	0x1a0
+#define MMC_XGMAC_RX_LPI_USEC		0x1a4
+#define MMC_XGMAC_RX_LPI_TRAN		0x1a8
+#define MMC_XGMAC_RX_DISCARD_PKT_GB	0x1ac
+#define MMC_XGMAC_RX_DISCARD_OCT_GB	0x1b4
+#define MMC_XGMAC_RX_ALIGN_ERR_PKT	0x1bc
+
+#define MMC_XGMAC_TX_FPE_FRAG		0x208
+#define MMC_XGMAC_TX_HOLD_REQ		0x20c
+#define MMC_XGMAC_RX_PKT_ASSEMBLY_ERR	0x228
+#define MMC_XGMAC_RX_PKT_SMD_ERR	0x22c
+#define MMC_XGMAC_RX_PKT_ASSEMBLY_OK	0x230
+#define MMC_XGMAC_RX_FPE_FRAG		0x234
+#define MMC_XGMAC_RX_IPC_INTR_MASK	0x25c
+
+static void dwmac_mmc_ctrl(void __iomem *mmcaddr, unsigned int mode)
+{
+	u32 value = readl(mmcaddr + MMC_CNTRL);
+
+	value |= (mode & 0x3F);
+
+	writel(value, mmcaddr + MMC_CNTRL);
+
+	pr_debug("stmmac: MMC ctrl register (offset 0x%x): 0x%08x\n",
+		 MMC_CNTRL, value);
+}
+
+/* To mask all all interrupts.*/
+static void dwmac_mmc_intr_all_mask(void __iomem *mmcaddr)
+{
+	writel(MMC_DEFAULT_MASK, mmcaddr + MMC_RX_INTR_MASK);
+	writel(MMC_DEFAULT_MASK, mmcaddr + MMC_TX_INTR_MASK);
+	writel(MMC_DEFAULT_MASK, mmcaddr + MMC_RX_IPC_INTR_MASK);
+}
+
+/* This reads the MAC core counters (if actaully supported).
+ * by default the MMC core is programmed to reset each
+ * counter after a read. So all the field of the mmc struct
+ * have to be incremented.
+ */
+static void dwmac_mmc_read(void __iomem *mmcaddr, struct stmmac_counters *mmc)
+{
+	mmc->mmc_tx_octetcount_gb += readl(mmcaddr + MMC_TX_OCTETCOUNT_GB);
+	mmc->mmc_tx_framecount_gb += readl(mmcaddr + MMC_TX_FRAMECOUNT_GB);
+	mmc->mmc_tx_broadcastframe_g += readl(mmcaddr +
+					      MMC_TX_BROADCASTFRAME_G);
+	mmc->mmc_tx_multicastframe_g += readl(mmcaddr +
+					      MMC_TX_MULTICASTFRAME_G);
+	mmc->mmc_tx_64_octets_gb += readl(mmcaddr + MMC_TX_64_OCTETS_GB);
+	mmc->mmc_tx_65_to_127_octets_gb +=
+	    readl(mmcaddr + MMC_TX_65_TO_127_OCTETS_GB);
+	mmc->mmc_tx_128_to_255_octets_gb +=
+	    readl(mmcaddr + MMC_TX_128_TO_255_OCTETS_GB);
+	mmc->mmc_tx_256_to_511_octets_gb +=
+	    readl(mmcaddr + MMC_TX_256_TO_511_OCTETS_GB);
+	mmc->mmc_tx_512_to_1023_octets_gb +=
+	    readl(mmcaddr + MMC_TX_512_TO_1023_OCTETS_GB);
+	mmc->mmc_tx_1024_to_max_octets_gb +=
+	    readl(mmcaddr + MMC_TX_1024_TO_MAX_OCTETS_GB);
+	mmc->mmc_tx_unicast_gb += readl(mmcaddr + MMC_TX_UNICAST_GB);
+	mmc->mmc_tx_multicast_gb += readl(mmcaddr + MMC_TX_MULTICAST_GB);
+	mmc->mmc_tx_broadcast_gb += readl(mmcaddr + MMC_TX_BROADCAST_GB);
+	mmc->mmc_tx_underflow_error += readl(mmcaddr + MMC_TX_UNDERFLOW_ERROR);
+	mmc->mmc_tx_singlecol_g += readl(mmcaddr + MMC_TX_SINGLECOL_G);
+	mmc->mmc_tx_multicol_g += readl(mmcaddr + MMC_TX_MULTICOL_G);
+	mmc->mmc_tx_deferred += readl(mmcaddr + MMC_TX_DEFERRED);
+	mmc->mmc_tx_latecol += readl(mmcaddr + MMC_TX_LATECOL);
+	mmc->mmc_tx_exesscol += readl(mmcaddr + MMC_TX_EXESSCOL);
+	mmc->mmc_tx_carrier_error += readl(mmcaddr + MMC_TX_CARRIER_ERROR);
+	mmc->mmc_tx_octetcount_g += readl(mmcaddr + MMC_TX_OCTETCOUNT_G);
+	mmc->mmc_tx_framecount_g += readl(mmcaddr + MMC_TX_FRAMECOUNT_G);
+	mmc->mmc_tx_excessdef += readl(mmcaddr + MMC_TX_EXCESSDEF);
+	mmc->mmc_tx_pause_frame += readl(mmcaddr + MMC_TX_PAUSE_FRAME);
+	mmc->mmc_tx_vlan_frame_g += readl(mmcaddr + MMC_TX_VLAN_FRAME_G);
+
+	/* MMC RX counter registers */
+	mmc->mmc_rx_framecount_gb += readl(mmcaddr + MMC_RX_FRAMECOUNT_GB);
+	mmc->mmc_rx_octetcount_gb += readl(mmcaddr + MMC_RX_OCTETCOUNT_GB);
+	mmc->mmc_rx_octetcount_g += readl(mmcaddr + MMC_RX_OCTETCOUNT_G);
+	mmc->mmc_rx_broadcastframe_g += readl(mmcaddr +
+					      MMC_RX_BROADCASTFRAME_G);
+	mmc->mmc_rx_multicastframe_g += readl(mmcaddr +
+					      MMC_RX_MULTICASTFRAME_G);
+	mmc->mmc_rx_crc_error += readl(mmcaddr + MMC_RX_CRC_ERROR);
+	mmc->mmc_rx_align_error += readl(mmcaddr + MMC_RX_ALIGN_ERROR);
+	mmc->mmc_rx_run_error += readl(mmcaddr + MMC_RX_RUN_ERROR);
+	mmc->mmc_rx_jabber_error += readl(mmcaddr + MMC_RX_JABBER_ERROR);
+	mmc->mmc_rx_undersize_g += readl(mmcaddr + MMC_RX_UNDERSIZE_G);
+	mmc->mmc_rx_oversize_g += readl(mmcaddr + MMC_RX_OVERSIZE_G);
+	mmc->mmc_rx_64_octets_gb += readl(mmcaddr + MMC_RX_64_OCTETS_GB);
+	mmc->mmc_rx_65_to_127_octets_gb +=
+	    readl(mmcaddr + MMC_RX_65_TO_127_OCTETS_GB);
+	mmc->mmc_rx_128_to_255_octets_gb +=
+	    readl(mmcaddr + MMC_RX_128_TO_255_OCTETS_GB);
+	mmc->mmc_rx_256_to_511_octets_gb +=
+	    readl(mmcaddr + MMC_RX_256_TO_511_OCTETS_GB);
+	mmc->mmc_rx_512_to_1023_octets_gb +=
+	    readl(mmcaddr + MMC_RX_512_TO_1023_OCTETS_GB);
+	mmc->mmc_rx_1024_to_max_octets_gb +=
+	    readl(mmcaddr + MMC_RX_1024_TO_MAX_OCTETS_GB);
+	mmc->mmc_rx_unicast_g += readl(mmcaddr + MMC_RX_UNICAST_G);
+	mmc->mmc_rx_length_error += readl(mmcaddr + MMC_RX_LENGTH_ERROR);
+	mmc->mmc_rx_autofrangetype += readl(mmcaddr + MMC_RX_AUTOFRANGETYPE);
+	mmc->mmc_rx_pause_frames += readl(mmcaddr + MMC_RX_PAUSE_FRAMES);
+	mmc->mmc_rx_fifo_overflow += readl(mmcaddr + MMC_RX_FIFO_OVERFLOW);
+	mmc->mmc_rx_vlan_frames_gb += readl(mmcaddr + MMC_RX_VLAN_FRAMES_GB);
+	mmc->mmc_rx_watchdog_error += readl(mmcaddr + MMC_RX_WATCHDOG_ERROR);
+	/* IPC */
+	mmc->mmc_rx_ipc_intr_mask += readl(mmcaddr + MMC_RX_IPC_INTR_MASK);
+	mmc->mmc_rx_ipc_intr += readl(mmcaddr + MMC_RX_IPC_INTR);
+	/* IPv4 */
+	mmc->mmc_rx_ipv4_gd += readl(mmcaddr + MMC_RX_IPV4_GD);
+	mmc->mmc_rx_ipv4_hderr += readl(mmcaddr + MMC_RX_IPV4_HDERR);
+	mmc->mmc_rx_ipv4_nopay += readl(mmcaddr + MMC_RX_IPV4_NOPAY);
+	mmc->mmc_rx_ipv4_frag += readl(mmcaddr + MMC_RX_IPV4_FRAG);
+	mmc->mmc_rx_ipv4_udsbl += readl(mmcaddr + MMC_RX_IPV4_UDSBL);
+
+	mmc->mmc_rx_ipv4_gd_octets += readl(mmcaddr + MMC_RX_IPV4_GD_OCTETS);
+	mmc->mmc_rx_ipv4_hderr_octets +=
+	    readl(mmcaddr + MMC_RX_IPV4_HDERR_OCTETS);
+	mmc->mmc_rx_ipv4_nopay_octets +=
+	    readl(mmcaddr + MMC_RX_IPV4_NOPAY_OCTETS);
+	mmc->mmc_rx_ipv4_frag_octets += readl(mmcaddr +
+					      MMC_RX_IPV4_FRAG_OCTETS);
+	mmc->mmc_rx_ipv4_udsbl_octets +=
+	    readl(mmcaddr + MMC_RX_IPV4_UDSBL_OCTETS);
+
+	/* IPV6 */
+	mmc->mmc_rx_ipv6_gd_octets += readl(mmcaddr + MMC_RX_IPV6_GD_OCTETS);
+	mmc->mmc_rx_ipv6_hderr_octets +=
+	    readl(mmcaddr + MMC_RX_IPV6_HDERR_OCTETS);
+	mmc->mmc_rx_ipv6_nopay_octets +=
+	    readl(mmcaddr + MMC_RX_IPV6_NOPAY_OCTETS);
+
+	mmc->mmc_rx_ipv6_gd += readl(mmcaddr + MMC_RX_IPV6_GD);
+	mmc->mmc_rx_ipv6_hderr += readl(mmcaddr + MMC_RX_IPV6_HDERR);
+	mmc->mmc_rx_ipv6_nopay += readl(mmcaddr + MMC_RX_IPV6_NOPAY);
+
+	/* Protocols */
+	mmc->mmc_rx_udp_gd += readl(mmcaddr + MMC_RX_UDP_GD);
+	mmc->mmc_rx_udp_err += readl(mmcaddr + MMC_RX_UDP_ERR);
+	mmc->mmc_rx_tcp_gd += readl(mmcaddr + MMC_RX_TCP_GD);
+	mmc->mmc_rx_tcp_err += readl(mmcaddr + MMC_RX_TCP_ERR);
+	mmc->mmc_rx_icmp_gd += readl(mmcaddr + MMC_RX_ICMP_GD);
+	mmc->mmc_rx_icmp_err += readl(mmcaddr + MMC_RX_ICMP_ERR);
+
+	mmc->mmc_rx_udp_gd_octets += readl(mmcaddr + MMC_RX_UDP_GD_OCTETS);
+	mmc->mmc_rx_udp_err_octets += readl(mmcaddr + MMC_RX_UDP_ERR_OCTETS);
+	mmc->mmc_rx_tcp_gd_octets += readl(mmcaddr + MMC_RX_TCP_GD_OCTETS);
+	mmc->mmc_rx_tcp_err_octets += readl(mmcaddr + MMC_RX_TCP_ERR_OCTETS);
+	mmc->mmc_rx_icmp_gd_octets += readl(mmcaddr + MMC_RX_ICMP_GD_OCTETS);
+	mmc->mmc_rx_icmp_err_octets += readl(mmcaddr + MMC_RX_ICMP_ERR_OCTETS);
+
+	mmc->mmc_tx_fpe_fragment_cntr += readl(mmcaddr + MMC_TX_FPE_FRAG);
+	mmc->mmc_tx_hold_req_cntr += readl(mmcaddr + MMC_TX_HOLD_REQ);
+	mmc->mmc_rx_packet_assembly_err_cntr +=
+		readl(mmcaddr + MMC_RX_PKT_ASSEMBLY_ERR);
+	mmc->mmc_rx_packet_smd_err_cntr += readl(mmcaddr + MMC_RX_PKT_SMD_ERR);
+	mmc->mmc_rx_packet_assembly_ok_cntr +=
+		readl(mmcaddr + MMC_RX_PKT_ASSEMBLY_OK);
+	mmc->mmc_rx_fpe_fragment_cntr += readl(mmcaddr + MMC_RX_FPE_FRAG);
+}
+
+const struct stmmac_mmc_ops dwmac_mmc_ops = {
+	.ctrl = dwmac_mmc_ctrl,
+	.intr_all_mask = dwmac_mmc_intr_all_mask,
+	.read = dwmac_mmc_read,
+};
+
+static void dwxgmac_mmc_ctrl(void __iomem *mmcaddr, unsigned int mode)
+{
+	u32 value = readl(mmcaddr + MMC_CNTRL);
+
+	value |= (mode & 0x3F);
+
+	writel(value, mmcaddr + MMC_CNTRL);
+}
+
+static void dwxgmac_mmc_intr_all_mask(void __iomem *mmcaddr)
+{
+	writel(0x0, mmcaddr + MMC_RX_INTR_MASK);
+	writel(0x0, mmcaddr + MMC_TX_INTR_MASK);
+	writel(MMC_DEFAULT_MASK, mmcaddr + MMC_XGMAC_RX_IPC_INTR_MASK);
+}
+
+static void dwxgmac_read_mmc_reg(void __iomem *addr, u32 reg, u32 *dest)
+{
+	u64 tmp = 0;
+
+	tmp += readl(addr + reg);
+	tmp += ((u64 )readl(addr + reg + 0x4)) << 32;
+	if (tmp > GENMASK(31, 0))
+		*dest = ~0x0;
+	else
+		*dest = *dest + tmp;
+}
+
+/* This reads the MAC core counters (if actaully supported).
+ * by default the MMC core is programmed to reset each
+ * counter after a read. So all the field of the mmc struct
+ * have to be incremented.
+ */
+static void dwxgmac_mmc_read(void __iomem *mmcaddr, struct stmmac_counters *mmc)
+{
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_OCTET_GB,
+			     &mmc->mmc_tx_octetcount_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_PKT_GB,
+			     &mmc->mmc_tx_framecount_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_BROAD_PKT_G,
+			     &mmc->mmc_tx_broadcastframe_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_MULTI_PKT_G,
+			     &mmc->mmc_tx_multicastframe_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_64OCT_GB,
+			     &mmc->mmc_tx_64_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_65OCT_GB,
+			     &mmc->mmc_tx_65_to_127_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_128OCT_GB,
+			     &mmc->mmc_tx_128_to_255_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_256OCT_GB,
+			     &mmc->mmc_tx_256_to_511_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_512OCT_GB,
+			     &mmc->mmc_tx_512_to_1023_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_1024OCT_GB,
+			     &mmc->mmc_tx_1024_to_max_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_UNI_PKT_GB,
+			     &mmc->mmc_tx_unicast_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_MULTI_PKT_GB,
+			     &mmc->mmc_tx_multicast_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_BROAD_PKT_GB,
+			     &mmc->mmc_tx_broadcast_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_UNDER,
+			     &mmc->mmc_tx_underflow_error);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_OCTET_G,
+			     &mmc->mmc_tx_octetcount_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_PKT_G,
+			     &mmc->mmc_tx_framecount_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_PAUSE,
+			     &mmc->mmc_tx_pause_frame);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_TX_VLAN_PKT_G,
+			     &mmc->mmc_tx_vlan_frame_g);
+
+	/* MMC RX counter registers */
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_PKT_GB,
+			     &mmc->mmc_rx_framecount_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_OCTET_GB,
+			     &mmc->mmc_rx_octetcount_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_OCTET_G,
+			     &mmc->mmc_rx_octetcount_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_BROAD_PKT_G,
+			     &mmc->mmc_rx_broadcastframe_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_MULTI_PKT_G,
+			     &mmc->mmc_rx_multicastframe_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_CRC_ERR,
+			     &mmc->mmc_rx_crc_error);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_CRC_ERR,
+			     &mmc->mmc_rx_crc_error);
+	mmc->mmc_rx_run_error += readl(mmcaddr + MMC_XGMAC_RX_RUNT_ERR);
+	mmc->mmc_rx_jabber_error += readl(mmcaddr + MMC_XGMAC_RX_JABBER_ERR);
+	mmc->mmc_rx_undersize_g += readl(mmcaddr + MMC_XGMAC_RX_UNDER);
+	mmc->mmc_rx_oversize_g += readl(mmcaddr + MMC_XGMAC_RX_OVER);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_64OCT_GB,
+			     &mmc->mmc_rx_64_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_65OCT_GB,
+			     &mmc->mmc_rx_65_to_127_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_128OCT_GB,
+			     &mmc->mmc_rx_128_to_255_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_256OCT_GB,
+			     &mmc->mmc_rx_256_to_511_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_512OCT_GB,
+			     &mmc->mmc_rx_512_to_1023_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_1024OCT_GB,
+			     &mmc->mmc_rx_1024_to_max_octets_gb);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_UNI_PKT_G,
+			     &mmc->mmc_rx_unicast_g);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_LENGTH_ERR,
+			     &mmc->mmc_rx_length_error);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_RANGE,
+			     &mmc->mmc_rx_autofrangetype);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_PAUSE,
+			     &mmc->mmc_rx_pause_frames);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_FIFOOVER_PKT,
+			     &mmc->mmc_rx_fifo_overflow);
+	dwxgmac_read_mmc_reg(mmcaddr, MMC_XGMAC_RX_VLAN_PKT_GB,
+			     &mmc->mmc_rx_vlan_frames_gb);
+	mmc->mmc_rx_watchdog_error += readl(mmcaddr + MMC_XGMAC_RX_WATCHDOG_ERR);
+
+	mmc->mmc_tx_fpe_fragment_cntr += readl(mmcaddr + MMC_XGMAC_TX_FPE_FRAG);
+	mmc->mmc_tx_hold_req_cntr += readl(mmcaddr + MMC_XGMAC_TX_HOLD_REQ);
+	mmc->mmc_rx_packet_assembly_err_cntr +=
+		readl(mmcaddr + MMC_XGMAC_RX_PKT_ASSEMBLY_ERR);
+	mmc->mmc_rx_packet_smd_err_cntr +=
+		readl(mmcaddr + MMC_XGMAC_RX_PKT_SMD_ERR);
+	mmc->mmc_rx_packet_assembly_ok_cntr +=
+		readl(mmcaddr + MMC_XGMAC_RX_PKT_ASSEMBLY_OK);
+	mmc->mmc_rx_fpe_fragment_cntr +=
+		readl(mmcaddr + MMC_XGMAC_RX_FPE_FRAG);
+}
+
+const struct stmmac_mmc_ops dwxgmac_mmc_ops = {
+	.ctrl = dwxgmac_mmc_ctrl,
+	.intr_all_mask = dwxgmac_mmc_intr_all_mask,
+	.read = dwxgmac_mmc_read,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/mmc.h b/net/rtnet/drivers/orange-pi-one/mmc.h
--- a/net/rtnet/drivers/orange-pi-one/mmc.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/mmc.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,130 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  MMC Header file
+
+  Copyright (C) 2011  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __MMC_H__
+#define __MMC_H__
+
+/* MMC control register */
+/* When set, all counter are reset */
+#define MMC_CNTRL_COUNTER_RESET		0x1
+/* When set, do not roll over zero after reaching the max value*/
+#define MMC_CNTRL_COUNTER_STOP_ROLLOVER	0x2
+#define MMC_CNTRL_RESET_ON_READ		0x4	/* Reset after reading */
+#define MMC_CNTRL_COUNTER_FREEZER	0x8	/* Freeze counter values to the
+						 * current value.*/
+#define MMC_CNTRL_PRESET		0x10
+#define MMC_CNTRL_FULL_HALF_PRESET	0x20
+
+#define MMC_GMAC4_OFFSET		0x700
+#define MMC_GMAC3_X_OFFSET		0x100
+#define MMC_XGMAC_OFFSET		0x800
+
+struct stmmac_counters {
+	unsigned int mmc_tx_octetcount_gb;
+	unsigned int mmc_tx_framecount_gb;
+	unsigned int mmc_tx_broadcastframe_g;
+	unsigned int mmc_tx_multicastframe_g;
+	unsigned int mmc_tx_64_octets_gb;
+	unsigned int mmc_tx_65_to_127_octets_gb;
+	unsigned int mmc_tx_128_to_255_octets_gb;
+	unsigned int mmc_tx_256_to_511_octets_gb;
+	unsigned int mmc_tx_512_to_1023_octets_gb;
+	unsigned int mmc_tx_1024_to_max_octets_gb;
+	unsigned int mmc_tx_unicast_gb;
+	unsigned int mmc_tx_multicast_gb;
+	unsigned int mmc_tx_broadcast_gb;
+	unsigned int mmc_tx_underflow_error;
+	unsigned int mmc_tx_singlecol_g;
+	unsigned int mmc_tx_multicol_g;
+	unsigned int mmc_tx_deferred;
+	unsigned int mmc_tx_latecol;
+	unsigned int mmc_tx_exesscol;
+	unsigned int mmc_tx_carrier_error;
+	unsigned int mmc_tx_octetcount_g;
+	unsigned int mmc_tx_framecount_g;
+	unsigned int mmc_tx_excessdef;
+	unsigned int mmc_tx_pause_frame;
+	unsigned int mmc_tx_vlan_frame_g;
+
+	/* MMC RX counter registers */
+	unsigned int mmc_rx_framecount_gb;
+	unsigned int mmc_rx_octetcount_gb;
+	unsigned int mmc_rx_octetcount_g;
+	unsigned int mmc_rx_broadcastframe_g;
+	unsigned int mmc_rx_multicastframe_g;
+	unsigned int mmc_rx_crc_error;
+	unsigned int mmc_rx_align_error;
+	unsigned int mmc_rx_run_error;
+	unsigned int mmc_rx_jabber_error;
+	unsigned int mmc_rx_undersize_g;
+	unsigned int mmc_rx_oversize_g;
+	unsigned int mmc_rx_64_octets_gb;
+	unsigned int mmc_rx_65_to_127_octets_gb;
+	unsigned int mmc_rx_128_to_255_octets_gb;
+	unsigned int mmc_rx_256_to_511_octets_gb;
+	unsigned int mmc_rx_512_to_1023_octets_gb;
+	unsigned int mmc_rx_1024_to_max_octets_gb;
+	unsigned int mmc_rx_unicast_g;
+	unsigned int mmc_rx_length_error;
+	unsigned int mmc_rx_autofrangetype;
+	unsigned int mmc_rx_pause_frames;
+	unsigned int mmc_rx_fifo_overflow;
+	unsigned int mmc_rx_vlan_frames_gb;
+	unsigned int mmc_rx_watchdog_error;
+	/* IPC */
+	unsigned int mmc_rx_ipc_intr_mask;
+	unsigned int mmc_rx_ipc_intr;
+	/* IPv4 */
+	unsigned int mmc_rx_ipv4_gd;
+	unsigned int mmc_rx_ipv4_hderr;
+	unsigned int mmc_rx_ipv4_nopay;
+	unsigned int mmc_rx_ipv4_frag;
+	unsigned int mmc_rx_ipv4_udsbl;
+
+	unsigned int mmc_rx_ipv4_gd_octets;
+	unsigned int mmc_rx_ipv4_hderr_octets;
+	unsigned int mmc_rx_ipv4_nopay_octets;
+	unsigned int mmc_rx_ipv4_frag_octets;
+	unsigned int mmc_rx_ipv4_udsbl_octets;
+
+	/* IPV6 */
+	unsigned int mmc_rx_ipv6_gd_octets;
+	unsigned int mmc_rx_ipv6_hderr_octets;
+	unsigned int mmc_rx_ipv6_nopay_octets;
+
+	unsigned int mmc_rx_ipv6_gd;
+	unsigned int mmc_rx_ipv6_hderr;
+	unsigned int mmc_rx_ipv6_nopay;
+
+	/* Protocols */
+	unsigned int mmc_rx_udp_gd;
+	unsigned int mmc_rx_udp_err;
+	unsigned int mmc_rx_tcp_gd;
+	unsigned int mmc_rx_tcp_err;
+	unsigned int mmc_rx_icmp_gd;
+	unsigned int mmc_rx_icmp_err;
+
+	unsigned int mmc_rx_udp_gd_octets;
+	unsigned int mmc_rx_udp_err_octets;
+	unsigned int mmc_rx_tcp_gd_octets;
+	unsigned int mmc_rx_tcp_err_octets;
+	unsigned int mmc_rx_icmp_gd_octets;
+	unsigned int mmc_rx_icmp_err_octets;
+
+	/* FPE */
+	unsigned int mmc_tx_fpe_fragment_cntr;
+	unsigned int mmc_tx_hold_req_cntr;
+	unsigned int mmc_rx_packet_assembly_err_cntr;
+	unsigned int mmc_rx_packet_smd_err_cntr;
+	unsigned int mmc_rx_packet_assembly_ok_cntr;
+	unsigned int mmc_rx_fpe_fragment_cntr;
+};
+
+#endif /* __MMC_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/norm_desc.c b/net/rtnet/drivers/orange-pi-one/norm_desc.c
--- a/net/rtnet/drivers/orange-pi-one/norm_desc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/norm_desc.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,329 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This contains the functions to handle the normal descriptors.
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/stmmac.h>
+#include "common.h"
+#include "descs_com.h"
+
+static int ndesc_get_tx_status(void *data, struct stmmac_extra_stats *x,
+			       struct dma_desc *p, void __iomem *ioaddr)
+{
+	struct net_device_stats *stats = (struct net_device_stats *)data;
+	unsigned int tdes0 = le32_to_cpu(p->des0);
+	unsigned int tdes1 = le32_to_cpu(p->des1);
+	int ret = tx_done;
+
+	/* Get tx owner first */
+	if (unlikely(tdes0 & TDES0_OWN))
+		return tx_dma_own;
+
+	/* Verify tx error by looking at the last segment. */
+	if (likely(!(tdes1 & TDES1_LAST_SEGMENT)))
+		return tx_not_ls;
+
+	if (unlikely(tdes0 & TDES0_ERROR_SUMMARY)) {
+		if (unlikely(tdes0 & TDES0_UNDERFLOW_ERROR)) {
+			x->tx_underflow++;
+			stats->tx_fifo_errors++;
+		}
+		if (unlikely(tdes0 & TDES0_NO_CARRIER)) {
+			x->tx_carrier++;
+			stats->tx_carrier_errors++;
+		}
+		if (unlikely(tdes0 & TDES0_LOSS_CARRIER)) {
+			x->tx_losscarrier++;
+			stats->tx_carrier_errors++;
+		}
+		if (unlikely((tdes0 & TDES0_EXCESSIVE_DEFERRAL) ||
+			     (tdes0 & TDES0_EXCESSIVE_COLLISIONS) ||
+			     (tdes0 & TDES0_LATE_COLLISION))) {
+			unsigned int collisions;
+
+			collisions = (tdes0 & TDES0_COLLISION_COUNT_MASK) >> 3;
+			stats->collisions += collisions;
+		}
+		ret = tx_err;
+	}
+
+	if (tdes0 & TDES0_VLAN_FRAME)
+		x->tx_vlan++;
+
+	if (unlikely(tdes0 & TDES0_DEFERRED))
+		x->tx_deferred++;
+
+	return ret;
+}
+
+static int ndesc_get_tx_len(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des1) & RDES1_BUFFER1_SIZE_MASK);
+}
+
+/* This function verifies if each incoming frame has some errors
+ * and, if required, updates the multicast statistics.
+ * In case of success, it returns good_frame because the GMAC device
+ * is supposed to be able to compute the csum in HW. */
+static int ndesc_get_rx_status(void *data, struct stmmac_extra_stats *x,
+			       struct dma_desc *p)
+{
+	int ret = good_frame;
+	unsigned int rdes0 = le32_to_cpu(p->des0);
+	struct net_device_stats *stats = (struct net_device_stats *)data;
+
+	if (unlikely(rdes0 & RDES0_OWN))
+		return dma_own;
+
+	if (unlikely(!(rdes0 & RDES0_LAST_DESCRIPTOR))) {
+		stats->rx_length_errors++;
+		return discard_frame;
+	}
+
+	if (unlikely(rdes0 & RDES0_ERROR_SUMMARY)) {
+		if (unlikely(rdes0 & RDES0_DESCRIPTOR_ERROR))
+			x->rx_desc++;
+		if (unlikely(rdes0 & RDES0_SA_FILTER_FAIL))
+			x->sa_filter_fail++;
+		if (unlikely(rdes0 & RDES0_OVERFLOW_ERROR))
+			x->overflow_error++;
+		if (unlikely(rdes0 & RDES0_IPC_CSUM_ERROR))
+			x->ipc_csum_error++;
+		if (unlikely(rdes0 & RDES0_COLLISION)) {
+			x->rx_collision++;
+			stats->collisions++;
+		}
+		if (unlikely(rdes0 & RDES0_CRC_ERROR)) {
+			x->rx_crc_errors++;
+			stats->rx_crc_errors++;
+		}
+		ret = discard_frame;
+	}
+	if (unlikely(rdes0 & RDES0_DRIBBLING))
+		x->dribbling_bit++;
+
+	if (unlikely(rdes0 & RDES0_LENGTH_ERROR)) {
+		x->rx_length++;
+		ret = discard_frame;
+	}
+	if (unlikely(rdes0 & RDES0_MII_ERROR)) {
+		x->rx_mii++;
+		ret = discard_frame;
+	}
+#ifdef STMMAC_VLAN_TAG_USED
+	if (rdes0 & RDES0_VLAN_TAG)
+		x->vlan_tag++;
+#endif
+	return ret;
+}
+
+static void ndesc_init_rx_desc(struct dma_desc *p, int disable_rx_ic, int mode,
+			       int end, int bfsize)
+{
+	int bfsize1;
+
+	p->des0 |= cpu_to_le32(RDES0_OWN);
+
+	bfsize1 = min(bfsize, BUF_SIZE_2KiB - 1);
+	p->des1 |= cpu_to_le32(bfsize1 & RDES1_BUFFER1_SIZE_MASK);
+
+	if (mode == STMMAC_CHAIN_MODE)
+		ndesc_rx_set_on_chain(p, end);
+	else
+		ndesc_rx_set_on_ring(p, end, bfsize);
+
+	if (disable_rx_ic)
+		p->des1 |= cpu_to_le32(RDES1_DISABLE_IC);
+}
+
+static void ndesc_init_tx_desc(struct dma_desc *p, int mode, int end)
+{
+	p->des0 &= cpu_to_le32(~TDES0_OWN);
+	if (mode == STMMAC_CHAIN_MODE)
+		ndesc_tx_set_on_chain(p);
+	else
+		ndesc_end_tx_desc_on_ring(p, end);
+}
+
+static int ndesc_get_tx_owner(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des0) & TDES0_OWN) >> 31;
+}
+
+static void ndesc_set_tx_owner(struct dma_desc *p)
+{
+	p->des0 |= cpu_to_le32(TDES0_OWN);
+}
+
+static void ndesc_set_rx_owner(struct dma_desc *p, int disable_rx_ic)
+{
+	p->des0 |= cpu_to_le32(RDES0_OWN);
+}
+
+static int ndesc_get_tx_ls(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des1) & TDES1_LAST_SEGMENT) >> 30;
+}
+
+static void ndesc_release_tx_desc(struct dma_desc *p, int mode)
+{
+	int ter = (le32_to_cpu(p->des1) & TDES1_END_RING) >> 25;
+
+	memset(p, 0, offsetof(struct dma_desc, des2));
+	if (mode == STMMAC_CHAIN_MODE)
+		ndesc_tx_set_on_chain(p);
+	else
+		ndesc_end_tx_desc_on_ring(p, ter);
+}
+
+static void ndesc_prepare_tx_desc(struct dma_desc *p, int is_fs, int len,
+				  bool csum_flag, int mode, bool tx_own,
+				  bool ls, unsigned int tot_pkt_len)
+{
+	unsigned int tdes1 = le32_to_cpu(p->des1);
+
+	if (is_fs)
+		tdes1 |= TDES1_FIRST_SEGMENT;
+	else
+		tdes1 &= ~TDES1_FIRST_SEGMENT;
+
+	if (likely(csum_flag))
+		tdes1 |= (TX_CIC_FULL) << TDES1_CHECKSUM_INSERTION_SHIFT;
+	else
+		tdes1 &= ~(TX_CIC_FULL << TDES1_CHECKSUM_INSERTION_SHIFT);
+
+	if (ls)
+		tdes1 |= TDES1_LAST_SEGMENT;
+
+	p->des1 = cpu_to_le32(tdes1);
+
+	if (mode == STMMAC_CHAIN_MODE)
+		norm_set_tx_desc_len_on_chain(p, len);
+	else
+		norm_set_tx_desc_len_on_ring(p, len);
+
+	if (tx_own)
+		p->des0 |= cpu_to_le32(TDES0_OWN);
+}
+
+static void ndesc_set_tx_ic(struct dma_desc *p)
+{
+	p->des1 |= cpu_to_le32(TDES1_INTERRUPT);
+}
+
+static int ndesc_get_rx_frame_len(struct dma_desc *p, int rx_coe_type)
+{
+	unsigned int csum = 0;
+
+	/* The type-1 checksum offload engines append the checksum at
+	 * the end of frame and the two bytes of checksum are added in
+	 * the length.
+	 * Adjust for that in the framelen for type-1 checksum offload
+	 * engines
+	 */
+	if (rx_coe_type == STMMAC_RX_COE_TYPE1)
+		csum = 2;
+
+	return (((le32_to_cpu(p->des0) & RDES0_FRAME_LEN_MASK)
+				>> RDES0_FRAME_LEN_SHIFT) -
+		csum);
+
+}
+
+static void ndesc_enable_tx_timestamp(struct dma_desc *p)
+{
+	p->des1 |= cpu_to_le32(TDES1_TIME_STAMP_ENABLE);
+}
+
+static int ndesc_get_tx_timestamp_status(struct dma_desc *p)
+{
+	return (le32_to_cpu(p->des0) & TDES0_TIME_STAMP_STATUS) >> 17;
+}
+
+static void ndesc_get_timestamp(void *desc, u32 ats, u64 *ts)
+{
+	struct dma_desc *p = (struct dma_desc *)desc;
+	u64 ns;
+
+	ns = le32_to_cpu(p->des2);
+	/* convert high/sec time stamp value to nanosecond */
+	ns += le32_to_cpu(p->des3) * 1000000000ULL;
+
+	*ts = ns;
+}
+
+static int ndesc_get_rx_timestamp_status(void *desc, void *next_desc, u32 ats)
+{
+	struct dma_desc *p = (struct dma_desc *)desc;
+
+	if ((le32_to_cpu(p->des2) == 0xffffffff) &&
+	    (le32_to_cpu(p->des3) == 0xffffffff))
+		/* timestamp is corrupted, hence don't store it */
+		return 0;
+	else
+		return 1;
+}
+
+static void ndesc_display_ring(void *head, unsigned int size, bool rx)
+{
+	struct dma_desc *p = (struct dma_desc *)head;
+	int i;
+
+	pr_info("%s descriptor ring:\n", rx ? "RX" : "TX");
+
+	for (i = 0; i < size; i++) {
+		u64 x;
+
+		x = *(u64 *)p;
+		pr_info("%03d [0x%x]: 0x%x 0x%x 0x%x 0x%x",
+			i, (unsigned int)virt_to_phys(p),
+			(unsigned int)x, (unsigned int)(x >> 32),
+			p->des2, p->des3);
+		p++;
+	}
+	pr_info("\n");
+}
+
+static void ndesc_get_addr(struct dma_desc *p, unsigned int *addr)
+{
+	*addr = le32_to_cpu(p->des2);
+}
+
+static void ndesc_set_addr(struct dma_desc *p, dma_addr_t addr)
+{
+	p->des2 = cpu_to_le32(addr);
+}
+
+static void ndesc_clear(struct dma_desc *p)
+{
+	p->des2 = 0;
+}
+
+const struct stmmac_desc_ops ndesc_ops = {
+	.tx_status = ndesc_get_tx_status,
+	.rx_status = ndesc_get_rx_status,
+	.get_tx_len = ndesc_get_tx_len,
+	.init_rx_desc = ndesc_init_rx_desc,
+	.init_tx_desc = ndesc_init_tx_desc,
+	.get_tx_owner = ndesc_get_tx_owner,
+	.release_tx_desc = ndesc_release_tx_desc,
+	.prepare_tx_desc = ndesc_prepare_tx_desc,
+	.set_tx_ic = ndesc_set_tx_ic,
+	.get_tx_ls = ndesc_get_tx_ls,
+	.set_tx_owner = ndesc_set_tx_owner,
+	.set_rx_owner = ndesc_set_rx_owner,
+	.get_rx_frame_len = ndesc_get_rx_frame_len,
+	.enable_tx_timestamp = ndesc_enable_tx_timestamp,
+	.get_tx_timestamp_status = ndesc_get_tx_timestamp_status,
+	.get_timestamp = ndesc_get_timestamp,
+	.get_rx_timestamp_status = ndesc_get_rx_timestamp_status,
+	.display_ring = ndesc_display_ring,
+	.get_addr = ndesc_get_addr,
+	.set_addr = ndesc_set_addr,
+	.clear = ndesc_clear,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/ring_mode.c b/net/rtnet/drivers/orange-pi-one/ring_mode.c
--- a/net/rtnet/drivers/orange-pi-one/ring_mode.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/ring_mode.c	2021-07-14 15:39:13.306124997 +0300
@@ -0,0 +1,148 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  Specialised functions for managing Ring mode
+
+  Copyright(C) 2011  STMicroelectronics Ltd
+
+  It defines all the functions used to handle the normal/enhanced
+  descriptors in case of the DMA is configured to work in chained or
+  in ring mode.
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include "stmmac.h"
+
+static int jumbo_frm(void *p, struct sk_buff *skb, int csum)
+{
+	struct stmmac_tx_queue *tx_q = (struct stmmac_tx_queue *)p;
+	unsigned int nopaged_len = skb_headlen(skb);
+	struct stmmac_priv *priv = tx_q->priv_data;
+	unsigned int entry = tx_q->cur_tx;
+	unsigned int bmax, len, des2;
+	struct dma_desc *desc;
+
+	if (priv->extend_desc)
+		desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+	else
+		desc = tx_q->dma_tx + entry;
+
+	if (priv->plat->enh_desc)
+		bmax = BUF_SIZE_8KiB;
+	else
+		bmax = BUF_SIZE_2KiB;
+
+	len = nopaged_len - bmax;
+
+	if (nopaged_len > BUF_SIZE_8KiB) {
+
+		des2 = dma_map_single(priv->device, skb->data, bmax,
+				      DMA_TO_DEVICE);
+		desc->des2 = cpu_to_le32(des2);
+		if (dma_mapping_error(priv->device, des2))
+			return -1;
+
+		tx_q->tx_skbuff_dma[entry].buf = des2;
+		tx_q->tx_skbuff_dma[entry].len = bmax;
+		tx_q->tx_skbuff_dma[entry].is_jumbo = true;
+
+		desc->des3 = cpu_to_le32(des2 + BUF_SIZE_4KiB);
+		stmmac_prepare_tx_desc(priv, desc, 1, bmax, csum,
+				STMMAC_RING_MODE, 0, false, skb->len);
+		tx_q->tx_skbuff[entry] = NULL;
+		entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
+
+		if (priv->extend_desc)
+			desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+		else
+			desc = tx_q->dma_tx + entry;
+
+		des2 = dma_map_single(priv->device, skb->data + bmax, len,
+				      DMA_TO_DEVICE);
+		desc->des2 = cpu_to_le32(des2);
+		if (dma_mapping_error(priv->device, des2))
+			return -1;
+		tx_q->tx_skbuff_dma[entry].buf = des2;
+		tx_q->tx_skbuff_dma[entry].len = len;
+		tx_q->tx_skbuff_dma[entry].is_jumbo = true;
+
+		desc->des3 = cpu_to_le32(des2 + BUF_SIZE_4KiB);
+		stmmac_prepare_tx_desc(priv, desc, 0, len, csum,
+				STMMAC_RING_MODE, 1, !skb_is_nonlinear(skb),
+				skb->len);
+	} else {
+		des2 = dma_map_single(priv->device, skb->data,
+				      nopaged_len, DMA_TO_DEVICE);
+		desc->des2 = cpu_to_le32(des2);
+		if (dma_mapping_error(priv->device, des2))
+			return -1;
+		tx_q->tx_skbuff_dma[entry].buf = des2;
+		tx_q->tx_skbuff_dma[entry].len = nopaged_len;
+		tx_q->tx_skbuff_dma[entry].is_jumbo = true;
+		desc->des3 = cpu_to_le32(des2 + BUF_SIZE_4KiB);
+		stmmac_prepare_tx_desc(priv, desc, 1, nopaged_len, csum,
+				STMMAC_RING_MODE, 0, !skb_is_nonlinear(skb),
+				skb->len);
+	}
+
+	tx_q->cur_tx = entry;
+
+	return entry;
+}
+
+static unsigned int is_jumbo_frm(int len, int enh_desc)
+{
+	unsigned int ret = 0;
+
+	if (len >= BUF_SIZE_4KiB)
+		ret = 1;
+
+	return ret;
+}
+
+static void refill_desc3(void *priv_ptr, struct dma_desc *p)
+{
+	struct stmmac_rx_queue *rx_q = priv_ptr;
+	struct stmmac_priv *priv = rx_q->priv_data;
+
+	/* Fill DES3 in case of RING mode */
+	if (priv->dma_buf_sz == BUF_SIZE_16KiB)
+		p->des3 = cpu_to_le32(le32_to_cpu(p->des2) + BUF_SIZE_8KiB);
+}
+
+/* In ring mode we need to fill the desc3 because it is used as buffer */
+static void init_desc3(struct dma_desc *p)
+{
+	p->des3 = cpu_to_le32(le32_to_cpu(p->des2) + BUF_SIZE_8KiB);
+}
+
+static void clean_desc3(void *priv_ptr, struct dma_desc *p)
+{
+	struct stmmac_tx_queue *tx_q = (struct stmmac_tx_queue *)priv_ptr;
+	struct stmmac_priv *priv = tx_q->priv_data;
+	unsigned int entry = tx_q->dirty_tx;
+
+	/* des3 is only used for jumbo frames tx or time stamping */
+	if (unlikely(tx_q->tx_skbuff_dma[entry].is_jumbo ||
+		     (tx_q->tx_skbuff_dma[entry].last_segment &&
+		      !priv->extend_desc && priv->hwts_tx_en)))
+		p->des3 = 0;
+}
+
+static int set_16kib_bfsize(int mtu)
+{
+	int ret = 0;
+	if (unlikely(mtu > BUF_SIZE_8KiB))
+		ret = BUF_SIZE_16KiB;
+	return ret;
+}
+
+const struct stmmac_mode_ops ring_mode_ops = {
+	.is_jumbo_frm = is_jumbo_frm,
+	.jumbo_frm = jumbo_frm,
+	.refill_desc3 = refill_desc3,
+	.init_desc3 = init_desc3,
+	.clean_desc3 = clean_desc3,
+	.set_16kib_bfsize = set_16kib_bfsize,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_ethtool.c b/net/rtnet/drivers/orange-pi-one/stmmac_ethtool.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_ethtool.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_ethtool.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,953 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  STMMAC Ethtool support
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/interrupt.h>
+#include <linux/mii.h>
+#include <linux/phylink.h>
+#include <linux/net_tstamp.h>
+#include <asm/io.h>
+
+#include "stmmac.h"
+#include "dwmac_dma.h"
+#include "dwxgmac2.h"
+
+#define REG_SPACE_SIZE	0x1060
+#define MAC100_ETHTOOL_NAME	"st_mac100"
+#define GMAC_ETHTOOL_NAME	"st_gmac"
+#define XGMAC_ETHTOOL_NAME	"st_xgmac"
+
+#define ETHTOOL_DMA_OFFSET	55
+
+struct stmmac_stats {
+	char stat_string[ETH_GSTRING_LEN];
+	int sizeof_stat;
+	int stat_offset;
+};
+
+#define STMMAC_STAT(m)	\
+	{ #m, sizeof_field(struct stmmac_extra_stats, m),	\
+	offsetof(struct stmmac_priv, xstats.m)}
+
+static const struct stmmac_stats stmmac_gstrings_stats[] = {
+	/* Transmit errors */
+	STMMAC_STAT(tx_underflow),
+	STMMAC_STAT(tx_carrier),
+	STMMAC_STAT(tx_losscarrier),
+	STMMAC_STAT(vlan_tag),
+	STMMAC_STAT(tx_deferred),
+	STMMAC_STAT(tx_vlan),
+	STMMAC_STAT(tx_jabber),
+	STMMAC_STAT(tx_frame_flushed),
+	STMMAC_STAT(tx_payload_error),
+	STMMAC_STAT(tx_ip_header_error),
+	/* Receive errors */
+	STMMAC_STAT(rx_desc),
+	STMMAC_STAT(sa_filter_fail),
+	STMMAC_STAT(overflow_error),
+	STMMAC_STAT(ipc_csum_error),
+	STMMAC_STAT(rx_collision),
+	STMMAC_STAT(rx_crc_errors),
+	STMMAC_STAT(dribbling_bit),
+	STMMAC_STAT(rx_length),
+	STMMAC_STAT(rx_mii),
+	STMMAC_STAT(rx_multicast),
+	STMMAC_STAT(rx_gmac_overflow),
+	STMMAC_STAT(rx_watchdog),
+	STMMAC_STAT(da_rx_filter_fail),
+	STMMAC_STAT(sa_rx_filter_fail),
+	STMMAC_STAT(rx_missed_cntr),
+	STMMAC_STAT(rx_overflow_cntr),
+	STMMAC_STAT(rx_vlan),
+	STMMAC_STAT(rx_split_hdr_pkt_n),
+	/* Tx/Rx IRQ error info */
+	STMMAC_STAT(tx_undeflow_irq),
+	STMMAC_STAT(tx_process_stopped_irq),
+	STMMAC_STAT(tx_jabber_irq),
+	STMMAC_STAT(rx_overflow_irq),
+	STMMAC_STAT(rx_buf_unav_irq),
+	STMMAC_STAT(rx_process_stopped_irq),
+	STMMAC_STAT(rx_watchdog_irq),
+	STMMAC_STAT(tx_early_irq),
+	STMMAC_STAT(fatal_bus_error_irq),
+	/* Tx/Rx IRQ Events */
+	STMMAC_STAT(rx_early_irq),
+	STMMAC_STAT(threshold),
+	STMMAC_STAT(tx_pkt_n),
+	STMMAC_STAT(rx_pkt_n),
+	STMMAC_STAT(normal_irq_n),
+	STMMAC_STAT(rx_normal_irq_n),
+	STMMAC_STAT(napi_poll),
+	STMMAC_STAT(tx_normal_irq_n),
+	STMMAC_STAT(tx_clean),
+	STMMAC_STAT(tx_set_ic_bit),
+	STMMAC_STAT(irq_receive_pmt_irq_n),
+	/* MMC info */
+	STMMAC_STAT(mmc_tx_irq_n),
+	STMMAC_STAT(mmc_rx_irq_n),
+	STMMAC_STAT(mmc_rx_csum_offload_irq_n),
+	/* EEE */
+	STMMAC_STAT(irq_tx_path_in_lpi_mode_n),
+	STMMAC_STAT(irq_tx_path_exit_lpi_mode_n),
+	STMMAC_STAT(irq_rx_path_in_lpi_mode_n),
+	STMMAC_STAT(irq_rx_path_exit_lpi_mode_n),
+	STMMAC_STAT(phy_eee_wakeup_error_n),
+	/* Extended RDES status */
+	STMMAC_STAT(ip_hdr_err),
+	STMMAC_STAT(ip_payload_err),
+	STMMAC_STAT(ip_csum_bypassed),
+	STMMAC_STAT(ipv4_pkt_rcvd),
+	STMMAC_STAT(ipv6_pkt_rcvd),
+	STMMAC_STAT(no_ptp_rx_msg_type_ext),
+	STMMAC_STAT(ptp_rx_msg_type_sync),
+	STMMAC_STAT(ptp_rx_msg_type_follow_up),
+	STMMAC_STAT(ptp_rx_msg_type_delay_req),
+	STMMAC_STAT(ptp_rx_msg_type_delay_resp),
+	STMMAC_STAT(ptp_rx_msg_type_pdelay_req),
+	STMMAC_STAT(ptp_rx_msg_type_pdelay_resp),
+	STMMAC_STAT(ptp_rx_msg_type_pdelay_follow_up),
+	STMMAC_STAT(ptp_rx_msg_type_announce),
+	STMMAC_STAT(ptp_rx_msg_type_management),
+	STMMAC_STAT(ptp_rx_msg_pkt_reserved_type),
+	STMMAC_STAT(ptp_frame_type),
+	STMMAC_STAT(ptp_ver),
+	STMMAC_STAT(timestamp_dropped),
+	STMMAC_STAT(av_pkt_rcvd),
+	STMMAC_STAT(av_tagged_pkt_rcvd),
+	STMMAC_STAT(vlan_tag_priority_val),
+	STMMAC_STAT(l3_filter_match),
+	STMMAC_STAT(l4_filter_match),
+	STMMAC_STAT(l3_l4_filter_no_match),
+	/* PCS */
+	STMMAC_STAT(irq_pcs_ane_n),
+	STMMAC_STAT(irq_pcs_link_n),
+	STMMAC_STAT(irq_rgmii_n),
+	/* DEBUG */
+	STMMAC_STAT(mtl_tx_status_fifo_full),
+	STMMAC_STAT(mtl_tx_fifo_not_empty),
+	STMMAC_STAT(mmtl_fifo_ctrl),
+	STMMAC_STAT(mtl_tx_fifo_read_ctrl_write),
+	STMMAC_STAT(mtl_tx_fifo_read_ctrl_wait),
+	STMMAC_STAT(mtl_tx_fifo_read_ctrl_read),
+	STMMAC_STAT(mtl_tx_fifo_read_ctrl_idle),
+	STMMAC_STAT(mac_tx_in_pause),
+	STMMAC_STAT(mac_tx_frame_ctrl_xfer),
+	STMMAC_STAT(mac_tx_frame_ctrl_idle),
+	STMMAC_STAT(mac_tx_frame_ctrl_wait),
+	STMMAC_STAT(mac_tx_frame_ctrl_pause),
+	STMMAC_STAT(mac_gmii_tx_proto_engine),
+	STMMAC_STAT(mtl_rx_fifo_fill_level_full),
+	STMMAC_STAT(mtl_rx_fifo_fill_above_thresh),
+	STMMAC_STAT(mtl_rx_fifo_fill_below_thresh),
+	STMMAC_STAT(mtl_rx_fifo_fill_level_empty),
+	STMMAC_STAT(mtl_rx_fifo_read_ctrl_flush),
+	STMMAC_STAT(mtl_rx_fifo_read_ctrl_read_data),
+	STMMAC_STAT(mtl_rx_fifo_read_ctrl_status),
+	STMMAC_STAT(mtl_rx_fifo_read_ctrl_idle),
+	STMMAC_STAT(mtl_rx_fifo_ctrl_active),
+	STMMAC_STAT(mac_rx_frame_ctrl_fifo),
+	STMMAC_STAT(mac_gmii_rx_proto_engine),
+	/* TSO */
+	STMMAC_STAT(tx_tso_frames),
+	STMMAC_STAT(tx_tso_nfrags),
+};
+#define STMMAC_STATS_LEN ARRAY_SIZE(stmmac_gstrings_stats)
+
+/* HW MAC Management counters (if supported) */
+#define STMMAC_MMC_STAT(m)	\
+	{ #m, sizeof_field(struct stmmac_counters, m),	\
+	offsetof(struct stmmac_priv, mmc.m)}
+
+static const struct stmmac_stats stmmac_mmc[] = {
+	STMMAC_MMC_STAT(mmc_tx_octetcount_gb),
+	STMMAC_MMC_STAT(mmc_tx_framecount_gb),
+	STMMAC_MMC_STAT(mmc_tx_broadcastframe_g),
+	STMMAC_MMC_STAT(mmc_tx_multicastframe_g),
+	STMMAC_MMC_STAT(mmc_tx_64_octets_gb),
+	STMMAC_MMC_STAT(mmc_tx_65_to_127_octets_gb),
+	STMMAC_MMC_STAT(mmc_tx_128_to_255_octets_gb),
+	STMMAC_MMC_STAT(mmc_tx_256_to_511_octets_gb),
+	STMMAC_MMC_STAT(mmc_tx_512_to_1023_octets_gb),
+	STMMAC_MMC_STAT(mmc_tx_1024_to_max_octets_gb),
+	STMMAC_MMC_STAT(mmc_tx_unicast_gb),
+	STMMAC_MMC_STAT(mmc_tx_multicast_gb),
+	STMMAC_MMC_STAT(mmc_tx_broadcast_gb),
+	STMMAC_MMC_STAT(mmc_tx_underflow_error),
+	STMMAC_MMC_STAT(mmc_tx_singlecol_g),
+	STMMAC_MMC_STAT(mmc_tx_multicol_g),
+	STMMAC_MMC_STAT(mmc_tx_deferred),
+	STMMAC_MMC_STAT(mmc_tx_latecol),
+	STMMAC_MMC_STAT(mmc_tx_exesscol),
+	STMMAC_MMC_STAT(mmc_tx_carrier_error),
+	STMMAC_MMC_STAT(mmc_tx_octetcount_g),
+	STMMAC_MMC_STAT(mmc_tx_framecount_g),
+	STMMAC_MMC_STAT(mmc_tx_excessdef),
+	STMMAC_MMC_STAT(mmc_tx_pause_frame),
+	STMMAC_MMC_STAT(mmc_tx_vlan_frame_g),
+	STMMAC_MMC_STAT(mmc_rx_framecount_gb),
+	STMMAC_MMC_STAT(mmc_rx_octetcount_gb),
+	STMMAC_MMC_STAT(mmc_rx_octetcount_g),
+	STMMAC_MMC_STAT(mmc_rx_broadcastframe_g),
+	STMMAC_MMC_STAT(mmc_rx_multicastframe_g),
+	STMMAC_MMC_STAT(mmc_rx_crc_error),
+	STMMAC_MMC_STAT(mmc_rx_align_error),
+	STMMAC_MMC_STAT(mmc_rx_run_error),
+	STMMAC_MMC_STAT(mmc_rx_jabber_error),
+	STMMAC_MMC_STAT(mmc_rx_undersize_g),
+	STMMAC_MMC_STAT(mmc_rx_oversize_g),
+	STMMAC_MMC_STAT(mmc_rx_64_octets_gb),
+	STMMAC_MMC_STAT(mmc_rx_65_to_127_octets_gb),
+	STMMAC_MMC_STAT(mmc_rx_128_to_255_octets_gb),
+	STMMAC_MMC_STAT(mmc_rx_256_to_511_octets_gb),
+	STMMAC_MMC_STAT(mmc_rx_512_to_1023_octets_gb),
+	STMMAC_MMC_STAT(mmc_rx_1024_to_max_octets_gb),
+	STMMAC_MMC_STAT(mmc_rx_unicast_g),
+	STMMAC_MMC_STAT(mmc_rx_length_error),
+	STMMAC_MMC_STAT(mmc_rx_autofrangetype),
+	STMMAC_MMC_STAT(mmc_rx_pause_frames),
+	STMMAC_MMC_STAT(mmc_rx_fifo_overflow),
+	STMMAC_MMC_STAT(mmc_rx_vlan_frames_gb),
+	STMMAC_MMC_STAT(mmc_rx_watchdog_error),
+	STMMAC_MMC_STAT(mmc_rx_ipc_intr_mask),
+	STMMAC_MMC_STAT(mmc_rx_ipc_intr),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_gd),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_hderr),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_nopay),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_frag),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_udsbl),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_gd_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_hderr_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_nopay_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_frag_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv4_udsbl_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv6_gd_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv6_hderr_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv6_nopay_octets),
+	STMMAC_MMC_STAT(mmc_rx_ipv6_gd),
+	STMMAC_MMC_STAT(mmc_rx_ipv6_hderr),
+	STMMAC_MMC_STAT(mmc_rx_ipv6_nopay),
+	STMMAC_MMC_STAT(mmc_rx_udp_gd),
+	STMMAC_MMC_STAT(mmc_rx_udp_err),
+	STMMAC_MMC_STAT(mmc_rx_tcp_gd),
+	STMMAC_MMC_STAT(mmc_rx_tcp_err),
+	STMMAC_MMC_STAT(mmc_rx_icmp_gd),
+	STMMAC_MMC_STAT(mmc_rx_icmp_err),
+	STMMAC_MMC_STAT(mmc_rx_udp_gd_octets),
+	STMMAC_MMC_STAT(mmc_rx_udp_err_octets),
+	STMMAC_MMC_STAT(mmc_rx_tcp_gd_octets),
+	STMMAC_MMC_STAT(mmc_rx_tcp_err_octets),
+	STMMAC_MMC_STAT(mmc_rx_icmp_gd_octets),
+	STMMAC_MMC_STAT(mmc_rx_icmp_err_octets),
+	STMMAC_MMC_STAT(mmc_tx_fpe_fragment_cntr),
+	STMMAC_MMC_STAT(mmc_tx_hold_req_cntr),
+	STMMAC_MMC_STAT(mmc_rx_packet_assembly_err_cntr),
+	STMMAC_MMC_STAT(mmc_rx_packet_smd_err_cntr),
+	STMMAC_MMC_STAT(mmc_rx_packet_assembly_ok_cntr),
+	STMMAC_MMC_STAT(mmc_rx_fpe_fragment_cntr),
+};
+#define STMMAC_MMC_STATS_LEN ARRAY_SIZE(stmmac_mmc)
+
+static void stmmac_ethtool_getdrvinfo(struct net_device *dev,
+				      struct ethtool_drvinfo *info)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if (priv->plat->has_gmac || priv->plat->has_gmac4)
+		strlcpy(info->driver, GMAC_ETHTOOL_NAME, sizeof(info->driver));
+	else if (priv->plat->has_xgmac)
+		strlcpy(info->driver, XGMAC_ETHTOOL_NAME, sizeof(info->driver));
+	else
+		strlcpy(info->driver, MAC100_ETHTOOL_NAME,
+			sizeof(info->driver));
+
+	strlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));
+}
+
+static int stmmac_ethtool_get_link_ksettings(struct net_device *dev,
+					     struct ethtool_link_ksettings *cmd)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if (priv->hw->pcs & STMMAC_PCS_RGMII ||
+	    priv->hw->pcs & STMMAC_PCS_SGMII) {
+		struct rgmii_adv adv;
+		u32 supported, advertising, lp_advertising;
+
+		if (!priv->xstats.pcs_link) {
+			cmd->base.speed = SPEED_UNKNOWN;
+			cmd->base.duplex = DUPLEX_UNKNOWN;
+			return 0;
+		}
+		cmd->base.duplex = priv->xstats.pcs_duplex;
+
+		cmd->base.speed = priv->xstats.pcs_speed;
+
+		/* Get and convert ADV/LP_ADV from the HW AN registers */
+		if (stmmac_pcs_get_adv_lp(priv, priv->ioaddr, &adv))
+			return -EOPNOTSUPP;	/* should never happen indeed */
+
+		/* Encoding of PSE bits is defined in 802.3z, 37.2.1.4 */
+
+		ethtool_convert_link_mode_to_legacy_u32(
+			&supported, cmd->link_modes.supported);
+		ethtool_convert_link_mode_to_legacy_u32(
+			&advertising, cmd->link_modes.advertising);
+		ethtool_convert_link_mode_to_legacy_u32(
+			&lp_advertising, cmd->link_modes.lp_advertising);
+
+		if (adv.pause & STMMAC_PCS_PAUSE)
+			advertising |= ADVERTISED_Pause;
+		if (adv.pause & STMMAC_PCS_ASYM_PAUSE)
+			advertising |= ADVERTISED_Asym_Pause;
+		if (adv.lp_pause & STMMAC_PCS_PAUSE)
+			lp_advertising |= ADVERTISED_Pause;
+		if (adv.lp_pause & STMMAC_PCS_ASYM_PAUSE)
+			lp_advertising |= ADVERTISED_Asym_Pause;
+
+		/* Reg49[3] always set because ANE is always supported */
+		cmd->base.autoneg = ADVERTISED_Autoneg;
+		supported |= SUPPORTED_Autoneg;
+		advertising |= ADVERTISED_Autoneg;
+		lp_advertising |= ADVERTISED_Autoneg;
+
+		if (adv.duplex) {
+			supported |= (SUPPORTED_1000baseT_Full |
+				      SUPPORTED_100baseT_Full |
+				      SUPPORTED_10baseT_Full);
+			advertising |= (ADVERTISED_1000baseT_Full |
+					ADVERTISED_100baseT_Full |
+					ADVERTISED_10baseT_Full);
+		} else {
+			supported |= (SUPPORTED_1000baseT_Half |
+				      SUPPORTED_100baseT_Half |
+				      SUPPORTED_10baseT_Half);
+			advertising |= (ADVERTISED_1000baseT_Half |
+					ADVERTISED_100baseT_Half |
+					ADVERTISED_10baseT_Half);
+		}
+		if (adv.lp_duplex)
+			lp_advertising |= (ADVERTISED_1000baseT_Full |
+					   ADVERTISED_100baseT_Full |
+					   ADVERTISED_10baseT_Full);
+		else
+			lp_advertising |= (ADVERTISED_1000baseT_Half |
+					   ADVERTISED_100baseT_Half |
+					   ADVERTISED_10baseT_Half);
+		cmd->base.port = PORT_OTHER;
+
+		ethtool_convert_legacy_u32_to_link_mode(
+			cmd->link_modes.supported, supported);
+		ethtool_convert_legacy_u32_to_link_mode(
+			cmd->link_modes.advertising, advertising);
+		ethtool_convert_legacy_u32_to_link_mode(
+			cmd->link_modes.lp_advertising, lp_advertising);
+
+		return 0;
+	}
+
+	return phylink_ethtool_ksettings_get(priv->phylink, cmd);
+}
+
+static int
+stmmac_ethtool_set_link_ksettings(struct net_device *dev,
+				  const struct ethtool_link_ksettings *cmd)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if (priv->hw->pcs & STMMAC_PCS_RGMII ||
+	    priv->hw->pcs & STMMAC_PCS_SGMII) {
+		u32 mask = ADVERTISED_Autoneg | ADVERTISED_Pause;
+
+		/* Only support ANE */
+		if (cmd->base.autoneg != AUTONEG_ENABLE)
+			return -EINVAL;
+
+		mask &= (ADVERTISED_1000baseT_Half |
+			ADVERTISED_1000baseT_Full |
+			ADVERTISED_100baseT_Half |
+			ADVERTISED_100baseT_Full |
+			ADVERTISED_10baseT_Half |
+			ADVERTISED_10baseT_Full);
+
+		rt_mutex_lock(&priv->lock);
+		stmmac_pcs_ctrl_ane(priv, priv->ioaddr, 1, priv->hw->ps, 0);
+		rt_mutex_unlock(&priv->lock);
+
+		return 0;
+	}
+
+	return phylink_ethtool_ksettings_set(priv->phylink, cmd);
+}
+
+static u32 stmmac_ethtool_getmsglevel(struct net_device *dev)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	return priv->msg_enable;
+}
+
+static void stmmac_ethtool_setmsglevel(struct net_device *dev, u32 level)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	priv->msg_enable = level;
+
+}
+
+static int stmmac_check_if_running(struct net_device *dev)
+{
+	if (!netif_running(dev))
+		return -EBUSY;
+	return 0;
+}
+
+static int stmmac_ethtool_get_regs_len(struct net_device *dev)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if (priv->plat->has_xgmac)
+		return XGMAC_REGSIZE * 4;
+	return REG_SPACE_SIZE;
+}
+
+static void stmmac_ethtool_gregs(struct net_device *dev,
+			  struct ethtool_regs *regs, void *space)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	u32 *reg_space = (u32 *) space;
+
+	stmmac_dump_mac_regs(priv, priv->hw, reg_space);
+	stmmac_dump_dma_regs(priv, priv->ioaddr, reg_space);
+
+	if (!priv->plat->has_xgmac) {
+		/* Copy DMA registers to where ethtool expects them */
+		memcpy(&reg_space[ETHTOOL_DMA_OFFSET],
+		       &reg_space[DMA_BUS_MODE / 4],
+		       NUM_DWMAC1000_DMA_REGS * 4);
+	}
+}
+
+static int stmmac_nway_reset(struct net_device *dev)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	return phylink_ethtool_nway_reset(priv->phylink);
+}
+
+static void
+stmmac_get_pauseparam(struct net_device *netdev,
+		      struct ethtool_pauseparam *pause)
+{
+	struct stmmac_priv *priv = netdev_priv(netdev);
+	struct rgmii_adv adv_lp;
+
+	if (priv->hw->pcs && !stmmac_pcs_get_adv_lp(priv, priv->ioaddr, &adv_lp)) {
+		pause->autoneg = 1;
+		if (!adv_lp.pause)
+			return;
+	} else {
+		phylink_ethtool_get_pauseparam(priv->phylink, pause);
+	}
+}
+
+static int
+stmmac_set_pauseparam(struct net_device *netdev,
+		      struct ethtool_pauseparam *pause)
+{
+	struct stmmac_priv *priv = netdev_priv(netdev);
+	struct rgmii_adv adv_lp;
+
+	if (priv->hw->pcs && !stmmac_pcs_get_adv_lp(priv, priv->ioaddr, &adv_lp)) {
+		pause->autoneg = 1;
+		if (!adv_lp.pause)
+			return -EOPNOTSUPP;
+		return 0;
+	} else {
+		return phylink_ethtool_set_pauseparam(priv->phylink, pause);
+	}
+}
+
+static void stmmac_get_ethtool_stats(struct net_device *dev,
+				 struct ethtool_stats *dummy, u64 *data)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	u32 rx_queues_count = priv->plat->rx_queues_to_use;
+	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+	unsigned long count;
+	int i, j = 0, ret;
+
+	if (priv->dma_cap.asp) {
+		for (i = 0; i < STMMAC_SAFETY_FEAT_SIZE; i++) {
+			if (!stmmac_safety_feat_dump(priv, &priv->sstats, i,
+						&count, NULL))
+				data[j++] = count;
+		}
+	}
+
+	/* Update the DMA HW counters for dwmac10/100 */
+	ret = stmmac_dma_diagnostic_fr(priv, &dev->stats, (void *) &priv->xstats,
+			priv->ioaddr);
+	if (ret) {
+		/* If supported, for new GMAC chips expose the MMC counters */
+		if (priv->dma_cap.rmon) {
+			stmmac_mmc_read(priv, priv->mmcaddr, &priv->mmc);
+
+			for (i = 0; i < STMMAC_MMC_STATS_LEN; i++) {
+				char *p;
+				p = (char *)priv + stmmac_mmc[i].stat_offset;
+
+				data[j++] = (stmmac_mmc[i].sizeof_stat ==
+					     sizeof(u64)) ? (*(u64 *)p) :
+					     (*(u32 *)p);
+			}
+		}
+		if (priv->eee_enabled) {
+			int val = phylink_get_eee_err(priv->phylink);
+			if (val)
+				priv->xstats.phy_eee_wakeup_error_n = val;
+		}
+
+		if (priv->synopsys_id >= DWMAC_CORE_3_50)
+			stmmac_mac_debug(priv, priv->ioaddr,
+					(void *)&priv->xstats,
+					rx_queues_count, tx_queues_count);
+	}
+	for (i = 0; i < STMMAC_STATS_LEN; i++) {
+		char *p = (char *)priv + stmmac_gstrings_stats[i].stat_offset;
+		data[j++] = (stmmac_gstrings_stats[i].sizeof_stat ==
+			     sizeof(u64)) ? (*(u64 *)p) : (*(u32 *)p);
+	}
+}
+
+static int stmmac_get_sset_count(struct net_device *netdev, int sset)
+{
+	struct stmmac_priv *priv = netdev_priv(netdev);
+	int i, len, safety_len = 0;
+
+	switch (sset) {
+	case ETH_SS_STATS:
+		len = STMMAC_STATS_LEN;
+
+		if (priv->dma_cap.rmon)
+			len += STMMAC_MMC_STATS_LEN;
+		if (priv->dma_cap.asp) {
+			for (i = 0; i < STMMAC_SAFETY_FEAT_SIZE; i++) {
+				if (!stmmac_safety_feat_dump(priv,
+							&priv->sstats, i,
+							NULL, NULL))
+					safety_len++;
+			}
+
+			len += safety_len;
+		}
+
+		return len;
+	case ETH_SS_TEST:
+		return stmmac_selftest_get_count(priv);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void stmmac_get_strings(struct net_device *dev, u32 stringset, u8 *data)
+{
+	int i;
+	u8 *p = data;
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	switch (stringset) {
+	case ETH_SS_STATS:
+		if (priv->dma_cap.asp) {
+			for (i = 0; i < STMMAC_SAFETY_FEAT_SIZE; i++) {
+				const char *desc;
+				if (!stmmac_safety_feat_dump(priv,
+							&priv->sstats, i,
+							NULL, &desc)) {
+					memcpy(p, desc, ETH_GSTRING_LEN);
+					p += ETH_GSTRING_LEN;
+				}
+			}
+		}
+		if (priv->dma_cap.rmon)
+			for (i = 0; i < STMMAC_MMC_STATS_LEN; i++) {
+				memcpy(p, stmmac_mmc[i].stat_string,
+				       ETH_GSTRING_LEN);
+				p += ETH_GSTRING_LEN;
+			}
+		for (i = 0; i < STMMAC_STATS_LEN; i++) {
+			memcpy(p, stmmac_gstrings_stats[i].stat_string,
+				ETH_GSTRING_LEN);
+			p += ETH_GSTRING_LEN;
+		}
+		break;
+	case ETH_SS_TEST:
+		stmmac_selftest_get_strings(priv, p);
+		break;
+	default:
+		WARN_ON(1);
+		break;
+	}
+}
+
+/* Currently only support WOL through Magic packet. */
+static void stmmac_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if (!priv->plat->pmt)
+		return phylink_ethtool_get_wol(priv->phylink, wol);
+
+	rt_mutex_lock(&priv->lock);
+	if (device_can_wakeup(priv->device)) {
+		wol->supported = WAKE_MAGIC | WAKE_UCAST;
+		if (priv->hw_cap_support && !priv->dma_cap.pmt_magic_frame)
+			wol->supported &= ~WAKE_MAGIC;
+		wol->wolopts = priv->wolopts;
+	}
+	rt_mutex_unlock(&priv->lock);
+}
+
+static int stmmac_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	u32 support = WAKE_MAGIC | WAKE_UCAST;
+
+	if (!device_can_wakeup(priv->device))
+		return -EOPNOTSUPP;
+
+	if (!priv->plat->pmt) {
+		int ret = phylink_ethtool_set_wol(priv->phylink, wol);
+
+		if (!ret)
+			device_set_wakeup_enable(priv->device, !!wol->wolopts);
+		return ret;
+	}
+
+	/* By default almost all GMAC devices support the WoL via
+	 * magic frame but we can disable it if the HW capability
+	 * register shows no support for pmt_magic_frame. */
+	if ((priv->hw_cap_support) && (!priv->dma_cap.pmt_magic_frame))
+		wol->wolopts &= ~WAKE_MAGIC;
+
+	if (wol->wolopts & ~support)
+		return -EINVAL;
+
+	if (wol->wolopts) {
+		pr_info("stmmac: wakeup enable\n");
+		device_set_wakeup_enable(priv->device, 1);
+		enable_irq_wake(priv->wol_irq);
+	} else {
+		device_set_wakeup_enable(priv->device, 0);
+		disable_irq_wake(priv->wol_irq);
+	}
+
+	rt_mutex_lock(&priv->lock);
+	priv->wolopts = wol->wolopts;
+	rt_mutex_unlock(&priv->lock);
+
+	return 0;
+}
+
+static int stmmac_ethtool_op_get_eee(struct net_device *dev,
+				     struct ethtool_eee *edata)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if (!priv->dma_cap.eee)
+		return -EOPNOTSUPP;
+
+	edata->eee_enabled = priv->eee_enabled;
+	edata->eee_active = priv->eee_active;
+	edata->tx_lpi_timer = priv->tx_lpi_timer;
+
+	return phylink_ethtool_get_eee(priv->phylink, edata);
+}
+
+static int stmmac_ethtool_op_set_eee(struct net_device *dev,
+				     struct ethtool_eee *edata)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int ret;
+
+	if (!edata->eee_enabled) {
+		stmmac_disable_eee_mode(priv);
+	} else {
+		/* We are asking for enabling the EEE but it is safe
+		 * to verify all by invoking the eee_init function.
+		 * In case of failure it will return an error.
+		 */
+		edata->eee_enabled = stmmac_eee_init(priv);
+		if (!edata->eee_enabled)
+			return -EOPNOTSUPP;
+	}
+
+	ret = phylink_ethtool_set_eee(priv->phylink, edata);
+	if (ret)
+		return ret;
+
+	priv->eee_enabled = edata->eee_enabled;
+	priv->tx_lpi_timer = edata->tx_lpi_timer;
+	return 0;
+}
+
+static u32 stmmac_usec2riwt(u32 usec, struct stmmac_priv *priv)
+{
+	unsigned long clk = clk_get_rate(priv->plat->stmmac_clk);
+
+	if (!clk) {
+		clk = priv->plat->clk_ref_rate;
+		if (!clk)
+			return 0;
+	}
+
+	return (usec * (clk / 1000000)) / 256;
+}
+
+static u32 stmmac_riwt2usec(u32 riwt, struct stmmac_priv *priv)
+{
+	unsigned long clk = clk_get_rate(priv->plat->stmmac_clk);
+
+	if (!clk) {
+		clk = priv->plat->clk_ref_rate;
+		if (!clk)
+			return 0;
+	}
+
+	return (riwt * 256) / (clk / 1000000);
+}
+
+static int stmmac_get_coalesce(struct net_device *dev,
+			       struct ethtool_coalesce *ec)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	ec->tx_coalesce_usecs = priv->tx_coal_timer;
+	ec->tx_max_coalesced_frames = priv->tx_coal_frames;
+
+	if (priv->use_riwt) {
+		ec->rx_max_coalesced_frames = priv->rx_coal_frames;
+		ec->rx_coalesce_usecs = stmmac_riwt2usec(priv->rx_riwt, priv);
+	}
+
+	return 0;
+}
+
+static int stmmac_set_coalesce(struct net_device *dev,
+			       struct ethtool_coalesce *ec)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	u32 rx_cnt = priv->plat->rx_queues_to_use;
+	unsigned int rx_riwt;
+
+	if (priv->use_riwt && (ec->rx_coalesce_usecs > 0)) {
+		rx_riwt = stmmac_usec2riwt(ec->rx_coalesce_usecs, priv);
+
+		if ((rx_riwt > MAX_DMA_RIWT) || (rx_riwt < MIN_DMA_RIWT))
+			return -EINVAL;
+
+		priv->rx_riwt = rx_riwt;
+		stmmac_rx_watchdog(priv, priv->ioaddr, priv->rx_riwt, rx_cnt);
+	}
+
+	if ((ec->tx_coalesce_usecs == 0) &&
+	    (ec->tx_max_coalesced_frames == 0))
+		return -EINVAL;
+
+	if ((ec->tx_coalesce_usecs > STMMAC_MAX_COAL_TX_TICK) ||
+	    (ec->tx_max_coalesced_frames > STMMAC_TX_MAX_FRAMES))
+		return -EINVAL;
+
+	/* Only copy relevant parameters, ignore all others. */
+	priv->tx_coal_frames = ec->tx_max_coalesced_frames;
+	priv->tx_coal_timer = ec->tx_coalesce_usecs;
+	priv->rx_coal_frames = ec->rx_max_coalesced_frames;
+	return 0;
+}
+
+static int stmmac_get_rxnfc(struct net_device *dev,
+			    struct ethtool_rxnfc *rxnfc, u32 *rule_locs)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	switch (rxnfc->cmd) {
+	case ETHTOOL_GRXRINGS:
+		rxnfc->data = priv->plat->rx_queues_to_use;
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static u32 stmmac_get_rxfh_key_size(struct net_device *dev)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	return sizeof(priv->rss.key);
+}
+
+static u32 stmmac_get_rxfh_indir_size(struct net_device *dev)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	return ARRAY_SIZE(priv->rss.table);
+}
+
+static int stmmac_get_rxfh(struct net_device *dev, u32 *indir, u8 *key,
+			   u8 *hfunc)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int i;
+
+	if (indir) {
+		for (i = 0; i < ARRAY_SIZE(priv->rss.table); i++)
+			indir[i] = priv->rss.table[i];
+	}
+
+	if (key)
+		memcpy(key, priv->rss.key, sizeof(priv->rss.key));
+	if (hfunc)
+		*hfunc = ETH_RSS_HASH_TOP;
+
+	return 0;
+}
+
+static int stmmac_set_rxfh(struct net_device *dev, const u32 *indir,
+			   const u8 *key, const u8 hfunc)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int i;
+
+	if ((hfunc != ETH_RSS_HASH_NO_CHANGE) && (hfunc != ETH_RSS_HASH_TOP))
+		return -EOPNOTSUPP;
+
+	if (indir) {
+		for (i = 0; i < ARRAY_SIZE(priv->rss.table); i++)
+			priv->rss.table[i] = indir[i];
+	}
+
+	if (key)
+		memcpy(priv->rss.key, key, sizeof(priv->rss.key));
+
+	return stmmac_rss_configure(priv, priv->hw, &priv->rss,
+				    priv->plat->rx_queues_to_use);
+}
+
+static int stmmac_get_ts_info(struct net_device *dev,
+			      struct ethtool_ts_info *info)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+
+	if ((priv->dma_cap.time_stamp || priv->dma_cap.atime_stamp)) {
+
+		info->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE |
+					SOF_TIMESTAMPING_TX_HARDWARE |
+					SOF_TIMESTAMPING_RX_SOFTWARE |
+					SOF_TIMESTAMPING_RX_HARDWARE |
+					SOF_TIMESTAMPING_SOFTWARE |
+					SOF_TIMESTAMPING_RAW_HARDWARE;
+
+		if (priv->ptp_clock)
+			info->phc_index = ptp_clock_index(priv->ptp_clock);
+
+		info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON);
+
+		info->rx_filters = ((1 << HWTSTAMP_FILTER_NONE) |
+				    (1 << HWTSTAMP_FILTER_PTP_V1_L4_EVENT) |
+				    (1 << HWTSTAMP_FILTER_PTP_V1_L4_SYNC) |
+				    (1 << HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ) |
+				    (1 << HWTSTAMP_FILTER_PTP_V2_L4_EVENT) |
+				    (1 << HWTSTAMP_FILTER_PTP_V2_L4_SYNC) |
+				    (1 << HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ) |
+				    (1 << HWTSTAMP_FILTER_PTP_V2_EVENT) |
+				    (1 << HWTSTAMP_FILTER_PTP_V2_SYNC) |
+				    (1 << HWTSTAMP_FILTER_PTP_V2_DELAY_REQ) |
+				    (1 << HWTSTAMP_FILTER_ALL));
+		return 0;
+	} else
+		return ethtool_op_get_ts_info(dev, info);
+}
+
+static int stmmac_get_tunable(struct net_device *dev,
+			      const struct ethtool_tunable *tuna, void *data)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int ret = 0;
+
+	switch (tuna->id) {
+	case ETHTOOL_RX_COPYBREAK:
+		*(u32 *)data = priv->rx_copybreak;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+static int stmmac_set_tunable(struct net_device *dev,
+			      const struct ethtool_tunable *tuna,
+			      const void *data)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int ret = 0;
+
+	switch (tuna->id) {
+	case ETHTOOL_RX_COPYBREAK:
+		priv->rx_copybreak = *(u32 *)data;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+static const struct ethtool_ops stmmac_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
+				     ETHTOOL_COALESCE_MAX_FRAMES,
+	.begin = stmmac_check_if_running,
+	.get_drvinfo = stmmac_ethtool_getdrvinfo,
+	.get_msglevel = stmmac_ethtool_getmsglevel,
+	.set_msglevel = stmmac_ethtool_setmsglevel,
+	.get_regs = stmmac_ethtool_gregs,
+	.get_regs_len = stmmac_ethtool_get_regs_len,
+	.get_link = ethtool_op_get_link,
+	.nway_reset = stmmac_nway_reset,
+	.get_pauseparam = stmmac_get_pauseparam,
+	.set_pauseparam = stmmac_set_pauseparam,
+	.self_test = stmmac_selftest_run,
+	.get_ethtool_stats = stmmac_get_ethtool_stats,
+	.get_strings = stmmac_get_strings,
+	.get_wol = stmmac_get_wol,
+	.set_wol = stmmac_set_wol,
+	.get_eee = stmmac_ethtool_op_get_eee,
+	.set_eee = stmmac_ethtool_op_set_eee,
+	.get_sset_count	= stmmac_get_sset_count,
+	.get_rxnfc = stmmac_get_rxnfc,
+	.get_rxfh_key_size = stmmac_get_rxfh_key_size,
+	.get_rxfh_indir_size = stmmac_get_rxfh_indir_size,
+	.get_rxfh = stmmac_get_rxfh,
+	.set_rxfh = stmmac_set_rxfh,
+	.get_ts_info = stmmac_get_ts_info,
+	.get_coalesce = stmmac_get_coalesce,
+	.set_coalesce = stmmac_set_coalesce,
+	.get_tunable = stmmac_get_tunable,
+	.set_tunable = stmmac_set_tunable,
+	.get_link_ksettings = stmmac_ethtool_get_link_ksettings,
+	.set_link_ksettings = stmmac_ethtool_set_link_ksettings,
+};
+
+void stmmac_set_ethtool_ops(struct net_device *netdev)
+{
+	netdev->ethtool_ops = &stmmac_ethtool_ops;
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac.h b/net/rtnet/drivers/orange-pi-one/stmmac.h
--- a/net/rtnet/drivers/orange-pi-one/stmmac.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac.h	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,297 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __STMMAC_H__
+#define __STMMAC_H__
+
+#define STMMAC_RESOURCE_NAME   "stmmaceth"
+#define DRV_MODULE_VERSION	"Jan_2016"
+
+#include <linux/clk.h>
+#include <linux/if_vlan.h>
+#include <linux/stmmac.h>
+#include <linux/phylink.h>
+#include <linux/pci.h>
+#include "common.h"
+#include <linux/ptp_clock_kernel.h>
+#include <linux/net_tstamp.h>
+#include <linux/reset.h>
+#include <net/page_pool.h>
+
+/* *** RTnet *** */
+#include <rtnet_port.h>
+#include <rtnet_multiple_queues.h>
+
+struct stmmac_resources {
+	void __iomem *addr;
+	const char *mac;
+	int wol_irq;
+	int lpi_irq;
+	int irq;
+};
+
+struct stmmac_tx_info {
+	dma_addr_t buf;
+	bool map_as_page;
+	unsigned len;
+	bool last_segment;
+	bool is_jumbo;
+};
+
+#define STMMAC_TBS_AVAIL	BIT(0)
+#define STMMAC_TBS_EN		BIT(1)
+
+/* Frequently used values are kept adjacent for cache effect */
+struct stmmac_tx_queue {
+	u32 tx_count_frames;
+	int tbs;
+	struct timer_list txtimer;
+	u32 queue_index;
+	struct stmmac_priv *priv_data;
+	struct dma_extended_desc *dma_etx ____cacheline_aligned_in_smp;
+	struct dma_edesc *dma_entx;
+	struct dma_desc *dma_tx;
+	struct rtskb **tx_skbuff;
+	struct stmmac_tx_info *tx_skbuff_dma;
+	unsigned int cur_tx;
+	unsigned int dirty_tx;
+	dma_addr_t dma_tx_phy;
+	u32 tx_tail_addr;
+	u32 mss;
+};
+
+struct stmmac_rx_buffer {
+	struct page *page;
+	struct page *sec_page;
+	dma_addr_t addr;
+	dma_addr_t sec_addr;
+};
+
+struct stmmac_rx_queue {
+	u32 rx_count_frames;
+	u32 queue_index;
+	struct page_pool *page_pool;
+	struct stmmac_rx_buffer *buf_pool;
+	struct stmmac_priv *priv_data;
+	struct dma_extended_desc *dma_erx;
+	struct dma_desc *dma_rx ____cacheline_aligned_in_smp;
+	unsigned int cur_rx;
+	unsigned int dirty_rx;
+	u32 rx_zeroc_thresh;
+	dma_addr_t dma_rx_phy;
+	u32 rx_tail_addr;
+	unsigned int state_saved;
+	struct {
+		struct rtskb *skb;
+		unsigned int len;
+		unsigned int error;
+	} state;
+};
+
+struct stmmac_channel {
+	struct napi_struct rx_napi ____cacheline_aligned_in_smp;
+	struct napi_struct tx_napi ____cacheline_aligned_in_smp;
+	struct stmmac_priv *priv_data;
+	raw_spinlock_t lock;
+	u32 index;
+};
+
+struct stmmac_tc_entry {
+	bool in_use;
+	bool in_hw;
+	bool is_last;
+	bool is_frag;
+	void *frag_ptr;
+	unsigned int table_pos;
+	u32 handle;
+	u32 prio;
+	struct {
+		u32 match_data;
+		u32 match_en;
+		u8 af:1;
+		u8 rf:1;
+		u8 im:1;
+		u8 nc:1;
+		u8 res1:4;
+		u8 frame_offset;
+		u8 ok_index;
+		u8 dma_ch_no;
+		u32 res2;
+	} __packed val;
+};
+
+#define STMMAC_PPS_MAX		4
+struct stmmac_pps_cfg {
+	bool available;
+	struct timespec64 start;
+	struct timespec64 period;
+};
+
+struct stmmac_rss {
+	int enable;
+	u8 key[STMMAC_RSS_HASH_KEY_SIZE];
+	u32 table[STMMAC_RSS_MAX_TABLE_SIZE];
+};
+
+#define STMMAC_FLOW_ACTION_DROP		BIT(0)
+struct stmmac_flow_entry {
+	unsigned long cookie;
+	unsigned long action;
+	u8 ip_proto;
+	int in_use;
+	int idx;
+	int is_l4;
+};
+
+struct stmmac_priv {
+	/* Frequently used values are kept adjacent for cache effect */
+	u32 tx_coal_frames;
+	u32 tx_coal_timer;
+	u32 rx_coal_frames;
+
+	int tx_coalesce;
+	int hwts_tx_en;
+	bool tx_path_in_lpi_mode;
+	bool tso;
+	int sph;
+	u32 sarc_type;
+
+	unsigned int dma_buf_sz;
+	unsigned int rx_copybreak;
+	u32 rx_riwt;
+	int hwts_rx_en;
+
+	void __iomem *ioaddr;
+	struct rtnet_device *dev;
+	struct net_device *dummy;
+	struct device *device;
+	struct mac_device_info *hw;
+	int (*hwif_quirks)(struct stmmac_priv *priv);
+	struct rt_mutex lock;
+	raw_spinlock_t rtnet_queue_lock;
+		
+	/* RX Queue */
+	struct stmmac_rx_queue rx_queue[MTL_MAX_RX_QUEUES];
+
+	/* TX Queue */
+	struct stmmac_tx_queue tx_queue[MTL_MAX_TX_QUEUES];
+
+	/* Generic channel for NAPI */
+	struct stmmac_channel channel[STMMAC_CH_MAX];
+
+	int speed;
+	unsigned int flow_ctrl;
+	unsigned int pause;
+	struct mii_bus *mii;
+	int mii_irq[PHY_MAX_ADDR];
+
+	struct phylink_config phylink_config;
+	struct phylink *phylink;
+
+	struct net_device_stats stats;
+	struct stmmac_extra_stats xstats ____cacheline_aligned_in_smp;
+	struct stmmac_safety_stats sstats;
+	struct plat_stmmacenet_data *plat;
+	struct dma_features dma_cap;
+	struct stmmac_counters mmc;
+	int hw_cap_support;
+	int synopsys_id;
+	u32 msg_enable;
+	int wolopts;
+	int wol_irq;
+	int clk_csr;
+	struct timer_list eee_ctrl_timer;
+	int lpi_irq;
+	int eee_enabled;
+	int eee_active;
+	int tx_lpi_timer;
+	unsigned int mode;
+	unsigned int chain_mode;
+	int extend_desc;
+	struct hwtstamp_config tstamp_config;
+	struct ptp_clock *ptp_clock;
+	struct ptp_clock_info ptp_clock_ops;
+	unsigned int default_addend;
+	u32 sub_second_inc;
+	u32 systime_flags;
+	u32 adv_ts;
+	int use_riwt;
+	int irq_wake;
+	spinlock_t ptp_lock;
+	void __iomem *mmcaddr;
+	void __iomem *ptpaddr;
+	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *dbgfs_dir;
+#endif
+
+	unsigned long state;
+	struct workqueue_struct *wq;
+	struct work_struct service_task;
+
+	/* TC Handling */
+	unsigned int tc_entries_max;
+	unsigned int tc_off_max;
+	struct stmmac_tc_entry *tc_entries;
+	unsigned int flow_entries_max;
+	struct stmmac_flow_entry *flow_entries;
+
+	/* Pulse Per Second output */
+	struct stmmac_pps_cfg pps[STMMAC_PPS_MAX];
+
+	/* Receive Side Scaling */
+	struct stmmac_rss rss;
+};
+
+enum stmmac_state {
+	STMMAC_DOWN,
+	STMMAC_RESET_REQUESTED,
+	STMMAC_RESETING,
+	STMMAC_SERVICE_SCHED,
+};
+
+int stmmac_mdio_unregister(struct net_device *ndev);
+int stmmac_mdio_register(struct net_device *ndev);
+int stmmac_mdio_reset(struct mii_bus *mii);
+void stmmac_set_ethtool_ops(struct net_device *netdev);
+
+void stmmac_ptp_register(struct stmmac_priv *priv);
+void stmmac_ptp_unregister(struct stmmac_priv *priv);
+int stmmac_resume(struct device *dev);
+int stmmac_suspend(struct device *dev);
+int stmmac_dvr_remove(struct device *dev);
+int stmmac_dvr_probe(struct device *device,
+		     struct plat_stmmacenet_data *plat_dat,
+		     struct stmmac_resources *res);
+void stmmac_disable_eee_mode(struct stmmac_priv *priv);
+bool stmmac_eee_init(struct stmmac_priv *priv);
+
+#if IS_ENABLED(CONFIG_STMMAC_SELFTESTS)
+void stmmac_selftest_run(struct net_device *dev,
+			 struct ethtool_test *etest, u64 *buf);
+void stmmac_selftest_get_strings(struct stmmac_priv *priv, u8 *data);
+int stmmac_selftest_get_count(struct stmmac_priv *priv);
+#else
+static inline void stmmac_selftest_run(struct net_device *dev,
+				       struct ethtool_test *etest, u64 *buf)
+{
+	/* Not enabled */
+}
+static inline void stmmac_selftest_get_strings(struct stmmac_priv *priv,
+					       u8 *data)
+{
+	/* Not enabled */
+}
+static inline int stmmac_selftest_get_count(struct stmmac_priv *priv)
+{
+	return -EOPNOTSUPP;
+}
+#endif /* CONFIG_STMMAC_SELFTESTS */
+
+#endif /* __STMMAC_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_hwtstamp.c b/net/rtnet/drivers/orange-pi-one/stmmac_hwtstamp.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_hwtstamp.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_hwtstamp.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,163 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  Copyright (C) 2013  Vayavya Labs Pvt Ltd
+
+  This implements all the API for managing HW timestamp & PTP.
+
+
+  Author: Rayagond Kokatanur <rayagond@vayavyalabs.com>
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/delay.h>
+#include "common.h"
+#include "stmmac_ptp.h"
+
+static void config_hw_tstamping(void __iomem *ioaddr, u32 data)
+{
+	writel(data, ioaddr + PTP_TCR);
+}
+
+static void config_sub_second_increment(void __iomem *ioaddr,
+		u32 ptp_clock, int gmac4, u32 *ssinc)
+{
+	u32 value = readl(ioaddr + PTP_TCR);
+	unsigned long data;
+	u32 reg_value;
+
+	/* For GMAC3.x, 4.x versions, in "fine adjustement mode" set sub-second
+	 * increment to twice the number of nanoseconds of a clock cycle.
+	 * The calculation of the default_addend value by the caller will set it
+	 * to mid-range = 2^31 when the remainder of this division is zero,
+	 * which will make the accumulator overflow once every 2 ptp_clock
+	 * cycles, adding twice the number of nanoseconds of a clock cycle :
+	 * 2000000000ULL / ptp_clock.
+	 */
+	if (value & PTP_TCR_TSCFUPDT)
+		data = (2000000000ULL / ptp_clock);
+	else
+		data = (1000000000ULL / ptp_clock);
+
+	/* 0.465ns accuracy */
+	if (!(value & PTP_TCR_TSCTRLSSR))
+		data = (data * 1000) / 465;
+
+	data &= PTP_SSIR_SSINC_MASK;
+
+	reg_value = data;
+	if (gmac4)
+		reg_value <<= GMAC4_PTP_SSIR_SSINC_SHIFT;
+
+	writel(reg_value, ioaddr + PTP_SSIR);
+
+	if (ssinc)
+		*ssinc = data;
+}
+
+static int init_systime(void __iomem *ioaddr, u32 sec, u32 nsec)
+{
+	u32 value;
+
+	writel(sec, ioaddr + PTP_STSUR);
+	writel(nsec, ioaddr + PTP_STNSUR);
+	/* issue command to initialize the system time value */
+	value = readl(ioaddr + PTP_TCR);
+	value |= PTP_TCR_TSINIT;
+	writel(value, ioaddr + PTP_TCR);
+
+	/* wait for present system time initialize to complete */
+	return readl_poll_timeout(ioaddr + PTP_TCR, value,
+				 !(value & PTP_TCR_TSINIT),
+				 10000, 100000);
+}
+
+static int config_addend(void __iomem *ioaddr, u32 addend)
+{
+	u32 value;
+	int limit;
+
+	writel(addend, ioaddr + PTP_TAR);
+	/* issue command to update the addend value */
+	value = readl(ioaddr + PTP_TCR);
+	value |= PTP_TCR_TSADDREG;
+	writel(value, ioaddr + PTP_TCR);
+
+	/* wait for present addend update to complete */
+	limit = 10;
+	while (limit--) {
+		if (!(readl(ioaddr + PTP_TCR) & PTP_TCR_TSADDREG))
+			break;
+		mdelay(10);
+	}
+	if (limit < 0)
+		return -EBUSY;
+
+	return 0;
+}
+
+static int adjust_systime(void __iomem *ioaddr, u32 sec, u32 nsec,
+		int add_sub, int gmac4)
+{
+	u32 value;
+	int limit;
+
+	if (add_sub) {
+		/* If the new sec value needs to be subtracted with
+		 * the system time, then MAC_STSUR reg should be
+		 * programmed with (2^32  <new_sec_value>)
+		 */
+		if (gmac4)
+			sec = -sec;
+
+		value = readl(ioaddr + PTP_TCR);
+		if (value & PTP_TCR_TSCTRLSSR)
+			nsec = (PTP_DIGITAL_ROLLOVER_MODE - nsec);
+		else
+			nsec = (PTP_BINARY_ROLLOVER_MODE - nsec);
+	}
+
+	writel(sec, ioaddr + PTP_STSUR);
+	value = (add_sub << PTP_STNSUR_ADDSUB_SHIFT) | nsec;
+	writel(value, ioaddr + PTP_STNSUR);
+
+	/* issue command to initialize the system time value */
+	value = readl(ioaddr + PTP_TCR);
+	value |= PTP_TCR_TSUPDT;
+	writel(value, ioaddr + PTP_TCR);
+
+	/* wait for present system time adjust/update to complete */
+	limit = 10;
+	while (limit--) {
+		if (!(readl(ioaddr + PTP_TCR) & PTP_TCR_TSUPDT))
+			break;
+		mdelay(10);
+	}
+	if (limit < 0)
+		return -EBUSY;
+
+	return 0;
+}
+
+static void get_systime(void __iomem *ioaddr, u64 *systime)
+{
+	u64 ns;
+
+	/* Get the TSSS value */
+	ns = readl(ioaddr + PTP_STNSR);
+	/* Get the TSS and convert sec time value to nanosecond */
+	ns += readl(ioaddr + PTP_STSR) * 1000000000ULL;
+
+	if (systime)
+		*systime = ns;
+}
+
+const struct stmmac_hwtimestamp stmmac_ptp = {
+	.config_hw_tstamping = config_hw_tstamping,
+	.init_systime = init_systime,
+	.config_sub_second_increment = config_sub_second_increment,
+	.config_addend = config_addend,
+	.adjust_systime = adjust_systime,
+	.get_systime = get_systime,
+};
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_main.c b/net/rtnet/drivers/orange-pi-one/stmmac_main.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_main.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_main.c	2021-07-14 15:39:13.314124941 +0300
@@ -0,0 +1,5586 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This is the driver for the ST MAC 10/100/1000 on-chip Ethernet controllers.
+  ST Ethernet IPs are built around a Synopsys IP Core.
+
+	Copyright(C) 2007-2011 STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+
+  Documentation available at:
+	http://www.stlinux.com
+  Support available at:
+	https://bugzilla.stlinux.com/
+*******************************************************************************/
+
+#include <linux/clk.h>
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/ip.h>
+#include <linux/tcp.h>
+#include <linux/skbuff.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/ethtool.h>
+#include <linux/if_ether.h>
+#include <linux/crc32.h>
+#include <linux/mii.h>
+#include <linux/if.h>
+#include <linux/if_vlan.h>
+#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+#include <linux/prefetch.h>
+#include <linux/pinctrl/consumer.h>
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#endif /* CONFIG_DEBUG_FS */
+#include <linux/net_tstamp.h>
+#include <linux/phylink.h>
+#include <linux/udp.h>
+#include <net/pkt_cls.h>
+#include "stmmac_ptp.h"
+#include "stmmac.h"
+#include <linux/reset.h>
+#include <linux/of_mdio.h>
+#include "dwmac1000.h"
+#include "dwxgmac2.h"
+#include "hwif.h"
+
+#define	STMMAC_ALIGN(x)		ALIGN(ALIGN(x, SMP_CACHE_BYTES), 16)
+#define	TSO_MAX_BUFF_SIZE	(SZ_16K - 1)
+
+/* Module parameters */
+#define TX_TIMEO	5000
+static int watchdog = TX_TIMEO;
+module_param(watchdog, int, 0644);
+MODULE_PARM_DESC(watchdog, "Transmit timeout in milliseconds (default 5s)");
+
+static int debug = -1;
+module_param(debug, int, 0644);
+MODULE_PARM_DESC(debug, "Message Level (-1: default, 0: no output, 16: all)");
+
+static int phyaddr = -1;
+module_param(phyaddr, int, 0444);
+MODULE_PARM_DESC(phyaddr, "Physical device address");
+
+#define STMMAC_TX_THRESH	(DMA_TX_SIZE / 4)
+#define STMMAC_RX_THRESH	(DMA_RX_SIZE / 4)
+
+static int flow_ctrl = FLOW_AUTO;
+module_param(flow_ctrl, int, 0644);
+MODULE_PARM_DESC(flow_ctrl, "Flow control ability [on/off]");
+
+static int pause = PAUSE_TIME;
+module_param(pause, int, 0644);
+MODULE_PARM_DESC(pause, "Flow Control Pause Time");
+
+#define TC_DEFAULT 64
+static int tc = TC_DEFAULT;
+module_param(tc, int, 0644);
+MODULE_PARM_DESC(tc, "DMA threshold control value");
+
+#define	DEFAULT_BUFSIZE	1536
+static int buf_sz = DEFAULT_BUFSIZE;
+module_param(buf_sz, int, 0644);
+MODULE_PARM_DESC(buf_sz, "DMA buffer size");
+
+#define	STMMAC_RX_COPYBREAK	256
+
+static const u32 default_msg_level = (NETIF_MSG_DRV | NETIF_MSG_PROBE |
+				      NETIF_MSG_LINK | NETIF_MSG_IFUP |
+				      NETIF_MSG_IFDOWN | NETIF_MSG_TIMER);
+
+#define STMMAC_DEFAULT_LPI_TIMER	1000
+static int eee_timer = STMMAC_DEFAULT_LPI_TIMER;
+module_param(eee_timer, int, 0644);
+MODULE_PARM_DESC(eee_timer, "LPI tx expiration time in msec");
+#define STMMAC_LPI_T(x) (jiffies + msecs_to_jiffies(x))
+
+/* By default the driver will use the ring mode to manage tx and rx descriptors,
+ * but allow user to force to use the chain instead of the ring
+ */
+static unsigned int chain_mode;
+module_param(chain_mode, int, 0444);
+MODULE_PARM_DESC(chain_mode, "To use chain instead of ring mode");
+
+static irqreturn_t stmmac_interrupt(int irq, void *dev_id);
+
+#ifdef CONFIG_DEBUG_FS
+static const struct net_device_ops stmmac_netdev_ops;
+static void stmmac_init_fs(struct rtnet_device *dev);
+static void stmmac_exit_fs(struct rtnet_device *dev);
+#endif
+
+#define STMMAC_COAL_TIMER(x) (jiffies + usecs_to_jiffies(x))
+
+#define RX_RING_SIZE	128
+static unsigned int stmmac_rtskb_pool_size = RX_RING_SIZE * 2;
+module_param(stmmac_rtskb_pool_size, uint, 0444);
+MODULE_PARM_DESC(stmmac_rtskb_pool_size, "Number of realtime socket buffers in proxy pool");
+
+static int stmmac_napi_poll_rx(void *vpriv);
+static int stmmac_napi_poll_tx(void *vpriv);
+static int stop_tx_task = 0, stop_rx_task = 0;
+static rtdm_event_t tx_event, rx_event;
+struct task_struct *tx_task, *rx_task;
+
+/**
+ * stmmac_verify_args - verify the driver parameters.
+ * Description: it checks the driver parameters and set a default in case of
+ * errors.
+ */
+static void stmmac_verify_args(void)
+{
+	if (unlikely(watchdog < 0))
+		watchdog = TX_TIMEO;
+	if (unlikely((buf_sz < DEFAULT_BUFSIZE) || (buf_sz > BUF_SIZE_16KiB)))
+		buf_sz = DEFAULT_BUFSIZE;
+	if (unlikely(flow_ctrl > 1))
+		flow_ctrl = FLOW_AUTO;
+	else if (likely(flow_ctrl < 0))
+		flow_ctrl = FLOW_OFF;
+	if (unlikely((pause < 0) || (pause > 0xffff)))
+		pause = PAUSE_TIME;
+	if (eee_timer < 0)
+		eee_timer = STMMAC_DEFAULT_LPI_TIMER;
+}
+
+/**
+ * stmmac_disable_all_queues - Disable all queues
+ * @priv: driver private structure
+ */
+static void stmmac_disable_all_queues(struct stmmac_priv *priv)
+{
+}
+
+/**
+ * stmmac_enable_all_queues - Enable all queues
+ * @priv: driver private structure
+ */
+static void stmmac_enable_all_queues(struct stmmac_priv *priv)
+{
+}
+
+/**
+ * stmmac_stop_all_queues - Stop all queues
+ * @priv: driver private structure
+ */
+static void stmmac_stop_all_queues(struct stmmac_priv *priv)
+{
+	u32 tx_queues_cnt = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	for (queue = 0; queue < tx_queues_cnt; queue++)
+		rtnetif_tx_stop_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, queue));
+}
+
+/**
+ * stmmac_start_all_queues - Start all queues
+ * @priv: driver private structure
+ */
+static void stmmac_start_all_queues(struct stmmac_priv *priv)
+{
+	u32 tx_queues_cnt = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	for (queue = 0; queue < tx_queues_cnt; queue++)
+		rtnetif_tx_start_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, queue));
+}
+
+static void stmmac_service_event_schedule(struct stmmac_priv *priv)
+{
+	if (!test_bit(STMMAC_DOWN, &priv->state) &&
+	    !test_and_set_bit(STMMAC_SERVICE_SCHED, &priv->state))
+		queue_work(priv->wq, &priv->service_task);
+}
+
+static void stmmac_global_err(struct stmmac_priv *priv)
+{
+	rtnetif_carrier_off(priv->dev);
+	set_bit(STMMAC_RESET_REQUESTED, &priv->state);
+	stmmac_service_event_schedule(priv);
+}
+
+/**
+ * stmmac_clk_csr_set - dynamically set the MDC clock
+ * @priv: driver private structure
+ * Description: this is to dynamically set the MDC clock according to the csr
+ * clock input.
+ * Note:
+ *	If a specific clk_csr value is passed from the platform
+ *	this means that the CSR Clock Range selection cannot be
+ *	changed at run-time and it is fixed (as reported in the driver
+ *	documentation). Viceversa the driver will try to set the MDC
+ *	clock dynamically according to the actual clock input.
+ */
+static void stmmac_clk_csr_set(struct stmmac_priv *priv)
+{
+	u32 clk_rate;
+
+	clk_rate = clk_get_rate(priv->plat->stmmac_clk);
+
+	/* Platform provided default clk_csr would be assumed valid
+	 * for all other cases except for the below mentioned ones.
+	 * For values higher than the IEEE 802.3 specified frequency
+	 * we can not estimate the proper divider as it is not known
+	 * the frequency of clk_csr_i. So we do not change the default
+	 * divider.
+	 */
+	if (!(priv->clk_csr & MAC_CSR_H_FRQ_MASK)) {
+		if (clk_rate < CSR_F_35M)
+			priv->clk_csr = STMMAC_CSR_20_35M;
+		else if ((clk_rate >= CSR_F_35M) && (clk_rate < CSR_F_60M))
+			priv->clk_csr = STMMAC_CSR_35_60M;
+		else if ((clk_rate >= CSR_F_60M) && (clk_rate < CSR_F_100M))
+			priv->clk_csr = STMMAC_CSR_60_100M;
+		else if ((clk_rate >= CSR_F_100M) && (clk_rate < CSR_F_150M))
+			priv->clk_csr = STMMAC_CSR_100_150M;
+		else if ((clk_rate >= CSR_F_150M) && (clk_rate < CSR_F_250M))
+			priv->clk_csr = STMMAC_CSR_150_250M;
+		else if ((clk_rate >= CSR_F_250M) && (clk_rate < CSR_F_300M))
+			priv->clk_csr = STMMAC_CSR_250_300M;
+	}
+
+	if (priv->plat->has_sun8i) {
+		if (clk_rate > 160000000)
+			priv->clk_csr = 0x03;
+		else if (clk_rate > 80000000)
+			priv->clk_csr = 0x02;
+		else if (clk_rate > 40000000)
+			priv->clk_csr = 0x01;
+		else
+			priv->clk_csr = 0;
+	}
+
+	if (priv->plat->has_xgmac) {
+		if (clk_rate > 400000000)
+			priv->clk_csr = 0x5;
+		else if (clk_rate > 350000000)
+			priv->clk_csr = 0x4;
+		else if (clk_rate > 300000000)
+			priv->clk_csr = 0x3;
+		else if (clk_rate > 250000000)
+			priv->clk_csr = 0x2;
+		else if (clk_rate > 150000000)
+			priv->clk_csr = 0x1;
+		else
+			priv->clk_csr = 0x0;
+	}
+}
+
+static void print_pkt(unsigned char *buf, int len)
+{
+	pr_debug("len = %d byte, buf addr: 0x%p\n", len, buf);
+	print_hex_dump_bytes("", DUMP_PREFIX_OFFSET, buf, len);
+}
+
+static inline u32 stmmac_tx_avail(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+	u32 avail;
+
+	if (tx_q->dirty_tx > tx_q->cur_tx)
+		avail = tx_q->dirty_tx - tx_q->cur_tx - 1;
+	else
+		avail = DMA_TX_SIZE - tx_q->cur_tx + tx_q->dirty_tx - 1;
+
+	return avail;
+}
+
+/**
+ * stmmac_rx_dirty - Get RX queue dirty
+ * @priv: driver private structure
+ * @queue: RX queue index
+ */
+static inline u32 stmmac_rx_dirty(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	u32 dirty;
+
+	if (rx_q->dirty_rx <= rx_q->cur_rx)
+		dirty = rx_q->cur_rx - rx_q->dirty_rx;
+	else
+		dirty = DMA_RX_SIZE - rx_q->dirty_rx + rx_q->cur_rx;
+
+	return dirty;
+}
+
+/**
+ * stmmac_enable_eee_mode - check and enter in LPI mode
+ * @priv: driver private structure
+ * Description: this function is to verify and enter in LPI mode in case of
+ * EEE.
+ */
+static void stmmac_enable_eee_mode(struct stmmac_priv *priv)
+{
+	u32 tx_cnt = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	/* check if all TX queues have the work finished */
+	for (queue = 0; queue < tx_cnt; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+
+		if (tx_q->dirty_tx != tx_q->cur_tx)
+			return; /* still unfinished work */
+	}
+
+	/* Check and enter in LPI mode */
+	if (!priv->tx_path_in_lpi_mode)
+		stmmac_set_eee_mode(priv, priv->hw,
+				priv->plat->en_tx_lpi_clockgating);
+}
+
+/**
+ * stmmac_disable_eee_mode - disable and exit from LPI mode
+ * @priv: driver private structure
+ * Description: this function is to exit and disable EEE in case of
+ * LPI state is true. This is called by the xmit.
+ */
+void stmmac_disable_eee_mode(struct stmmac_priv *priv)
+{
+	stmmac_reset_eee_mode(priv, priv->hw);
+	del_timer_sync(&priv->eee_ctrl_timer);
+	priv->tx_path_in_lpi_mode = false;
+}
+
+/**
+ * stmmac_eee_ctrl_timer - EEE TX SW timer.
+ * @arg : data hook
+ * Description:
+ *  if there is no data transfer and if we are not in LPI state,
+ *  then MAC Transmitter can be moved to LPI state.
+ */
+static void stmmac_eee_ctrl_timer(struct timer_list *t)
+{
+	struct stmmac_priv *priv = from_timer(priv, t, eee_ctrl_timer);
+
+	stmmac_enable_eee_mode(priv);
+	mod_timer(&priv->eee_ctrl_timer, STMMAC_LPI_T(eee_timer));
+}
+
+/**
+ * stmmac_eee_init - init EEE
+ * @priv: driver private structure
+ * Description:
+ *  if the GMAC supports the EEE (from the HW cap reg) and the phy device
+ *  can also manage EEE, this function enable the LPI state and start related
+ *  timer.
+ */
+bool stmmac_eee_init(struct stmmac_priv *priv)
+{
+	int tx_lpi_timer = priv->tx_lpi_timer;
+
+	/* Using PCS we cannot dial with the phy registers at this stage
+	 * so we do not support extra feature like EEE.
+	 */
+	if (priv->hw->pcs == STMMAC_PCS_TBI ||
+	    priv->hw->pcs == STMMAC_PCS_RTBI)
+		return false;
+
+	/* Check if MAC core supports the EEE feature. */
+	if (!priv->dma_cap.eee)
+		return false;
+
+	rt_mutex_lock(&priv->lock);
+
+	/* Check if it needs to be deactivated */
+	if (!priv->eee_active) {
+		if (priv->eee_enabled) {
+			netdev_dbg(priv->dummy, "disable EEE\n");
+			del_timer_sync(&priv->eee_ctrl_timer);
+			stmmac_set_eee_timer(priv, priv->hw, 0, tx_lpi_timer);
+		}
+		rt_mutex_unlock(&priv->lock);
+		return false;
+	}
+
+	if (priv->eee_active && !priv->eee_enabled) {
+		timer_setup(&priv->eee_ctrl_timer, stmmac_eee_ctrl_timer, 0);
+		mod_timer(&priv->eee_ctrl_timer, STMMAC_LPI_T(eee_timer));
+		stmmac_set_eee_timer(priv, priv->hw, STMMAC_DEFAULT_LIT_LS,
+				     tx_lpi_timer);
+	}
+
+	rt_mutex_unlock(&priv->lock);
+	netdev_dbg(priv->dummy, "Energy-Efficient Ethernet initialized\n");
+	return true;
+}
+
+/* stmmac_get_tx_hwtstamp - get HW TX timestamps
+ * @priv: driver private structure
+ * @p : descriptor pointer
+ * @skb : the socket buffer
+ * Description :
+ * This function will read timestamp from the descriptor & pass it to stack.
+ * and also perform some sanity checks.
+ */
+static void stmmac_get_tx_hwtstamp(struct stmmac_priv *priv,
+				   struct dma_desc *p, struct rtskb *skb)
+{
+#if 0
+	struct skb_shared_hwtstamps shhwtstamp;
+	bool found = false;
+	u64 ns = 0;
+#endif
+	
+	if (!priv->hwts_tx_en)
+		return;
+
+	/* RTnet does not know about skb_shinfo */
+#if 0
+	/* exit if skb doesn't support hw tstamp */
+	if (likely(!skb || !(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS)))
+		return;
+	
+	/* check tx tstamp status */
+	if (stmmac_get_tx_timestamp_status(priv, p)) {
+		stmmac_get_timestamp(priv, p, priv->adv_ts, &ns);
+		found = true;
+	} else if (!stmmac_get_mac_tx_timestamp(priv, priv->hw, &ns)) {
+		found = true;
+	}
+
+	if (found) {
+		memset(&shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));
+		shhwtstamp.hwtstamp = ns_to_ktime(ns);
+
+		netdev_dbg(priv->dummy, "get valid TX hw timestamp %llu\n", ns);
+		/* pass tstamp to stack */
+		skb_tstamp_tx(skb, &shhwtstamp);
+	}
+#endif
+}
+
+/* stmmac_get_rx_hwtstamp - get HW RX timestamps
+ * @priv: driver private structure
+ * @p : descriptor pointer
+ * @np : next descriptor pointer
+ * @skb : the socket buffer
+ * Description :
+ * This function will read received packet's timestamp from the descriptor
+ * and pass it to stack. It also perform some sanity checks.
+ */
+static void stmmac_get_rx_hwtstamp(struct stmmac_priv *priv, struct dma_desc *p,
+				   struct dma_desc *np, struct rtskb *skb)
+{
+#if 0
+	struct skb_shared_hwtstamps *shhwtstamp = NULL;
+	struct dma_desc *desc = p;
+	u64 ns = 0;
+#endif
+	
+	/* RTnet does not know about skb_hwtstamps */
+#if 0
+	if (!priv->hwts_rx_en)
+		return;
+	/* For GMAC4, the valid timestamp is from CTX next desc. */
+	if (priv->plat->has_gmac4 || priv->plat->has_xgmac)
+		desc = np;
+
+	/* Check if timestamp is available */
+	if (stmmac_get_rx_timestamp_status(priv, p, np, priv->adv_ts)) {
+		stmmac_get_timestamp(priv, desc, priv->adv_ts, &ns);
+		netdev_dbg(priv->dummy, "get valid RX hw timestamp %llu\n", ns);
+		shhwtstamp = skb_hwtstamps(skb);
+		memset(shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));
+		shhwtstamp->hwtstamp = ns_to_ktime(ns);
+	} else  {
+		netdev_dbg(priv->dummy, "cannot get RX hw timestamp\n");
+	}
+#endif
+}
+
+/**
+ *  stmmac_hwtstamp_set - control hardware timestamping.
+ *  @dev: device pointer.
+ *  @ifr: An IOCTL specific structure, that can contain a pointer to
+ *  a proprietary structure used to pass information to the driver.
+ *  Description:
+ *  This function configures the MAC to enable/disable both outgoing(TX)
+ *  and incoming(RX) packets time stamping based on user input.
+ *  Return Value:
+ *  0 on success and an appropriate -ve integer on failure.
+ */
+#if 0
+static int stmmac_hwtstamp_set(struct rtnet_device *dev, struct ifreq *ifr)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	struct hwtstamp_config config;
+	struct timespec64 now;
+	u64 temp = 0;
+	u32 ptp_v2 = 0;
+	u32 tstamp_all = 0;
+	u32 ptp_over_ipv4_udp = 0;
+	u32 ptp_over_ipv6_udp = 0;
+	u32 ptp_over_ethernet = 0;
+	u32 snap_type_sel = 0;
+	u32 ts_master_en = 0;
+	u32 ts_event_en = 0;
+	u32 sec_inc = 0;
+	u32 value = 0;
+	bool xmac;
+
+	xmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;
+
+	if (!(priv->dma_cap.time_stamp || priv->adv_ts)) {
+		netdev_alert(priv->dummy, "No support for HW time stamping\n");
+		priv->hwts_tx_en = 0;
+		priv->hwts_rx_en = 0;
+
+		return -EOPNOTSUPP;
+	}
+
+	if (copy_from_user(&config, ifr->ifr_data,
+			   sizeof(config)))
+		return -EFAULT;
+
+	netdev_dbg(priv->dummy, "%s config flags:0x%x, tx_type:0x%x, rx_filter:0x%x\n",
+		   __func__, config.flags, config.tx_type, config.rx_filter);
+
+	/* reserved for future extensions */
+	if (config.flags)
+		return -EINVAL;
+
+	if (config.tx_type != HWTSTAMP_TX_OFF &&
+	    config.tx_type != HWTSTAMP_TX_ON)
+		return -ERANGE;
+
+	if (priv->adv_ts) {
+		switch (config.rx_filter) {
+		case HWTSTAMP_FILTER_NONE:
+			/* time stamp no incoming packet at all */
+			config.rx_filter = HWTSTAMP_FILTER_NONE;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+			/* PTP v1, UDP, any kind of event packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_EVENT;
+			/* 'xmac' hardware can support Sync, Pdelay_Req and
+			 * Pdelay_resp by setting bit14 and bits17/16 to 01
+			 * This leaves Delay_Req timestamps out.
+			 * Enable all events *and* general purpose message
+			 * timestamping
+			 */
+			snap_type_sel = PTP_TCR_SNAPTYPSEL_1;
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+			/* PTP v1, UDP, Sync packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_SYNC;
+			/* take time stamp for SYNC messages only */
+			ts_event_en = PTP_TCR_TSEVNTENA;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+			/* PTP v1, UDP, Delay_req packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ;
+			/* take time stamp for Delay_Req messages only */
+			ts_master_en = PTP_TCR_TSMSTRENA;
+			ts_event_en = PTP_TCR_TSEVNTENA;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+			/* PTP v2, UDP, any kind of event packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_EVENT;
+			ptp_v2 = PTP_TCR_TSVER2ENA;
+			/* take time stamp for all event messages */
+			snap_type_sel = PTP_TCR_SNAPTYPSEL_1;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+			/* PTP v2, UDP, Sync packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_SYNC;
+			ptp_v2 = PTP_TCR_TSVER2ENA;
+			/* take time stamp for SYNC messages only */
+			ts_event_en = PTP_TCR_TSEVNTENA;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+			/* PTP v2, UDP, Delay_req packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ;
+			ptp_v2 = PTP_TCR_TSVER2ENA;
+			/* take time stamp for Delay_Req messages only */
+			ts_master_en = PTP_TCR_TSMSTRENA;
+			ts_event_en = PTP_TCR_TSEVNTENA;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V2_EVENT:
+			/* PTP v2/802.AS1 any layer, any kind of event packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V2_EVENT;
+			ptp_v2 = PTP_TCR_TSVER2ENA;
+			snap_type_sel = PTP_TCR_SNAPTYPSEL_1;
+			if (priv->synopsys_id != DWMAC_CORE_5_10)
+				ts_event_en = PTP_TCR_TSEVNTENA;
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			ptp_over_ethernet = PTP_TCR_TSIPENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V2_SYNC:
+			/* PTP v2/802.AS1, any layer, Sync packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V2_SYNC;
+			ptp_v2 = PTP_TCR_TSVER2ENA;
+			/* take time stamp for SYNC messages only */
+			ts_event_en = PTP_TCR_TSEVNTENA;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			ptp_over_ethernet = PTP_TCR_TSIPENA;
+			break;
+
+		case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+			/* PTP v2/802.AS1, any layer, Delay_req packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V2_DELAY_REQ;
+			ptp_v2 = PTP_TCR_TSVER2ENA;
+			/* take time stamp for Delay_Req messages only */
+			ts_master_en = PTP_TCR_TSMSTRENA;
+			ts_event_en = PTP_TCR_TSEVNTENA;
+
+			ptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;
+			ptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;
+			ptp_over_ethernet = PTP_TCR_TSIPENA;
+			break;
+
+		case HWTSTAMP_FILTER_NTP_ALL:
+		case HWTSTAMP_FILTER_ALL:
+			/* time stamp any incoming packet */
+			config.rx_filter = HWTSTAMP_FILTER_ALL;
+			tstamp_all = PTP_TCR_TSENALL;
+			break;
+
+		default:
+			return -ERANGE;
+		}
+	} else {
+		switch (config.rx_filter) {
+		case HWTSTAMP_FILTER_NONE:
+			config.rx_filter = HWTSTAMP_FILTER_NONE;
+			break;
+		default:
+			/* PTP v1, UDP, any kind of event packet */
+			config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_EVENT;
+			break;
+		}
+	}
+	priv->hwts_rx_en = ((config.rx_filter == HWTSTAMP_FILTER_NONE) ? 0 : 1);
+	priv->hwts_tx_en = config.tx_type == HWTSTAMP_TX_ON;
+
+	if (!priv->hwts_tx_en && !priv->hwts_rx_en)
+		stmmac_config_hw_tstamping(priv, priv->ptpaddr, 0);
+	else {
+		value = (PTP_TCR_TSENA | PTP_TCR_TSCFUPDT | PTP_TCR_TSCTRLSSR |
+			 tstamp_all | ptp_v2 | ptp_over_ethernet |
+			 ptp_over_ipv6_udp | ptp_over_ipv4_udp | ts_event_en |
+			 ts_master_en | snap_type_sel);
+		stmmac_config_hw_tstamping(priv, priv->ptpaddr, value);
+
+		/* program Sub Second Increment reg */
+		stmmac_config_sub_second_increment(priv,
+				priv->ptpaddr, priv->plat->clk_ptp_rate,
+				xmac, &sec_inc);
+		temp = div_u64(1000000000ULL, sec_inc);
+
+		/* Store sub second increment and flags for later use */
+		priv->sub_second_inc = sec_inc;
+		priv->systime_flags = value;
+
+		/* calculate default added value:
+		 * formula is :
+		 * addend = (2^32)/freq_div_ratio;
+		 * where, freq_div_ratio = 1e9ns/sec_inc
+		 */
+		temp = (u64)(temp << 32);
+		priv->default_addend = div_u64(temp, priv->plat->clk_ptp_rate);
+		stmmac_config_addend(priv, priv->ptpaddr, priv->default_addend);
+
+		/* initialize system time */
+		ktime_get_real_ts64(&now);
+
+		/* lower 32 bits of tv_sec are safe until y2106 */
+		stmmac_init_systime(priv, priv->ptpaddr,
+				(u32)now.tv_sec, now.tv_nsec);
+	}
+
+	memcpy(&priv->tstamp_config, &config, sizeof(config));
+
+	return copy_to_user(ifr->ifr_data, &config,
+			    sizeof(config)) ? -EFAULT : 0;
+}
+
+/**
+ *  stmmac_hwtstamp_get - read hardware timestamping.
+ *  @dev: device pointer.
+ *  @ifr: An IOCTL specific structure, that can contain a pointer to
+ *  a proprietary structure used to pass information to the driver.
+ *  Description:
+ *  This function obtain the current hardware timestamping settings
+    as requested.
+ */
+static int stmmac_hwtstamp_get(struct rtnet_device *dev, struct ifreq *ifr)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	struct hwtstamp_config *config = &priv->tstamp_config;
+
+	if (!(priv->dma_cap.time_stamp || priv->dma_cap.atime_stamp))
+		return -EOPNOTSUPP;
+
+	return copy_to_user(ifr->ifr_data, config,
+			    sizeof(*config)) ? -EFAULT : 0;
+}
+#endif
+
+/**
+ * stmmac_init_ptp - init PTP
+ * @priv: driver private structure
+ * Description: this is to verify if the HW supports the PTPv1 or PTPv2.
+ * This is done by looking at the HW cap. register.
+ * This function also registers the ptp driver.
+ */
+static int stmmac_init_ptp(struct stmmac_priv *priv)
+{
+	bool xmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;
+
+	if (!(priv->dma_cap.time_stamp || priv->dma_cap.atime_stamp))
+		return -EOPNOTSUPP;
+
+	priv->adv_ts = 0;
+	/* Check if adv_ts can be enabled for dwmac 4.x / xgmac core */
+	if (xmac && priv->dma_cap.atime_stamp)
+		priv->adv_ts = 1;
+	/* Dwmac 3.x core with extend_desc can support adv_ts */
+	else if (priv->extend_desc && priv->dma_cap.atime_stamp)
+		priv->adv_ts = 1;
+
+	if (priv->dma_cap.time_stamp)
+		netdev_info(priv->dummy, "IEEE 1588-2002 Timestamp supported\n");
+
+	if (priv->adv_ts)
+		netdev_info(priv->dummy,
+			    "IEEE 1588-2008 Advanced Timestamp supported\n");
+
+	priv->hwts_tx_en = 0;
+	priv->hwts_rx_en = 0;
+
+	stmmac_ptp_register(priv);
+
+	return 0;
+}
+
+static void stmmac_release_ptp(struct stmmac_priv *priv)
+{
+	if (priv->plat->clk_ptp_ref)
+		clk_disable_unprepare(priv->plat->clk_ptp_ref);
+	stmmac_ptp_unregister(priv);
+}
+
+/**
+ *  stmmac_mac_flow_ctrl - Configure flow control in all queues
+ *  @priv: driver private structure
+ *  Description: It is used for configuring the flow control in all queues
+ */
+static void stmmac_mac_flow_ctrl(struct stmmac_priv *priv, u32 duplex)
+{
+	u32 tx_cnt = priv->plat->tx_queues_to_use;
+
+	stmmac_flow_ctrl(priv, priv->hw, duplex, priv->flow_ctrl,
+			priv->pause, tx_cnt);
+}
+
+static void stmmac_validate(struct phylink_config *config,
+			    unsigned long *supported,
+			    struct phylink_link_state *state)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv *)
+		netdev_priv(to_net_dev(config->dev)))->rtdev);
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mac_supported) = { 0, };
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
+	int tx_cnt = priv->plat->tx_queues_to_use;
+	int max_speed = priv->plat->max_speed;
+
+	phylink_set(mac_supported, 10baseT_Half);
+	phylink_set(mac_supported, 10baseT_Full);
+	phylink_set(mac_supported, 100baseT_Half);
+	phylink_set(mac_supported, 100baseT_Full);
+	phylink_set(mac_supported, 1000baseT_Half);
+	phylink_set(mac_supported, 1000baseT_Full);
+	phylink_set(mac_supported, 1000baseKX_Full);
+
+	phylink_set(mac_supported, Autoneg);
+	phylink_set(mac_supported, Pause);
+	phylink_set(mac_supported, Asym_Pause);
+	phylink_set_port_modes(mac_supported);
+
+	/* Cut down 1G if asked to */
+	if ((max_speed > 0) && (max_speed < 1000)) {
+		phylink_set(mask, 1000baseT_Full);
+		phylink_set(mask, 1000baseX_Full);
+	} else if (priv->plat->has_xgmac) {
+		if (!max_speed || (max_speed >= 2500)) {
+			phylink_set(mac_supported, 2500baseT_Full);
+			phylink_set(mac_supported, 2500baseX_Full);
+		}
+		if (!max_speed || (max_speed >= 5000)) {
+			phylink_set(mac_supported, 5000baseT_Full);
+		}
+		if (!max_speed || (max_speed >= 10000)) {
+			phylink_set(mac_supported, 10000baseSR_Full);
+			phylink_set(mac_supported, 10000baseLR_Full);
+			phylink_set(mac_supported, 10000baseER_Full);
+			phylink_set(mac_supported, 10000baseLRM_Full);
+			phylink_set(mac_supported, 10000baseT_Full);
+			phylink_set(mac_supported, 10000baseKX4_Full);
+			phylink_set(mac_supported, 10000baseKR_Full);
+		}
+		if (!max_speed || (max_speed >= 25000)) {
+			phylink_set(mac_supported, 25000baseCR_Full);
+			phylink_set(mac_supported, 25000baseKR_Full);
+			phylink_set(mac_supported, 25000baseSR_Full);
+		}
+		if (!max_speed || (max_speed >= 40000)) {
+			phylink_set(mac_supported, 40000baseKR4_Full);
+			phylink_set(mac_supported, 40000baseCR4_Full);
+			phylink_set(mac_supported, 40000baseSR4_Full);
+			phylink_set(mac_supported, 40000baseLR4_Full);
+		}
+		if (!max_speed || (max_speed >= 50000)) {
+			phylink_set(mac_supported, 50000baseCR2_Full);
+			phylink_set(mac_supported, 50000baseKR2_Full);
+			phylink_set(mac_supported, 50000baseSR2_Full);
+			phylink_set(mac_supported, 50000baseKR_Full);
+			phylink_set(mac_supported, 50000baseSR_Full);
+			phylink_set(mac_supported, 50000baseCR_Full);
+			phylink_set(mac_supported, 50000baseLR_ER_FR_Full);
+			phylink_set(mac_supported, 50000baseDR_Full);
+		}
+		if (!max_speed || (max_speed >= 100000)) {
+			phylink_set(mac_supported, 100000baseKR4_Full);
+			phylink_set(mac_supported, 100000baseSR4_Full);
+			phylink_set(mac_supported, 100000baseCR4_Full);
+			phylink_set(mac_supported, 100000baseLR4_ER4_Full);
+			phylink_set(mac_supported, 100000baseKR2_Full);
+			phylink_set(mac_supported, 100000baseSR2_Full);
+			phylink_set(mac_supported, 100000baseCR2_Full);
+			phylink_set(mac_supported, 100000baseLR2_ER2_FR2_Full);
+			phylink_set(mac_supported, 100000baseDR2_Full);
+		}
+	}
+
+	/* Half-Duplex can only work with single queue */
+	if (tx_cnt > 1) {
+		phylink_set(mask, 10baseT_Half);
+		phylink_set(mask, 100baseT_Half);
+		phylink_set(mask, 1000baseT_Half);
+	}
+
+	linkmode_and(supported, supported, mac_supported);
+	linkmode_andnot(supported, supported, mask);
+
+	linkmode_and(state->advertising, state->advertising, mac_supported);
+	linkmode_andnot(state->advertising, state->advertising, mask);
+
+	/* If PCS is supported, check which modes it supports. */
+	stmmac_xpcs_validate(priv, &priv->hw->xpcs_args, supported, state);
+}
+
+static void stmmac_mac_pcs_get_state(struct phylink_config *config,
+				     struct phylink_link_state *state)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv *)
+		netdev_priv(to_net_dev(config->dev)))->rtdev);
+
+	state->link = 0;
+	stmmac_xpcs_get_state(priv, &priv->hw->xpcs_args, state);
+}
+
+static void stmmac_mac_config(struct phylink_config *config, unsigned int mode,
+			      const struct phylink_link_state *state)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv *)
+		netdev_priv(to_net_dev(config->dev)))->rtdev);
+
+	stmmac_xpcs_config(priv, &priv->hw->xpcs_args, state);
+}
+
+static void stmmac_mac_an_restart(struct phylink_config *config)
+{
+	/* Not Supported */
+}
+
+static void stmmac_mac_link_down(struct phylink_config *config,
+				 unsigned int mode, phy_interface_t interface)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv *)
+		netdev_priv(to_net_dev(config->dev)))->rtdev);
+
+	stmmac_mac_set(priv, priv->ioaddr, false);
+	priv->eee_active = false;
+	stmmac_eee_init(priv);
+	stmmac_set_eee_pls(priv, priv->hw, false);
+}
+
+static void stmmac_mac_link_up(struct phylink_config *config,
+			       struct phy_device *phy,
+			       unsigned int mode, phy_interface_t interface,
+			       int speed, int duplex,
+			       bool tx_pause, bool rx_pause)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv *)
+		netdev_priv(to_net_dev(config->dev)))->rtdev);
+	u32 ctrl;
+
+	stmmac_xpcs_link_up(priv, &priv->hw->xpcs_args, speed, interface);
+
+	ctrl = readl(priv->ioaddr + MAC_CTRL_REG);
+	ctrl &= ~priv->hw->link.speed_mask;
+
+	if (interface == PHY_INTERFACE_MODE_USXGMII) {
+		switch (speed) {
+		case SPEED_10000:
+			ctrl |= priv->hw->link.xgmii.speed10000;
+			break;
+		case SPEED_5000:
+			ctrl |= priv->hw->link.xgmii.speed5000;
+			break;
+		case SPEED_2500:
+			ctrl |= priv->hw->link.xgmii.speed2500;
+			break;
+		default:
+			return;
+		}
+	} else if (interface == PHY_INTERFACE_MODE_XLGMII) {
+		switch (speed) {
+		case SPEED_100000:
+			ctrl |= priv->hw->link.xlgmii.speed100000;
+			break;
+		case SPEED_50000:
+			ctrl |= priv->hw->link.xlgmii.speed50000;
+			break;
+		case SPEED_40000:
+			ctrl |= priv->hw->link.xlgmii.speed40000;
+			break;
+		case SPEED_25000:
+			ctrl |= priv->hw->link.xlgmii.speed25000;
+			break;
+		case SPEED_10000:
+			ctrl |= priv->hw->link.xgmii.speed10000;
+			break;
+		case SPEED_2500:
+			ctrl |= priv->hw->link.speed2500;
+			break;
+		case SPEED_1000:
+			ctrl |= priv->hw->link.speed1000;
+			break;
+		default:
+			return;
+		}
+	} else {
+		switch (speed) {
+		case SPEED_2500:
+			ctrl |= priv->hw->link.speed2500;
+			break;
+		case SPEED_1000:
+			ctrl |= priv->hw->link.speed1000;
+			break;
+		case SPEED_100:
+			ctrl |= priv->hw->link.speed100;
+			break;
+		case SPEED_10:
+			ctrl |= priv->hw->link.speed10;
+			break;
+		default:
+			return;
+		}
+	}
+
+	priv->speed = speed;
+
+	if (priv->plat->fix_mac_speed)
+		priv->plat->fix_mac_speed(priv->plat->bsp_priv, speed);
+
+	if (!duplex)
+		ctrl &= ~priv->hw->link.duplex;
+	else
+		ctrl |= priv->hw->link.duplex;
+
+	/* Flow Control operation */
+	if (tx_pause && rx_pause)
+		stmmac_mac_flow_ctrl(priv, duplex);
+
+	writel(ctrl, priv->ioaddr + MAC_CTRL_REG);
+
+	stmmac_mac_set(priv, priv->ioaddr, true);
+	if (phy && priv->dma_cap.eee) {
+		priv->eee_active = phy_init_eee(phy, 1) >= 0;
+		priv->eee_enabled = stmmac_eee_init(priv);
+		stmmac_set_eee_pls(priv, priv->hw, true);
+	}
+}
+
+static const struct phylink_mac_ops stmmac_phylink_mac_ops = {
+	.validate = stmmac_validate,
+	.mac_pcs_get_state = stmmac_mac_pcs_get_state,
+	.mac_config = stmmac_mac_config,
+	.mac_an_restart = stmmac_mac_an_restart,
+	.mac_link_down = stmmac_mac_link_down,
+	.mac_link_up = stmmac_mac_link_up,
+};
+
+/**
+ * stmmac_check_pcs_mode - verify if RGMII/SGMII is supported
+ * @priv: driver private structure
+ * Description: this is to verify if the HW supports the PCS.
+ * Physical Coding Sublayer (PCS) interface that can be used when the MAC is
+ * configured for the TBI, RTBI, or SGMII PHY interface.
+ */
+static void stmmac_check_pcs_mode(struct stmmac_priv *priv)
+{
+	int interface = priv->plat->interface;
+
+	if (priv->dma_cap.pcs) {
+		if ((interface == PHY_INTERFACE_MODE_RGMII) ||
+		    (interface == PHY_INTERFACE_MODE_RGMII_ID) ||
+		    (interface == PHY_INTERFACE_MODE_RGMII_RXID) ||
+		    (interface == PHY_INTERFACE_MODE_RGMII_TXID)) {
+			netdev_dbg(priv->dummy, "PCS RGMII support enabled\n");
+			priv->hw->pcs = STMMAC_PCS_RGMII;
+		} else if (interface == PHY_INTERFACE_MODE_SGMII) {
+			netdev_dbg(priv->dummy, "PCS SGMII support enabled\n");
+			priv->hw->pcs = STMMAC_PCS_SGMII;
+		}
+	}
+}
+
+/**
+ * stmmac_init_phy - PHY initialization
+ * @dev: net device structure
+ * Description: it initializes the driver's PHY state, and attaches the PHY
+ * to the mac driver.
+ *  Return value:
+ *  0 on success
+ */
+static int stmmac_init_phy(struct rtnet_device *dev)
+{
+#if 0
+	struct ethtool_wolinfo wol = { .cmd = ETHTOOL_GWOL };
+#endif
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	struct device_node *node;
+	int ret;
+
+	node = priv->plat->phylink_node;
+
+	if (node)
+		ret = phylink_of_phy_connect(priv->phylink, node, 0);
+
+	/* Some DT bindings do not set-up the PHY handle. Let's try to
+	 * manually parse it
+	 */
+	if (!node || ret) {
+		int addr = priv->plat->phy_addr;
+		struct phy_device *phydev;
+
+		phydev = mdiobus_get_phy(priv->mii, addr);
+		if (!phydev) {
+			netdev_err(priv->dummy, "no phy at addr %d\n", addr);
+			return -ENODEV;
+		}
+
+		ret = phylink_connect_phy(priv->phylink, phydev);
+	}
+
+#if 0
+	phylink_ethtool_get_wol(priv->phylink, &wol);
+	device_set_wakeup_capable(priv->device, !!wol.supported);
+#endif
+	
+	return ret;
+}
+
+static int stmmac_phy_setup(struct stmmac_priv *priv)
+{
+	struct fwnode_handle *fwnode = of_fwnode_handle(priv->plat->phylink_node);
+	int mode = priv->plat->phy_interface;
+	struct phylink *phylink;
+
+	priv->phylink_config.dev = &priv->dummy->dev;
+	priv->phylink_config.type = PHYLINK_NETDEV;
+	priv->phylink_config.pcs_poll = true;
+
+	if (!fwnode)
+		fwnode = dev_fwnode(priv->device);
+
+	phylink = phylink_create(&priv->phylink_config, fwnode,
+				 mode, &stmmac_phylink_mac_ops);
+	if (IS_ERR(phylink))
+		return PTR_ERR(phylink);
+
+	priv->phylink = phylink;
+	return 0;
+}
+
+static void stmmac_display_rx_rings(struct stmmac_priv *priv)
+{
+	u32 rx_cnt = priv->plat->rx_queues_to_use;
+	void *head_rx;
+	u32 queue;
+
+	/* Display RX rings */
+	for (queue = 0; queue < rx_cnt; queue++) {
+		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+		pr_info("\tRX Queue %u rings\n", queue);
+
+		if (priv->extend_desc)
+			head_rx = (void *)rx_q->dma_erx;
+		else
+			head_rx = (void *)rx_q->dma_rx;
+
+		/* Display RX ring */
+		stmmac_display_ring(priv, head_rx, DMA_RX_SIZE, true);
+	}
+}
+
+static void stmmac_display_tx_rings(struct stmmac_priv *priv)
+{
+	u32 tx_cnt = priv->plat->tx_queues_to_use;
+	void *head_tx;
+	u32 queue;
+
+	/* Display TX rings */
+	for (queue = 0; queue < tx_cnt; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+
+		pr_info("\tTX Queue %d rings\n", queue);
+
+		if (priv->extend_desc)
+			head_tx = (void *)tx_q->dma_etx;
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			head_tx = (void *)tx_q->dma_entx;
+		else
+			head_tx = (void *)tx_q->dma_tx;
+
+		stmmac_display_ring(priv, head_tx, DMA_TX_SIZE, false);
+	}
+}
+
+static void stmmac_display_rings(struct stmmac_priv *priv)
+{
+	/* Display RX ring */
+	stmmac_display_rx_rings(priv);
+
+	/* Display TX ring */
+	stmmac_display_tx_rings(priv);
+}
+
+static int stmmac_set_bfsize(int mtu, int bufsize)
+{
+	int ret = bufsize;
+
+	if (mtu >= BUF_SIZE_8KiB)
+		ret = BUF_SIZE_16KiB;
+	else if (mtu >= BUF_SIZE_4KiB)
+		ret = BUF_SIZE_8KiB;
+	else if (mtu >= BUF_SIZE_2KiB)
+		ret = BUF_SIZE_4KiB;
+	else if (mtu > DEFAULT_BUFSIZE)
+		ret = BUF_SIZE_2KiB;
+	else
+		ret = DEFAULT_BUFSIZE;
+
+	return ret;
+}
+
+/**
+ * stmmac_clear_rx_descriptors - clear RX descriptors
+ * @priv: driver private structure
+ * @queue: RX queue index
+ * Description: this function is called to clear the RX descriptors
+ * in case of both basic and extended descriptors are used.
+ */
+static void stmmac_clear_rx_descriptors(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	int i;
+
+	/* Clear the RX descriptors */
+	for (i = 0; i < DMA_RX_SIZE; i++)
+		if (priv->extend_desc)
+			stmmac_init_rx_desc(priv, &rx_q->dma_erx[i].basic,
+					priv->use_riwt, priv->mode,
+					(i == DMA_RX_SIZE - 1),
+					priv->dma_buf_sz);
+		else
+			stmmac_init_rx_desc(priv, &rx_q->dma_rx[i],
+					priv->use_riwt, priv->mode,
+					(i == DMA_RX_SIZE - 1),
+					priv->dma_buf_sz);
+}
+
+/**
+ * stmmac_clear_tx_descriptors - clear tx descriptors
+ * @priv: driver private structure
+ * @queue: TX queue index.
+ * Description: this function is called to clear the TX descriptors
+ * in case of both basic and extended descriptors are used.
+ */
+static void stmmac_clear_tx_descriptors(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+	int i;
+
+	/* Clear the TX descriptors */
+	for (i = 0; i < DMA_TX_SIZE; i++) {
+		int last = (i == (DMA_TX_SIZE - 1));
+		struct dma_desc *p;
+
+		if (priv->extend_desc)
+			p = &tx_q->dma_etx[i].basic;
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			p = &tx_q->dma_entx[i].basic;
+		else
+			p = &tx_q->dma_tx[i];
+
+		stmmac_init_tx_desc(priv, p, priv->mode, last);
+	}
+}
+
+/**
+ * stmmac_clear_descriptors - clear descriptors
+ * @priv: driver private structure
+ * Description: this function is called to clear the TX and RX descriptors
+ * in case of both basic and extended descriptors are used.
+ */
+static void stmmac_clear_descriptors(struct stmmac_priv *priv)
+{
+	u32 rx_queue_cnt = priv->plat->rx_queues_to_use;
+	u32 tx_queue_cnt = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	/* Clear the RX descriptors */
+	for (queue = 0; queue < rx_queue_cnt; queue++)
+		stmmac_clear_rx_descriptors(priv, queue);
+
+	/* Clear the TX descriptors */
+	for (queue = 0; queue < tx_queue_cnt; queue++)
+		stmmac_clear_tx_descriptors(priv, queue);
+}
+
+/**
+ * stmmac_init_rx_buffers - init the RX descriptor buffer.
+ * @priv: driver private structure
+ * @p: descriptor pointer
+ * @i: descriptor index
+ * @flags: gfp flag
+ * @queue: RX queue index
+ * Description: this function is called to allocate a receive buffer, perform
+ * the DMA mapping and init the descriptor.
+ */
+static int stmmac_init_rx_buffers(struct stmmac_priv *priv, struct dma_desc *p,
+				  int i, gfp_t flags, u32 queue)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	struct stmmac_rx_buffer *buf = &rx_q->buf_pool[i];
+
+	buf->page = page_pool_dev_alloc_pages(rx_q->page_pool);
+	if (!buf->page)
+		return -ENOMEM;
+
+	if (priv->sph) {
+		buf->sec_page = page_pool_dev_alloc_pages(rx_q->page_pool);
+		if (!buf->sec_page)
+			return -ENOMEM;
+
+		buf->sec_addr = page_pool_get_dma_addr(buf->sec_page);
+		stmmac_set_desc_sec_addr(priv, p, buf->sec_addr);
+	} else {
+		buf->sec_page = NULL;
+	}
+
+	buf->addr = page_pool_get_dma_addr(buf->page);
+	stmmac_set_desc_addr(priv, p, buf->addr);
+	if (priv->dma_buf_sz == BUF_SIZE_16KiB)
+		stmmac_init_desc3(priv, p);
+
+	return 0;
+}
+
+/**
+ * stmmac_free_rx_buffer - free RX dma buffers
+ * @priv: private structure
+ * @queue: RX queue index
+ * @i: buffer index.
+ */
+static void stmmac_free_rx_buffer(struct stmmac_priv *priv, u32 queue, int i)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	struct stmmac_rx_buffer *buf = &rx_q->buf_pool[i];
+
+	if (buf->page)
+		page_pool_put_full_page(rx_q->page_pool, buf->page, false);
+	buf->page = NULL;
+
+	if (buf->sec_page)
+		page_pool_put_full_page(rx_q->page_pool, buf->sec_page, false);
+	buf->sec_page = NULL;
+}
+
+/**
+ * stmmac_free_tx_buffer - free RX dma buffers
+ * @priv: private structure
+ * @queue: RX queue index
+ * @i: buffer index.
+ */
+static void stmmac_free_tx_buffer(struct stmmac_priv *priv, u32 queue, int i)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+
+	if (tx_q->tx_skbuff_dma[i].buf) {
+		if (tx_q->tx_skbuff_dma[i].map_as_page)
+			dma_unmap_page(priv->device,
+				       tx_q->tx_skbuff_dma[i].buf,
+				       tx_q->tx_skbuff_dma[i].len,
+				       DMA_TO_DEVICE);
+		else
+			dma_unmap_single(priv->device,
+					 tx_q->tx_skbuff_dma[i].buf,
+					 tx_q->tx_skbuff_dma[i].len,
+					 DMA_TO_DEVICE);
+	}
+
+	if (tx_q->tx_skbuff[i]) {
+#if 0
+		dev_kfree_skb_any(tx_q->tx_skbuff[i]);
+#endif
+		dev_kfree_rtskb(tx_q->tx_skbuff[i]);
+		tx_q->tx_skbuff[i] = NULL;
+		tx_q->tx_skbuff_dma[i].buf = 0;
+		tx_q->tx_skbuff_dma[i].map_as_page = false;
+	}
+}
+
+/**
+ * init_dma_rx_desc_rings - init the RX descriptor rings
+ * @dev: net device structure
+ * @flags: gfp flag.
+ * Description: this function initializes the DMA RX descriptors
+ * and allocates the socket buffers. It supports the chained and ring
+ * modes.
+ */
+static int init_dma_rx_desc_rings(struct rtnet_device *dev, gfp_t flags)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	u32 rx_count = priv->plat->rx_queues_to_use;
+	int ret = -ENOMEM;
+	int queue;
+	int i;
+
+	/* RX INITIALIZATION */
+	netif_dbg(priv, probe, priv->dummy,
+		  "SKB addresses:\nskb\t\tskb data\tdma data\n");
+
+	for (queue = 0; queue < rx_count; queue++) {
+		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+		netif_dbg(priv, probe, priv->dummy,
+			  "(%s) dma_rx_phy=0x%08x\n", __func__,
+			  (u32)rx_q->dma_rx_phy);
+
+		stmmac_clear_rx_descriptors(priv, queue);
+
+		for (i = 0; i < DMA_RX_SIZE; i++) {
+			struct dma_desc *p;
+
+			if (priv->extend_desc)
+				p = &((rx_q->dma_erx + i)->basic);
+			else
+				p = rx_q->dma_rx + i;
+
+			ret = stmmac_init_rx_buffers(priv, p, i, flags,
+						     queue);
+			if (ret)
+				goto err_init_rx_buffers;
+		}
+
+		rx_q->cur_rx = 0;
+		rx_q->dirty_rx = (unsigned int)(i - DMA_RX_SIZE);
+
+		/* Setup the chained descriptor addresses */
+		if (priv->mode == STMMAC_CHAIN_MODE) {
+			if (priv->extend_desc)
+				stmmac_mode_init(priv, rx_q->dma_erx,
+						rx_q->dma_rx_phy, DMA_RX_SIZE, 1);
+			else
+				stmmac_mode_init(priv, rx_q->dma_rx,
+						rx_q->dma_rx_phy, DMA_RX_SIZE, 0);
+		}
+	}
+
+	return 0;
+
+err_init_rx_buffers:
+	while (queue >= 0) {
+		while (--i >= 0)
+			stmmac_free_rx_buffer(priv, queue, i);
+
+		if (queue == 0)
+			break;
+
+		i = DMA_RX_SIZE;
+		queue--;
+	}
+
+	return ret;
+}
+
+/**
+ * init_dma_tx_desc_rings - init the TX descriptor rings
+ * @dev: net device structure.
+ * Description: this function initializes the DMA TX descriptors
+ * and allocates the socket buffers. It supports the chained and ring
+ * modes.
+ */
+static int init_dma_tx_desc_rings(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	u32 tx_queue_cnt = priv->plat->tx_queues_to_use;
+	u32 queue;
+	int i;
+
+	for (queue = 0; queue < tx_queue_cnt; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+
+		netif_dbg(priv, probe, priv->dummy,
+			  "(%s) dma_tx_phy=0x%08x\n", __func__,
+			 (u32)tx_q->dma_tx_phy);
+
+		/* Setup the chained descriptor addresses */
+		if (priv->mode == STMMAC_CHAIN_MODE) {
+			if (priv->extend_desc)
+				stmmac_mode_init(priv, tx_q->dma_etx,
+						tx_q->dma_tx_phy, DMA_TX_SIZE, 1);
+			else if (!(tx_q->tbs & STMMAC_TBS_AVAIL))
+				stmmac_mode_init(priv, tx_q->dma_tx,
+						tx_q->dma_tx_phy, DMA_TX_SIZE, 0);
+		}
+
+		for (i = 0; i < DMA_TX_SIZE; i++) {
+			struct dma_desc *p;
+			if (priv->extend_desc)
+				p = &((tx_q->dma_etx + i)->basic);
+			else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+				p = &((tx_q->dma_entx + i)->basic);
+			else
+				p = tx_q->dma_tx + i;
+
+			stmmac_clear_desc(priv, p);
+
+			tx_q->tx_skbuff_dma[i].buf = 0;
+			tx_q->tx_skbuff_dma[i].map_as_page = false;
+			tx_q->tx_skbuff_dma[i].len = 0;
+			tx_q->tx_skbuff_dma[i].last_segment = false;
+			tx_q->tx_skbuff[i] = NULL;
+		}
+
+		tx_q->dirty_tx = 0;
+		tx_q->cur_tx = 0;
+		tx_q->mss = 0;
+
+		rtnetdev_tx_reset_queue(rtnetdev_get_tx_queue(priv->dev, queue));
+	}
+
+	return 0;
+}
+
+/**
+ * init_dma_desc_rings - init the RX/TX descriptor rings
+ * @dev: net device structure
+ * @flags: gfp flag.
+ * Description: this function initializes the DMA RX/TX descriptors
+ * and allocates the socket buffers. It supports the chained and ring
+ * modes.
+ */
+static int init_dma_desc_rings(struct rtnet_device *dev, gfp_t flags)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	int ret;
+
+	ret = init_dma_rx_desc_rings(dev, flags);
+	if (ret)
+		return ret;
+
+	ret = init_dma_tx_desc_rings(dev);
+
+	stmmac_clear_descriptors(priv);
+
+	if (netif_msg_hw(priv))
+		stmmac_display_rings(priv);
+
+	return ret;
+}
+
+/**
+ * dma_free_rx_skbufs - free RX dma buffers
+ * @priv: private structure
+ * @queue: RX queue index
+ */
+static void dma_free_rx_skbufs(struct stmmac_priv *priv, u32 queue)
+{
+	int i;
+
+	for (i = 0; i < DMA_RX_SIZE; i++)
+		stmmac_free_rx_buffer(priv, queue, i);
+}
+
+/**
+ * dma_free_tx_skbufs - free TX dma buffers
+ * @priv: private structure
+ * @queue: TX queue index
+ */
+static void dma_free_tx_skbufs(struct stmmac_priv *priv, u32 queue)
+{
+	int i;
+
+	for (i = 0; i < DMA_TX_SIZE; i++)
+		stmmac_free_tx_buffer(priv, queue, i);
+}
+
+/**
+ * free_dma_rx_desc_resources - free RX dma desc resources
+ * @priv: private structure
+ */
+static void free_dma_rx_desc_resources(struct stmmac_priv *priv)
+{
+	u32 rx_count = priv->plat->rx_queues_to_use;
+	u32 queue;
+
+	/* Free RX queue resources */
+	for (queue = 0; queue < rx_count; queue++) {
+		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+		/* Release the DMA RX socket buffers */
+		dma_free_rx_skbufs(priv, queue);
+
+		/* Free DMA regions of consistent memory previously allocated */
+		if (!priv->extend_desc)
+			dma_free_coherent(priv->device,
+					  DMA_RX_SIZE * sizeof(struct dma_desc),
+					  rx_q->dma_rx, rx_q->dma_rx_phy);
+		else
+			dma_free_coherent(priv->device, DMA_RX_SIZE *
+					  sizeof(struct dma_extended_desc),
+					  rx_q->dma_erx, rx_q->dma_rx_phy);
+
+		kfree(rx_q->buf_pool);
+		if (rx_q->page_pool)
+			page_pool_destroy(rx_q->page_pool);
+	}
+}
+
+/**
+ * free_dma_tx_desc_resources - free TX dma desc resources
+ * @priv: private structure
+ */
+static void free_dma_tx_desc_resources(struct stmmac_priv *priv)
+{
+	u32 tx_count = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	/* Free TX queue resources */
+	for (queue = 0; queue < tx_count; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+		size_t size;
+		void *addr;
+
+		/* Release the DMA TX socket buffers */
+		dma_free_tx_skbufs(priv, queue);
+
+		if (priv->extend_desc) {
+			size = sizeof(struct dma_extended_desc);
+			addr = tx_q->dma_etx;
+		} else if (tx_q->tbs & STMMAC_TBS_AVAIL) {
+			size = sizeof(struct dma_edesc);
+			addr = tx_q->dma_entx;
+		} else {
+			size = sizeof(struct dma_desc);
+			addr = tx_q->dma_tx;
+		}
+
+		size *= DMA_TX_SIZE;
+
+		dma_free_coherent(priv->device, size, addr, tx_q->dma_tx_phy);
+
+		kfree(tx_q->tx_skbuff_dma);
+		kfree(tx_q->tx_skbuff);
+	}
+}
+
+/**
+ * alloc_dma_rx_desc_resources - alloc RX resources.
+ * @priv: private structure
+ * Description: according to which descriptor can be used (extend or basic)
+ * this function allocates the resources for TX and RX paths. In case of
+ * reception, for example, it pre-allocated the RX socket buffer in order to
+ * allow zero-copy mechanism.
+ */
+static int alloc_dma_rx_desc_resources(struct stmmac_priv *priv)
+{
+	u32 rx_count = priv->plat->rx_queues_to_use;
+	int ret = -ENOMEM;
+	u32 queue;
+
+	/* RX queues buffers and DMA */
+	for (queue = 0; queue < rx_count; queue++) {
+		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+		struct page_pool_params pp_params = { 0 };
+		unsigned int num_pages;
+
+		rx_q->queue_index = queue;
+		rx_q->priv_data = priv;
+
+		pp_params.flags = PP_FLAG_DMA_MAP;
+		pp_params.pool_size = DMA_RX_SIZE;
+		num_pages = DIV_ROUND_UP(priv->dma_buf_sz, PAGE_SIZE);
+		pp_params.order = ilog2(num_pages);
+		pp_params.nid = dev_to_node(priv->device);
+		pp_params.dev = priv->device;
+		pp_params.dma_dir = DMA_FROM_DEVICE;
+
+		rx_q->page_pool = page_pool_create(&pp_params);
+		if (IS_ERR(rx_q->page_pool)) {
+			ret = PTR_ERR(rx_q->page_pool);
+			rx_q->page_pool = NULL;
+			goto err_dma;
+		}
+
+		rx_q->buf_pool = kcalloc(DMA_RX_SIZE, sizeof(*rx_q->buf_pool),
+					 GFP_KERNEL);
+		if (!rx_q->buf_pool)
+			goto err_dma;
+
+		if (priv->extend_desc) {
+			rx_q->dma_erx = dma_alloc_coherent(priv->device,
+							   DMA_RX_SIZE * sizeof(struct dma_extended_desc),
+							   &rx_q->dma_rx_phy,
+							   GFP_KERNEL);
+			if (!rx_q->dma_erx)
+				goto err_dma;
+
+		} else {
+			rx_q->dma_rx = dma_alloc_coherent(priv->device,
+							  DMA_RX_SIZE * sizeof(struct dma_desc),
+							  &rx_q->dma_rx_phy,
+							  GFP_KERNEL);
+			if (!rx_q->dma_rx)
+				goto err_dma;
+		}
+	}
+
+	return 0;
+
+err_dma:
+	free_dma_rx_desc_resources(priv);
+
+	return ret;
+}
+
+/**
+ * alloc_dma_tx_desc_resources - alloc TX resources.
+ * @priv: private structure
+ * Description: according to which descriptor can be used (extend or basic)
+ * this function allocates the resources for TX and RX paths. In case of
+ * reception, for example, it pre-allocated the RX socket buffer in order to
+ * allow zero-copy mechanism.
+ */
+static int alloc_dma_tx_desc_resources(struct stmmac_priv *priv)
+{
+	u32 tx_count = priv->plat->tx_queues_to_use;
+	int ret = -ENOMEM;
+	u32 queue;
+
+	/* TX queues buffers and DMA */
+	for (queue = 0; queue < tx_count; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+		size_t size;
+		void *addr;
+
+		tx_q->queue_index = queue;
+		tx_q->priv_data = priv;
+
+		tx_q->tx_skbuff_dma = kcalloc(DMA_TX_SIZE,
+					      sizeof(*tx_q->tx_skbuff_dma),
+					      GFP_KERNEL);
+		if (!tx_q->tx_skbuff_dma)
+			goto err_dma;
+
+		tx_q->tx_skbuff = kcalloc(DMA_TX_SIZE,
+					  sizeof(struct rtskb *),
+					  GFP_KERNEL);
+		if (!tx_q->tx_skbuff)
+			goto err_dma;
+
+		if (priv->extend_desc)
+			size = sizeof(struct dma_extended_desc);
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			size = sizeof(struct dma_edesc);
+		else
+			size = sizeof(struct dma_desc);
+
+		size *= DMA_TX_SIZE;
+
+		addr = dma_alloc_coherent(priv->device, size,
+					  &tx_q->dma_tx_phy, GFP_KERNEL);
+		if (!addr)
+			goto err_dma;
+
+		if (priv->extend_desc)
+			tx_q->dma_etx = addr;
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			tx_q->dma_entx = addr;
+		else
+			tx_q->dma_tx = addr;
+	}
+
+	return 0;
+
+err_dma:
+	free_dma_tx_desc_resources(priv);
+	return ret;
+}
+
+/**
+ * alloc_dma_desc_resources - alloc TX/RX resources.
+ * @priv: private structure
+ * Description: according to which descriptor can be used (extend or basic)
+ * this function allocates the resources for TX and RX paths. In case of
+ * reception, for example, it pre-allocated the RX socket buffer in order to
+ * allow zero-copy mechanism.
+ */
+static int alloc_dma_desc_resources(struct stmmac_priv *priv)
+{
+	/* RX Allocation */
+	int ret = alloc_dma_rx_desc_resources(priv);
+
+	if (ret)
+		return ret;
+
+	ret = alloc_dma_tx_desc_resources(priv);
+
+	return ret;
+}
+
+/**
+ * free_dma_desc_resources - free dma desc resources
+ * @priv: private structure
+ */
+static void free_dma_desc_resources(struct stmmac_priv *priv)
+{
+	/* Release the DMA RX socket buffers */
+	free_dma_rx_desc_resources(priv);
+
+	/* Release the DMA TX socket buffers */
+	free_dma_tx_desc_resources(priv);
+}
+
+/**
+ *  stmmac_mac_enable_rx_queues - Enable MAC rx queues
+ *  @priv: driver private structure
+ *  Description: It is used for enabling the rx queues in the MAC
+ */
+static void stmmac_mac_enable_rx_queues(struct stmmac_priv *priv)
+{
+	u32 rx_queues_count = priv->plat->rx_queues_to_use;
+	int queue;
+	u8 mode;
+
+	for (queue = 0; queue < rx_queues_count; queue++) {
+		mode = priv->plat->rx_queues_cfg[queue].mode_to_use;
+		stmmac_rx_queue_enable(priv, priv->hw, mode, queue);
+	}
+}
+
+/**
+ * stmmac_start_rx_dma - start RX DMA channel
+ * @priv: driver private structure
+ * @chan: RX channel index
+ * Description:
+ * This starts a RX DMA channel
+ */
+static void stmmac_start_rx_dma(struct stmmac_priv *priv, u32 chan)
+{
+	netdev_dbg(priv->dummy, "DMA RX processes started in channel %d\n", chan);
+	stmmac_start_rx(priv, priv->ioaddr, chan);
+}
+
+/**
+ * stmmac_start_tx_dma - start TX DMA channel
+ * @priv: driver private structure
+ * @chan: TX channel index
+ * Description:
+ * This starts a TX DMA channel
+ */
+static void stmmac_start_tx_dma(struct stmmac_priv *priv, u32 chan)
+{
+	netdev_dbg(priv->dummy, "DMA TX processes started in channel %d\n", chan);
+	stmmac_start_tx(priv, priv->ioaddr, chan);
+}
+
+/**
+ * stmmac_stop_rx_dma - stop RX DMA channel
+ * @priv: driver private structure
+ * @chan: RX channel index
+ * Description:
+ * This stops a RX DMA channel
+ */
+static void stmmac_stop_rx_dma(struct stmmac_priv *priv, u32 chan)
+{
+	netdev_dbg(priv->dummy, "DMA RX processes stopped in channel %d\n", chan);
+	stmmac_stop_rx(priv, priv->ioaddr, chan);
+}
+
+/**
+ * stmmac_stop_tx_dma - stop TX DMA channel
+ * @priv: driver private structure
+ * @chan: TX channel index
+ * Description:
+ * This stops a TX DMA channel
+ */
+static void stmmac_stop_tx_dma(struct stmmac_priv *priv, u32 chan)
+{
+	netdev_dbg(priv->dummy, "DMA TX processes stopped in channel %d\n", chan);
+	stmmac_stop_tx(priv, priv->ioaddr, chan);
+}
+
+/**
+ * stmmac_start_all_dma - start all RX and TX DMA channels
+ * @priv: driver private structure
+ * Description:
+ * This starts all the RX and TX DMA channels
+ */
+static void stmmac_start_all_dma(struct stmmac_priv *priv)
+{
+	u32 rx_channels_count = priv->plat->rx_queues_to_use;
+	u32 tx_channels_count = priv->plat->tx_queues_to_use;
+	u32 chan = 0;
+
+	for (chan = 0; chan < rx_channels_count; chan++)
+		stmmac_start_rx_dma(priv, chan);
+
+	for (chan = 0; chan < tx_channels_count; chan++)
+		stmmac_start_tx_dma(priv, chan);
+}
+
+/**
+ * stmmac_stop_all_dma - stop all RX and TX DMA channels
+ * @priv: driver private structure
+ * Description:
+ * This stops the RX and TX DMA channels
+ */
+static void stmmac_stop_all_dma(struct stmmac_priv *priv)
+{
+	u32 rx_channels_count = priv->plat->rx_queues_to_use;
+	u32 tx_channels_count = priv->plat->tx_queues_to_use;
+	u32 chan = 0;
+
+	for (chan = 0; chan < rx_channels_count; chan++)
+		stmmac_stop_rx_dma(priv, chan);
+
+	for (chan = 0; chan < tx_channels_count; chan++)
+		stmmac_stop_tx_dma(priv, chan);
+}
+
+/**
+ *  stmmac_dma_operation_mode - HW DMA operation mode
+ *  @priv: driver private structure
+ *  Description: it is used for configuring the DMA operation mode register in
+ *  order to program the tx/rx DMA thresholds or Store-And-Forward mode.
+ */
+static void stmmac_dma_operation_mode(struct stmmac_priv *priv)
+{
+	u32 rx_channels_count = priv->plat->rx_queues_to_use;
+	u32 tx_channels_count = priv->plat->tx_queues_to_use;
+	int rxfifosz = priv->plat->rx_fifo_size;
+	int txfifosz = priv->plat->tx_fifo_size;
+	u32 txmode = 0;
+	u32 rxmode = 0;
+	u32 chan = 0;
+	u8 qmode = 0;
+
+	if (rxfifosz == 0)
+		rxfifosz = priv->dma_cap.rx_fifo_size;
+	if (txfifosz == 0)
+		txfifosz = priv->dma_cap.tx_fifo_size;
+
+	/* Adjust for real per queue fifo size */
+	rxfifosz /= rx_channels_count;
+	txfifosz /= tx_channels_count;
+
+	if (priv->plat->force_thresh_dma_mode) {
+		txmode = tc;
+		rxmode = tc;
+	} else if (priv->plat->force_sf_dma_mode || priv->plat->tx_coe) {
+		/*
+		 * In case of GMAC, SF mode can be enabled
+		 * to perform the TX COE in HW. This depends on:
+		 * 1) TX COE if actually supported
+		 * 2) There is no bugged Jumbo frame support
+		 *    that needs to not insert csum in the TDES.
+		 */
+		txmode = SF_DMA_MODE;
+		rxmode = SF_DMA_MODE;
+		priv->xstats.threshold = SF_DMA_MODE;
+	} else {
+		txmode = tc;
+		rxmode = SF_DMA_MODE;
+	}
+
+	/* configure all channels */
+	for (chan = 0; chan < rx_channels_count; chan++) {
+		qmode = priv->plat->rx_queues_cfg[chan].mode_to_use;
+
+		stmmac_dma_rx_mode(priv, priv->ioaddr, rxmode, chan,
+				rxfifosz, qmode);
+		stmmac_set_dma_bfsize(priv, priv->ioaddr, priv->dma_buf_sz,
+				chan);
+	}
+
+	for (chan = 0; chan < tx_channels_count; chan++) {
+		qmode = priv->plat->tx_queues_cfg[chan].mode_to_use;
+
+		stmmac_dma_tx_mode(priv, priv->ioaddr, txmode, chan,
+				txfifosz, qmode);
+	}
+}
+
+/**
+ * stmmac_tx_clean - to manage the transmission completion
+ * @priv: driver private structure
+ * @queue: TX queue index
+ * Description: it reclaims the transmit resources after transmission completes.
+ */
+static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+	unsigned int bytes_compl = 0, pkts_compl = 0;
+	unsigned int entry, count = 0;
+
+	/* RTnet is single queue */
+	raw_spin_lock(&priv->rtnet_queue_lock);
+#if 0
+	__rtnetif_tx_lock_bh(rtnetdev_get_tx_queue(priv->dev, queue));
+#endif
+	
+	priv->xstats.tx_clean++;
+
+	entry = tx_q->dirty_tx;
+	while ((entry != tx_q->cur_tx) && (count < budget)) {
+		struct rtskb *skb = tx_q->tx_skbuff[entry];
+		struct dma_desc *p;
+		int status;
+
+		if (priv->extend_desc)
+			p = (struct dma_desc *)(tx_q->dma_etx + entry);
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			p = &tx_q->dma_entx[entry].basic;
+		else
+			p = tx_q->dma_tx + entry;
+
+		status = stmmac_tx_status(priv, &priv->stats,
+				&priv->xstats, p, priv->ioaddr);
+		/* Check if the descriptor is owned by the DMA */
+		if (unlikely(status & tx_dma_own))
+			break;
+
+		count++;
+
+		/* Make sure descriptor fields are read after reading
+		 * the own bit.
+		 */
+		dma_rmb();
+
+		/* Just consider the last segment and ...*/
+		if (likely(!(status & tx_not_ls))) {
+			/* ... verify the status error condition */
+			if (unlikely(status & tx_err)) {
+				priv->stats.tx_errors++;
+			} else {
+				priv->stats.tx_packets++;
+				priv->xstats.tx_pkt_n++;
+			}
+			stmmac_get_tx_hwtstamp(priv, p, skb);
+		}
+
+		if (likely(tx_q->tx_skbuff_dma[entry].buf)) {
+			if (tx_q->tx_skbuff_dma[entry].map_as_page)
+				dma_unmap_page(priv->device,
+					       tx_q->tx_skbuff_dma[entry].buf,
+					       tx_q->tx_skbuff_dma[entry].len,
+					       DMA_TO_DEVICE);
+			else
+				dma_unmap_single(priv->device,
+						 tx_q->tx_skbuff_dma[entry].buf,
+						 tx_q->tx_skbuff_dma[entry].len,
+						 DMA_TO_DEVICE);
+			tx_q->tx_skbuff_dma[entry].buf = 0;
+			tx_q->tx_skbuff_dma[entry].len = 0;
+			tx_q->tx_skbuff_dma[entry].map_as_page = false;
+		}
+
+		stmmac_clean_desc3(priv, tx_q, p);
+
+		tx_q->tx_skbuff_dma[entry].last_segment = false;
+		tx_q->tx_skbuff_dma[entry].is_jumbo = false;
+
+		if (likely(skb != NULL)) {
+			pkts_compl++;
+			bytes_compl += skb->len;
+			dev_kfree_rtskb(skb);
+			tx_q->tx_skbuff[entry] = NULL;
+		}
+
+		stmmac_release_tx_desc(priv, p, priv->mode);
+
+		entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
+	}
+	tx_q->dirty_tx = entry;
+
+	rtnetdev_tx_completed_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, queue),
+				  pkts_compl, bytes_compl);
+
+	if (unlikely(rtnetif_tx_queue_stopped(priv->dev, rtnetdev_get_tx_queue(priv->dev,
+								queue))) &&
+	    stmmac_tx_avail(priv, queue) > STMMAC_TX_THRESH) {
+
+		netif_dbg(priv, tx_done, priv->dummy,
+			  "%s: restart transmit\n", __func__);
+		rtnetif_tx_wake_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, queue));
+	}
+
+	if ((priv->eee_enabled) && (!priv->tx_path_in_lpi_mode)) {
+		stmmac_enable_eee_mode(priv);
+		mod_timer(&priv->eee_ctrl_timer, STMMAC_LPI_T(eee_timer));
+	}
+
+
+	/* We still have pending packets, let's call for a new scheduling */
+	if (tx_q->dirty_tx != tx_q->cur_tx) {
+#if 0		
+		trace_printk("tx_q->dirty_tx != tx_q->cur_tx\n");
+		mod_timer(&tx_q->txtimer, STMMAC_COAL_TIMER(priv->tx_coal_timer));
+#endif
+	}
+	
+	/* RTnet is single queue */
+	raw_spin_unlock(&priv->rtnet_queue_lock);
+#if 0
+	__rtnetif_tx_unlock_bh(rtnetdev_get_tx_queue(priv->dev, queue));
+#endif
+	
+	return count;
+}
+
+/**
+ * stmmac_tx_err - to manage the tx error
+ * @priv: driver private structure
+ * @chan: channel index
+ * Description: it cleans the descriptors and restarts the transmission
+ * in case of transmission errors.
+ */
+static void stmmac_tx_err(struct stmmac_priv *priv, u32 chan)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[chan];
+
+	rtnetif_tx_stop_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, chan));
+
+	stmmac_stop_tx_dma(priv, chan);
+	dma_free_tx_skbufs(priv, chan);
+	stmmac_clear_tx_descriptors(priv, chan);
+	tx_q->dirty_tx = 0;
+	tx_q->cur_tx = 0;
+	tx_q->mss = 0;
+	rtnetdev_tx_reset_queue(rtnetdev_get_tx_queue(priv->dev, chan));
+	stmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,
+			    tx_q->dma_tx_phy, chan);
+	stmmac_start_tx_dma(priv, chan);
+
+	priv->stats.tx_errors++;
+	rtnetif_tx_wake_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, chan));
+}
+
+/**
+ *  stmmac_set_dma_operation_mode - Set DMA operation mode by channel
+ *  @priv: driver private structure
+ *  @txmode: TX operating mode
+ *  @rxmode: RX operating mode
+ *  @chan: channel index
+ *  Description: it is used for configuring of the DMA operation mode in
+ *  runtime in order to program the tx/rx DMA thresholds or Store-And-Forward
+ *  mode.
+ */
+static void stmmac_set_dma_operation_mode(struct stmmac_priv *priv, u32 txmode,
+					  u32 rxmode, u32 chan)
+{
+	u8 rxqmode = priv->plat->rx_queues_cfg[chan].mode_to_use;
+	u8 txqmode = priv->plat->tx_queues_cfg[chan].mode_to_use;
+	u32 rx_channels_count = priv->plat->rx_queues_to_use;
+	u32 tx_channels_count = priv->plat->tx_queues_to_use;
+	int rxfifosz = priv->plat->rx_fifo_size;
+	int txfifosz = priv->plat->tx_fifo_size;
+
+	if (rxfifosz == 0)
+		rxfifosz = priv->dma_cap.rx_fifo_size;
+	if (txfifosz == 0)
+		txfifosz = priv->dma_cap.tx_fifo_size;
+
+	/* Adjust for real per queue fifo size */
+	rxfifosz /= rx_channels_count;
+	txfifosz /= tx_channels_count;
+
+	stmmac_dma_rx_mode(priv, priv->ioaddr, rxmode, chan, rxfifosz, rxqmode);
+	stmmac_dma_tx_mode(priv, priv->ioaddr, txmode, chan, txfifosz, txqmode);
+}
+
+static bool stmmac_safety_feat_interrupt(struct stmmac_priv *priv)
+{
+	int ret;
+
+	ret = stmmac_safety_feat_irq_status(priv, priv->dummy,
+			priv->ioaddr, priv->dma_cap.asp, &priv->sstats);
+	if (ret && (ret != -EINVAL)) {
+		stmmac_global_err(priv);
+		return true;
+	}
+
+	return false;
+}
+
+static int stmmac_napi_check(struct stmmac_priv *priv, u32 chan)
+{
+	int status = stmmac_dma_interrupt_status(priv, priv->ioaddr,
+						 &priv->xstats, chan);
+	struct stmmac_channel *ch;
+	unsigned long flags;
+	ch = &priv->channel[chan];
+	
+	if ((status & handle_rx) && (chan < priv->plat->rx_queues_to_use)) {
+		raw_spin_lock_irqsave(&ch->lock, flags);
+		stmmac_disable_dma_irq(priv, priv->ioaddr, chan, 1, 0);
+		raw_spin_unlock_irqrestore(&ch->lock, flags);
+		
+		rtdm_event_signal_one(&rx_event);
+	}
+
+	if ((status & handle_tx) && (chan < priv->plat->tx_queues_to_use)) {
+		raw_spin_lock_irqsave(&ch->lock, flags);
+		stmmac_disable_dma_irq(priv, priv->ioaddr, chan, 0, 1);
+		raw_spin_unlock_irqrestore(&ch->lock, flags);
+		
+		rtdm_event_signal_one(&tx_event);
+	}
+
+	return status;
+}
+
+/**
+ * stmmac_dma_interrupt - DMA ISR
+ * @priv: driver private structure
+ * Description: this is the DMA ISR. It is called by the main ISR.
+ * It calls the dwmac dma routine and schedule poll method in case of some
+ * work can be done.
+ */
+static irqreturn_t stmmac_dma_interrupt(int irq, void *dev_id)
+{
+	struct rtnet_device *dev = (struct rtnet_device *)dev_id;
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	u32 tx_channel_count = priv->plat->tx_queues_to_use;
+	u32 rx_channel_count = priv->plat->rx_queues_to_use;
+	u32 channels_to_check = tx_channel_count > rx_channel_count ?
+				tx_channel_count : rx_channel_count;
+	u32 chan;
+	int status[max_t(u32, MTL_MAX_TX_QUEUES, MTL_MAX_RX_QUEUES)];
+
+	/* Make sure we never check beyond our status buffer. */
+	if (WARN_ON_ONCE(channels_to_check > ARRAY_SIZE(status)))
+		channels_to_check = ARRAY_SIZE(status);
+
+	for (chan = 0; chan < channels_to_check; chan++)
+		status[chan] = stmmac_napi_check(priv, chan);
+
+	for (chan = 0; chan < tx_channel_count; chan++) {
+		if (unlikely(status[chan] & tx_hard_error_bump_tc)) {
+			/* Try to bump up the dma threshold on this failure */
+			if (unlikely(priv->xstats.threshold != SF_DMA_MODE) &&
+			    (tc <= 256)) {
+				tc += 64;
+				if (priv->plat->force_thresh_dma_mode)
+					stmmac_set_dma_operation_mode(priv,
+								      tc,
+								      tc,
+								      chan);
+				else
+					stmmac_set_dma_operation_mode(priv,
+								    tc,
+								    SF_DMA_MODE,
+								    chan);
+				priv->xstats.threshold = tc;
+			}
+		} else if (unlikely(status[chan] == tx_hard_error)) {
+			trace_printk("stmmac_tx_err\n");
+			stmmac_tx_err(priv, chan);
+		}
+	}
+	
+	return IRQ_HANDLED;
+}
+
+/**
+ * stmmac_mmc_setup: setup the Mac Management Counters (MMC)
+ * @priv: driver private structure
+ * Description: this masks the MMC irq, in fact, the counters are managed in SW.
+ */
+static void stmmac_mmc_setup(struct stmmac_priv *priv)
+{
+	unsigned int mode = MMC_CNTRL_RESET_ON_READ | MMC_CNTRL_COUNTER_RESET |
+			    MMC_CNTRL_PRESET | MMC_CNTRL_FULL_HALF_PRESET;
+
+	stmmac_mmc_intr_all_mask(priv, priv->mmcaddr);
+
+	if (priv->dma_cap.rmon) {
+		stmmac_mmc_ctrl(priv, priv->mmcaddr, mode);
+		memset(&priv->mmc, 0, sizeof(struct stmmac_counters));
+	} else
+		netdev_info(priv->dummy, "No MAC Management Counters available\n");
+}
+
+/**
+ * stmmac_get_hw_features - get MAC capabilities from the HW cap. register.
+ * @priv: driver private structure
+ * Description:
+ *  new GMAC chip generations have a new register to indicate the
+ *  presence of the optional feature/functions.
+ *  This can be also used to override the value passed through the
+ *  platform and necessary for old MAC10/100 and GMAC chips.
+ */
+static int stmmac_get_hw_features(struct stmmac_priv *priv)
+{
+	return stmmac_get_hw_feature(priv, priv->ioaddr, &priv->dma_cap) == 0;
+}
+
+/**
+ * stmmac_check_ether_addr - check if the MAC addr is valid
+ * @priv: driver private structure
+ * Description:
+ * it is to verify if the MAC address is valid, in case of failures it
+ * generates a random MAC address
+ */
+static void stmmac_check_ether_addr(struct stmmac_priv *priv)
+{
+	if (!is_valid_ether_addr(priv->dev->dev_addr)) {
+		stmmac_get_umac_addr(priv, priv->hw, priv->dev->dev_addr, 0);
+		if (!is_valid_ether_addr(priv->dev->dev_addr))
+			eth_random_addr(priv->dev->dev_addr);
+		dev_info(priv->device, "device MAC address %pM\n",
+			 priv->dev->dev_addr);
+	}
+}
+
+/**
+ * stmmac_init_dma_engine - DMA init.
+ * @priv: driver private structure
+ * Description:
+ * It inits the DMA invoking the specific MAC/GMAC callback.
+ * Some DMA parameters can be passed from the platform;
+ * in case of these are not passed a default is kept for the MAC or GMAC.
+ */
+static int stmmac_init_dma_engine(struct stmmac_priv *priv)
+{
+	u32 rx_channels_count = priv->plat->rx_queues_to_use;
+	u32 tx_channels_count = priv->plat->tx_queues_to_use;
+	u32 dma_csr_ch = max(rx_channels_count, tx_channels_count);
+	struct stmmac_rx_queue *rx_q;
+	struct stmmac_tx_queue *tx_q;
+	u32 chan = 0;
+	int atds = 0;
+	int ret = 0;
+
+	if (!priv->plat->dma_cfg || !priv->plat->dma_cfg->pbl) {
+		dev_err(priv->device, "Invalid DMA configuration\n");
+		return -EINVAL;
+	}
+
+	if (priv->extend_desc && (priv->mode == STMMAC_RING_MODE))
+		atds = 1;
+
+	ret = stmmac_reset(priv, priv->ioaddr);
+	if (ret) {
+		dev_err(priv->device, "Failed to reset the dma\n");
+		return ret;
+	}
+
+	/* DMA Configuration */
+	stmmac_dma_init(priv, priv->ioaddr, priv->plat->dma_cfg, atds);
+
+	if (priv->plat->axi)
+		stmmac_axi(priv, priv->ioaddr, priv->plat->axi);
+
+	/* DMA CSR Channel configuration */
+	for (chan = 0; chan < dma_csr_ch; chan++)
+		stmmac_init_chan(priv, priv->ioaddr, priv->plat->dma_cfg, chan);
+
+	/* DMA RX Channel Configuration */
+	for (chan = 0; chan < rx_channels_count; chan++) {
+		rx_q = &priv->rx_queue[chan];
+
+		stmmac_init_rx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,
+				    rx_q->dma_rx_phy, chan);
+
+		rx_q->rx_tail_addr = rx_q->dma_rx_phy +
+			    (DMA_RX_SIZE * sizeof(struct dma_desc));
+		stmmac_set_rx_tail_ptr(priv, priv->ioaddr,
+				       rx_q->rx_tail_addr, chan);
+	}
+
+	/* DMA TX Channel Configuration */
+	for (chan = 0; chan < tx_channels_count; chan++) {
+		tx_q = &priv->tx_queue[chan];
+
+		stmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,
+				    tx_q->dma_tx_phy, chan);
+
+		tx_q->tx_tail_addr = tx_q->dma_tx_phy;
+		stmmac_set_tx_tail_ptr(priv, priv->ioaddr,
+				       tx_q->tx_tail_addr, chan);
+	}
+
+	return ret;
+}
+
+static void stmmac_tx_timer_arm(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+	struct stmmac_channel *ch;
+#if 0
+	mod_timer(&tx_q->txtimer, STMMAC_COAL_TIMER(priv->tx_coal_timer));
+#endif
+
+	ch = &priv->channel[tx_q->queue_index];
+	{
+		unsigned long flags;
+		raw_spin_lock_irqsave(&ch->lock, flags);
+		stmmac_disable_dma_irq(priv, priv->ioaddr, ch->index, 0, 1);
+		raw_spin_unlock_irqrestore(&ch->lock, flags);
+		
+		rtdm_event_signal_one(&tx_event);
+	}
+}
+
+/**
+ * stmmac_tx_timer - mitigation sw timer for tx.
+ * @data: data pointer
+ * Description:
+ * This is the timer handler to directly invoke the stmmac_tx_clean.
+ */
+#if 0
+static void stmmac_tx_timer(struct timer_list *t)
+{
+	struct stmmac_tx_queue *tx_q = from_timer(tx_q, t, txtimer);
+	struct stmmac_priv *priv = tx_q->priv_data;
+	struct stmmac_channel *ch;
+
+	ch = &priv->channel[tx_q->queue_index];
+
+	{
+		unsigned long flags;
+		raw_spin_lock_irqsave(&ch->lock, flags);
+		stmmac_disable_dma_irq(priv, priv->ioaddr, ch->index, 0, 1);
+		raw_spin_unlock_irqrestore(&ch->lock, flags);
+		
+		rtdm_event_signal_one(&tx_event);
+	}
+}
+#endif
+
+/**
+ * stmmac_init_coalesce - init mitigation options.
+ * @priv: driver private structure
+ * Description:
+ * This inits the coalesce parameters: i.e. timer rate,
+ * timer handler and default threshold used for enabling the
+ * interrupt on completion bit.
+ */
+static void stmmac_init_coalesce(struct stmmac_priv *priv)
+{
+#if 0
+	u32 tx_channel_count = priv->plat->tx_queues_to_use;
+	u32 chan;
+#endif
+	
+	priv->tx_coal_frames = STMMAC_TX_FRAMES;
+	priv->tx_coal_timer = STMMAC_COAL_TX_TIMER;
+	priv->rx_coal_frames = STMMAC_RX_FRAMES;
+
+#if 0
+	for (chan = 0; chan < tx_channel_count; chan++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[chan];
+
+		timer_setup(&tx_q->txtimer, stmmac_tx_timer, 0);
+	}
+#endif
+}
+
+static void stmmac_set_rings_length(struct stmmac_priv *priv)
+{
+	u32 rx_channels_count = priv->plat->rx_queues_to_use;
+	u32 tx_channels_count = priv->plat->tx_queues_to_use;
+	u32 chan;
+
+	/* set TX ring length */
+	for (chan = 0; chan < tx_channels_count; chan++)
+		stmmac_set_tx_ring_len(priv, priv->ioaddr,
+				(DMA_TX_SIZE - 1), chan);
+
+	/* set RX ring length */
+	for (chan = 0; chan < rx_channels_count; chan++)
+		stmmac_set_rx_ring_len(priv, priv->ioaddr,
+				(DMA_RX_SIZE - 1), chan);
+}
+
+/**
+ *  stmmac_set_tx_queue_weight - Set TX queue weight
+ *  @priv: driver private structure
+ *  Description: It is used for setting TX queues weight
+ */
+static void stmmac_set_tx_queue_weight(struct stmmac_priv *priv)
+{
+	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+	u32 weight;
+	u32 queue;
+
+	for (queue = 0; queue < tx_queues_count; queue++) {
+		weight = priv->plat->tx_queues_cfg[queue].weight;
+		stmmac_set_mtl_tx_queue_weight(priv, priv->hw, weight, queue);
+	}
+}
+
+/**
+ *  stmmac_configure_cbs - Configure CBS in TX queue
+ *  @priv: driver private structure
+ *  Description: It is used for configuring CBS in AVB TX queues
+ */
+static void stmmac_configure_cbs(struct stmmac_priv *priv)
+{
+	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+	u32 mode_to_use;
+	u32 queue;
+
+	/* queue 0 is reserved for legacy traffic */
+	for (queue = 1; queue < tx_queues_count; queue++) {
+		mode_to_use = priv->plat->tx_queues_cfg[queue].mode_to_use;
+		if (mode_to_use == MTL_QUEUE_DCB)
+			continue;
+
+		stmmac_config_cbs(priv, priv->hw,
+				priv->plat->tx_queues_cfg[queue].send_slope,
+				priv->plat->tx_queues_cfg[queue].idle_slope,
+				priv->plat->tx_queues_cfg[queue].high_credit,
+				priv->plat->tx_queues_cfg[queue].low_credit,
+				queue);
+	}
+}
+
+/**
+ *  stmmac_rx_queue_dma_chan_map - Map RX queue to RX dma channel
+ *  @priv: driver private structure
+ *  Description: It is used for mapping RX queues to RX dma channels
+ */
+static void stmmac_rx_queue_dma_chan_map(struct stmmac_priv *priv)
+{
+	u32 rx_queues_count = priv->plat->rx_queues_to_use;
+	u32 queue;
+	u32 chan;
+
+	for (queue = 0; queue < rx_queues_count; queue++) {
+		chan = priv->plat->rx_queues_cfg[queue].chan;
+		stmmac_map_mtl_to_dma(priv, priv->hw, queue, chan);
+	}
+}
+
+/**
+ *  stmmac_mac_config_rx_queues_prio - Configure RX Queue priority
+ *  @priv: driver private structure
+ *  Description: It is used for configuring the RX Queue Priority
+ */
+static void stmmac_mac_config_rx_queues_prio(struct stmmac_priv *priv)
+{
+	u32 rx_queues_count = priv->plat->rx_queues_to_use;
+	u32 queue;
+	u32 prio;
+
+	for (queue = 0; queue < rx_queues_count; queue++) {
+		if (!priv->plat->rx_queues_cfg[queue].use_prio)
+			continue;
+
+		prio = priv->plat->rx_queues_cfg[queue].prio;
+		stmmac_rx_queue_prio(priv, priv->hw, prio, queue);
+	}
+}
+
+/**
+ *  stmmac_mac_config_tx_queues_prio - Configure TX Queue priority
+ *  @priv: driver private structure
+ *  Description: It is used for configuring the TX Queue Priority
+ */
+static void stmmac_mac_config_tx_queues_prio(struct stmmac_priv *priv)
+{
+	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+	u32 queue;
+	u32 prio;
+
+	for (queue = 0; queue < tx_queues_count; queue++) {
+		if (!priv->plat->tx_queues_cfg[queue].use_prio)
+			continue;
+
+		prio = priv->plat->tx_queues_cfg[queue].prio;
+		stmmac_tx_queue_prio(priv, priv->hw, prio, queue);
+	}
+}
+
+/**
+ *  stmmac_mac_config_rx_queues_routing - Configure RX Queue Routing
+ *  @priv: driver private structure
+ *  Description: It is used for configuring the RX queue routing
+ */
+static void stmmac_mac_config_rx_queues_routing(struct stmmac_priv *priv)
+{
+	u32 rx_queues_count = priv->plat->rx_queues_to_use;
+	u32 queue;
+	u8 packet;
+
+	for (queue = 0; queue < rx_queues_count; queue++) {
+		/* no specific packet type routing specified for the queue */
+		if (priv->plat->rx_queues_cfg[queue].pkt_route == 0x0)
+			continue;
+
+		packet = priv->plat->rx_queues_cfg[queue].pkt_route;
+		stmmac_rx_queue_routing(priv, priv->hw, packet, queue);
+	}
+}
+
+static void stmmac_mac_config_rss(struct stmmac_priv *priv)
+{
+	if (!priv->dma_cap.rssen || !priv->plat->rss_en) {
+		priv->rss.enable = false;
+		return;
+	}
+
+	if (priv->dev->features & NETIF_F_RXHASH)
+		priv->rss.enable = true;
+	else
+		priv->rss.enable = false;
+
+	stmmac_rss_configure(priv, priv->hw, &priv->rss,
+			     priv->plat->rx_queues_to_use);
+}
+
+/**
+ *  stmmac_mtl_configuration - Configure MTL
+ *  @priv: driver private structure
+ *  Description: It is used for configurring MTL
+ */
+static void stmmac_mtl_configuration(struct stmmac_priv *priv)
+{
+	u32 rx_queues_count = priv->plat->rx_queues_to_use;
+	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+
+	if (tx_queues_count > 1)
+		stmmac_set_tx_queue_weight(priv);
+
+	/* Configure MTL RX algorithms */
+	if (rx_queues_count > 1)
+		stmmac_prog_mtl_rx_algorithms(priv, priv->hw,
+				priv->plat->rx_sched_algorithm);
+
+	/* Configure MTL TX algorithms */
+	if (tx_queues_count > 1)
+		stmmac_prog_mtl_tx_algorithms(priv, priv->hw,
+				priv->plat->tx_sched_algorithm);
+
+	/* Configure CBS in AVB TX queues */
+	if (tx_queues_count > 1)
+		stmmac_configure_cbs(priv);
+
+	/* Map RX MTL to DMA channels */
+	stmmac_rx_queue_dma_chan_map(priv);
+
+	/* Enable MAC RX Queues */
+	stmmac_mac_enable_rx_queues(priv);
+
+	/* Set RX priorities */
+	if (rx_queues_count > 1)
+		stmmac_mac_config_rx_queues_prio(priv);
+
+	/* Set TX priorities */
+	if (tx_queues_count > 1)
+		stmmac_mac_config_tx_queues_prio(priv);
+
+	/* Set RX routing */
+	if (rx_queues_count > 1)
+		stmmac_mac_config_rx_queues_routing(priv);
+
+	/* Receive Side Scaling */
+	if (rx_queues_count > 1)
+		stmmac_mac_config_rss(priv);
+}
+
+static void stmmac_safety_feat_configuration(struct stmmac_priv *priv)
+{
+	if (priv->dma_cap.asp) {
+		netdev_info(priv->dummy, "Enabling Safety Features\n");
+		stmmac_safety_feat_config(priv, priv->ioaddr, priv->dma_cap.asp);
+	} else {
+		netdev_info(priv->dummy, "No Safety Features support found\n");
+	}
+}
+
+/**
+ * stmmac_hw_setup - setup mac in a usable state.
+ *  @dev : pointer to the device structure.
+ *  Description:
+ *  this is the main function to setup the HW in a usable state because the
+ *  dma engine is reset, the core registers are configured (e.g. AXI,
+ *  Checksum features, timers). The DMA is ready to start receiving and
+ *  transmitting.
+ *  Return value:
+ *  0 on success and an appropriate (-)ve integer as defined in errno.h
+ *  file on failure.
+ */
+static int stmmac_hw_setup(struct rtnet_device *dev, bool init_ptp)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	u32 rx_cnt = priv->plat->rx_queues_to_use;
+	u32 tx_cnt = priv->plat->tx_queues_to_use;
+	u32 chan;
+	int ret;
+
+	/* DMA initialization and SW reset */
+	ret = stmmac_init_dma_engine(priv);
+	if (ret < 0) {
+		netdev_err(priv->dummy, "%s: DMA engine initialization failed\n",
+			   __func__);
+		return ret;
+	}
+
+	/* Copy the MAC addr into the HW  */
+	stmmac_set_umac_addr(priv, priv->hw, dev->dev_addr, 0);
+
+	/* PS and related bits will be programmed according to the speed */
+	if (priv->hw->pcs) {
+		int speed = priv->plat->mac_port_sel_speed;
+
+		if ((speed == SPEED_10) || (speed == SPEED_100) ||
+		    (speed == SPEED_1000)) {
+			priv->hw->ps = speed;
+		} else {
+			dev_warn(priv->device, "invalid port speed\n");
+			priv->hw->ps = 0;
+		}
+	}
+
+	/* Initialize the MAC Core */
+	stmmac_core_init(priv, priv->hw, priv->dummy);
+
+	/* Initialize MTL*/
+	stmmac_mtl_configuration(priv);
+
+	/* Initialize Safety Features */
+	stmmac_safety_feat_configuration(priv);
+
+	ret = stmmac_rx_ipc(priv, priv->hw);
+	if (!ret) {
+		netdev_warn(priv->dummy, "RX IPC Checksum Offload disabled\n");
+		priv->plat->rx_coe = STMMAC_RX_COE_NONE;
+		priv->hw->rx_csum = 0;
+	}
+
+	/* Enable the MAC Rx/Tx */
+	stmmac_mac_set(priv, priv->ioaddr, true);
+
+	/* Set the HW DMA mode and the COE */
+	stmmac_dma_operation_mode(priv);
+
+	stmmac_mmc_setup(priv);
+
+	if (init_ptp) {
+		ret = clk_prepare_enable(priv->plat->clk_ptp_ref);
+		if (ret < 0)
+			netdev_warn(priv->dummy, "failed to enable PTP reference clock: %d\n", ret);
+
+		ret = stmmac_init_ptp(priv);
+		if (ret == -EOPNOTSUPP)
+			netdev_warn(priv->dummy, "PTP not supported by HW\n");
+		else if (ret)
+			netdev_warn(priv->dummy, "PTP init failed\n");
+	}
+
+	priv->tx_lpi_timer = STMMAC_DEFAULT_TWT_LS;
+
+	if (priv->use_riwt) {
+		if (!priv->rx_riwt)
+			priv->rx_riwt = DEF_DMA_RIWT;
+
+		ret = stmmac_rx_watchdog(priv, priv->ioaddr, priv->rx_riwt, rx_cnt);
+	}
+
+	if (priv->hw->pcs)
+		stmmac_pcs_ctrl_ane(priv, priv->ioaddr, 1, priv->hw->ps, 0);
+
+	/* set TX and RX rings length */
+	stmmac_set_rings_length(priv);
+
+	/* Enable TSO */
+	if (priv->tso) {
+		for (chan = 0; chan < tx_cnt; chan++)
+			stmmac_enable_tso(priv, priv->ioaddr, 1, chan);
+	}
+
+	/* Enable Split Header */
+	if (priv->sph && priv->hw->rx_csum) {
+		for (chan = 0; chan < rx_cnt; chan++)
+			stmmac_enable_sph(priv, priv->ioaddr, 1, chan);
+	}
+
+	/* VLAN Tag Insertion */
+	if (priv->dma_cap.vlins)
+		stmmac_enable_vlan(priv, priv->hw, STMMAC_VLAN_INSERT);
+
+	/* TBS */
+	for (chan = 0; chan < tx_cnt; chan++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[chan];
+		int enable = tx_q->tbs & STMMAC_TBS_AVAIL;
+
+		stmmac_enable_tbs(priv, priv->ioaddr, enable, chan);
+	}
+
+	/* Start the ball rolling... */
+	stmmac_start_all_dma(priv);
+
+	return 0;
+}
+
+static void stmmac_hw_teardown(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	clk_disable_unprepare(priv->plat->clk_ptp_ref);
+}
+
+struct net_device_stats * stmmac_get_stats(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	return &priv->stats;
+}
+
+/**
+ *  stmmac_open - open entry point of the driver
+ *  @dev : pointer to the device structure.
+ *  Description:
+ *  This function is the open entry point of the driver.
+ *  Return value:
+ *  0 on success and an appropriate (-)ve integer as defined in errno.h
+ *  file on failure.
+ */
+static int stmmac_open(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	int bfsize = 0;
+	u32 chan;
+	int ret;
+	/* soft interrupts have priority MAX_RT_PRIO / 2 in preempt_rt */
+	struct sched_param tx_schedule_param = 
+		{ .sched_priority = RTNET_TX_THREAD_RT_PRIO };
+	struct sched_param rx_schedule_param = 
+		{ .sched_priority = RTNET_RX_THREAD_RT_PRIO };
+
+	if (priv->hw->pcs != STMMAC_PCS_TBI &&
+	    priv->hw->pcs != STMMAC_PCS_RTBI &&
+	    priv->hw->xpcs == NULL) {
+		ret = stmmac_init_phy(dev);
+		if (ret) {
+			netdev_err(priv->dummy,
+				   "%s: Cannot attach to PHY (error: %d)\n",
+				   __func__, ret);
+			return ret;
+		}
+	}
+
+	/* Extra statistics */
+	memset(&priv->xstats, 0, sizeof(struct stmmac_extra_stats));
+	priv->xstats.threshold = tc;
+
+	bfsize = stmmac_set_16kib_bfsize(priv, dev->mtu);
+	if (bfsize < 0)
+		bfsize = 0;
+
+	if (bfsize < BUF_SIZE_16KiB)
+		bfsize = stmmac_set_bfsize(dev->mtu, priv->dma_buf_sz);
+
+	priv->dma_buf_sz = bfsize;
+	buf_sz = bfsize;
+
+	priv->rx_copybreak = STMMAC_RX_COPYBREAK;
+
+	/* Earlier check for TBS */
+	for (chan = 0; chan < priv->plat->tx_queues_to_use; chan++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[chan];
+		int tbs_en = priv->plat->tx_queues_cfg[chan].tbs_en;
+
+		tx_q->tbs |= tbs_en ? STMMAC_TBS_AVAIL : 0;
+		if (stmmac_enable_tbs(priv, priv->ioaddr, tbs_en, chan))
+			tx_q->tbs &= ~STMMAC_TBS_AVAIL;
+	}
+
+	ret = alloc_dma_desc_resources(priv);
+	if (ret < 0) {
+		netdev_err(priv->dummy, "%s: DMA descriptors allocation failed\n",
+			   __func__);
+		goto dma_desc_error;
+	}
+
+	ret = init_dma_desc_rings(dev, GFP_KERNEL);
+	if (ret < 0) {
+		netdev_err(priv->dummy, "%s: DMA descriptors initialization failed\n",
+			   __func__);
+		goto init_error;
+	}
+
+	ret = stmmac_hw_setup(dev, true);
+	if (ret < 0) {
+		netdev_err(priv->dummy, "%s: Hw setup failed\n", __func__);
+		goto init_error;
+	}
+
+	stmmac_init_coalesce(priv);
+
+	rtnl_lock();
+	phylink_start(priv->phylink);
+	/* We may have called phylink_speed_down before */
+	phylink_speed_up(priv->phylink);
+	rtnl_unlock();
+	
+	/* Request the IRQ lines */
+	ret = request_irq(dev->irq, stmmac_dma_interrupt,
+			  IRQF_NO_THREAD, dev->name, dev);
+	if (unlikely(ret < 0)) {
+		netdev_err(priv->dummy,
+			   "%s: ERROR: allocating the IRQ %d (error: %d)\n",
+			   __func__, dev->irq, ret);
+		goto irq_error;
+	}
+
+#if 0
+	/* Request the Wake IRQ in case of another line is used for WoL */
+	if (priv->wol_irq != dev->irq) {
+		ret = request_irq(priv->wol_irq, stmmac_interrupt,
+				  IRQF_NO_THREAD, dev->name, dev);
+		if (unlikely(ret < 0)) {
+			netdev_err(priv->dummy,
+				   "%s: ERROR: allocating the WoL IRQ %d (%d)\n",
+				   __func__, priv->wol_irq, ret);
+			goto wolirq_error;
+		}
+	}
+#endif
+	
+	/* Request the IRQ lines */
+	if (priv->lpi_irq > 0) {
+		ret = request_irq(priv->lpi_irq, stmmac_interrupt, IRQF_NO_THREAD,
+				  dev->name, dev);
+		if (unlikely(ret < 0)) {
+			netdev_err(priv->dummy,
+				   "%s: ERROR: allocating the LPI IRQ %d (%d)\n",
+				   __func__, priv->lpi_irq, ret);
+			goto lpiirq_error;
+		}
+	}
+
+	rt_stack_connect(dev, &STACK_manager);
+	
+	/* setup the rx task */
+	rtdm_event_init(&rx_event, 0);
+    stop_rx_task = 0;
+    rx_task = kthread_create(stmmac_napi_poll_rx, priv, "stmmac_napi_poll_rx");
+    if (!rx_task) {
+		ret = -ENOMEM;
+		goto lpiirq_error;
+	}
+    sched_setscheduler(rx_task, SCHED_FIFO, &rx_schedule_param);
+    wake_up_process(rx_task);
+
+	/* setup the tx task */
+	rtdm_event_init(&tx_event, 0);
+    stop_tx_task = 0;
+    tx_task = kthread_create(stmmac_napi_poll_tx, priv, "stmmac_napi_poll_tx");
+    if (!tx_task) {
+		ret = -ENOMEM;
+		goto lpiirq_error;
+	}
+    sched_setscheduler(tx_task, SCHED_FIFO, &tx_schedule_param);
+    wake_up_process(tx_task);
+	
+	stmmac_enable_all_queues(priv);
+	stmmac_start_all_queues(priv);
+
+	return 0;
+
+lpiirq_error:
+	if (priv->wol_irq != dev->irq)
+		free_irq(priv->wol_irq, dev);
+#if 0
+wolirq_error:
+	free_irq(dev->irq, dev);
+#endif
+irq_error:
+	phylink_stop(priv->phylink);
+
+#if 0
+	for (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)
+		del_timer_sync(&priv->tx_queue[chan].txtimer);
+#endif
+	
+	stmmac_hw_teardown(dev);
+init_error:
+	free_dma_desc_resources(priv);
+dma_desc_error:
+	phylink_disconnect_phy(priv->phylink);
+	return ret;
+}
+
+/**
+ *  stmmac_release - close entry point of the driver
+ *  @dev : device pointer.
+ *  Description:
+ *  This is the stop entry point of the driver.
+ */
+static int stmmac_release(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+#if 0
+	u32 chan;
+#endif
+	
+	if (priv->eee_enabled)
+		del_timer_sync(&priv->eee_ctrl_timer);
+
+	if (device_may_wakeup(priv->device))
+		phylink_speed_down(priv->phylink, false);
+	/* Stop and disconnect the PHY */
+	phylink_stop(priv->phylink);
+	phylink_disconnect_phy(priv->phylink);
+
+	stop_rx_task = 1;
+	stop_tx_task = 1;
+	smp_mb();
+	rtdm_event_signal_one(&rx_event);
+    kthread_stop(rx_task);
+#if 0
+    put_task_struct(rx_task);
+#endif
+	rtdm_event_signal_one(&tx_event);
+    kthread_stop(tx_task);
+#if 0
+    put_task_struct(tx_task);
+#endif
+	
+	stmmac_stop_all_queues(priv);
+
+	stmmac_disable_all_queues(priv);
+
+#if 0
+	for (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)
+		del_timer_sync(&priv->tx_queue[chan].txtimer);
+#endif
+	
+	/* Free the IRQ lines */
+	free_irq(dev->irq, dev);
+	if (priv->wol_irq != dev->irq)
+		free_irq(priv->wol_irq, dev);
+	if (priv->lpi_irq > 0)
+		free_irq(priv->lpi_irq, dev);
+
+	/* Stop TX/RX DMA and clear the descriptors */
+	stmmac_stop_all_dma(priv);
+
+	/* Release and free the Rx/Tx resources */
+	free_dma_desc_resources(priv);
+
+	/* Disable the MAC Rx/Tx */
+	stmmac_mac_set(priv, priv->ioaddr, false);
+
+	rtnetif_carrier_off(dev);
+
+	stmmac_release_ptp(priv);
+
+	return 0;
+}
+
+static bool stmmac_vlan_insert(struct stmmac_priv *priv, struct rtskb *skb,
+			       struct stmmac_tx_queue *tx_q)
+{
+#if 0
+	u16 tag = 0x0, inner_tag = 0x0;
+	u32 inner_type = 0x0;
+	struct dma_desc *p;
+#endif
+	
+	/* RTnet does not know about vlans */
+	return false;
+#if 0
+	if (!priv->dma_cap.vlins)
+		return false;
+	if (!skb_vlan_tag_present(skb))
+		return false;
+	if (skb->vlan_proto == htons(ETH_P_8021AD)) {
+		inner_tag = skb_vlan_tag_get(skb);
+		inner_type = STMMAC_VLAN_INSERT;
+	}
+
+	tag = skb_vlan_tag_get(skb);
+
+	if (tx_q->tbs & STMMAC_TBS_AVAIL)
+		p = &tx_q->dma_entx[tx_q->cur_tx].basic;
+	else
+		p = &tx_q->dma_tx[tx_q->cur_tx];
+
+	if (stmmac_set_desc_vlan_tag(priv, p, tag, inner_tag, inner_type))
+		return false;
+
+	stmmac_set_tx_owner(priv, p);
+	tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+	return true;
+#endif
+}
+
+/**
+ *  stmmac_tso_allocator - close entry point of the driver
+ *  @priv: driver private structure
+ *  @des: buffer start address
+ *  @total_len: total length to fill in descriptors
+ *  @last_segmant: condition for the last descriptor
+ *  @queue: TX queue index
+ *  Description:
+ *  This function fills descriptor and request new descriptors according to
+ *  buffer length to fill
+ */
+#if 0
+static void stmmac_tso_allocator(struct stmmac_priv *priv, dma_addr_t des,
+				 int total_len, bool last_segment, u32 queue)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+	struct dma_desc *desc;
+	u32 buff_size;
+	int tmp_len;
+
+	tmp_len = total_len;
+
+	while (tmp_len > 0) {
+		dma_addr_t curr_addr;
+
+		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+		WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]);
+
+		if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			desc = &tx_q->dma_entx[tx_q->cur_tx].basic;
+		else
+			desc = &tx_q->dma_tx[tx_q->cur_tx];
+
+		curr_addr = des + (total_len - tmp_len);
+		if (priv->dma_cap.addr64 <= 32)
+			desc->des0 = cpu_to_le32(curr_addr);
+		else
+			stmmac_set_desc_addr(priv, desc, curr_addr);
+
+		buff_size = tmp_len >= TSO_MAX_BUFF_SIZE ?
+			    TSO_MAX_BUFF_SIZE : tmp_len;
+
+		stmmac_prepare_tso_tx_desc(priv, desc, 0, buff_size,
+				0, 1,
+				(last_segment) && (tmp_len <= TSO_MAX_BUFF_SIZE),
+				0, 0);
+
+		tmp_len -= TSO_MAX_BUFF_SIZE;
+	}
+}
+#endif
+
+/**
+ *  stmmac_tso_xmit - Tx entry point of the driver for oversized frames (TSO)
+ *  @skb : the socket buffer
+ *  @dev : device pointer
+ *  Description: this is the transmit function that is called on TSO frames
+ *  (support available on GMAC4 and newer chips).
+ *  Diagram below show the ring programming in case of TSO frames:
+ *
+ *  First Descriptor
+ *   --------
+ *   | DES0 |---> buffer1 = L2/L3/L4 header
+ *   | DES1 |---> TCP Payload (can continue on next descr...)
+ *   | DES2 |---> buffer 1 and 2 len
+ *   | DES3 |---> must set TSE, TCP hdr len-> [22:19]. TCP payload len [17:0]
+ *   --------
+ *	|
+ *     ...
+ *	|
+ *   --------
+ *   | DES0 | --| Split TCP Payload on Buffers 1 and 2
+ *   | DES1 | --|
+ *   | DES2 | --> buffer 1 and 2 len
+ *   | DES3 |
+ *   --------
+ *
+ * mss is fixed when enable tso, so w/o programming the TDES3 ctx field.
+ */
+static netdev_tx_t stmmac_tso_xmit(struct rtskb *skb, struct rtnet_device *dev)
+{
+	/* RTnet does not supprot large packets */
+	return NETDEV_TX_OK;
+	
+#if 0
+	struct dma_desc *desc, *first, *mss_desc = NULL;
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	int desc_size, tmp_pay_len = 0, first_tx;
+	int nfrags = skb_shinfo(skb)->nr_frags;
+	u32 queue = skb_get_queue_mapping(skb);
+	unsigned int first_entry, tx_packets;
+	struct stmmac_tx_queue *tx_q;
+	bool has_vlan, set_ic;
+	u8 proto_hdr_len, hdr;
+	u32 pay_len, mss;
+	dma_addr_t des;
+	int i;
+
+	tx_q = &priv->tx_queue[queue];
+	first_tx = tx_q->cur_tx;
+
+	/* Compute header lengths */
+	if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {
+		proto_hdr_len = skb_transport_offset(skb) + sizeof(struct udphdr);
+		hdr = sizeof(struct udphdr);
+	} else {
+		proto_hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
+		hdr = tcp_hdrlen(skb);
+	}
+
+	/* Desc availability based on threshold should be enough safe */
+	if (unlikely(stmmac_tx_avail(priv, queue) <
+		(((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) {
+		if (!rtnetif_tx_queue_stopped(priv->dev, netdev_get_tx_queue(dev, queue))) {
+			rtnetif_tx_stop_queue(priv->dev, netdev_get_tx_queue(priv->dev,
+								queue));
+			/* This is a hard error, log it. */
+			netdev_err(priv->dummy,
+				   "%s: Tx Ring full when queue awake\n",
+				   __func__);
+		}
+		return NETDEV_TX_BUSY;
+	}
+
+	pay_len = skb_headlen(skb) - proto_hdr_len; /* no frags */
+
+	mss = skb_shinfo(skb)->gso_size;
+
+	/* set new MSS value if needed */
+	if (mss != tx_q->mss) {
+		if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			mss_desc = &tx_q->dma_entx[tx_q->cur_tx].basic;
+		else
+			mss_desc = &tx_q->dma_tx[tx_q->cur_tx];
+
+		stmmac_set_mss(priv, mss_desc, mss);
+		tx_q->mss = mss;
+		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+		WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]);
+	}
+
+	if (netif_msg_tx_queued(priv)) {
+		pr_info("%s: hdrlen %d, hdr_len %d, pay_len %d, mss %d\n",
+			__func__, hdr, proto_hdr_len, pay_len, mss);
+		pr_info("\tskb->len %d, skb->data_len %d\n", skb->len,
+			skb->data_len);
+	}
+
+	/* Check if VLAN can be inserted by HW */
+	has_vlan = stmmac_vlan_insert(priv, skb, tx_q);
+
+	first_entry = tx_q->cur_tx;
+	WARN_ON(tx_q->tx_skbuff[first_entry]);
+
+	if (tx_q->tbs & STMMAC_TBS_AVAIL)
+		desc = &tx_q->dma_entx[first_entry].basic;
+	else
+		desc = &tx_q->dma_tx[first_entry];
+	first = desc;
+
+	if (has_vlan)
+		stmmac_set_desc_vlan(priv, first, STMMAC_VLAN_INSERT);
+
+	/* first descriptor: fill Headers on Buf1 */
+	des = dma_map_single(priv->device, skb->data, skb_headlen(skb),
+			     DMA_TO_DEVICE);
+	if (dma_mapping_error(priv->device, des))
+		goto dma_map_err;
+
+	tx_q->tx_skbuff_dma[first_entry].buf = des;
+	tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);
+
+	if (priv->dma_cap.addr64 <= 32) {
+		first->des0 = cpu_to_le32(des);
+
+		/* Fill start of payload in buff2 of first descriptor */
+		if (pay_len)
+			first->des1 = cpu_to_le32(des + proto_hdr_len);
+
+		/* If needed take extra descriptors to fill the remaining payload */
+		tmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE;
+	} else {
+		stmmac_set_desc_addr(priv, first, des);
+		tmp_pay_len = pay_len;
+		des += proto_hdr_len;
+		pay_len = 0;
+	}
+
+	stmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue);
+
+	/* Prepare fragments */
+	for (i = 0; i < nfrags; i++) {
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+
+		des = skb_frag_dma_map(priv->device, frag, 0,
+				       skb_frag_size(frag),
+				       DMA_TO_DEVICE);
+		if (dma_mapping_error(priv->device, des))
+			goto dma_map_err;
+
+		stmmac_tso_allocator(priv, des, skb_frag_size(frag),
+				     (i == nfrags - 1), queue);
+
+		tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des;
+		tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag);
+		tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true;
+	}
+
+	tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true;
+
+	/* Only the last descriptor gets to point to the skb. */
+	tx_q->tx_skbuff[tx_q->cur_tx] = skb;
+
+	/* Manage tx mitigation */
+	tx_packets = (tx_q->cur_tx + 1) - first_tx;
+	tx_q->tx_count_frames += tx_packets;
+
+	if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)
+		set_ic = true;
+	else if (!priv->tx_coal_frames)
+		set_ic = false;
+	else if (tx_packets > priv->tx_coal_frames)
+		set_ic = true;
+	else if ((tx_q->tx_count_frames % priv->tx_coal_frames) < tx_packets)
+		set_ic = true;
+	else
+		set_ic = false;
+
+	if (set_ic) {
+		if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			desc = &tx_q->dma_entx[tx_q->cur_tx].basic;
+		else
+			desc = &tx_q->dma_tx[tx_q->cur_tx];
+
+		tx_q->tx_count_frames = 0;
+		stmmac_set_tx_ic(priv, desc);
+		priv->xstats.tx_set_ic_bit++;
+	}
+
+	/* We've used all descriptors we need for this skb, however,
+	 * advance cur_tx so that it references a fresh descriptor.
+	 * ndo_start_xmit will fill this descriptor the next time it's
+	 * called and stmmac_tx_clean may clean up to this descriptor.
+	 */
+	tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+
+	if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {
+		netif_dbg(priv, hw, priv->dummy, "%s: stop transmitted packets\n",
+			  __func__);
+		rtnetif_tx_stop_queue(priv->dev, netdev_get_tx_queue(priv->dev, queue));
+	}
+
+	dev->stats.tx_bytes += skb->len;
+	priv->xstats.tx_tso_frames++;
+	priv->xstats.tx_tso_nfrags += nfrags;
+
+	if (priv->sarc_type)
+		stmmac_set_desc_sarc(priv, first, priv->sarc_type);
+
+	rtskb_tx_timestamp(skb);
+
+	if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+		     priv->hwts_tx_en)) {
+		/* declare that device is doing timestamping */
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+		stmmac_enable_tx_timestamp(priv, first);
+	}
+
+	/* Complete the first descriptor before granting the DMA */
+	stmmac_prepare_tso_tx_desc(priv, first, 1,
+			proto_hdr_len,
+			pay_len,
+			1, tx_q->tx_skbuff_dma[first_entry].last_segment,
+			hdr / 4, (skb->len - proto_hdr_len));
+
+	/* If context desc is used to change MSS */
+	if (mss_desc) {
+		/* Make sure that first descriptor has been completely
+		 * written, including its own bit. This is because MSS is
+		 * actually before first descriptor, so we need to make
+		 * sure that MSS's own bit is the last thing written.
+		 */
+		dma_wmb();
+		stmmac_set_tx_owner(priv, mss_desc);
+	}
+
+	/* The own bit must be the latest setting done when prepare the
+	 * descriptor and then barrier is needed to make sure that
+	 * all is coherent before granting the DMA engine.
+	 */
+	wmb();
+
+	if (netif_msg_pktdata(priv)) {
+		pr_info("%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\n",
+			__func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry,
+			tx_q->cur_tx, first, nfrags);
+		pr_info(">>> frame to be transmitted: ");
+		print_pkt(skb->data, skb_headlen(skb));
+	}
+
+	rtnetdev_tx_sent_queue(rtnetdev_get_tx_queue(dev, queue), skb->len);
+
+	if (tx_q->tbs & STMMAC_TBS_AVAIL)
+		desc_size = sizeof(struct dma_edesc);
+	else
+		desc_size = sizeof(struct dma_desc);
+
+	tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx * desc_size);
+	stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr, queue);
+	stmmac_tx_timer_arm(priv, queue);
+	
+	return NETDEV_TX_OK;
+
+dma_map_err:
+	dev_err(priv->device, "Tx dma map failed\n");
+	dev_kfree_skb(skb);
+	priv->stats.tx_dropped++;
+	return NETDEV_TX_OK;
+#endif
+}
+
+/**
+ *  stmmac_xmit - Tx entry point of the driver
+ *  @skb : the socket buffer
+ *  @dev : device pointer
+ *  Description : this is the tx entry point of the driver.
+ *  It programs the chain or the ring and supports oversized frames
+ *  and SG feature.
+ */
+static netdev_tx_t stmmac_xmit(struct rtskb *skb, struct rtnet_device *dev)
+{
+	unsigned int first_entry, tx_packets, enh_desc;
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	unsigned int nopaged_len = rtskb_headlen(skb);
+	int csum_insertion = 0, is_jumbo = 0;
+	u32 queue = rtskb_get_queue_mapping(skb);
+#if 0
+	int nfrags = skb_shinfo(skb)->nr_frags;
+	int gso = skb_shinfo(skb)->gso_type;
+#endif
+	int nfrags = 0;
+	int gso = 0;
+	struct dma_edesc *tbs_desc = NULL;
+	int entry, desc_size, first_tx;
+	struct dma_desc *desc, *first;
+	struct stmmac_tx_queue *tx_q;
+	bool has_vlan, set_ic;
+	dma_addr_t des;
+
+	tx_q = &priv->tx_queue[queue];
+	first_tx = tx_q->cur_tx;
+
+	if (priv->tx_path_in_lpi_mode)
+		stmmac_disable_eee_mode(priv);
+
+	/* Manage oversized TCP frames for GMAC4 device */
+	if (rtskb_is_gso(skb) && priv->tso) {
+		if (gso & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))
+			return stmmac_tso_xmit(skb, dev);
+		if (priv->plat->has_gmac4 && (gso & SKB_GSO_UDP_L4))
+			return stmmac_tso_xmit(skb, dev);
+	}
+	
+	if (unlikely(stmmac_tx_avail(priv, queue) < nfrags + 1)) {
+		if (!rtnetif_tx_queue_stopped(priv->dev, rtnetdev_get_tx_queue(dev, queue))) {
+			rtnetif_tx_stop_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev,
+								queue));
+			/* This is a hard error, log it. */
+			netdev_err(priv->dummy,
+				   "%s: Tx Ring full when queue awake\n",
+				   __func__);
+		}
+		return NETDEV_TX_BUSY;
+	}
+
+	/* Check if VLAN can be inserted by HW */
+	has_vlan = stmmac_vlan_insert(priv, skb, tx_q);
+
+	entry = tx_q->cur_tx;
+	first_entry = entry;
+	WARN_ON(tx_q->tx_skbuff[first_entry]);
+
+	csum_insertion = (skb->ip_summed == CHECKSUM_PARTIAL);
+
+	if (likely(priv->extend_desc))
+		desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+	else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+		desc = &tx_q->dma_entx[entry].basic;
+	else
+		desc = tx_q->dma_tx + entry;
+
+	first = desc;
+
+	if (has_vlan)
+		stmmac_set_desc_vlan(priv, first, STMMAC_VLAN_INSERT);
+
+	enh_desc = priv->plat->enh_desc;
+	/* To program the descriptors according to the size of the frame */
+	if (enh_desc)
+		is_jumbo = stmmac_is_jumbo_frm(priv, skb->len, enh_desc);
+
+#if 0
+	/* RTnet does not know about jumbo frames */
+	if (unlikely(is_jumbo)) {
+		entry = stmmac_jumbo_frm(priv, tx_q, skb, csum_insertion);
+		if (unlikely(entry < 0) && (entry != -EINVAL))
+			goto dma_map_err;
+	}
+#endif
+	
+#if 0
+	for (i = 0; i < nfrags; i++) {
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+		int len = skb_frag_size(frag);
+		bool last_segment = (i == (nfrags - 1));
+
+		entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
+		WARN_ON(tx_q->tx_skbuff[entry]);
+
+		if (likely(priv->extend_desc))
+			desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			desc = &tx_q->dma_entx[entry].basic;
+		else
+			desc = tx_q->dma_tx + entry;
+
+		des = skb_frag_dma_map(priv->device, frag, 0, len,
+				       DMA_TO_DEVICE);
+		if (dma_mapping_error(priv->device, des))
+			goto dma_map_err; /* should reuse desc w/o issues */
+
+		tx_q->tx_skbuff_dma[entry].buf = des;
+
+		stmmac_set_desc_addr(priv, desc, des);
+
+		tx_q->tx_skbuff_dma[entry].map_as_page = true;
+		tx_q->tx_skbuff_dma[entry].len = len;
+		tx_q->tx_skbuff_dma[entry].last_segment = last_segment;
+
+		/* Prepare the descriptor and set the own bit too */
+		stmmac_prepare_tx_desc(priv, desc, 0, len, csum_insertion,
+				priv->mode, 1, last_segment, skb->len);
+	}
+#endif
+	
+	/* Only the last descriptor gets to point to the skb. */
+	tx_q->tx_skbuff[entry] = skb;
+
+	/* According to the coalesce parameter the IC bit for the latest
+	 * segment is reset and the timer re-started to clean the tx status.
+	 * This approach takes care about the fragments: desc is the first
+	 * element in case of no SG.
+	 */
+	tx_packets = (entry + 1) - first_tx;
+	tx_q->tx_count_frames += tx_packets;
+
+#if 0
+	if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)
+		set_ic = true;
+	else if (!priv->tx_coal_frames)
+		set_ic = false;
+	else if (tx_packets > priv->tx_coal_frames)
+		set_ic = true;
+	else if ((tx_q->tx_count_frames % priv->tx_coal_frames) < tx_packets)
+		set_ic = true;
+	else
+		set_ic = false;
+#endif
+	set_ic = false;
+	
+	if (set_ic) {
+		if (likely(priv->extend_desc))
+			desc = &tx_q->dma_etx[entry].basic;
+		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+			desc = &tx_q->dma_entx[entry].basic;
+		else
+			desc = &tx_q->dma_tx[entry];
+
+		tx_q->tx_count_frames = 0;
+		stmmac_set_tx_ic(priv, desc);
+		priv->xstats.tx_set_ic_bit++;
+	}
+
+	/* We've used all descriptors we need for this skb, however,
+	 * advance cur_tx so that it references a fresh descriptor.
+	 * ndo_start_xmit will fill this descriptor the next time it's
+	 * called and stmmac_tx_clean may clean up to this descriptor.
+	 */
+	entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
+	tx_q->cur_tx = entry;
+
+	if (netif_msg_pktdata(priv)) {
+		netdev_dbg(priv->dummy,
+			   "%s: curr=%d dirty=%d f=%d, e=%d, first=%p, nfrags=%d",
+			   __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry,
+			   entry, first, nfrags);
+
+		netdev_dbg(priv->dummy, ">>> frame to be transmitted: ");
+		print_pkt(skb->data, skb->len);
+	}
+
+	if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {
+		netif_dbg(priv, hw, priv->dummy, "%s: stop transmitted packets\n",
+			  __func__);
+		rtnetif_tx_stop_queue(priv->dev, rtnetdev_get_tx_queue(priv->dev, queue));
+	}
+
+	priv->stats.tx_bytes += skb->len;
+
+	if (priv->sarc_type)
+		stmmac_set_desc_sarc(priv, first, priv->sarc_type);
+
+	rtskb_tx_timestamp(skb);
+
+	/* Ready to fill the first descriptor and set the OWN bit w/o any
+	 * problems because all the descriptors are actually ready to be
+	 * passed to the DMA engine.
+	 */
+	if (likely(!is_jumbo)) {
+		bool last_segment = (nfrags == 0);
+
+		des = dma_map_single(priv->device, skb->data,
+				     nopaged_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(priv->device, des))
+			goto dma_map_err;
+
+		tx_q->tx_skbuff_dma[first_entry].buf = des;
+
+		stmmac_set_desc_addr(priv, first, des);
+
+		tx_q->tx_skbuff_dma[first_entry].len = nopaged_len;
+		tx_q->tx_skbuff_dma[first_entry].last_segment = last_segment;
+
+		/* RTnet does not know about skb_shinfo */
+#if 0
+		if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+			     priv->hwts_tx_en)) {
+			/* declare that device is doing timestamping */
+			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+			stmmac_enable_tx_timestamp(priv, first);
+		}
+#endif
+		
+		/* Prepare the first descriptor setting the OWN bit too */
+		stmmac_prepare_tx_desc(priv, first, 1, nopaged_len,
+				csum_insertion, priv->mode, 0, last_segment,
+				skb->len);
+	}
+
+	if (tx_q->tbs & STMMAC_TBS_EN) {
+		struct timespec64 ts = ns_to_timespec64(skb->time_stamp);
+
+		tbs_desc = &tx_q->dma_entx[first_entry];
+		stmmac_set_desc_tbs(priv, tbs_desc, ts.tv_sec, ts.tv_nsec);
+	}
+
+	stmmac_set_tx_owner(priv, first);
+
+	/* The own bit must be the latest setting done when prepare the
+	 * descriptor and then barrier is needed to make sure that
+	 * all is coherent before granting the DMA engine.
+	 */
+	wmb();
+
+	rtnetdev_tx_sent_queue(rtnetdev_get_tx_queue(priv->dev, queue), skb->len);
+
+	stmmac_enable_dma_transmission(priv, priv->ioaddr);
+
+	if (likely(priv->extend_desc))
+		desc_size = sizeof(struct dma_extended_desc);
+	else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+		desc_size = sizeof(struct dma_edesc);
+	else
+		desc_size = sizeof(struct dma_desc);
+
+	tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx * desc_size);
+	stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr, queue);
+	stmmac_tx_timer_arm(priv, queue);
+
+	return NETDEV_TX_OK;
+
+dma_map_err:
+	netdev_err(priv->dummy, "Tx DMA map failed\n");
+	dev_kfree_rtskb(skb);
+	priv->stats.tx_dropped++;
+	return NETDEV_TX_OK;
+}
+
+static void stmmac_rx_vlan(struct rtnet_device *dev, struct rtskb *skb)
+{
+	/* RTnet does not know about vlans */
+#if 0
+	struct vlan_ethhdr *veth;
+	__be16 vlan_proto;
+	u16 vlanid;
+
+	veth = (struct vlan_ethhdr *)skb->data;
+	vlan_proto = veth->h_vlan_proto;
+
+	if ((vlan_proto == htons(ETH_P_8021Q) &&
+	     dev->features & NETIF_F_HW_VLAN_CTAG_RX) ||
+	    (vlan_proto == htons(ETH_P_8021AD) &&
+	     dev->features & NETIF_F_HW_VLAN_STAG_RX)) {
+		/* pop the vlan tag */
+		vlanid = ntohs(veth->h_vlan_TCI);
+		memmove(skb->data + VLAN_HLEN, veth, ETH_ALEN * 2);
+		rtskb_pull(skb, VLAN_HLEN);
+		__vlan_hwaccel_put_tag(skb, vlan_proto, vlanid);
+	}
+#endif
+}
+
+/**
+ * stmmac_rx_refill - refill used skb preallocated buffers
+ * @priv: driver private structure
+ * @queue: RX queue index
+ * Description : this is to reallocate the skb for the reception process
+ * that is based on zero-copy.
+ */
+static inline void stmmac_rx_refill(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	int len, dirty = stmmac_rx_dirty(priv, queue);
+	unsigned int entry = rx_q->dirty_rx;
+
+	len = DIV_ROUND_UP(priv->dma_buf_sz, PAGE_SIZE) * PAGE_SIZE;
+
+	while (dirty-- > 0) {
+		struct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];
+		struct dma_desc *p;
+		bool use_rx_wd;
+
+		if (priv->extend_desc)
+			p = (struct dma_desc *)(rx_q->dma_erx + entry);
+		else
+			p = rx_q->dma_rx + entry;
+
+		if (!buf->page) {
+			buf->page = page_pool_dev_alloc_pages(rx_q->page_pool);
+			if (!buf->page)
+				break;
+		}
+
+		if (priv->sph && !buf->sec_page) {
+			buf->sec_page = page_pool_dev_alloc_pages(rx_q->page_pool);
+			if (!buf->sec_page)
+				break;
+
+			buf->sec_addr = page_pool_get_dma_addr(buf->sec_page);
+
+			dma_sync_single_for_device(priv->device, buf->sec_addr,
+						   len, DMA_FROM_DEVICE);
+		}
+
+		buf->addr = page_pool_get_dma_addr(buf->page);
+
+		/* Sync whole allocation to device. This will invalidate old
+		 * data.
+		 */
+		dma_sync_single_for_device(priv->device, buf->addr, len,
+					   DMA_FROM_DEVICE);
+
+		stmmac_set_desc_addr(priv, p, buf->addr);
+		stmmac_set_desc_sec_addr(priv, p, buf->sec_addr);
+		stmmac_refill_desc3(priv, rx_q, p);
+
+		rx_q->rx_count_frames++;
+		rx_q->rx_count_frames += priv->rx_coal_frames;
+		if (rx_q->rx_count_frames > priv->rx_coal_frames)
+			rx_q->rx_count_frames = 0;
+
+		use_rx_wd = !priv->rx_coal_frames;
+		use_rx_wd |= rx_q->rx_count_frames > 0;
+		if (!priv->use_riwt)
+			use_rx_wd = false;
+
+		dma_wmb();
+		stmmac_set_rx_owner(priv, p, use_rx_wd);
+
+		entry = STMMAC_GET_ENTRY(entry, DMA_RX_SIZE);
+	}
+	rx_q->dirty_rx = entry;
+	rx_q->rx_tail_addr = rx_q->dma_rx_phy +
+			    (rx_q->dirty_rx * sizeof(struct dma_desc));
+	stmmac_set_rx_tail_ptr(priv, priv->ioaddr, rx_q->rx_tail_addr, queue);
+}
+
+static unsigned int stmmac_rx_buf1_len(struct stmmac_priv *priv,
+				       struct dma_desc *p,
+				       int status, unsigned int len)
+{
+	int ret, coe = priv->hw->rx_csum;
+	unsigned int plen = 0, hlen = 0;
+
+	/* Not first descriptor, buffer is always zero */
+	if (priv->sph && len)
+		return 0;
+
+	/* First descriptor, get split header length */
+	ret = stmmac_get_rx_header_len(priv, p, &hlen);
+	if (priv->sph && hlen) {
+		priv->xstats.rx_split_hdr_pkt_n++;
+		return hlen;
+	}
+
+	/* First descriptor, not last descriptor and not split header */
+	if (status & rx_not_ls)
+		return priv->dma_buf_sz;
+
+	plen = stmmac_get_rx_frame_len(priv, p, coe);
+
+	/* First descriptor and last descriptor and not split header */
+	return min_t(unsigned int, priv->dma_buf_sz, plen);
+}
+
+static unsigned int stmmac_rx_buf2_len(struct stmmac_priv *priv,
+				       struct dma_desc *p,
+				       int status, unsigned int len)
+{
+	int coe = priv->hw->rx_csum;
+	unsigned int plen = 0;
+
+	/* Not split header, buffer is not available */
+	if (!priv->sph)
+		return 0;
+
+	/* Not last descriptor */
+	if (status & rx_not_ls)
+		return priv->dma_buf_sz;
+
+	plen = stmmac_get_rx_frame_len(priv, p, coe);
+
+	/* Last descriptor */
+	return plen - len;
+}
+
+/**
+ * stmmac_rx - manage the receive process
+ * @priv: driver private structure
+ * @limit: napi bugget
+ * @queue: RX queue index.
+ * Description :  this the function called by the napi poll method.
+ * It gets all the frames inside the ring.
+ */
+static int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	unsigned int count = 0, error = 0, len = 0;
+	int status = 0, coe = priv->hw->rx_csum;
+	unsigned int next_entry = rx_q->cur_rx;
+	struct rtskb *skb = NULL;
+	
+	if (netif_msg_rx_status(priv)) {
+		void *rx_head;
+
+		netdev_dbg(priv->dummy, "%s: descriptor ring:\n", __func__);
+		if (priv->extend_desc)
+			rx_head = (void *)rx_q->dma_erx;
+		else
+			rx_head = (void *)rx_q->dma_rx;
+
+		stmmac_display_ring(priv, rx_head, DMA_RX_SIZE, true);
+	}
+	while (count < limit) {
+		unsigned int buf1_len = 0, buf2_len = 0;
+		struct stmmac_rx_buffer *buf;
+		struct dma_desc *np, *p;
+		int entry;
+		len = 0;
+#if 0
+		enum pkt_hash_types hash_type;
+		u32 hash;
+#endif
+		
+		if (!count && rx_q->state_saved) {
+			trace_printk("!count && rx_q->state_saved\n");
+			skb = rx_q->state.skb;
+			error = rx_q->state.error;
+			len = rx_q->state.len;
+		} else {
+			rx_q->state_saved = false;
+			skb = NULL;
+			error = 0;
+			len = 0;
+		}
+
+		if (count >= limit)
+			break;
+
+#if 0
+read_again:
+#endif
+		buf1_len = 0;
+		buf2_len = 0;
+		entry = next_entry;
+		buf = &rx_q->buf_pool[entry];
+
+		if (priv->extend_desc)
+			p = (struct dma_desc *)(rx_q->dma_erx + entry);
+		else
+			p = rx_q->dma_rx + entry;
+
+		/* read the status of the incoming frame */
+		status = stmmac_rx_status(priv, &priv->stats,
+				&priv->xstats, p);
+		/* check if managed by the DMA otherwise go ahead */
+		if (unlikely(status & dma_own))
+			break;
+
+		rx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx, DMA_RX_SIZE);
+		next_entry = rx_q->cur_rx;
+
+		if (priv->extend_desc)
+			np = (struct dma_desc *)(rx_q->dma_erx + next_entry);
+		else
+			np = rx_q->dma_rx + next_entry;
+
+		prefetch(np);
+
+		if (priv->extend_desc)
+			stmmac_rx_extended_status(priv, &priv->stats,
+					&priv->xstats, rx_q->dma_erx + entry);
+		if (unlikely(status == discard_frame)) {
+			page_pool_recycle_direct(rx_q->page_pool, buf->page);
+			buf->page = NULL;
+			error = 1;
+			if (!priv->hwts_rx_en)
+				priv->stats.rx_errors++;
+		}
+
+#if 0
+		/* status & rx_not_ls is always false for orange pi one */
+		if (unlikely(error && (status & rx_not_ls))) {
+			trace_printk("error && (status & rx_not_ls): goto read_again\n");
+			goto read_again;
+		}
+#endif
+		
+		if (unlikely(error)) {
+			trace_printk("error=%d\n", error);
+			dev_kfree_rtskb(skb);
+			skb = NULL;
+			count++;
+			continue;
+		}
+
+		/* Buffer is good. Go on. */
+
+		prefetch(page_address(buf->page));
+		if (buf->sec_page)
+			prefetch(page_address(buf->sec_page));
+
+		buf1_len = stmmac_rx_buf1_len(priv, p, status, len);
+		len += buf1_len;
+
+		buf2_len = stmmac_rx_buf2_len(priv, p, status, len);
+		len += buf2_len;
+
+		/* ACS is set; GMAC core strips PAD/FCS for IEEE 802.3
+		 * Type frames (LLC/LLC-SNAP)
+		 *
+		 * llc_snap is never checked in GMAC >= 4, so this ACS
+		 * feature is always disabled and packets need to be
+		 * stripped manually.
+		 */
+		if (likely(!(status & rx_not_ls)) &&
+		    (likely(priv->synopsys_id >= DWMAC_CORE_4_00) ||
+		     unlikely(status != llc_snap))) {
+			if (buf2_len)
+				buf2_len -= ETH_FCS_LEN;
+			else
+				buf1_len -= ETH_FCS_LEN;
+
+			len -= ETH_FCS_LEN;
+		}
+		
+		if (!skb) {
+			skb = rtnetdev_alloc_rtskb(priv->dev, buf1_len);
+			if (!skb) {
+				priv->stats.rx_dropped++;
+				count++;
+				trace_printk("rtnetdev_alloc_rtskb returns NULL\n");
+				goto drain_data;
+			}
+#if 0
+			rtskb_reserve(skb, NET_IP_ALIGN);
+#endif
+			dma_sync_single_for_cpu(priv->device, buf->addr,
+						buf1_len, DMA_FROM_DEVICE);
+			rtskb_copy_to_linear_data(skb, page_address(buf->page),
+						buf1_len);
+			rtskb_put(skb, buf1_len);
+
+			/* Data payload copied into SKB, page ready for recycle */
+			page_pool_recycle_direct(rx_q->page_pool, buf->page);
+			buf->page = NULL;
+		} 
+		/* RTnet does not know about skb_add_rx_frag */
+		else {
+			printk(KERN_ERR "%s: skb is not null\n", __func__);
+		}
+#if 0
+		else if (buf1_len) {
+			dma_sync_single_for_cpu(priv->device, buf->addr,
+						buf1_len, DMA_FROM_DEVICE);
+			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+					buf->page, 0, buf1_len,
+					priv->dma_buf_sz);
+
+			/* Data payload appended into SKB */
+			page_pool_release_page(rx_q->page_pool, buf->page);
+			buf->page = NULL;
+		}
+		if (buf2_len) {
+			dma_sync_single_for_cpu(priv->device, buf->sec_addr,
+						buf2_len, DMA_FROM_DEVICE);
+			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+					buf->sec_page, 0, buf2_len,
+					priv->dma_buf_sz);
+
+			/* Data payload appended into SKB */
+			page_pool_release_page(rx_q->page_pool, buf->sec_page);
+			buf->sec_page = NULL;
+		}
+#endif
+		
+drain_data:
+#if 0
+		/* status & rx_not_ls is always false for orange pi one */
+		if (likely(status & rx_not_ls)) {
+			trace_printk("status & rx_not_ls: goto read_again\n");
+			goto read_again;
+		}
+#endif
+		if (!skb)
+			continue;
+
+		/* Got entire packet into SKB. Finish it. */
+
+		stmmac_get_rx_hwtstamp(priv, p, np, skb);
+		stmmac_rx_vlan(priv->dev, skb);
+		skb->protocol = rt_eth_type_trans(skb, priv->dev);
+
+		if (unlikely(!coe))
+			rtskb_checksum_none_assert(skb);
+		else
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+#if 0
+		/* RTnet does not know about skb_set_hash. */
+		if (!stmmac_get_rx_hash(priv, p, &hash, &hash_type))
+			skb_set_hash(skb, hash, hash_type);
+#endif
+		
+		rtskb_record_rx_queue(skb, queue);
+		rtnetif_rx(skb);
+		skb = NULL;
+
+		priv->stats.rx_packets++;
+		priv->stats.rx_bytes += len;
+		count++;
+	}
+
+#if 0
+	/* status & rx_not_ls is always false for orange pi one */
+	if (status & rx_not_ls || skb) {
+		trace_printk("status & rx_not_ls || skb: save state\n");
+		rx_q->state_saved = true;
+		rx_q->state.skb = skb;
+		rx_q->state.error = error;
+		rx_q->state.len = len;
+	}
+#endif
+	
+	stmmac_rx_refill(priv, queue);
+
+	priv->xstats.rx_pkt_n += count;
+
+	return count;
+}
+
+static int stmmac_napi_poll_rx(void *vpriv)
+{
+	struct stmmac_priv *priv = (struct stmmac_priv *)vpriv;
+	/* sun8i has one queue only */
+	struct stmmac_channel *ch = &priv->channel[0];
+	u32 chan = ch->index;
+	int work_done;
+	int budget = NAPI_POLL_WEIGHT;
+	static int nrx = 0;
+	
+    while (!stop_rx_task) {
+        if (rtdm_event_wait_one(&rx_event) < 0) {
+			printk(KERN_ERR "%s rtdm_event_wait_one error\n", __func__);
+            break;
+		}
+
+		if(stop_rx_task)
+			break;
+
+		do {
+			priv->xstats.napi_poll++;
+
+			work_done = stmmac_rx(priv, budget, chan);
+			if(work_done)
+				rt_mark_stack_mgr(priv->dev);
+		} while (work_done >= budget);
+		if (work_done < budget) {
+			unsigned long flags;
+			raw_spin_lock_irqsave(&ch->lock, flags);
+			stmmac_enable_dma_irq(priv, priv->ioaddr, chan, 1, 0);
+			raw_spin_unlock_irqrestore(&ch->lock, flags);
+		}
+		nrx += work_done;
+	}
+	
+	do_exit(0);
+	return 0;
+}
+
+static int stmmac_napi_poll_tx(void *vpriv)
+{
+	struct stmmac_priv *priv = (struct stmmac_priv *)vpriv;
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[0];
+	/* sun8i has one queue only */
+	struct stmmac_channel *ch = &priv->channel[0];
+	u32 chan = ch->index;
+	int work_done;
+	int budget = NAPI_POLL_WEIGHT;
+	static int ntx = 0;
+	
+    while (!stop_tx_task) {
+        if (rtdm_event_wait_one(&tx_event) < 0) {
+			printk(KERN_ERR "%s rtdm_event_wait_one error\n", __func__);
+            break;
+		}
+		
+		if(stop_tx_task)
+			break;
+
+		priv->xstats.napi_poll++;
+
+		do {
+			work_done = stmmac_tx_clean(priv, DMA_TX_SIZE, chan);
+			work_done = min(work_done, budget);
+#if 0
+		} while(work_done >= budget);
+#endif
+		} while((work_done >= budget) || (tx_q->dirty_tx != tx_q->cur_tx));
+		if (work_done < budget) {
+			unsigned long flags;
+			raw_spin_lock_irqsave(&ch->lock, flags);
+			stmmac_enable_dma_irq(priv, priv->ioaddr, chan, 0, 1);
+			raw_spin_unlock_irqrestore(&ch->lock, flags);
+		}
+		ntx += work_done;
+	}
+	
+	do_exit(0);
+	return 0;
+}
+
+/**
+ *  stmmac_tx_timeout
+ *  @dev : Pointer to net device structure
+ *  Description: this function is called when a packet transmission fails to
+ *   complete within a reasonable time. The driver will mark the error in the
+ *   netdev structure and arrange for the device to be reset to a sane state
+ *   in order to transmit a new packet.
+ */
+#if 0
+static void stmmac_tx_timeout(struct rtnet_device *dev, unsigned int txqueue)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	stmmac_global_err(priv);
+}
+#endif
+
+/**
+ *  stmmac_set_rx_mode - entry point for multicast addressing
+ *  @dev : pointer to the device structure
+ *  Description:
+ *  This function is a driver entry point which gets called by the kernel
+ *  whenever multicast addresses must be enabled/disabled.
+ *  Return value:
+ *  void.
+ */
+#if 0
+static void stmmac_set_rx_mode(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	stmmac_set_filter(priv, priv->hw, priv->dummy);
+}
+#endif
+
+/**
+ *  stmmac_change_mtu - entry point to change MTU size for the device.
+ *  @dev : device pointer.
+ *  @new_mtu : the new MTU size for the device.
+ *  Description: the Maximum Transfer Unit (MTU) is used by the network layer
+ *  to drive packet transmission. Ethernet has an MTU of 1500 octets
+ *  (ETH_DATA_LEN). This value can be changed with ifconfig.
+ *  Return value:
+ *  0 on success and an appropriate (-)ve integer as defined in errno.h
+ *  file on failure.
+ */
+#if 0
+static int stmmac_change_mtu(struct rtnet_device *dev, int new_mtu)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	int txfifosz = priv->plat->tx_fifo_size;
+
+	if (txfifosz == 0)
+		txfifosz = priv->dma_cap.tx_fifo_size;
+
+	txfifosz /= priv->plat->tx_queues_to_use;
+
+	if (rtnetif_running(dev)) {
+		netdev_err(priv->dummy, "must be stopped to change its MTU\n");
+		return -EBUSY;
+	}
+
+	new_mtu = STMMAC_ALIGN(new_mtu);
+
+	/* If condition true, FIFO is too small or MTU too large */
+	if ((txfifosz < new_mtu) || (new_mtu > BUF_SIZE_16KiB))
+		return -EINVAL;
+
+	dev->mtu = new_mtu;
+
+	rtnetdev_update_features(dev);
+
+	return 0;
+}
+
+static netdev_features_t stmmac_fix_features(struct rtnet_device *dev,
+					     netdev_features_t features)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	if (priv->plat->rx_coe == STMMAC_RX_COE_NONE)
+		features &= ~NETIF_F_RXCSUM;
+
+	if (!priv->plat->tx_coe)
+		features &= ~NETIF_F_CSUM_MASK;
+
+	/* Some GMAC devices have a bugged Jumbo frame support that
+	 * needs to have the Tx COE disabled for oversized frames
+	 * (due to limited buffer sizes). In this case we disable
+	 * the TX csum insertion in the TDES and not use SF.
+	 */
+	if (priv->plat->bugged_jumbo && (dev->mtu > ETH_DATA_LEN))
+		features &= ~NETIF_F_CSUM_MASK;
+
+	/* Disable tso if asked by ethtool */
+	if ((priv->plat->tso_en) && (priv->dma_cap.tsoen)) {
+		if (features & NETIF_F_TSO)
+			priv->tso = true;
+		else
+			priv->tso = false;
+	}
+
+	return features;
+}
+
+static int stmmac_set_features(struct rtnet_device *netdev,
+			       netdev_features_t features)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(netdev);
+	bool sph_en;
+	u32 chan;
+
+	/* Keep the COE Type in case of csum is supporting */
+	if (features & NETIF_F_RXCSUM)
+		priv->hw->rx_csum = priv->plat->rx_coe;
+	else
+		priv->hw->rx_csum = 0;
+	/* No check needed because rx_coe has been set before and it will be
+	 * fixed in case of issue.
+	 */
+	stmmac_rx_ipc(priv, priv->hw);
+
+	sph_en = (priv->hw->rx_csum > 0) && priv->sph;
+	for (chan = 0; chan < priv->plat->rx_queues_to_use; chan++)
+		stmmac_enable_sph(priv, priv->ioaddr, sph_en, chan);
+
+	return 0;
+}
+#endif
+
+/**
+ *  stmmac_interrupt - main ISR
+ *  @irq: interrupt number.
+ *  @dev_id: to pass the net device pointer (must be valid).
+ *  Description: this is the main driver interrupt service routine.
+ *  It can call:
+ *  o DMA service routine (to manage incoming frame reception and transmission
+ *    status)
+ *  o Core interrupts to manage: remote wake-up, management counter, LPI
+ *    interrupts.
+ */
+static irqreturn_t stmmac_interrupt(int irq, void *dev_id)
+{
+	struct rtnet_device *dev = (struct rtnet_device *)dev_id;
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	u32 rx_cnt = priv->plat->rx_queues_to_use;
+	u32 tx_cnt = priv->plat->tx_queues_to_use;
+	u32 queues_count;
+	u32 queue;
+	bool xmac;
+
+	xmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;
+	queues_count = (rx_cnt > tx_cnt) ? rx_cnt : tx_cnt;
+
+#if 0
+	if (priv->irq_wake)
+		pm_wakeup_event(priv->device, 0);
+#endif
+	
+	/* Check if adapter is up */
+	if (test_bit(STMMAC_DOWN, &priv->state))
+		return IRQ_HANDLED;
+	/* Check if a fatal error happened */
+	if (stmmac_safety_feat_interrupt(priv))
+		return IRQ_HANDLED;
+
+	/* To handle GMAC own interrupts */
+	if ((priv->plat->has_gmac) || xmac) {
+		int status = stmmac_host_irq_status(priv, priv->hw, &priv->xstats);
+		int mtl_status;
+
+		if (unlikely(status)) {
+			/* For LPI we need to save the tx status */
+			if (status & CORE_IRQ_TX_PATH_IN_LPI_MODE)
+				priv->tx_path_in_lpi_mode = true;
+			if (status & CORE_IRQ_TX_PATH_EXIT_LPI_MODE)
+				priv->tx_path_in_lpi_mode = false;
+		}
+
+		for (queue = 0; queue < queues_count; queue++) {
+			struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+			mtl_status = stmmac_host_mtl_irq_status(priv, priv->hw,
+								queue);
+			if (mtl_status != -EINVAL)
+				status |= mtl_status;
+
+			if (status & CORE_IRQ_MTL_RX_OVERFLOW)
+				stmmac_set_rx_tail_ptr(priv, priv->ioaddr,
+						       rx_q->rx_tail_addr,
+						       queue);
+		}
+
+		/* PCS link status */
+		if (priv->hw->pcs) {
+			if (priv->xstats.pcs_link)
+				rtnetif_carrier_on(dev);
+			else
+				rtnetif_carrier_off(dev);
+		}
+	}
+	
+#if 0
+	/* To handle DMA interrupts */
+	stmmac_dma_interrupt(dev);
+#endif
+	
+	return IRQ_HANDLED;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/* Polling receive - used by NETCONSOLE and other diagnostic tools
+ * to allow network I/O with interrupts disabled.
+ */
+static void stmmac_poll_controller(struct rtnet_device *dev)
+{
+	disable_irq(dev->irq);
+	stmmac_interrupt(dev->irq, dev);
+	enable_irq(dev->irq);
+}
+#endif
+
+/**
+ *  stmmac_ioctl - Entry point for the Ioctl
+ *  @dev: Device pointer.
+ *  @rq: An IOCTL specefic structure, that can contain a pointer to
+ *  a proprietary structure used to pass information to the driver.
+ *  @cmd: IOCTL command
+ *  Description:
+ *  Currently it supports the phy_mii_ioctl(...) and HW time stamping.
+ */
+#if 0
+static int stmmac_ioctl(struct rtnet_device *dev, struct ifreq *rq, int cmd)
+{
+	struct stmmac_priv *priv = rtnetdev_priv (dev);
+	int ret = -EOPNOTSUPP;
+
+	if (!rtnetif_running(dev))
+		return -EINVAL;
+
+	switch (cmd) {
+	case SIOCGMIIPHY:
+	case SIOCGMIIREG:
+	case SIOCSMIIREG:
+		ret = phylink_mii_ioctl(priv->phylink, rq, cmd);
+		break;
+	case SIOCSHWTSTAMP:
+		ret = stmmac_hwtstamp_set(dev, rq);
+		break;
+	case SIOCGHWTSTAMP:
+		ret = stmmac_hwtstamp_get(dev, rq);
+		break;
+	default:
+		break;
+	}
+
+	return ret;
+}
+
+static int stmmac_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
+				    void *cb_priv)
+{
+	struct stmmac_priv *priv = cb_priv;
+	int ret = -EOPNOTSUPP;
+
+	if (!tc_cls_can_offload_and_chain0(priv->dummy, type_data))
+		return ret;
+
+	stmmac_disable_all_queues(priv);
+
+	switch (type) {
+	case TC_SETUP_CLSU32:
+		ret = stmmac_tc_setup_cls_u32(priv, priv, type_data);
+		break;
+	case TC_SETUP_CLSFLOWER:
+		ret = stmmac_tc_setup_cls(priv, priv, type_data);
+		break;
+	default:
+		break;
+	}
+
+	stmmac_enable_all_queues(priv);
+	return ret;
+}
+
+static LIST_HEAD(stmmac_block_cb_list);
+
+static int stmmac_setup_tc(struct rtnet_device *ndev, enum tc_setup_type type,
+			   void *type_data)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+
+	switch (type) {
+	case TC_SETUP_BLOCK:
+		return flow_block_cb_setup_simple(type_data,
+						  &stmmac_block_cb_list,
+						  stmmac_setup_tc_block_cb,
+						  priv, priv, true);
+	case TC_SETUP_QDISC_CBS:
+		return stmmac_tc_setup_cbs(priv, priv, type_data);
+	case TC_SETUP_QDISC_TAPRIO:
+		return stmmac_tc_setup_taprio(priv, priv, type_data);
+	case TC_SETUP_QDISC_ETF:
+		return stmmac_tc_setup_etf(priv, priv, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static u16 stmmac_select_queue(struct rtnet_device *dev, struct rtskb *skb,
+			       struct rtnet_device *sb_dev)
+{
+#if SOMETHING_NOT_DEFINED
+	int gso = skb_shinfo(skb)->gso_type;
+
+	if (gso & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6 | SKB_GSO_UDP_L4)) {
+		/*
+		 * There is no way to determine the number of TSO/USO
+		 * capable Queues. Let's use always the Queue 0
+		 * because if TSO/USO is supported then at least this
+		 * one will be capable.
+		 */
+		return 0;
+	}
+
+	return netdev_pick_tx(dev, skb, NULL) % dev->real_num_tx_queues;
+#endif
+	return 0;
+}
+
+static int stmmac_set_mac_address(struct rtnet_device *ndev, void *addr)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+	int ret = 0;
+
+	ret = eth_mac_addr(ndev, addr);
+	if (ret)
+		return ret;
+	if (rtnetif_running(ndev))
+		return -EBUSY;
+	if (!is_valid_ether_addr(((struct sockaddr *)addr)->sa_data))
+		return -EADDRNOTAVAIL;
+	memcpy(ndev->dev_addr, ((struct sockaddr *)addr)->sa_data, ETH_ALEN);
+	ret = 0;
+
+	stmmac_set_umac_addr(priv, priv->hw, ndev->dev_addr, 0);
+
+	return ret;
+}
+#endif
+
+#ifdef CONFIG_DEBUG_FS
+static struct dentry *stmmac_fs_dir;
+
+static void sysfs_display_ring(void *head, int size, int extend_desc,
+			       struct seq_file *seq)
+{
+	int i;
+	struct dma_extended_desc *ep = (struct dma_extended_desc *)head;
+	struct dma_desc *p = (struct dma_desc *)head;
+
+	for (i = 0; i < size; i++) {
+		if (extend_desc) {
+			seq_printf(seq, "%d [0x%x]: 0x%x 0x%x 0x%x 0x%x\n",
+				   i, (unsigned int)virt_to_phys(ep),
+				   le32_to_cpu(ep->basic.des0),
+				   le32_to_cpu(ep->basic.des1),
+				   le32_to_cpu(ep->basic.des2),
+				   le32_to_cpu(ep->basic.des3));
+			ep++;
+		} else {
+			seq_printf(seq, "%d [0x%x]: 0x%x 0x%x 0x%x 0x%x\n",
+				   i, (unsigned int)virt_to_phys(p),
+				   le32_to_cpu(p->des0), le32_to_cpu(p->des1),
+				   le32_to_cpu(p->des2), le32_to_cpu(p->des3));
+			p++;
+		}
+		seq_printf(seq, "\n");
+	}
+}
+
+static int stmmac_rings_status_show(struct seq_file *seq, void *v)
+{
+	struct rtnet_device *dev = seq->private;
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+	u32 rx_count = priv->plat->rx_queues_to_use;
+	u32 tx_count = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	if ((dev->flags & IFF_UP) == 0)
+		return 0;
+
+	for (queue = 0; queue < rx_count; queue++) {
+		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+		seq_printf(seq, "RX Queue %d:\n", queue);
+
+		if (priv->extend_desc) {
+			seq_printf(seq, "Extended descriptor ring:\n");
+			sysfs_display_ring((void *)rx_q->dma_erx,
+					   DMA_RX_SIZE, 1, seq);
+		} else {
+			seq_printf(seq, "Descriptor ring:\n");
+			sysfs_display_ring((void *)rx_q->dma_rx,
+					   DMA_RX_SIZE, 0, seq);
+		}
+	}
+
+	for (queue = 0; queue < tx_count; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+
+		seq_printf(seq, "TX Queue %d:\n", queue);
+
+		if (priv->extend_desc) {
+			seq_printf(seq, "Extended descriptor ring:\n");
+			sysfs_display_ring((void *)tx_q->dma_etx,
+					   DMA_TX_SIZE, 1, seq);
+		} else if (!(tx_q->tbs & STMMAC_TBS_AVAIL)) {
+			seq_printf(seq, "Descriptor ring:\n");
+			sysfs_display_ring((void *)tx_q->dma_tx,
+					   DMA_TX_SIZE, 0, seq);
+		}
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(stmmac_rings_status);
+
+static int stmmac_dma_cap_show(struct seq_file *seq, void *v)
+{
+	struct rtnet_device *dev = seq->private;
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	if (!priv->hw_cap_support) {
+		seq_printf(seq, "DMA HW features not supported\n");
+		return 0;
+	}
+
+	seq_printf(seq, "==============================\n");
+	seq_printf(seq, "\tDMA HW features\n");
+	seq_printf(seq, "==============================\n");
+
+	seq_printf(seq, "\t10/100 Mbps: %s\n",
+		   (priv->dma_cap.mbps_10_100) ? "Y" : "N");
+	seq_printf(seq, "\t1000 Mbps: %s\n",
+		   (priv->dma_cap.mbps_1000) ? "Y" : "N");
+	seq_printf(seq, "\tHalf duplex: %s\n",
+		   (priv->dma_cap.half_duplex) ? "Y" : "N");
+	seq_printf(seq, "\tHash Filter: %s\n",
+		   (priv->dma_cap.hash_filter) ? "Y" : "N");
+	seq_printf(seq, "\tMultiple MAC address registers: %s\n",
+		   (priv->dma_cap.multi_addr) ? "Y" : "N");
+	seq_printf(seq, "\tPCS (TBI/SGMII/RTBI PHY interfaces): %s\n",
+		   (priv->dma_cap.pcs) ? "Y" : "N");
+	seq_printf(seq, "\tSMA (MDIO) Interface: %s\n",
+		   (priv->dma_cap.sma_mdio) ? "Y" : "N");
+	seq_printf(seq, "\tPMT Remote wake up: %s\n",
+		   (priv->dma_cap.pmt_remote_wake_up) ? "Y" : "N");
+	seq_printf(seq, "\tPMT Magic Frame: %s\n",
+		   (priv->dma_cap.pmt_magic_frame) ? "Y" : "N");
+	seq_printf(seq, "\tRMON module: %s\n",
+		   (priv->dma_cap.rmon) ? "Y" : "N");
+	seq_printf(seq, "\tIEEE 1588-2002 Time Stamp: %s\n",
+		   (priv->dma_cap.time_stamp) ? "Y" : "N");
+	seq_printf(seq, "\tIEEE 1588-2008 Advanced Time Stamp: %s\n",
+		   (priv->dma_cap.atime_stamp) ? "Y" : "N");
+	seq_printf(seq, "\t802.3az - Energy-Efficient Ethernet (EEE): %s\n",
+		   (priv->dma_cap.eee) ? "Y" : "N");
+	seq_printf(seq, "\tAV features: %s\n", (priv->dma_cap.av) ? "Y" : "N");
+	seq_printf(seq, "\tChecksum Offload in TX: %s\n",
+		   (priv->dma_cap.tx_coe) ? "Y" : "N");
+	if (priv->synopsys_id >= DWMAC_CORE_4_00) {
+		seq_printf(seq, "\tIP Checksum Offload in RX: %s\n",
+			   (priv->dma_cap.rx_coe) ? "Y" : "N");
+	} else {
+		seq_printf(seq, "\tIP Checksum Offload (type1) in RX: %s\n",
+			   (priv->dma_cap.rx_coe_type1) ? "Y" : "N");
+		seq_printf(seq, "\tIP Checksum Offload (type2) in RX: %s\n",
+			   (priv->dma_cap.rx_coe_type2) ? "Y" : "N");
+	}
+	seq_printf(seq, "\tRXFIFO > 2048bytes: %s\n",
+		   (priv->dma_cap.rxfifo_over_2048) ? "Y" : "N");
+	seq_printf(seq, "\tNumber of Additional RX channel: %d\n",
+		   priv->dma_cap.number_rx_channel);
+	seq_printf(seq, "\tNumber of Additional TX channel: %d\n",
+		   priv->dma_cap.number_tx_channel);
+	seq_printf(seq, "\tNumber of Additional RX queues: %d\n",
+		   priv->dma_cap.number_rx_queues);
+	seq_printf(seq, "\tNumber of Additional TX queues: %d\n",
+		   priv->dma_cap.number_tx_queues);
+	seq_printf(seq, "\tEnhanced descriptors: %s\n",
+		   (priv->dma_cap.enh_desc) ? "Y" : "N");
+	seq_printf(seq, "\tTX Fifo Size: %d\n", priv->dma_cap.tx_fifo_size);
+	seq_printf(seq, "\tRX Fifo Size: %d\n", priv->dma_cap.rx_fifo_size);
+	seq_printf(seq, "\tHash Table Size: %d\n", priv->dma_cap.hash_tb_sz);
+	seq_printf(seq, "\tTSO: %s\n", priv->dma_cap.tsoen ? "Y" : "N");
+	seq_printf(seq, "\tNumber of PPS Outputs: %d\n",
+		   priv->dma_cap.pps_out_num);
+	seq_printf(seq, "\tSafety Features: %s\n",
+		   priv->dma_cap.asp ? "Y" : "N");
+	seq_printf(seq, "\tFlexible RX Parser: %s\n",
+		   priv->dma_cap.frpsel ? "Y" : "N");
+	seq_printf(seq, "\tEnhanced Addressing: %d\n",
+		   priv->dma_cap.addr64);
+	seq_printf(seq, "\tReceive Side Scaling: %s\n",
+		   priv->dma_cap.rssen ? "Y" : "N");
+	seq_printf(seq, "\tVLAN Hash Filtering: %s\n",
+		   priv->dma_cap.vlhash ? "Y" : "N");
+	seq_printf(seq, "\tSplit Header: %s\n",
+		   priv->dma_cap.sphen ? "Y" : "N");
+	seq_printf(seq, "\tVLAN TX Insertion: %s\n",
+		   priv->dma_cap.vlins ? "Y" : "N");
+	seq_printf(seq, "\tDouble VLAN: %s\n",
+		   priv->dma_cap.dvlan ? "Y" : "N");
+	seq_printf(seq, "\tNumber of L3/L4 Filters: %d\n",
+		   priv->dma_cap.l3l4fnum);
+	seq_printf(seq, "\tARP Offloading: %s\n",
+		   priv->dma_cap.arpoffsel ? "Y" : "N");
+	seq_printf(seq, "\tEnhancements to Scheduled Traffic (EST): %s\n",
+		   priv->dma_cap.estsel ? "Y" : "N");
+	seq_printf(seq, "\tFrame Preemption (FPE): %s\n",
+		   priv->dma_cap.fpesel ? "Y" : "N");
+	seq_printf(seq, "\tTime-Based Scheduling (TBS): %s\n",
+		   priv->dma_cap.tbssel ? "Y" : "N");
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(stmmac_dma_cap);
+
+/* Use network device events to rename debugfs file entries.
+ */
+static int stmmac_device_event(struct notifier_block *unused,
+			       unsigned long event, void *ptr)
+{
+	struct net_device *dummy = netdev_notifier_info_to_dev(ptr);
+	struct stmmac_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv *)netdev_priv(dummy))->rtdev);
+
+	if (dummy->netdev_ops != &stmmac_netdev_ops)
+		goto done;
+
+	switch (event) {
+	case NETDEV_CHANGENAME:
+		if (priv->dbgfs_dir)
+			priv->dbgfs_dir = debugfs_rename(stmmac_fs_dir,
+							 priv->dbgfs_dir,
+							 stmmac_fs_dir,
+							 priv->dev->name);
+		break;
+	}
+done:
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block stmmac_notifier = {
+	.notifier_call = stmmac_device_event,
+};
+
+static void stmmac_init_fs(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	rtnl_lock();
+
+	/* Create per netdev entries */
+	priv->dbgfs_dir = debugfs_create_dir(dev->name, stmmac_fs_dir);
+
+	/* Entry to report DMA RX/TX rings */
+	debugfs_create_file("descriptors_status", 0444, priv->dbgfs_dir, dev,
+			    &stmmac_rings_status_fops);
+
+	/* Entry to report the DMA HW features */
+	debugfs_create_file("dma_cap", 0444, priv->dbgfs_dir, dev,
+			    &stmmac_dma_cap_fops);
+
+	rtnl_unlock();
+}
+
+static void stmmac_exit_fs(struct rtnet_device *dev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(dev);
+
+	debugfs_remove_recursive(priv->dbgfs_dir);
+}
+#endif /* CONFIG_DEBUG_FS */
+
+#if 0
+static u32 stmmac_vid_crc32_le(__le16 vid_le)
+{
+	unsigned char *data = (unsigned char *)&vid_le;
+	unsigned char data_byte = 0;
+	u32 crc = ~0x0;
+	u32 temp = 0;
+	int i, bits;
+
+	bits = get_bitmask_order(VLAN_VID_MASK);
+	for (i = 0; i < bits; i++) {
+		if ((i % 8) == 0)
+			data_byte = data[i / 8];
+
+		temp = ((crc & 1) ^ data_byte) & 1;
+		crc >>= 1;
+		data_byte >>= 1;
+
+		if (temp)
+			crc ^= 0xedb88320;
+	}
+
+	return crc;
+}
+
+static int stmmac_vlan_update(struct stmmac_priv *priv, bool is_double)
+{
+	u32 crc, hash = 0;
+	__le16 pmatch = 0;
+	int count = 0;
+	u16 vid = 0;
+
+	for_each_set_bit(vid, priv->active_vlans, VLAN_N_VID) {
+		__le16 vid_le = cpu_to_le16(vid);
+		crc = bitrev32(~stmmac_vid_crc32_le(vid_le)) >> 28;
+		hash |= (1 << crc);
+		count++;
+	}
+
+	if (!priv->dma_cap.vlhash) {
+		if (count > 2) /* VID = 0 always passes filter */
+			return -EOPNOTSUPP;
+
+		pmatch = cpu_to_le16(vid);
+		hash = 0;
+	}
+
+	return stmmac_update_vlan_hash(priv, priv->hw, hash, pmatch, is_double);
+}
+
+static int stmmac_vlan_rx_add_vid(struct rtnet_device *ndev, __be16 proto, u16 vid)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+	bool is_double = false;
+	int ret;
+
+	if (be16_to_cpu(proto) == ETH_P_8021AD)
+		is_double = true;
+
+	set_bit(vid, priv->active_vlans);
+	ret = stmmac_vlan_update(priv, is_double);
+	if (ret) {
+		clear_bit(vid, priv->active_vlans);
+		return ret;
+	}
+
+	if (priv->hw->num_vlan) {
+		ret = stmmac_add_hw_vlan_rx_fltr(priv, priv->dummy, priv->hw, proto, vid);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int stmmac_vlan_rx_kill_vid(struct rtnet_device *ndev, __be16 proto, u16 vid)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+	bool is_double = false;
+	int ret;
+
+	if (be16_to_cpu(proto) == ETH_P_8021AD)
+		is_double = true;
+
+	clear_bit(vid, priv->active_vlans);
+
+	if (priv->hw->num_vlan) {
+		ret = stmmac_del_hw_vlan_rx_fltr(priv, priv->dummy, priv->hw, proto, vid);
+		if (ret)
+			return ret;
+	}
+
+	return stmmac_vlan_update(priv, is_double);
+}
+#endif
+
+static const struct net_device_ops stmmac_netdev_ops = {
+#if 0
+	.ndo_open = stmmac_open,
+	.ndo_start_xmit = stmmac_xmit,
+	.ndo_stop = stmmac_release,
+	.ndo_change_mtu = stmmac_change_mtu,
+	.ndo_fix_features = stmmac_fix_features,
+	.ndo_set_features = stmmac_set_features,
+	.ndo_set_rx_mode = stmmac_set_rx_mode,
+	.ndo_tx_timeout = stmmac_tx_timeout,
+	.ndo_do_ioctl = stmmac_ioctl,
+	.ndo_setup_tc = stmmac_setup_tc,
+	.ndo_select_queue = stmmac_select_queue,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller = stmmac_poll_controller,
+#endif
+	.ndo_set_mac_address = stmmac_set_mac_address,
+	.ndo_vlan_rx_add_vid = stmmac_vlan_rx_add_vid,
+	.ndo_vlan_rx_kill_vid = stmmac_vlan_rx_kill_vid,
+#endif
+};
+
+static void stmmac_reset_subtask(struct stmmac_priv *priv)
+{
+	if (!test_and_clear_bit(STMMAC_RESET_REQUESTED, &priv->state))
+		return;
+	if (test_bit(STMMAC_DOWN, &priv->state))
+		return;
+
+	netdev_err(priv->dummy, "Reset adapter.\n");
+
+	rtnl_lock();
+	rtnetif_trans_update(priv->dev);
+	while (test_and_set_bit(STMMAC_RESETING, &priv->state))
+		usleep_range(1000, 2000);
+
+	set_bit(STMMAC_DOWN, &priv->state);
+	rtdev_close(priv->dev);
+	rtdev_open(priv->dev);
+	clear_bit(STMMAC_DOWN, &priv->state);
+	clear_bit(STMMAC_RESETING, &priv->state);
+	rtnl_unlock();
+}
+
+static void stmmac_service_task(struct work_struct *work)
+{
+	struct stmmac_priv *priv = container_of(work, struct stmmac_priv,
+			service_task);
+
+	stmmac_reset_subtask(priv);
+	clear_bit(STMMAC_SERVICE_SCHED, &priv->state);
+}
+
+/**
+ *  stmmac_hw_init - Init the MAC device
+ *  @priv: driver private structure
+ *  Description: this function is to configure the MAC device according to
+ *  some platform parameters or the HW capability register. It prepares the
+ *  driver to use either ring or chain modes and to setup either enhanced or
+ *  normal descriptors.
+ */
+static int stmmac_hw_init(struct stmmac_priv *priv)
+{
+	int ret;
+
+	/* dwmac-sun8i only work in chain mode */
+	if (priv->plat->has_sun8i)
+		chain_mode = 1;
+	priv->chain_mode = chain_mode;
+
+	/* Initialize HW Interface */
+	ret = stmmac_hwif_init(priv);
+	if (ret)
+		return ret;
+
+	/* Get the HW capability (new GMAC newer than 3.50a) */
+	priv->hw_cap_support = stmmac_get_hw_features(priv);
+	if (priv->hw_cap_support) {
+		dev_info(priv->device, "DMA HW capability register supported\n");
+
+		/* We can override some gmac/dma configuration fields: e.g.
+		 * enh_desc, tx_coe (e.g. that are passed through the
+		 * platform) with the values from the HW capability
+		 * register (if supported).
+		 */
+		priv->plat->enh_desc = priv->dma_cap.enh_desc;
+		priv->plat->pmt = priv->dma_cap.pmt_remote_wake_up;
+		priv->hw->pmt = priv->plat->pmt;
+		if (priv->dma_cap.hash_tb_sz) {
+			priv->hw->multicast_filter_bins =
+					(BIT(priv->dma_cap.hash_tb_sz) << 5);
+			priv->hw->mcast_bits_log2 =
+					ilog2(priv->hw->multicast_filter_bins);
+		}
+
+		/* TXCOE doesn't work in thresh DMA mode */
+		if (priv->plat->force_thresh_dma_mode)
+			priv->plat->tx_coe = 0;
+		else
+			priv->plat->tx_coe = priv->dma_cap.tx_coe;
+
+		/* In case of GMAC4 rx_coe is from HW cap register. */
+		priv->plat->rx_coe = priv->dma_cap.rx_coe;
+
+		if (priv->dma_cap.rx_coe_type2)
+			priv->plat->rx_coe = STMMAC_RX_COE_TYPE2;
+		else if (priv->dma_cap.rx_coe_type1)
+			priv->plat->rx_coe = STMMAC_RX_COE_TYPE1;
+
+	} else {
+		dev_info(priv->device, "No HW DMA feature register supported\n");
+	}
+
+	if (priv->plat->rx_coe) {
+		priv->hw->rx_csum = priv->plat->rx_coe;
+		dev_info(priv->device, "RX Checksum Offload Engine supported\n");
+		if (priv->synopsys_id < DWMAC_CORE_4_00)
+			dev_info(priv->device, "COE Type %d\n", priv->hw->rx_csum);
+	}
+	if (priv->plat->tx_coe)
+		dev_info(priv->device, "TX Checksum insertion supported\n");
+
+	if (priv->plat->pmt) {
+		dev_info(priv->device, "Wake-Up On Lan supported\n");
+		device_set_wakeup_capable(priv->device, 1);
+	}
+
+	if (priv->dma_cap.tsoen)
+		dev_info(priv->device, "TSO supported\n");
+
+	/* Run HW quirks, if any */
+	if (priv->hwif_quirks) {
+		ret = priv->hwif_quirks(priv);
+		if (ret)
+			return ret;
+	}
+
+	/* Rx Watchdog is available in the COREs newer than the 3.40.
+	 * In some case, for example on bugged HW this feature
+	 * has to be disable and this can be done by passing the
+	 * riwt_off field from the platform.
+	 */
+	if (((priv->synopsys_id >= DWMAC_CORE_3_50) ||
+	    (priv->plat->has_xgmac)) && (!priv->plat->riwt_off)) {
+		priv->use_riwt = 1;
+		dev_info(priv->device,
+			 "Enable RX Mitigation via HW Watchdog Timer\n");
+	}
+
+	return 0;
+}
+
+/**
+ * stmmac_dvr_probe
+ * @device: device pointer
+ * @plat_dat: platform data pointer
+ * @res: stmmac resource pointer
+ * Description: this is the main probe function used to
+ * call the alloc_etherdev, allocate the priv structure.
+ * Return:
+ * returns 0 on success, otherwise errno.
+ */
+int stmmac_dvr_probe(struct device *device,
+		     struct plat_stmmacenet_data *plat_dat,
+		     struct stmmac_resources *res)
+{
+	struct rtnet_device *ndev = NULL;
+	struct net_device *dummy = NULL;
+	struct dummy_rtnetdev_priv *dummy_priv = NULL;
+	struct stmmac_priv *priv;
+	u32 queue, rxq, maxq;
+	int i, ret = 0;
+
+	ndev = rt_alloc_etherdev_mqs(sizeof(*priv), stmmac_rtskb_pool_size, 
+							   MTL_MAX_TX_QUEUES, MTL_MAX_RX_QUEUES);
+	if (!ndev) {
+		dev_err(device, "can't allocate rtnet device\n");
+		return -ENOMEM;
+	}
+    rtdev_alloc_name(ndev, "rteth%d");
+	ndev->vers = RTDEV_VERS_2_0;
+    rt_rtdev_connect(ndev, &RTDEV_manager);
+	
+	dummy = alloc_etherdev(sizeof(struct dummy_rtnetdev_priv));
+	if (!dummy) {
+		dev_err(device, "can't allocate dummy net device\n");
+		ret = -ENOMEM;
+		goto error_5;
+	}
+	dev_alloc_name(dummy, "dummy_rteth%d");	
+	SET_NETDEV_DEV(dummy, device);
+
+	priv = rtnetdev_priv(ndev);
+	priv->device = device;
+	priv->dev = ndev;
+	priv->dummy = dummy;
+	dummy_priv = netdev_priv(dummy);
+	dummy_priv->rtdev = ndev;
+
+	raw_spin_lock_init(&priv->rtnet_queue_lock);
+	
+#if 0
+	stmmac_set_ethtool_ops(ndev);
+#endif
+	priv->pause = pause;
+	priv->plat = plat_dat;
+	priv->ioaddr = res->addr;
+	priv->dev->base_addr = (unsigned long)res->addr;
+
+	priv->dev->irq = res->irq;
+	priv->wol_irq = res->wol_irq;
+	priv->lpi_irq = res->lpi_irq;
+
+	if (!IS_ERR_OR_NULL(res->mac))
+		memcpy(priv->dev->dev_addr, res->mac, ETH_ALEN);
+
+	dev_set_drvdata(device, priv->dev);
+
+	/* Verify driver arguments */
+	stmmac_verify_args();
+
+	/* Allocate workqueue */
+	priv->wq = create_singlethread_workqueue("stmmac_wq");
+	if (!priv->wq) {
+		dev_err(priv->device, "failed to create workqueue\n");
+		return -ENOMEM;
+	}
+
+	INIT_WORK(&priv->service_task, stmmac_service_task);
+
+	/* Override with kernel parameters if supplied XXX CRS XXX
+	 * this needs to have multiple instances
+	 */
+	if ((phyaddr >= 0) && (phyaddr <= 31))
+		priv->plat->phy_addr = phyaddr;
+
+	if (priv->plat->stmmac_rst) {
+		ret = reset_control_assert(priv->plat->stmmac_rst);
+		reset_control_deassert(priv->plat->stmmac_rst);
+		/* Some reset controllers have only reset callback instead of
+		 * assert + deassert callbacks pair.
+		 */
+		if (ret == -ENOTSUPP)
+			reset_control_reset(priv->plat->stmmac_rst);
+	}
+
+	/* Init MAC and get the capabilities */
+	ret = stmmac_hw_init(priv);
+	if (ret)
+		goto error_hw_init;
+
+	stmmac_check_ether_addr(priv);
+
+	/* Configure real RX and TX queues */
+	printk(KERN_INFO "%s priv->plat->rx_queues_to_use=%d priv->plat->tx_queues_to_use=%d\n",
+		   __func__, priv->plat->rx_queues_to_use, priv->plat->tx_queues_to_use);
+	rtnetif_set_real_num_rx_queues(ndev, priv->plat->rx_queues_to_use);
+	rtnetif_set_real_num_tx_queues(ndev, priv->plat->tx_queues_to_use);
+
+	/* The RTnet specific entries in the device structure. */
+	ndev->open = stmmac_open;
+	ndev->stop = stmmac_release;
+	ndev->hard_header = &rt_eth_header;
+	ndev->hard_start_xmit = stmmac_xmit;
+	ndev->start_xmit = ndev->hard_start_xmit;
+	ndev->get_stats = stmmac_get_stats;
+	dummy->netdev_ops	= &stmmac_netdev_ops;
+
+	ndev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+			    NETIF_F_RXCSUM;
+
+	ret = stmmac_tc_init(priv, priv);
+	if (!ret) {
+		ndev->features |= NETIF_F_HW_TC;
+	}
+
+	if ((priv->plat->tso_en) && (priv->dma_cap.tsoen)) {
+		ndev->features |= NETIF_F_TSO | NETIF_F_TSO6;
+		if (priv->plat->has_gmac4)
+			ndev->features |= NETIF_F_GSO_UDP_L4;
+		priv->tso = true;
+		dev_info(priv->device, "TSO feature enabled\n");
+	}
+
+	if (priv->dma_cap.sphen) {
+		ndev->features |= NETIF_F_GRO;
+		priv->sph = true;
+		dev_info(priv->device, "SPH feature enabled\n");
+	}
+
+	if (priv->dma_cap.addr64) {
+		ret = dma_set_mask_and_coherent(device,
+				DMA_BIT_MASK(priv->dma_cap.addr64));
+		if (!ret) {
+			dev_info(priv->device, "Using %d bits DMA width\n",
+				 priv->dma_cap.addr64);
+
+			/*
+			 * If more than 32 bits can be addressed, make sure to
+			 * enable enhanced addressing mode.
+			 */
+			if (IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT))
+				priv->plat->dma_cfg->eame = true;
+		} else {
+			ret = dma_set_mask_and_coherent(device, DMA_BIT_MASK(32));
+			if (ret) {
+				dev_err(priv->device, "Failed to set DMA Mask\n");
+				goto error_hw_init;
+			}
+
+			priv->dma_cap.addr64 = 32;
+		}
+	}
+
+	ndev->features |= ndev->features | NETIF_F_HIGHDMA;
+#if 0
+	/* RTnet does not know about watchdog_timeo */
+	ndev->watchdog_timeo = msecs_to_jiffies(watchdog);
+#endif
+	
+#ifdef STMMAC_VLAN_TAG_USED
+	/* Both mac100 and gmac support receive VLAN tag detection */
+	ndev->features |= NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_STAG_RX;
+	if (priv->dma_cap.vlhash) {
+		ndev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+		ndev->features |= NETIF_F_HW_VLAN_STAG_FILTER;
+	}
+	if (priv->dma_cap.vlins) {
+		ndev->features |= NETIF_F_HW_VLAN_CTAG_TX;
+		if (priv->dma_cap.dvlan)
+			ndev->features |= NETIF_F_HW_VLAN_STAG_TX;
+	}
+#endif
+	priv->msg_enable = netif_msg_init(debug, default_msg_level);
+
+	/* Initialize RSS */
+	rxq = priv->plat->rx_queues_to_use;
+	netdev_rss_key_fill(priv->rss.key, sizeof(priv->rss.key));
+	for (i = 0; i < ARRAY_SIZE(priv->rss.table); i++)
+		priv->rss.table[i] = ethtool_rxfh_indir_default(i, rxq);
+
+	if (priv->dma_cap.rssen && priv->plat->rss_en)
+		ndev->features |= NETIF_F_RXHASH;
+
+	/* MTU range: 46 - hw-specific max */
+#if 0
+	/* RTnet does not know min_mtu.
+	 * rtdev->mtu is 1500;  (eth_mtu)
+	 */ 
+	ndev->min_mtu = ETH_ZLEN - ETH_HLEN;
+	if (priv->plat->has_xgmac)
+		ndev->max_mtu = XGMAC_JUMBO_LEN;
+	else if ((priv->plat->enh_desc) || (priv->synopsys_id >= DWMAC_CORE_4_00))
+		ndev->max_mtu = JUMBO_LEN;
+	else
+		ndev->max_mtu = SKB_MAX_HEAD(NET_SKB_PAD + NET_IP_ALIGN);
+	/* Will not overwrite ndev->max_mtu if plat->maxmtu > ndev->max_mtu
+	 * as well as plat->maxmtu < ndev->min_mtu which is a invalid range.
+	 */
+	printk(KERN_INFO "%s priv->plat->maxmtu=%d ndev->max_mtu=%d\n",
+		   __func__, priv->plat->maxmtu, ndev->max_mtu);
+	if ((priv->plat->maxmtu < ndev->max_mtu) &&
+	    (priv->plat->maxmtu >= ndev->min_mtu))
+		ndev->max_mtu = priv->plat->maxmtu;
+	else if (priv->plat->maxmtu < ndev->min_mtu)
+		dev_warn(priv->device,
+			 "%s: warning: maxmtu having invalid value (%d)\n",
+			 __func__, priv->plat->maxmtu);
+#endif
+	
+	if (flow_ctrl)
+		priv->flow_ctrl = FLOW_AUTO;	/* RX/TX pause on */
+
+	/* Setup channels NAPI */
+	maxq = max(priv->plat->rx_queues_to_use, priv->plat->tx_queues_to_use);
+
+	for (queue = 0; queue < maxq; queue++) {
+		struct stmmac_channel *ch = &priv->channel[queue];
+
+		raw_spin_lock_init(&ch->lock);
+		ch->priv_data = priv;
+		ch->index = queue;
+	}
+
+	rt_mutex_init(&priv->lock);
+
+	/* If a specific clk_csr value is passed from the platform
+	 * this means that the CSR Clock Range selection cannot be
+	 * changed at run-time and it is fixed. Viceversa the driver'll try to
+	 * set the MDC clock dynamically according to the csr actual
+	 * clock input.
+	 */
+	if (priv->plat->clk_csr >= 0)
+		priv->clk_csr = priv->plat->clk_csr;
+	else
+		stmmac_clk_csr_set(priv);
+
+	stmmac_check_pcs_mode(priv);
+
+	if (priv->hw->pcs != STMMAC_PCS_TBI &&
+	    priv->hw->pcs != STMMAC_PCS_RTBI) {
+		/* MDIO bus Registration */
+		ret = stmmac_mdio_register(dummy);
+		if (ret < 0) {
+			dev_err(priv->device,
+				"%s: MDIO bus (id: %d) registration failed",
+				__func__, priv->plat->bus_id);
+			goto error_mdio_register;
+		}
+	}
+
+	ret = stmmac_phy_setup(priv);
+	if (ret) {
+		netdev_err(priv->dummy, "failed to setup phy (%d)\n", ret);
+		goto error_phy_setup;
+	}
+
+	ret = rt_register_rtnetdev(ndev);
+	if (ret) {
+		dev_err(device, "%s: ERROR %i registering the device\n",
+			__func__, ret);
+		goto error_netdev_register;
+	}
+	ret = register_netdev(dummy);
+	if (ret) {
+		dev_err(device, "register_netdev err=%d\n", ret);
+		printk(KERN_ERR "register_netdev err=%d\n", ret);
+		goto error_0;
+	} else {
+		printk(KERN_INFO "dummy-rteth0: register_netdev err=%d"
+			   " dummy->state=%lx\n", 
+			   ret, dummy->state);
+	}
+
+	if (priv->plat->serdes_powerup) {
+		ret = priv->plat->serdes_powerup(priv->dummy,
+						 priv->plat->bsp_priv);
+
+		if (ret < 0)
+			goto error_serdes_powerup;
+	}
+
+#ifdef CONFIG_DEBUG_FS
+	stmmac_init_fs(ndev);
+#endif
+
+	return ret;
+
+error_0:
+	rt_unregister_rtnetdev(priv->dev);
+error_serdes_powerup:
+	unregister_netdev(dummy);
+error_netdev_register:
+	phylink_destroy(priv->phylink);
+error_phy_setup:
+	if (priv->hw->pcs != STMMAC_PCS_TBI &&
+	    priv->hw->pcs != STMMAC_PCS_RTBI)
+		stmmac_mdio_unregister(dummy);
+error_mdio_register:
+error_hw_init:
+	destroy_workqueue(priv->wq);
+error_5:
+	rt_rtdev_disconnect(priv->dev);
+	rtdev_free(priv->dev);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(stmmac_dvr_probe);
+
+/**
+ * stmmac_dvr_remove
+ * @dev: device pointer
+ * Description: this function resets the TX/RX processes, disables the MAC RX/TX
+ * changes the link status, releases the DMA descriptor rings.
+ */
+int stmmac_dvr_remove(struct device *dev)
+{
+	struct rtnet_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+
+	netdev_info(priv->dummy, "%s: removing driver", __func__);
+
+	stmmac_stop_all_dma(priv);
+
+	if (priv->plat->serdes_powerdown)
+		priv->plat->serdes_powerdown(priv->dummy, priv->plat->bsp_priv);
+
+	stmmac_mac_set(priv, priv->ioaddr, false);
+	rtnetif_carrier_off(ndev);
+	
+	rt_unregister_rtnetdev(priv->dev);
+	rt_stack_disconnect(priv->dev);
+	rt_rtdev_disconnect(priv->dev);
+	rtdev_free(priv->dev);
+	
+#ifdef CONFIG_DEBUG_FS
+	stmmac_exit_fs(ndev);
+#endif
+	phylink_destroy(priv->phylink);
+	if (priv->plat->stmmac_rst)
+		reset_control_assert(priv->plat->stmmac_rst);
+	clk_disable_unprepare(priv->plat->pclk);
+	clk_disable_unprepare(priv->plat->stmmac_clk);
+	if (priv->hw->pcs != STMMAC_PCS_TBI &&
+	    priv->hw->pcs != STMMAC_PCS_RTBI)
+		stmmac_mdio_unregister(priv->dummy);
+	destroy_workqueue(priv->wq);
+	rt_mutex_destroy(&priv->lock);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(stmmac_dvr_remove);
+
+/**
+ * stmmac_suspend - suspend callback
+ * @dev: device pointer
+ * Description: this is the function to suspend the device and it is called
+ * by the platform driver to stop the network queue, release the resources,
+ * program the PMT register (for WoL), clean and release driver resources.
+ */
+#if 0
+int stmmac_suspend(struct device *dev)
+{
+	struct rtnet_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+#if 0
+	u32 chan;
+#endif
+	
+	if (!ndev || !rtnetif_running(ndev))
+		return 0;
+
+	phylink_mac_change(priv->phylink, false);
+
+	rt_mutex_lock(&priv->lock);
+
+	rtnetif_device_detach(ndev);
+	stmmac_stop_all_queues(priv);
+
+	stmmac_disable_all_queues(priv);
+
+#if 0
+	for (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)
+		del_timer_sync(&priv->tx_queue[chan].txtimer);
+#endif
+	
+	/* Stop TX/RX DMA */
+	stmmac_stop_all_dma(priv);
+
+	if (priv->plat->serdes_powerdown)
+		priv->plat->serdes_powerdown(priv->dummy, priv->plat->bsp_priv);
+
+	/* Enable Power down mode by programming the PMT regs */
+	if (device_may_wakeup(priv->device) && priv->plat->pmt) {
+		stmmac_pmt(priv, priv->hw, priv->wolopts);
+		priv->irq_wake = 1;
+	} else {
+		rt_mutex_unlock(&priv->lock);
+		rtnl_lock();
+		if (device_may_wakeup(priv->device))
+			phylink_speed_down(priv->phylink, false);
+		phylink_stop(priv->phylink);
+		rtnl_unlock();
+		rt_mutex_lock(&priv->lock);
+
+		stmmac_mac_set(priv, priv->ioaddr, false);
+		pinctrl_pm_select_sleep_state(priv->device);
+		/* Disable clock in case of PWM is off */
+		if (priv->plat->clk_ptp_ref)
+			clk_disable_unprepare(priv->plat->clk_ptp_ref);
+		clk_disable_unprepare(priv->plat->pclk);
+		clk_disable_unprepare(priv->plat->stmmac_clk);
+	}
+	rt_mutex_unlock(&priv->lock);
+
+	priv->speed = SPEED_UNKNOWN;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(stmmac_suspend);
+
+/**
+ * stmmac_reset_queues_param - reset queue parameters
+ * @dev: device pointer
+ */
+static void stmmac_reset_queues_param(struct stmmac_priv *priv)
+{
+	u32 rx_cnt = priv->plat->rx_queues_to_use;
+	u32 tx_cnt = priv->plat->tx_queues_to_use;
+	u32 queue;
+
+	for (queue = 0; queue < rx_cnt; queue++) {
+		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+		rx_q->cur_rx = 0;
+		rx_q->dirty_rx = 0;
+	}
+
+	for (queue = 0; queue < tx_cnt; queue++) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+
+		tx_q->cur_tx = 0;
+		tx_q->dirty_tx = 0;
+		tx_q->mss = 0;
+	}
+}
+
+/**
+ * stmmac_resume - resume callback
+ * @dev: device pointer
+ * Description: when resume this function is invoked to setup the DMA and CORE
+ * in a usable state.
+ */
+int stmmac_resume(struct device *dev)
+{
+	struct rtnet_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = rtnetdev_priv(ndev);
+	int ret;
+
+	if (!rtnetif_running(ndev))
+		return 0;
+
+	/* Power Down bit, into the PM register, is cleared
+	 * automatically as soon as a magic packet or a Wake-up frame
+	 * is received. Anyway, it's better to manually clear
+	 * this bit because it can generate problems while resuming
+	 * from another devices (e.g. serial console).
+	 */
+	if (device_may_wakeup(priv->device) && priv->plat->pmt) {
+		rt_mutex_lock(&priv->lock);
+		stmmac_pmt(priv, priv->hw, 0);
+		rt_mutex_unlock(&priv->lock);
+		priv->irq_wake = 0;
+	} else {
+		pinctrl_pm_select_default_state(priv->device);
+		/* enable the clk previously disabled */
+		clk_prepare_enable(priv->plat->stmmac_clk);
+		clk_prepare_enable(priv->plat->pclk);
+		if (priv->plat->clk_ptp_ref)
+			clk_prepare_enable(priv->plat->clk_ptp_ref);
+		/* reset the phy so that it's ready */
+		if (priv->mii)
+			stmmac_mdio_reset(priv->mii);
+	}
+
+	if (priv->plat->serdes_powerup) {
+		ret = priv->plat->serdes_powerup(priv->dummy,
+						 priv->plat->bsp_priv);
+
+		if (ret < 0)
+			return ret;
+	}
+
+	rt_mutex_lock(&priv->lock);
+
+	stmmac_reset_queues_param(priv);
+
+	stmmac_clear_descriptors(priv);
+
+	stmmac_hw_setup(ndev, false);
+	stmmac_init_coalesce(priv);
+	stmmac_set_rx_mode(ndev);
+
+	stmmac_restore_hw_vlan_rx_fltr(priv, priv->dummy, priv->hw);
+
+	stmmac_enable_all_queues(priv);
+
+	stmmac_start_all_queues(priv);
+
+	rt_mutex_unlock(&priv->lock);
+
+	if (!device_may_wakeup(priv->device) || !priv->plat->pmt) {
+		rtnl_lock();
+		phylink_start(priv->phylink);
+		/* We may have called phylink_speed_down before */
+		phylink_speed_up(priv->phylink);
+		rtnl_unlock();
+	}
+
+	phylink_mac_change(priv->phylink, true);
+
+	rtnetif_device_attach(ndev);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(stmmac_resume);
+#endif
+
+#ifndef MODULE
+static int __init stmmac_cmdline_opt(char *str)
+{
+	char *opt;
+
+	if (!str || !*str)
+		return -EINVAL;
+	while ((opt = strsep(&str, ",")) != NULL) {
+		if (!strncmp(opt, "debug:", 6)) {
+			if (kstrtoint(opt + 6, 0, &debug))
+				goto err;
+		} else if (!strncmp(opt, "phyaddr:", 8)) {
+			if (kstrtoint(opt + 8, 0, &phyaddr))
+				goto err;
+		} else if (!strncmp(opt, "buf_sz:", 7)) {
+			if (kstrtoint(opt + 7, 0, &buf_sz))
+				goto err;
+		} else if (!strncmp(opt, "tc:", 3)) {
+			if (kstrtoint(opt + 3, 0, &tc))
+				goto err;
+		} else if (!strncmp(opt, "watchdog:", 9)) {
+			if (kstrtoint(opt + 9, 0, &watchdog))
+				goto err;
+		} else if (!strncmp(opt, "flow_ctrl:", 10)) {
+			if (kstrtoint(opt + 10, 0, &flow_ctrl))
+				goto err;
+		} else if (!strncmp(opt, "pause:", 6)) {
+			if (kstrtoint(opt + 6, 0, &pause))
+				goto err;
+		} else if (!strncmp(opt, "eee_timer:", 10)) {
+			if (kstrtoint(opt + 10, 0, &eee_timer))
+				goto err;
+		} else if (!strncmp(opt, "chain_mode:", 11)) {
+			if (kstrtoint(opt + 11, 0, &chain_mode))
+				goto err;
+		}
+	}
+	return 0;
+
+err:
+	pr_err("%s: ERROR broken module parameter conversion", __func__);
+	return -EINVAL;
+}
+
+__setup("stmmaceth=", stmmac_cmdline_opt);
+#endif /* MODULE */
+
+static int __init stmmac_init(void)
+{
+#ifdef CONFIG_DEBUG_FS
+	/* Create debugfs main directory if it doesn't exist yet */
+	if (!stmmac_fs_dir)
+		stmmac_fs_dir = debugfs_create_dir(STMMAC_RESOURCE_NAME, NULL);
+	register_netdevice_notifier(&stmmac_notifier);
+#endif
+
+	return 0;
+}
+
+static void __exit stmmac_exit(void)
+{
+#ifdef CONFIG_DEBUG_FS
+	unregister_netdevice_notifier(&stmmac_notifier);
+	debugfs_remove_recursive(stmmac_fs_dir);
+#endif
+}
+
+module_init(stmmac_init)
+module_exit(stmmac_exit)
+
+MODULE_DESCRIPTION("STMMAC 10/100/1000 Ethernet device driver");
+MODULE_AUTHOR("Giuseppe Cavallaro <peppe.cavallaro@st.com>");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_mdio.c b/net/rtnet/drivers/orange-pi-one/stmmac_mdio.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_mdio.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_mdio.c	2021-07-14 15:39:13.302125025 +0300
@@ -0,0 +1,498 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  STMMAC Ethernet Driver -- MDIO bus implementation
+  Provides Bus interface for MII registers
+
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Carl Shaw <carl.shaw@st.com>
+  Maintainer: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/gpio/consumer.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/mii.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/property.h>
+#include <linux/slab.h>
+
+#include "dwxgmac2.h"
+#include "stmmac.h"
+
+#define MII_BUSY 0x00000001
+#define MII_WRITE 0x00000002
+#define MII_DATA_MASK GENMASK(15, 0)
+
+/* GMAC4 defines */
+#define MII_GMAC4_GOC_SHIFT		2
+#define MII_GMAC4_REG_ADDR_SHIFT	16
+#define MII_GMAC4_WRITE			(1 << MII_GMAC4_GOC_SHIFT)
+#define MII_GMAC4_READ			(3 << MII_GMAC4_GOC_SHIFT)
+#define MII_GMAC4_C45E			BIT(1)
+
+/* XGMAC defines */
+#define MII_XGMAC_SADDR			BIT(18)
+#define MII_XGMAC_CMD_SHIFT		16
+#define MII_XGMAC_WRITE			(1 << MII_XGMAC_CMD_SHIFT)
+#define MII_XGMAC_READ			(3 << MII_XGMAC_CMD_SHIFT)
+#define MII_XGMAC_BUSY			BIT(22)
+#define MII_XGMAC_MAX_C22ADDR		3
+#define MII_XGMAC_C22P_MASK		GENMASK(MII_XGMAC_MAX_C22ADDR, 0)
+#define MII_XGMAC_PA_SHIFT		16
+#define MII_XGMAC_DA_SHIFT		21
+
+static int stmmac_xgmac2_c45_format(struct stmmac_priv *priv, int phyaddr,
+				    int phyreg, u32 *hw_addr)
+{
+	u32 tmp;
+
+	/* Set port as Clause 45 */
+	tmp = readl(priv->ioaddr + XGMAC_MDIO_C22P);
+	tmp &= ~BIT(phyaddr);
+	writel(tmp, priv->ioaddr + XGMAC_MDIO_C22P);
+
+	*hw_addr = (phyaddr << MII_XGMAC_PA_SHIFT) | (phyreg & 0xffff);
+	*hw_addr |= (phyreg >> MII_DEVADDR_C45_SHIFT) << MII_XGMAC_DA_SHIFT;
+	return 0;
+}
+
+static int stmmac_xgmac2_c22_format(struct stmmac_priv *priv, int phyaddr,
+				    int phyreg, u32 *hw_addr)
+{
+	u32 tmp;
+
+	/* HW does not support C22 addr >= 4 */
+	if (phyaddr > MII_XGMAC_MAX_C22ADDR)
+		return -ENODEV;
+
+	/* Set port as Clause 22 */
+	tmp = readl(priv->ioaddr + XGMAC_MDIO_C22P);
+	tmp &= ~MII_XGMAC_C22P_MASK;
+	tmp |= BIT(phyaddr);
+	writel(tmp, priv->ioaddr + XGMAC_MDIO_C22P);
+
+	*hw_addr = (phyaddr << MII_XGMAC_PA_SHIFT) | (phyreg & 0x1f);
+	return 0;
+}
+
+static int stmmac_xgmac2_mdio_read(struct mii_bus *bus, int phyaddr, int phyreg)
+{
+	struct net_device *ndev = bus->priv;
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+	unsigned int mii_address = priv->hw->mii.addr;
+	unsigned int mii_data = priv->hw->mii.data;
+	u32 tmp, addr, value = MII_XGMAC_BUSY;
+	int ret;
+
+	/* Wait until any existing MII operation is complete */
+	if (readl_poll_timeout(priv->ioaddr + mii_data, tmp,
+			       !(tmp & MII_XGMAC_BUSY), 100, 10000))
+		return -EBUSY;
+
+	if (phyreg & MII_ADDR_C45) {
+		phyreg &= ~MII_ADDR_C45;
+
+		ret = stmmac_xgmac2_c45_format(priv, phyaddr, phyreg, &addr);
+		if (ret)
+			return ret;
+	} else {
+		ret = stmmac_xgmac2_c22_format(priv, phyaddr, phyreg, &addr);
+		if (ret)
+			return ret;
+
+		value |= MII_XGMAC_SADDR;
+	}
+
+	value |= (priv->clk_csr << priv->hw->mii.clk_csr_shift)
+		& priv->hw->mii.clk_csr_mask;
+	value |= MII_XGMAC_READ;
+
+	/* Wait until any existing MII operation is complete */
+	if (readl_poll_timeout(priv->ioaddr + mii_data, tmp,
+			       !(tmp & MII_XGMAC_BUSY), 100, 10000))
+		return -EBUSY;
+
+	/* Set the MII address register to read */
+	writel(addr, priv->ioaddr + mii_address);
+	writel(value, priv->ioaddr + mii_data);
+
+	/* Wait until any existing MII operation is complete */
+	if (readl_poll_timeout(priv->ioaddr + mii_data, tmp,
+			       !(tmp & MII_XGMAC_BUSY), 100, 10000))
+		return -EBUSY;
+
+	/* Read the data from the MII data register */
+	return readl(priv->ioaddr + mii_data) & GENMASK(15, 0);
+}
+
+static int stmmac_xgmac2_mdio_write(struct mii_bus *bus, int phyaddr,
+				    int phyreg, u16 phydata)
+{
+	struct net_device *ndev = bus->priv;
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+	unsigned int mii_address = priv->hw->mii.addr;
+	unsigned int mii_data = priv->hw->mii.data;
+	u32 addr, tmp, value = MII_XGMAC_BUSY;
+	int ret;
+
+	/* Wait until any existing MII operation is complete */
+	if (readl_poll_timeout(priv->ioaddr + mii_data, tmp,
+			       !(tmp & MII_XGMAC_BUSY), 100, 10000))
+		return -EBUSY;
+
+	if (phyreg & MII_ADDR_C45) {
+		phyreg &= ~MII_ADDR_C45;
+
+		ret = stmmac_xgmac2_c45_format(priv, phyaddr, phyreg, &addr);
+		if (ret)
+			return ret;
+	} else {
+		ret = stmmac_xgmac2_c22_format(priv, phyaddr, phyreg, &addr);
+		if (ret)
+			return ret;
+
+		value |= MII_XGMAC_SADDR;
+	}
+
+	value |= (priv->clk_csr << priv->hw->mii.clk_csr_shift)
+		& priv->hw->mii.clk_csr_mask;
+	value |= phydata;
+	value |= MII_XGMAC_WRITE;
+
+	/* Wait until any existing MII operation is complete */
+	if (readl_poll_timeout(priv->ioaddr + mii_data, tmp,
+			       !(tmp & MII_XGMAC_BUSY), 100, 10000))
+		return -EBUSY;
+
+	/* Set the MII address register to write */
+	writel(addr, priv->ioaddr + mii_address);
+	writel(value, priv->ioaddr + mii_data);
+
+	/* Wait until any existing MII operation is complete */
+	return readl_poll_timeout(priv->ioaddr + mii_data, tmp,
+				  !(tmp & MII_XGMAC_BUSY), 100, 10000);
+}
+
+/**
+ * stmmac_mdio_read
+ * @bus: points to the mii_bus structure
+ * @phyaddr: MII addr
+ * @phyreg: MII reg
+ * Description: it reads data from the MII register from within the phy device.
+ * For the 7111 GMAC, we must set the bit 0 in the MII address register while
+ * accessing the PHY registers.
+ * Fortunately, it seems this has no drawback for the 7109 MAC.
+ */
+static int stmmac_mdio_read(struct mii_bus *bus, int phyaddr, int phyreg)
+{
+	struct net_device *ndev = bus->priv;
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+	unsigned int mii_address = priv->hw->mii.addr;
+	unsigned int mii_data = priv->hw->mii.data;
+	u32 value = MII_BUSY;
+	int data = 0;
+	u32 v;
+
+	value |= (phyaddr << priv->hw->mii.addr_shift)
+		& priv->hw->mii.addr_mask;
+	value |= (phyreg << priv->hw->mii.reg_shift) & priv->hw->mii.reg_mask;
+	value |= (priv->clk_csr << priv->hw->mii.clk_csr_shift)
+		& priv->hw->mii.clk_csr_mask;
+	if (priv->plat->has_gmac4) {
+		value |= MII_GMAC4_READ;
+		if (phyreg & MII_ADDR_C45) {
+			value |= MII_GMAC4_C45E;
+			value &= ~priv->hw->mii.reg_mask;
+			value |= ((phyreg >> MII_DEVADDR_C45_SHIFT) <<
+			       priv->hw->mii.reg_shift) &
+			       priv->hw->mii.reg_mask;
+
+			data |= (phyreg & MII_REGADDR_C45_MASK) <<
+				MII_GMAC4_REG_ADDR_SHIFT;
+		}
+	}
+
+	if (readl_poll_timeout(priv->ioaddr + mii_address, v, !(v & MII_BUSY),
+			       100, 10000))
+		return -EBUSY;
+
+	writel(data, priv->ioaddr + mii_data);
+	writel(value, priv->ioaddr + mii_address);
+
+	if (readl_poll_timeout(priv->ioaddr + mii_address, v, !(v & MII_BUSY),
+			       100, 10000))
+		return -EBUSY;
+
+	/* Read the data from the MII data register */
+	data = (int)readl(priv->ioaddr + mii_data) & MII_DATA_MASK;
+
+	return data;
+}
+
+/**
+ * stmmac_mdio_write
+ * @bus: points to the mii_bus structure
+ * @phyaddr: MII addr
+ * @phyreg: MII reg
+ * @phydata: phy data
+ * Description: it writes the data into the MII register from within the device.
+ */
+static int stmmac_mdio_write(struct mii_bus *bus, int phyaddr, int phyreg,
+			     u16 phydata)
+{
+	struct net_device *ndev = bus->priv;
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+	unsigned int mii_address = priv->hw->mii.addr;
+	unsigned int mii_data = priv->hw->mii.data;
+	u32 value = MII_BUSY;
+	int data = phydata;
+	u32 v;
+
+	value |= (phyaddr << priv->hw->mii.addr_shift)
+		& priv->hw->mii.addr_mask;
+	value |= (phyreg << priv->hw->mii.reg_shift) & priv->hw->mii.reg_mask;
+
+	value |= (priv->clk_csr << priv->hw->mii.clk_csr_shift)
+		& priv->hw->mii.clk_csr_mask;
+	if (priv->plat->has_gmac4) {
+		value |= MII_GMAC4_WRITE;
+		if (phyreg & MII_ADDR_C45) {
+			value |= MII_GMAC4_C45E;
+			value &= ~priv->hw->mii.reg_mask;
+			value |= ((phyreg >> MII_DEVADDR_C45_SHIFT) <<
+			       priv->hw->mii.reg_shift) &
+			       priv->hw->mii.reg_mask;
+
+			data |= (phyreg & MII_REGADDR_C45_MASK) <<
+				MII_GMAC4_REG_ADDR_SHIFT;
+		}
+	} else {
+		value |= MII_WRITE;
+	}
+
+	/* Wait until any existing MII operation is complete */
+	if (readl_poll_timeout(priv->ioaddr + mii_address, v, !(v & MII_BUSY),
+			       100, 10000))
+		return -EBUSY;
+
+	/* Set the MII address register to write */
+	writel(data, priv->ioaddr + mii_data);
+	writel(value, priv->ioaddr + mii_address);
+
+	/* Wait until any existing MII operation is complete */
+	return readl_poll_timeout(priv->ioaddr + mii_address, v, !(v & MII_BUSY),
+				  100, 10000);
+}
+
+/**
+ * stmmac_mdio_reset
+ * @bus: points to the mii_bus structure
+ * Description: reset the MII bus
+ */
+int stmmac_mdio_reset(struct mii_bus *bus)
+{
+#if IS_ENABLED(CONFIG_RTNET_STMMAC_PLATFORM)
+	struct net_device *ndev = bus->priv;
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+	unsigned int mii_address = priv->hw->mii.addr;
+
+#ifdef CONFIG_OF
+	if (priv->device->of_node) {
+		struct gpio_desc *reset_gpio;
+		u32 delays[3] = { 0, 0, 0 };
+
+		reset_gpio = devm_gpiod_get_optional(priv->device,
+						     "snps,reset",
+						     GPIOD_OUT_LOW);
+		if (IS_ERR(reset_gpio))
+			return PTR_ERR(reset_gpio);
+
+		device_property_read_u32_array(priv->device,
+					       "snps,reset-delays-us",
+					       delays, ARRAY_SIZE(delays));
+
+		if (delays[0])
+			msleep(DIV_ROUND_UP(delays[0], 1000));
+
+		gpiod_set_value_cansleep(reset_gpio, 1);
+		if (delays[1])
+			msleep(DIV_ROUND_UP(delays[1], 1000));
+
+		gpiod_set_value_cansleep(reset_gpio, 0);
+		if (delays[2])
+			msleep(DIV_ROUND_UP(delays[2], 1000));
+	}
+#endif
+
+	/* This is a workaround for problems with the STE101P PHY.
+	 * It doesn't complete its reset until at least one clock cycle
+	 * on MDC, so perform a dummy mdio read. To be updated for GMAC4
+	 * if needed.
+	 */
+	if (!priv->plat->has_gmac4)
+		writel(0, priv->ioaddr + mii_address);
+#endif
+	return 0;
+}
+
+/**
+ * stmmac_mdio_register
+ * @ndev: net device structure
+ * Description: it registers the MII bus
+ */
+int stmmac_mdio_register(struct net_device *ndev)
+{
+	int err = 0;
+	struct mii_bus *new_bus;
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+	struct stmmac_mdio_bus_data *mdio_bus_data = priv->plat->mdio_bus_data;
+	struct device_node *mdio_node = priv->plat->mdio_node;
+	struct device *dev = ndev->dev.parent;
+	int addr, found, max_addr;
+
+	if (!mdio_bus_data)
+		return 0;
+
+	new_bus = mdiobus_alloc();
+	if (!new_bus)
+		return -ENOMEM;
+
+	if (mdio_bus_data->irqs)
+		memcpy(new_bus->irq, mdio_bus_data->irqs, sizeof(new_bus->irq));
+
+	new_bus->name = "stmmac";
+
+	if (priv->plat->has_xgmac) {
+		new_bus->read = &stmmac_xgmac2_mdio_read;
+		new_bus->write = &stmmac_xgmac2_mdio_write;
+
+		/* Right now only C22 phys are supported */
+		max_addr = MII_XGMAC_MAX_C22ADDR + 1;
+
+		/* Check if DT specified an unsupported phy addr */
+		if (priv->plat->phy_addr > MII_XGMAC_MAX_C22ADDR)
+			dev_err(dev, "Unsupported phy_addr (max=%d)\n",
+					MII_XGMAC_MAX_C22ADDR);
+	} else {
+		new_bus->read = &stmmac_mdio_read;
+		new_bus->write = &stmmac_mdio_write;
+		max_addr = PHY_MAX_ADDR;
+	}
+
+	if (mdio_bus_data->has_xpcs) {
+		priv->hw->xpcs = mdio_xpcs_get_ops();
+		if (!priv->hw->xpcs) {
+			err = -ENODEV;
+			goto bus_register_fail;
+		}
+	}
+
+	if (mdio_bus_data->needs_reset)
+		new_bus->reset = &stmmac_mdio_reset;
+
+	snprintf(new_bus->id, MII_BUS_ID_SIZE, "%s-%x",
+		 new_bus->name, priv->plat->bus_id);
+	new_bus->priv = ndev;
+	new_bus->phy_mask = mdio_bus_data->phy_mask;
+	new_bus->parent = priv->device;
+
+	err = of_mdiobus_register(new_bus, mdio_node);
+	if (err != 0) {
+		dev_err(dev, "Cannot register the MDIO bus\n");
+		goto bus_register_fail;
+	}
+
+	/* Looks like we need a dummy read for XGMAC only and C45 PHYs */
+	if (priv->plat->has_xgmac)
+		stmmac_xgmac2_mdio_read(new_bus, 0, MII_ADDR_C45);
+
+	if (priv->plat->phy_node || mdio_node)
+		goto bus_register_done;
+
+	found = 0;
+	for (addr = 0; addr < max_addr; addr++) {
+		struct phy_device *phydev = mdiobus_get_phy(new_bus, addr);
+
+		if (!phydev)
+			continue;
+
+		/*
+		 * If an IRQ was provided to be assigned after
+		 * the bus probe, do it here.
+		 */
+		if (!mdio_bus_data->irqs &&
+		    (mdio_bus_data->probed_phy_irq > 0)) {
+			new_bus->irq[addr] = mdio_bus_data->probed_phy_irq;
+			phydev->irq = mdio_bus_data->probed_phy_irq;
+		}
+
+		/*
+		 * If we're going to bind the MAC to this PHY bus,
+		 * and no PHY number was provided to the MAC,
+		 * use the one probed here.
+		 */
+		if (priv->plat->phy_addr == -1)
+			priv->plat->phy_addr = addr;
+
+		phy_attached_info(phydev);
+		found = 1;
+	}
+
+	/* Try to probe the XPCS by scanning all addresses. */
+	if (priv->hw->xpcs) {
+		struct mdio_xpcs_args *xpcs = &priv->hw->xpcs_args;
+		int ret, mode = priv->plat->phy_interface;
+		max_addr = PHY_MAX_ADDR;
+
+		xpcs->bus = new_bus;
+
+		for (addr = 0; addr < max_addr; addr++) {
+			xpcs->addr = addr;
+
+			ret = stmmac_xpcs_probe(priv, xpcs, mode);
+			if (!ret) {
+				found = 1;
+				break;
+			}
+		}
+	}
+
+	if (!found && !mdio_node) {
+		dev_warn(dev, "No PHY found\n");
+		mdiobus_unregister(new_bus);
+		mdiobus_free(new_bus);
+		return -ENODEV;
+	}
+
+bus_register_done:
+	priv->mii = new_bus;
+
+	return 0;
+
+bus_register_fail:
+	mdiobus_free(new_bus);
+	return err;
+}
+
+/**
+ * stmmac_mdio_unregister
+ * @ndev: net device structure
+ * Description: it unregisters the MII bus
+ */
+int stmmac_mdio_unregister(struct net_device *ndev)
+{
+	struct stmmac_priv *priv = rtnetdev_priv(((struct dummy_rtnetdev_priv *)netdev_priv(ndev))->rtdev);
+
+	if (!priv->mii)
+		return 0;
+
+	mdiobus_unregister(priv->mii);
+	priv->mii->priv = NULL;
+	mdiobus_free(priv->mii);
+	priv->mii = NULL;
+
+	return 0;
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_pci.c b/net/rtnet/drivers/orange-pi-one/stmmac_pci.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_pci.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_pci.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,301 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This contains the functions to handle the pci driver.
+
+  Copyright (C) 2011-2012  Vayavya Labs Pvt Ltd
+
+
+  Author: Rayagond Kokatanur <rayagond@vayavyalabs.com>
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/clk-provider.h>
+#include <linux/pci.h>
+#include <linux/dmi.h>
+
+#include "stmmac.h"
+
+struct stmmac_pci_info {
+	int (*setup)(struct pci_dev *pdev, struct plat_stmmacenet_data *plat);
+};
+
+static void common_default_data(struct plat_stmmacenet_data *plat)
+{
+	plat->clk_csr = 2;	/* clk_csr_i = 20-35MHz & MDC = clk_csr_i/16 */
+	plat->has_gmac = 1;
+	plat->force_sf_dma_mode = 1;
+
+	plat->mdio_bus_data->needs_reset = true;
+
+	/* Set default value for multicast hash bins */
+	plat->multicast_filter_bins = HASH_TABLE_SIZE;
+
+	/* Set default value for unicast filter entries */
+	plat->unicast_filter_entries = 1;
+
+	/* Set the maxmtu to a default of JUMBO_LEN */
+	plat->maxmtu = JUMBO_LEN;
+
+	/* Set default number of RX and TX queues to use */
+	plat->tx_queues_to_use = 1;
+	plat->rx_queues_to_use = 1;
+
+	/* Disable Priority config by default */
+	plat->tx_queues_cfg[0].use_prio = false;
+	plat->rx_queues_cfg[0].use_prio = false;
+
+	/* Disable RX queues routing by default */
+	plat->rx_queues_cfg[0].pkt_route = 0x0;
+}
+
+static int stmmac_default_data(struct pci_dev *pdev,
+			       struct plat_stmmacenet_data *plat)
+{
+	/* Set common default data first */
+	common_default_data(plat);
+
+	plat->bus_id = 1;
+	plat->phy_addr = 0;
+	plat->phy_interface = PHY_INTERFACE_MODE_GMII;
+
+	plat->dma_cfg->pbl = 32;
+	plat->dma_cfg->pblx8 = true;
+	/* TODO: AXI */
+
+	return 0;
+}
+
+static const struct stmmac_pci_info stmmac_pci_info = {
+	.setup = stmmac_default_data,
+};
+
+static int snps_gmac5_default_data(struct pci_dev *pdev,
+				   struct plat_stmmacenet_data *plat)
+{
+	int i;
+
+	plat->clk_csr = 5;
+	plat->has_gmac4 = 1;
+	plat->force_sf_dma_mode = 1;
+	plat->tso_en = 1;
+	plat->pmt = 1;
+
+	/* Set default value for multicast hash bins */
+	plat->multicast_filter_bins = HASH_TABLE_SIZE;
+
+	/* Set default value for unicast filter entries */
+	plat->unicast_filter_entries = 1;
+
+	/* Set the maxmtu to a default of JUMBO_LEN */
+	plat->maxmtu = JUMBO_LEN;
+
+	/* Set default number of RX and TX queues to use */
+	plat->tx_queues_to_use = 4;
+	plat->rx_queues_to_use = 4;
+
+	plat->tx_sched_algorithm = MTL_TX_ALGORITHM_WRR;
+	for (i = 0; i < plat->tx_queues_to_use; i++) {
+		plat->tx_queues_cfg[i].use_prio = false;
+		plat->tx_queues_cfg[i].mode_to_use = MTL_QUEUE_DCB;
+		plat->tx_queues_cfg[i].weight = 25;
+		if (i > 0)
+			plat->tx_queues_cfg[i].tbs_en = 1;
+	}
+
+	plat->rx_sched_algorithm = MTL_RX_ALGORITHM_SP;
+	for (i = 0; i < plat->rx_queues_to_use; i++) {
+		plat->rx_queues_cfg[i].use_prio = false;
+		plat->rx_queues_cfg[i].mode_to_use = MTL_QUEUE_DCB;
+		plat->rx_queues_cfg[i].pkt_route = 0x0;
+		plat->rx_queues_cfg[i].chan = i;
+	}
+
+	plat->bus_id = 1;
+	plat->phy_addr = -1;
+	plat->phy_interface = PHY_INTERFACE_MODE_GMII;
+
+	plat->dma_cfg->pbl = 32;
+	plat->dma_cfg->pblx8 = true;
+
+	/* Axi Configuration */
+	plat->axi = devm_kzalloc(&pdev->dev, sizeof(*plat->axi), GFP_KERNEL);
+	if (!plat->axi)
+		return -ENOMEM;
+
+	plat->axi->axi_wr_osr_lmt = 31;
+	plat->axi->axi_rd_osr_lmt = 31;
+
+	plat->axi->axi_fb = false;
+	plat->axi->axi_blen[0] = 4;
+	plat->axi->axi_blen[1] = 8;
+	plat->axi->axi_blen[2] = 16;
+	plat->axi->axi_blen[3] = 32;
+
+	return 0;
+}
+
+static const struct stmmac_pci_info snps_gmac5_pci_info = {
+	.setup = snps_gmac5_default_data,
+};
+
+/**
+ * stmmac_pci_probe
+ *
+ * @pdev: pci device pointer
+ * @id: pointer to table of device id/id's.
+ *
+ * Description: This probing function gets called for all PCI devices which
+ * match the ID table and are not "owned" by other driver yet. This function
+ * gets passed a "struct pci_dev *" for each device whose entry in the ID table
+ * matches the device. The probe functions returns zero when the driver choose
+ * to take "ownership" of the device or an error code(-ve no) otherwise.
+ */
+static int stmmac_pci_probe(struct pci_dev *pdev,
+			    const struct pci_device_id *id)
+{
+	struct stmmac_pci_info *info = (struct stmmac_pci_info *)id->driver_data;
+	struct plat_stmmacenet_data *plat;
+	struct stmmac_resources res;
+	int i;
+	int ret;
+
+	plat = devm_kzalloc(&pdev->dev, sizeof(*plat), GFP_KERNEL);
+	if (!plat)
+		return -ENOMEM;
+
+	plat->mdio_bus_data = devm_kzalloc(&pdev->dev,
+					   sizeof(*plat->mdio_bus_data),
+					   GFP_KERNEL);
+	if (!plat->mdio_bus_data)
+		return -ENOMEM;
+
+	plat->dma_cfg = devm_kzalloc(&pdev->dev, sizeof(*plat->dma_cfg),
+				     GFP_KERNEL);
+	if (!plat->dma_cfg)
+		return -ENOMEM;
+
+	/* Enable pci device */
+	ret = pci_enable_device(pdev);
+	if (ret) {
+		dev_err(&pdev->dev, "%s: ERROR: failed to enable device\n",
+			__func__);
+		return ret;
+	}
+
+	/* Get the base address of device */
+	for (i = 0; i < PCI_STD_NUM_BARS; i++) {
+		if (pci_resource_len(pdev, i) == 0)
+			continue;
+		ret = pcim_iomap_regions(pdev, BIT(i), pci_name(pdev));
+		if (ret)
+			return ret;
+		break;
+	}
+
+	pci_set_master(pdev);
+
+	ret = info->setup(pdev, plat);
+	if (ret)
+		return ret;
+
+	pci_enable_msi(pdev);
+
+	memset(&res, 0, sizeof(res));
+	res.addr = pcim_iomap_table(pdev)[i];
+	res.wol_irq = pdev->irq;
+	res.irq = pdev->irq;
+
+	return stmmac_dvr_probe(&pdev->dev, plat, &res);
+}
+
+/**
+ * stmmac_pci_remove
+ *
+ * @pdev: platform device pointer
+ * Description: this function calls the main to free the net resources
+ * and releases the PCI resources.
+ */
+static void stmmac_pci_remove(struct pci_dev *pdev)
+{
+	int i;
+
+	stmmac_dvr_remove(&pdev->dev);
+
+	for (i = 0; i < PCI_STD_NUM_BARS; i++) {
+		if (pci_resource_len(pdev, i) == 0)
+			continue;
+		pcim_iounmap_regions(pdev, BIT(i));
+		break;
+	}
+
+	pci_disable_device(pdev);
+}
+
+static int __maybe_unused stmmac_pci_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int ret;
+
+	ret = stmmac_suspend(dev);
+	if (ret)
+		return ret;
+
+	ret = pci_save_state(pdev);
+	if (ret)
+		return ret;
+
+	pci_disable_device(pdev);
+	pci_wake_from_d3(pdev, true);
+	return 0;
+}
+
+static int __maybe_unused stmmac_pci_resume(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int ret;
+
+	pci_restore_state(pdev);
+	pci_set_power_state(pdev, PCI_D0);
+
+	ret = pci_enable_device(pdev);
+	if (ret)
+		return ret;
+
+	pci_set_master(pdev);
+
+	return stmmac_resume(dev);
+}
+
+static SIMPLE_DEV_PM_OPS(stmmac_pm_ops, stmmac_pci_suspend, stmmac_pci_resume);
+
+/* synthetic ID, no official vendor */
+#define PCI_VENDOR_ID_STMMAC		0x0700
+
+#define PCI_DEVICE_ID_STMMAC_STMMAC		0x1108
+#define PCI_DEVICE_ID_SYNOPSYS_GMAC5_ID		0x7102
+
+static const struct pci_device_id stmmac_id_table[] = {
+	{ PCI_DEVICE_DATA(STMMAC, STMMAC, &stmmac_pci_info) },
+	{ PCI_DEVICE_DATA(STMICRO, MAC, &stmmac_pci_info) },
+	{ PCI_DEVICE_DATA(SYNOPSYS, GMAC5_ID, &snps_gmac5_pci_info) },
+	{}
+};
+
+MODULE_DEVICE_TABLE(pci, stmmac_id_table);
+
+static struct pci_driver stmmac_pci_driver = {
+	.name = STMMAC_RESOURCE_NAME,
+	.id_table = stmmac_id_table,
+	.probe = stmmac_pci_probe,
+	.remove = stmmac_pci_remove,
+	.driver         = {
+		.pm     = &stmmac_pm_ops,
+	},
+};
+
+module_pci_driver(stmmac_pci_driver);
+
+MODULE_DESCRIPTION("STMMAC 10/100/1000 Ethernet PCI driver");
+MODULE_AUTHOR("Rayagond Kokatanur <rayagond.kokatanur@vayavyalabs.com>");
+MODULE_AUTHOR("Giuseppe Cavallaro <peppe.cavallaro@st.com>");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_pcs.h b/net/rtnet/drivers/orange-pi-one/stmmac_pcs.h
--- a/net/rtnet/drivers/orange-pi-one/stmmac_pcs.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_pcs.h	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,155 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * stmmac_pcs.h: Physical Coding Sublayer Header File
+ *
+ * Copyright (C) 2016 STMicroelectronics (R&D) Limited
+ * Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+ */
+
+#ifndef __STMMAC_PCS_H__
+#define __STMMAC_PCS_H__
+
+#include <linux/slab.h>
+#include <linux/io.h>
+#include "common.h"
+
+/* PCS registers (AN/TBI/SGMII/RGMII) offsets */
+#define GMAC_AN_CTRL(x)		(x)		/* AN control */
+#define GMAC_AN_STATUS(x)	(x + 0x4)	/* AN status */
+#define GMAC_ANE_ADV(x)		(x + 0x8)	/* ANE Advertisement */
+#define GMAC_ANE_LPA(x)		(x + 0xc)	/* ANE link partener ability */
+#define GMAC_ANE_EXP(x)		(x + 0x10)	/* ANE expansion */
+#define GMAC_TBI(x)		(x + 0x14)	/* TBI extend status */
+
+/* AN Configuration defines */
+#define GMAC_AN_CTRL_RAN	BIT(9)	/* Restart Auto-Negotiation */
+#define GMAC_AN_CTRL_ANE	BIT(12)	/* Auto-Negotiation Enable */
+#define GMAC_AN_CTRL_ELE	BIT(14)	/* External Loopback Enable */
+#define GMAC_AN_CTRL_ECD	BIT(16)	/* Enable Comma Detect */
+#define GMAC_AN_CTRL_LR		BIT(17)	/* Lock to Reference */
+#define GMAC_AN_CTRL_SGMRAL	BIT(18)	/* SGMII RAL Control */
+
+/* AN Status defines */
+#define GMAC_AN_STATUS_LS	BIT(2)	/* Link Status 0:down 1:up */
+#define GMAC_AN_STATUS_ANA	BIT(3)	/* Auto-Negotiation Ability */
+#define GMAC_AN_STATUS_ANC	BIT(5)	/* Auto-Negotiation Complete */
+#define GMAC_AN_STATUS_ES	BIT(8)	/* Extended Status */
+
+/* ADV and LPA defines */
+#define GMAC_ANE_FD		BIT(5)
+#define GMAC_ANE_HD		BIT(6)
+#define GMAC_ANE_PSE		GENMASK(8, 7)
+#define GMAC_ANE_PSE_SHIFT	7
+#define GMAC_ANE_RFE		GENMASK(13, 12)
+#define GMAC_ANE_RFE_SHIFT	12
+#define GMAC_ANE_ACK		BIT(14)
+
+/**
+ * dwmac_pcs_isr - TBI, RTBI, or SGMII PHY ISR
+ * @ioaddr: IO registers pointer
+ * @reg: Base address of the AN Control Register.
+ * @intr_status: GMAC core interrupt status
+ * @x: pointer to log these events as stats
+ * Description: it is the ISR for PCS events: Auto-Negotiation Completed and
+ * Link status.
+ */
+static inline void dwmac_pcs_isr(void __iomem *ioaddr, u32 reg,
+				 unsigned int intr_status,
+				 struct stmmac_extra_stats *x)
+{
+	u32 val = readl(ioaddr + GMAC_AN_STATUS(reg));
+
+	if (intr_status & PCS_ANE_IRQ) {
+		x->irq_pcs_ane_n++;
+		if (val & GMAC_AN_STATUS_ANC)
+			pr_info("stmmac_pcs: ANE process completed\n");
+	}
+
+	if (intr_status & PCS_LINK_IRQ) {
+		x->irq_pcs_link_n++;
+		if (val & GMAC_AN_STATUS_LS)
+			pr_info("stmmac_pcs: Link Up\n");
+		else
+			pr_info("stmmac_pcs: Link Down\n");
+	}
+}
+
+/**
+ * dwmac_rane - To restart ANE
+ * @ioaddr: IO registers pointer
+ * @reg: Base address of the AN Control Register.
+ * @restart: to restart ANE
+ * Description: this is to just restart the Auto-Negotiation.
+ */
+static inline void dwmac_rane(void __iomem *ioaddr, u32 reg, bool restart)
+{
+	u32 value = readl(ioaddr + GMAC_AN_CTRL(reg));
+
+	if (restart)
+		value |= GMAC_AN_CTRL_RAN;
+
+	writel(value, ioaddr + GMAC_AN_CTRL(reg));
+}
+
+/**
+ * dwmac_ctrl_ane - To program the AN Control Register.
+ * @ioaddr: IO registers pointer
+ * @reg: Base address of the AN Control Register.
+ * @ane: to enable the auto-negotiation
+ * @srgmi_ral: to manage MAC-2-MAC SGMII connections.
+ * @loopback: to cause the PHY to loopback tx data into rx path.
+ * Description: this is the main function to configure the AN control register
+ * and init the ANE, select loopback (usually for debugging purpose) and
+ * configure SGMII RAL.
+ */
+static inline void dwmac_ctrl_ane(void __iomem *ioaddr, u32 reg, bool ane,
+				  bool srgmi_ral, bool loopback)
+{
+	u32 value = readl(ioaddr + GMAC_AN_CTRL(reg));
+
+	/* Enable and restart the Auto-Negotiation */
+	if (ane)
+		value |= GMAC_AN_CTRL_ANE | GMAC_AN_CTRL_RAN;
+
+	/* In case of MAC-2-MAC connection, block is configured to operate
+	 * according to MAC conf register.
+	 */
+	if (srgmi_ral)
+		value |= GMAC_AN_CTRL_SGMRAL;
+
+	if (loopback)
+		value |= GMAC_AN_CTRL_ELE;
+
+	writel(value, ioaddr + GMAC_AN_CTRL(reg));
+}
+
+/**
+ * dwmac_get_adv_lp - Get ADV and LP cap
+ * @ioaddr: IO registers pointer
+ * @reg: Base address of the AN Control Register.
+ * @adv_lp: structure to store the adv,lp status
+ * Description: this is to expose the ANE advertisement and Link partner ability
+ * status to ethtool support.
+ */
+static inline void dwmac_get_adv_lp(void __iomem *ioaddr, u32 reg,
+				    struct rgmii_adv *adv_lp)
+{
+	u32 value = readl(ioaddr + GMAC_ANE_ADV(reg));
+
+	if (value & GMAC_ANE_FD)
+		adv_lp->duplex = DUPLEX_FULL;
+	if (value & GMAC_ANE_HD)
+		adv_lp->duplex |= DUPLEX_HALF;
+
+	adv_lp->pause = (value & GMAC_ANE_PSE) >> GMAC_ANE_PSE_SHIFT;
+
+	value = readl(ioaddr + GMAC_ANE_LPA(reg));
+
+	if (value & GMAC_ANE_FD)
+		adv_lp->lp_duplex = DUPLEX_FULL;
+	if (value & GMAC_ANE_HD)
+		adv_lp->lp_duplex = DUPLEX_HALF;
+
+	adv_lp->lp_pause = (value & GMAC_ANE_PSE) >> GMAC_ANE_PSE_SHIFT;
+}
+#endif /* __STMMAC_PCS_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_platform.c b/net/rtnet/drivers/orange-pi-one/stmmac_platform.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_platform.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_platform.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,760 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  This contains the functions to handle the platform driver.
+
+  Copyright (C) 2007-2011  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#include <linux/platform_device.h>
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/of_device.h>
+#include <linux/of_mdio.h>
+
+#include "stmmac.h"
+#include "stmmac_platform.h"
+
+#ifdef CONFIG_OF
+
+/**
+ * dwmac1000_validate_mcast_bins - validates the number of Multicast filter bins
+ * @dev: struct device of the platform device
+ * @mcast_bins: Multicast filtering bins
+ * Description:
+ * this function validates the number of Multicast filtering bins specified
+ * by the configuration through the device tree. The Synopsys GMAC supports
+ * 64 bins, 128 bins, or 256 bins. "bins" refer to the division of CRC
+ * number space. 64 bins correspond to 6 bits of the CRC, 128 corresponds
+ * to 7 bits, and 256 refers to 8 bits of the CRC. Any other setting is
+ * invalid and will cause the filtering algorithm to use Multicast
+ * promiscuous mode.
+ */
+static int dwmac1000_validate_mcast_bins(struct device *dev, int mcast_bins)
+{
+	int x = mcast_bins;
+
+	switch (x) {
+	case HASH_TABLE_SIZE:
+	case 128:
+	case 256:
+		break;
+	default:
+		x = 0;
+		dev_info(dev, "Hash table entries set to unexpected value %d\n",
+			 mcast_bins);
+		break;
+	}
+	return x;
+}
+
+/**
+ * dwmac1000_validate_ucast_entries - validate the Unicast address entries
+ * @dev: struct device of the platform device
+ * @ucast_entries: number of Unicast address entries
+ * Description:
+ * This function validates the number of Unicast address entries supported
+ * by a particular Synopsys 10/100/1000 controller. The Synopsys controller
+ * supports 1..32, 64, or 128 Unicast filter entries for it's Unicast filter
+ * logic. This function validates a valid, supported configuration is
+ * selected, and defaults to 1 Unicast address if an unsupported
+ * configuration is selected.
+ */
+static int dwmac1000_validate_ucast_entries(struct device *dev,
+					    int ucast_entries)
+{
+	int x = ucast_entries;
+
+	switch (x) {
+	case 1 ... 32:
+	case 64:
+	case 128:
+		break;
+	default:
+		x = 1;
+		dev_info(dev, "Unicast table entries set to unexpected value %d\n",
+			 ucast_entries);
+		break;
+	}
+	return x;
+}
+
+/**
+ * stmmac_axi_setup - parse DT parameters for programming the AXI register
+ * @pdev: platform device
+ * Description:
+ * if required, from device-tree the AXI internal register can be tuned
+ * by using platform parameters.
+ */
+static struct stmmac_axi *stmmac_axi_setup(struct platform_device *pdev)
+{
+	struct device_node *np;
+	struct stmmac_axi *axi;
+
+	np = of_parse_phandle(pdev->dev.of_node, "snps,axi-config", 0);
+	if (!np)
+		return NULL;
+
+	axi = devm_kzalloc(&pdev->dev, sizeof(*axi), GFP_KERNEL);
+	if (!axi) {
+		of_node_put(np);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	axi->axi_lpi_en = of_property_read_bool(np, "snps,lpi_en");
+	axi->axi_xit_frm = of_property_read_bool(np, "snps,xit_frm");
+	axi->axi_kbbe = of_property_read_bool(np, "snps,axi_kbbe");
+	axi->axi_fb = of_property_read_bool(np, "snps,axi_fb");
+	axi->axi_mb = of_property_read_bool(np, "snps,axi_mb");
+	axi->axi_rb =  of_property_read_bool(np, "snps,axi_rb");
+
+	if (of_property_read_u32(np, "snps,wr_osr_lmt", &axi->axi_wr_osr_lmt))
+		axi->axi_wr_osr_lmt = 1;
+	if (of_property_read_u32(np, "snps,rd_osr_lmt", &axi->axi_rd_osr_lmt))
+		axi->axi_rd_osr_lmt = 1;
+	of_property_read_u32_array(np, "snps,blen", axi->axi_blen, AXI_BLEN);
+	of_node_put(np);
+
+	return axi;
+}
+
+/**
+ * stmmac_mtl_setup - parse DT parameters for multiple queues configuration
+ * @pdev: platform device
+ */
+static int stmmac_mtl_setup(struct platform_device *pdev,
+			    struct plat_stmmacenet_data *plat)
+{
+	struct device_node *q_node;
+	struct device_node *rx_node;
+	struct device_node *tx_node;
+	u8 queue = 0;
+	int ret = 0;
+
+	/* For backwards-compatibility with device trees that don't have any
+	 * snps,mtl-rx-config or snps,mtl-tx-config properties, we fall back
+	 * to one RX and TX queues each.
+	 */
+	plat->rx_queues_to_use = 1;
+	plat->tx_queues_to_use = 1;
+
+	/* First Queue must always be in DCB mode. As MTL_QUEUE_DCB = 1 we need
+	 * to always set this, otherwise Queue will be classified as AVB
+	 * (because MTL_QUEUE_AVB = 0).
+	 */
+	plat->rx_queues_cfg[0].mode_to_use = MTL_QUEUE_DCB;
+	plat->tx_queues_cfg[0].mode_to_use = MTL_QUEUE_DCB;
+
+	rx_node = of_parse_phandle(pdev->dev.of_node, "snps,mtl-rx-config", 0);
+	if (!rx_node)
+		return ret;
+
+	tx_node = of_parse_phandle(pdev->dev.of_node, "snps,mtl-tx-config", 0);
+	if (!tx_node) {
+		of_node_put(rx_node);
+		return ret;
+	}
+
+	/* Processing RX queues common config */
+	if (of_property_read_u32(rx_node, "snps,rx-queues-to-use",
+				 &plat->rx_queues_to_use))
+		plat->rx_queues_to_use = 1;
+
+	if (of_property_read_bool(rx_node, "snps,rx-sched-sp"))
+		plat->rx_sched_algorithm = MTL_RX_ALGORITHM_SP;
+	else if (of_property_read_bool(rx_node, "snps,rx-sched-wsp"))
+		plat->rx_sched_algorithm = MTL_RX_ALGORITHM_WSP;
+	else
+		plat->rx_sched_algorithm = MTL_RX_ALGORITHM_SP;
+
+	/* Processing individual RX queue config */
+	for_each_child_of_node(rx_node, q_node) {
+		if (queue >= plat->rx_queues_to_use)
+			break;
+
+		if (of_property_read_bool(q_node, "snps,dcb-algorithm"))
+			plat->rx_queues_cfg[queue].mode_to_use = MTL_QUEUE_DCB;
+		else if (of_property_read_bool(q_node, "snps,avb-algorithm"))
+			plat->rx_queues_cfg[queue].mode_to_use = MTL_QUEUE_AVB;
+		else
+			plat->rx_queues_cfg[queue].mode_to_use = MTL_QUEUE_DCB;
+
+		if (of_property_read_u32(q_node, "snps,map-to-dma-channel",
+					 &plat->rx_queues_cfg[queue].chan))
+			plat->rx_queues_cfg[queue].chan = queue;
+		/* TODO: Dynamic mapping to be included in the future */
+
+		if (of_property_read_u32(q_node, "snps,priority",
+					&plat->rx_queues_cfg[queue].prio)) {
+			plat->rx_queues_cfg[queue].prio = 0;
+			plat->rx_queues_cfg[queue].use_prio = false;
+		} else {
+			plat->rx_queues_cfg[queue].use_prio = true;
+		}
+
+		/* RX queue specific packet type routing */
+		if (of_property_read_bool(q_node, "snps,route-avcp"))
+			plat->rx_queues_cfg[queue].pkt_route = PACKET_AVCPQ;
+		else if (of_property_read_bool(q_node, "snps,route-ptp"))
+			plat->rx_queues_cfg[queue].pkt_route = PACKET_PTPQ;
+		else if (of_property_read_bool(q_node, "snps,route-dcbcp"))
+			plat->rx_queues_cfg[queue].pkt_route = PACKET_DCBCPQ;
+		else if (of_property_read_bool(q_node, "snps,route-up"))
+			plat->rx_queues_cfg[queue].pkt_route = PACKET_UPQ;
+		else if (of_property_read_bool(q_node, "snps,route-multi-broad"))
+			plat->rx_queues_cfg[queue].pkt_route = PACKET_MCBCQ;
+		else
+			plat->rx_queues_cfg[queue].pkt_route = 0x0;
+
+		queue++;
+	}
+	if (queue != plat->rx_queues_to_use) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "Not all RX queues were configured\n");
+		goto out;
+	}
+
+	/* Processing TX queues common config */
+	if (of_property_read_u32(tx_node, "snps,tx-queues-to-use",
+				 &plat->tx_queues_to_use))
+		plat->tx_queues_to_use = 1;
+
+	if (of_property_read_bool(tx_node, "snps,tx-sched-wrr"))
+		plat->tx_sched_algorithm = MTL_TX_ALGORITHM_WRR;
+	else if (of_property_read_bool(tx_node, "snps,tx-sched-wfq"))
+		plat->tx_sched_algorithm = MTL_TX_ALGORITHM_WFQ;
+	else if (of_property_read_bool(tx_node, "snps,tx-sched-dwrr"))
+		plat->tx_sched_algorithm = MTL_TX_ALGORITHM_DWRR;
+	else if (of_property_read_bool(tx_node, "snps,tx-sched-sp"))
+		plat->tx_sched_algorithm = MTL_TX_ALGORITHM_SP;
+	else
+		plat->tx_sched_algorithm = MTL_TX_ALGORITHM_SP;
+
+	queue = 0;
+
+	/* Processing individual TX queue config */
+	for_each_child_of_node(tx_node, q_node) {
+		if (queue >= plat->tx_queues_to_use)
+			break;
+
+		if (of_property_read_u32(q_node, "snps,weight",
+					 &plat->tx_queues_cfg[queue].weight))
+			plat->tx_queues_cfg[queue].weight = 0x10 + queue;
+
+		if (of_property_read_bool(q_node, "snps,dcb-algorithm")) {
+			plat->tx_queues_cfg[queue].mode_to_use = MTL_QUEUE_DCB;
+		} else if (of_property_read_bool(q_node,
+						 "snps,avb-algorithm")) {
+			plat->tx_queues_cfg[queue].mode_to_use = MTL_QUEUE_AVB;
+
+			/* Credit Base Shaper parameters used by AVB */
+			if (of_property_read_u32(q_node, "snps,send_slope",
+				&plat->tx_queues_cfg[queue].send_slope))
+				plat->tx_queues_cfg[queue].send_slope = 0x0;
+			if (of_property_read_u32(q_node, "snps,idle_slope",
+				&plat->tx_queues_cfg[queue].idle_slope))
+				plat->tx_queues_cfg[queue].idle_slope = 0x0;
+			if (of_property_read_u32(q_node, "snps,high_credit",
+				&plat->tx_queues_cfg[queue].high_credit))
+				plat->tx_queues_cfg[queue].high_credit = 0x0;
+			if (of_property_read_u32(q_node, "snps,low_credit",
+				&plat->tx_queues_cfg[queue].low_credit))
+				plat->tx_queues_cfg[queue].low_credit = 0x0;
+		} else {
+			plat->tx_queues_cfg[queue].mode_to_use = MTL_QUEUE_DCB;
+		}
+
+		if (of_property_read_u32(q_node, "snps,priority",
+					&plat->tx_queues_cfg[queue].prio)) {
+			plat->tx_queues_cfg[queue].prio = 0;
+			plat->tx_queues_cfg[queue].use_prio = false;
+		} else {
+			plat->tx_queues_cfg[queue].use_prio = true;
+		}
+
+		queue++;
+	}
+	if (queue != plat->tx_queues_to_use) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "Not all TX queues were configured\n");
+		goto out;
+	}
+
+out:
+	of_node_put(rx_node);
+	of_node_put(tx_node);
+	of_node_put(q_node);
+
+	return ret;
+}
+
+/**
+ * stmmac_dt_phy - parse device-tree driver parameters to allocate PHY resources
+ * @plat: driver data platform structure
+ * @np: device tree node
+ * @dev: device pointer
+ * Description:
+ * The mdio bus will be allocated in case of a phy transceiver is on board;
+ * it will be NULL if the fixed-link is configured.
+ * If there is the "snps,dwmac-mdio" sub-node the mdio will be allocated
+ * in any case (for DSA, mdio must be registered even if fixed-link).
+ * The table below sums the supported configurations:
+ *	-------------------------------
+ *	snps,phy-addr	|     Y
+ *	-------------------------------
+ *	phy-handle	|     Y
+ *	-------------------------------
+ *	fixed-link	|     N
+ *	-------------------------------
+ *	snps,dwmac-mdio	|
+ *	  even if	|     Y
+ *	fixed-link	|
+ *	-------------------------------
+ *
+ * It returns 0 in case of success otherwise -ENODEV.
+ */
+static int stmmac_dt_phy(struct plat_stmmacenet_data *plat,
+			 struct device_node *np, struct device *dev)
+{
+	bool mdio = !of_phy_is_fixed_link(np);
+	static const struct of_device_id need_mdio_ids[] = {
+		{ .compatible = "snps,dwc-qos-ethernet-4.10" },
+		{},
+	};
+
+	if (of_match_node(need_mdio_ids, np)) {
+		plat->mdio_node = of_get_child_by_name(np, "mdio");
+	} else {
+		/**
+		 * If snps,dwmac-mdio is passed from DT, always register
+		 * the MDIO
+		 */
+		for_each_child_of_node(np, plat->mdio_node) {
+			if (of_device_is_compatible(plat->mdio_node,
+						    "snps,dwmac-mdio"))
+				break;
+		}
+	}
+
+	if (plat->mdio_node) {
+		dev_dbg(dev, "Found MDIO subnode\n");
+		mdio = true;
+	}
+
+	if (mdio) {
+		plat->mdio_bus_data =
+			devm_kzalloc(dev, sizeof(struct stmmac_mdio_bus_data),
+				     GFP_KERNEL);
+		if (!plat->mdio_bus_data)
+			return -ENOMEM;
+
+		plat->mdio_bus_data->needs_reset = true;
+	}
+
+	return 0;
+}
+
+/**
+ * stmmac_of_get_mac_mode - retrieves the interface of the MAC
+ * @np - device-tree node
+ * Description:
+ * Similar to `of_get_phy_mode()`, this function will retrieve (from
+ * the device-tree) the interface mode on the MAC side. This assumes
+ * that there is mode converter in-between the MAC & PHY
+ * (e.g. GMII-to-RGMII).
+ */
+static int stmmac_of_get_mac_mode(struct device_node *np)
+{
+	const char *pm;
+	int err, i;
+
+	err = of_property_read_string(np, "mac-mode", &pm);
+	if (err < 0)
+		return err;
+
+	for (i = 0; i < PHY_INTERFACE_MODE_MAX; i++) {
+		if (!strcasecmp(pm, phy_modes(i)))
+			return i;
+	}
+
+	return -ENODEV;
+}
+
+/**
+ * stmmac_probe_config_dt - parse device-tree driver parameters
+ * @pdev: platform_device structure
+ * @mac: MAC address to use
+ * Description:
+ * this function is to read the driver parameters from device-tree and
+ * set some private fields that will be used by the main at runtime.
+ */
+struct plat_stmmacenet_data *
+stmmac_probe_config_dt(struct platform_device *pdev, const char **mac)
+{
+	struct device_node *np = pdev->dev.of_node;
+	struct plat_stmmacenet_data *plat;
+	struct stmmac_dma_cfg *dma_cfg;
+	int rc;
+
+	plat = devm_kzalloc(&pdev->dev, sizeof(*plat), GFP_KERNEL);
+	if (!plat)
+		return ERR_PTR(-ENOMEM);
+
+	*mac = of_get_mac_address(np);
+	if (IS_ERR(*mac)) {
+		if (PTR_ERR(*mac) == -EPROBE_DEFER)
+			return ERR_CAST(*mac);
+
+		*mac = NULL;
+	}
+
+	plat->phy_interface = device_get_phy_mode(&pdev->dev);
+	if (plat->phy_interface < 0)
+		return ERR_PTR(plat->phy_interface);
+
+	plat->interface = stmmac_of_get_mac_mode(np);
+	if (plat->interface < 0)
+		plat->interface = plat->phy_interface;
+
+	/* Some wrapper drivers still rely on phy_node. Let's save it while
+	 * they are not converted to phylink. */
+	plat->phy_node = of_parse_phandle(np, "phy-handle", 0);
+
+	/* PHYLINK automatically parses the phy-handle property */
+	plat->phylink_node = np;
+
+	/* Get max speed of operation from device tree */
+	if (of_property_read_u32(np, "max-speed", &plat->max_speed))
+		plat->max_speed = -1;
+
+	plat->bus_id = of_alias_get_id(np, "ethernet");
+	if (plat->bus_id < 0)
+		plat->bus_id = 0;
+
+	/* Default to phy auto-detection */
+	plat->phy_addr = -1;
+
+	/* Default to get clk_csr from stmmac_clk_crs_set(),
+	 * or get clk_csr from device tree.
+	 */
+	plat->clk_csr = -1;
+	of_property_read_u32(np, "clk_csr", &plat->clk_csr);
+
+	/* "snps,phy-addr" is not a standard property. Mark it as deprecated
+	 * and warn of its use. Remove this when phy node support is added.
+	 */
+	if (of_property_read_u32(np, "snps,phy-addr", &plat->phy_addr) == 0)
+		dev_warn(&pdev->dev, "snps,phy-addr property is deprecated\n");
+
+	/* To Configure PHY by using all device-tree supported properties */
+	rc = stmmac_dt_phy(plat, np, &pdev->dev);
+	if (rc)
+		return ERR_PTR(rc);
+
+	of_property_read_u32(np, "tx-fifo-depth", &plat->tx_fifo_size);
+
+	of_property_read_u32(np, "rx-fifo-depth", &plat->rx_fifo_size);
+
+	plat->force_sf_dma_mode =
+		of_property_read_bool(np, "snps,force_sf_dma_mode");
+
+	plat->en_tx_lpi_clockgating =
+		of_property_read_bool(np, "snps,en-tx-lpi-clockgating");
+
+	/* Set the maxmtu to a default of JUMBO_LEN in case the
+	 * parameter is not present in the device tree.
+	 */
+	plat->maxmtu = JUMBO_LEN;
+
+	/* Set default value for multicast hash bins */
+	plat->multicast_filter_bins = HASH_TABLE_SIZE;
+
+	/* Set default value for unicast filter entries */
+	plat->unicast_filter_entries = 1;
+
+	/*
+	 * Currently only the properties needed on SPEAr600
+	 * are provided. All other properties should be added
+	 * once needed on other platforms.
+	 */
+	if (of_device_is_compatible(np, "st,spear600-gmac") ||
+		of_device_is_compatible(np, "snps,dwmac-3.50a") ||
+		of_device_is_compatible(np, "snps,dwmac-3.70a") ||
+		of_device_is_compatible(np, "snps,dwmac")) {
+		/* Note that the max-frame-size parameter as defined in the
+		 * ePAPR v1.1 spec is defined as max-frame-size, it's
+		 * actually used as the IEEE definition of MAC Client
+		 * data, or MTU. The ePAPR specification is confusing as
+		 * the definition is max-frame-size, but usage examples
+		 * are clearly MTUs
+		 */
+		of_property_read_u32(np, "max-frame-size", &plat->maxmtu);
+		of_property_read_u32(np, "snps,multicast-filter-bins",
+				     &plat->multicast_filter_bins);
+		of_property_read_u32(np, "snps,perfect-filter-entries",
+				     &plat->unicast_filter_entries);
+		plat->unicast_filter_entries = dwmac1000_validate_ucast_entries(
+				&pdev->dev, plat->unicast_filter_entries);
+		plat->multicast_filter_bins = dwmac1000_validate_mcast_bins(
+				&pdev->dev, plat->multicast_filter_bins);
+		plat->has_gmac = 1;
+		plat->pmt = 1;
+	}
+
+	if (of_device_is_compatible(np, "snps,dwmac-4.00") ||
+	    of_device_is_compatible(np, "snps,dwmac-4.10a") ||
+	    of_device_is_compatible(np, "snps,dwmac-4.20a") ||
+	    of_device_is_compatible(np, "snps,dwmac-5.10a")) {
+		plat->has_gmac4 = 1;
+		plat->has_gmac = 0;
+		plat->pmt = 1;
+		plat->tso_en = of_property_read_bool(np, "snps,tso");
+	}
+
+	if (of_device_is_compatible(np, "snps,dwmac-3.610") ||
+		of_device_is_compatible(np, "snps,dwmac-3.710")) {
+		plat->enh_desc = 1;
+		plat->bugged_jumbo = 1;
+		plat->force_sf_dma_mode = 1;
+	}
+
+	if (of_device_is_compatible(np, "snps,dwxgmac")) {
+		plat->has_xgmac = 1;
+		plat->pmt = 1;
+		plat->tso_en = of_property_read_bool(np, "snps,tso");
+	}
+
+	dma_cfg = devm_kzalloc(&pdev->dev, sizeof(*dma_cfg),
+			       GFP_KERNEL);
+	if (!dma_cfg) {
+		stmmac_remove_config_dt(pdev, plat);
+		return ERR_PTR(-ENOMEM);
+	}
+	plat->dma_cfg = dma_cfg;
+
+	of_property_read_u32(np, "snps,pbl", &dma_cfg->pbl);
+	if (!dma_cfg->pbl)
+		dma_cfg->pbl = DEFAULT_DMA_PBL;
+	of_property_read_u32(np, "snps,txpbl", &dma_cfg->txpbl);
+	of_property_read_u32(np, "snps,rxpbl", &dma_cfg->rxpbl);
+	dma_cfg->pblx8 = !of_property_read_bool(np, "snps,no-pbl-x8");
+
+	dma_cfg->aal = of_property_read_bool(np, "snps,aal");
+	dma_cfg->fixed_burst = of_property_read_bool(np, "snps,fixed-burst");
+	dma_cfg->mixed_burst = of_property_read_bool(np, "snps,mixed-burst");
+
+	plat->force_thresh_dma_mode = of_property_read_bool(np, "snps,force_thresh_dma_mode");
+	if (plat->force_thresh_dma_mode) {
+		plat->force_sf_dma_mode = 0;
+		dev_warn(&pdev->dev,
+			 "force_sf_dma_mode is ignored if force_thresh_dma_mode is set.\n");
+	}
+
+	of_property_read_u32(np, "snps,ps-speed", &plat->mac_port_sel_speed);
+
+	plat->axi = stmmac_axi_setup(pdev);
+
+	rc = stmmac_mtl_setup(pdev, plat);
+	if (rc) {
+		stmmac_remove_config_dt(pdev, plat);
+		return ERR_PTR(rc);
+	}
+
+	/* clock setup */
+	if (!of_device_is_compatible(np, "snps,dwc-qos-ethernet-4.10")) {
+		plat->stmmac_clk = devm_clk_get(&pdev->dev,
+						STMMAC_RESOURCE_NAME);
+		if (IS_ERR(plat->stmmac_clk)) {
+			dev_warn(&pdev->dev, "Cannot get CSR clock\n");
+			plat->stmmac_clk = NULL;
+		}
+		clk_prepare_enable(plat->stmmac_clk);
+	}
+
+	plat->pclk = devm_clk_get(&pdev->dev, "pclk");
+	if (IS_ERR(plat->pclk)) {
+		if (PTR_ERR(plat->pclk) == -EPROBE_DEFER)
+			goto error_pclk_get;
+
+		plat->pclk = NULL;
+	}
+	clk_prepare_enable(plat->pclk);
+
+	/* Fall-back to main clock in case of no PTP ref is passed */
+	plat->clk_ptp_ref = devm_clk_get(&pdev->dev, "ptp_ref");
+	if (IS_ERR(plat->clk_ptp_ref)) {
+		plat->clk_ptp_rate = clk_get_rate(plat->stmmac_clk);
+		plat->clk_ptp_ref = NULL;
+		dev_info(&pdev->dev, "PTP uses main clock\n");
+	} else {
+		plat->clk_ptp_rate = clk_get_rate(plat->clk_ptp_ref);
+		dev_dbg(&pdev->dev, "PTP rate %d\n", plat->clk_ptp_rate);
+	}
+
+	plat->stmmac_rst = devm_reset_control_get(&pdev->dev,
+						  STMMAC_RESOURCE_NAME);
+	if (IS_ERR(plat->stmmac_rst)) {
+		if (PTR_ERR(plat->stmmac_rst) == -EPROBE_DEFER)
+			goto error_hw_init;
+
+		dev_info(&pdev->dev, "no reset control found\n");
+		plat->stmmac_rst = NULL;
+	}
+
+	return plat;
+
+error_hw_init:
+	clk_disable_unprepare(plat->pclk);
+error_pclk_get:
+	clk_disable_unprepare(plat->stmmac_clk);
+
+	return ERR_PTR(-EPROBE_DEFER);
+}
+
+/**
+ * stmmac_remove_config_dt - undo the effects of stmmac_probe_config_dt()
+ * @pdev: platform_device structure
+ * @plat: driver data platform structure
+ *
+ * Release resources claimed by stmmac_probe_config_dt().
+ */
+void stmmac_remove_config_dt(struct platform_device *pdev,
+			     struct plat_stmmacenet_data *plat)
+{
+	of_node_put(plat->phy_node);
+	of_node_put(plat->mdio_node);
+}
+#else
+struct plat_stmmacenet_data *
+stmmac_probe_config_dt(struct platform_device *pdev, const char **mac)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+void stmmac_remove_config_dt(struct platform_device *pdev,
+			     struct plat_stmmacenet_data *plat)
+{
+}
+#endif /* CONFIG_OF */
+EXPORT_SYMBOL_GPL(stmmac_probe_config_dt);
+EXPORT_SYMBOL_GPL(stmmac_remove_config_dt);
+
+int stmmac_get_platform_resources(struct platform_device *pdev,
+				  struct stmmac_resources *stmmac_res)
+{
+	memset(stmmac_res, 0, sizeof(*stmmac_res));
+
+	/* Get IRQ information early to have an ability to ask for deferred
+	 * probe if needed before we went too far with resource allocation.
+	 */
+	stmmac_res->irq = platform_get_irq_byname(pdev, "macirq");
+	if (stmmac_res->irq < 0)
+		return stmmac_res->irq;
+
+	/* On some platforms e.g. SPEAr the wake up irq differs from the mac irq
+	 * The external wake up irq can be passed through the platform code
+	 * named as "eth_wake_irq"
+	 *
+	 * In case the wake up interrupt is not passed from the platform
+	 * so the driver will continue to use the mac irq (ndev->irq)
+	 */
+	stmmac_res->wol_irq =
+		platform_get_irq_byname_optional(pdev, "eth_wake_irq");
+	if (stmmac_res->wol_irq < 0) {
+		if (stmmac_res->wol_irq == -EPROBE_DEFER)
+			return -EPROBE_DEFER;
+		dev_info(&pdev->dev, "IRQ eth_wake_irq not found\n");
+		stmmac_res->wol_irq = stmmac_res->irq;
+	}
+
+	stmmac_res->lpi_irq =
+		platform_get_irq_byname_optional(pdev, "eth_lpi");
+	if (stmmac_res->lpi_irq < 0) {
+		if (stmmac_res->lpi_irq == -EPROBE_DEFER)
+			return -EPROBE_DEFER;
+		dev_info(&pdev->dev, "IRQ eth_lpi not found\n");
+	}
+
+	stmmac_res->addr = devm_platform_ioremap_resource(pdev, 0);
+
+	return PTR_ERR_OR_ZERO(stmmac_res->addr);
+}
+EXPORT_SYMBOL_GPL(stmmac_get_platform_resources);
+
+/**
+ * stmmac_pltfr_remove
+ * @pdev: platform device pointer
+ * Description: this function calls the main to free the net resources
+ * and calls the platforms hook and release the resources (e.g. mem).
+ */
+int stmmac_pltfr_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+	struct plat_stmmacenet_data *plat = priv->plat;
+	int ret = stmmac_dvr_remove(&pdev->dev);
+
+	if (plat->exit)
+		plat->exit(pdev, plat->bsp_priv);
+
+	stmmac_remove_config_dt(pdev, plat);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(stmmac_pltfr_remove);
+
+#if 0
+#ifdef CONFIG_PM_SLEEP
+/**
+ * stmmac_pltfr_suspend
+ * @dev: device pointer
+ * Description: this function is invoked when suspend the driver and it direcly
+ * call the main suspend function and then, if required, on some platform, it
+ * can call an exit helper.
+ */
+static int stmmac_pltfr_suspend(struct device *dev)
+{
+	int ret;
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+	struct platform_device *pdev = to_platform_device(dev);
+
+	ret = stmmac_suspend(dev);
+	if (priv->plat->exit)
+		priv->plat->exit(pdev, priv->plat->bsp_priv);
+
+	return ret;
+}
+
+/**
+ * stmmac_pltfr_resume
+ * @dev: device pointer
+ * Description: this function is invoked when resume the driver before calling
+ * the main resume function, on some platforms, it can call own init helper
+ * if required.
+ */
+static int stmmac_pltfr_resume(struct device *dev)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+	struct platform_device *pdev = to_platform_device(dev);
+
+	if (priv->plat->init)
+		priv->plat->init(pdev, priv->plat->bsp_priv);
+
+	return stmmac_resume(dev);
+}
+#endif /* CONFIG_PM_SLEEP */
+
+SIMPLE_DEV_PM_OPS(stmmac_pltfr_pm_ops, stmmac_pltfr_suspend,
+				       stmmac_pltfr_resume);
+EXPORT_SYMBOL_GPL(stmmac_pltfr_pm_ops);
+#endif /* 0 */
+
+MODULE_DESCRIPTION("STMMAC 10/100/1000 Ethernet platform support");
+MODULE_AUTHOR("Giuseppe Cavallaro <peppe.cavallaro@st.com>");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_platform.h b/net/rtnet/drivers/orange-pi-one/stmmac_platform.h
--- a/net/rtnet/drivers/orange-pi-one/stmmac_platform.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_platform.h	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*******************************************************************************
+  Copyright (C) 2007-2009  STMicroelectronics Ltd
+
+
+  Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>
+*******************************************************************************/
+
+#ifndef __STMMAC_PLATFORM_H__
+#define __STMMAC_PLATFORM_H__
+
+#include "stmmac.h"
+
+struct plat_stmmacenet_data *
+stmmac_probe_config_dt(struct platform_device *pdev, const char **mac);
+void stmmac_remove_config_dt(struct platform_device *pdev,
+			     struct plat_stmmacenet_data *plat);
+
+int stmmac_get_platform_resources(struct platform_device *pdev,
+				  struct stmmac_resources *stmmac_res);
+
+int stmmac_pltfr_remove(struct platform_device *pdev);
+extern const struct dev_pm_ops stmmac_pltfr_pm_ops;
+
+static inline void *get_stmmac_bsp_priv(struct device *dev)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct stmmac_priv *priv = netdev_priv(ndev);
+
+	return priv->plat->bsp_priv;
+}
+
+#endif /* __STMMAC_PLATFORM_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_ptp.c b/net/rtnet/drivers/orange-pi-one/stmmac_ptp.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_ptp.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_ptp.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,232 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*******************************************************************************
+  PTP 1588 clock using the STMMAC.
+
+  Copyright (C) 2013  Vayavya Labs Pvt Ltd
+
+
+  Author: Rayagond Kokatanur <rayagond@vayavyalabs.com>
+*******************************************************************************/
+#include "stmmac.h"
+#include "stmmac_ptp.h"
+
+/**
+ * stmmac_adjust_freq
+ *
+ * @ptp: pointer to ptp_clock_info structure
+ * @ppb: desired period change in parts ber billion
+ *
+ * Description: this function will adjust the frequency of hardware clock.
+ */
+static int stmmac_adjust_freq(struct ptp_clock_info *ptp, s32 ppb)
+{
+	struct stmmac_priv *priv =
+	    container_of(ptp, struct stmmac_priv, ptp_clock_ops);
+	unsigned long flags;
+	u32 diff, addend;
+	int neg_adj = 0;
+	u64 adj;
+
+	if (ppb < 0) {
+		neg_adj = 1;
+		ppb = -ppb;
+	}
+
+	addend = priv->default_addend;
+	adj = addend;
+	adj *= ppb;
+	diff = div_u64(adj, 1000000000ULL);
+	addend = neg_adj ? (addend - diff) : (addend + diff);
+
+	spin_lock_irqsave(&priv->ptp_lock, flags);
+	stmmac_config_addend(priv, priv->ptpaddr, addend);
+	spin_unlock_irqrestore(&priv->ptp_lock, flags);
+
+	return 0;
+}
+
+/**
+ * stmmac_adjust_time
+ *
+ * @ptp: pointer to ptp_clock_info structure
+ * @delta: desired change in nanoseconds
+ *
+ * Description: this function will shift/adjust the hardware clock time.
+ */
+static int stmmac_adjust_time(struct ptp_clock_info *ptp, s64 delta)
+{
+	struct stmmac_priv *priv =
+	    container_of(ptp, struct stmmac_priv, ptp_clock_ops);
+	unsigned long flags;
+	u32 sec, nsec;
+	u32 quotient, reminder;
+	int neg_adj = 0;
+	bool xmac;
+
+	xmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;
+
+	if (delta < 0) {
+		neg_adj = 1;
+		delta = -delta;
+	}
+
+	quotient = div_u64_rem(delta, 1000000000ULL, &reminder);
+	sec = quotient;
+	nsec = reminder;
+
+	spin_lock_irqsave(&priv->ptp_lock, flags);
+	stmmac_adjust_systime(priv, priv->ptpaddr, sec, nsec, neg_adj, xmac);
+	spin_unlock_irqrestore(&priv->ptp_lock, flags);
+
+	return 0;
+}
+
+/**
+ * stmmac_get_time
+ *
+ * @ptp: pointer to ptp_clock_info structure
+ * @ts: pointer to hold time/result
+ *
+ * Description: this function will read the current time from the
+ * hardware clock and store it in @ts.
+ */
+static int stmmac_get_time(struct ptp_clock_info *ptp, struct timespec64 *ts)
+{
+	struct stmmac_priv *priv =
+	    container_of(ptp, struct stmmac_priv, ptp_clock_ops);
+	unsigned long flags;
+	u64 ns = 0;
+
+	spin_lock_irqsave(&priv->ptp_lock, flags);
+	stmmac_get_systime(priv, priv->ptpaddr, &ns);
+	spin_unlock_irqrestore(&priv->ptp_lock, flags);
+
+	*ts = ns_to_timespec64(ns);
+
+	return 0;
+}
+
+/**
+ * stmmac_set_time
+ *
+ * @ptp: pointer to ptp_clock_info structure
+ * @ts: time value to set
+ *
+ * Description: this function will set the current time on the
+ * hardware clock.
+ */
+static int stmmac_set_time(struct ptp_clock_info *ptp,
+			   const struct timespec64 *ts)
+{
+	struct stmmac_priv *priv =
+	    container_of(ptp, struct stmmac_priv, ptp_clock_ops);
+	unsigned long flags;
+
+	spin_lock_irqsave(&priv->ptp_lock, flags);
+	stmmac_init_systime(priv, priv->ptpaddr, ts->tv_sec, ts->tv_nsec);
+	spin_unlock_irqrestore(&priv->ptp_lock, flags);
+
+	return 0;
+}
+
+static int stmmac_enable(struct ptp_clock_info *ptp,
+			 struct ptp_clock_request *rq, int on)
+{
+	struct stmmac_priv *priv =
+	    container_of(ptp, struct stmmac_priv, ptp_clock_ops);
+	struct stmmac_pps_cfg *cfg;
+	int ret = -EOPNOTSUPP;
+	unsigned long flags;
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_PEROUT:
+		/* Reject requests with unsupported flags */
+		if (rq->perout.flags)
+			return -EOPNOTSUPP;
+
+		cfg = &priv->pps[rq->perout.index];
+
+		cfg->start.tv_sec = rq->perout.start.sec;
+		cfg->start.tv_nsec = rq->perout.start.nsec;
+		cfg->period.tv_sec = rq->perout.period.sec;
+		cfg->period.tv_nsec = rq->perout.period.nsec;
+
+		spin_lock_irqsave(&priv->ptp_lock, flags);
+		ret = stmmac_flex_pps_config(priv, priv->ioaddr,
+					     rq->perout.index, cfg, on,
+					     priv->sub_second_inc,
+					     priv->systime_flags);
+		spin_unlock_irqrestore(&priv->ptp_lock, flags);
+		break;
+	default:
+		break;
+	}
+
+	return ret;
+}
+
+/* structure describing a PTP hardware clock */
+static struct ptp_clock_info stmmac_ptp_clock_ops = {
+	.owner = THIS_MODULE,
+	.name = "stmmac ptp",
+	.max_adj = 62500000,
+	.n_alarm = 0,
+	.n_ext_ts = 0,
+	.n_per_out = 0, /* will be overwritten in stmmac_ptp_register */
+	.n_pins = 0,
+	.pps = 0,
+	.adjfreq = stmmac_adjust_freq,
+	.adjtime = stmmac_adjust_time,
+	.gettime64 = stmmac_get_time,
+	.settime64 = stmmac_set_time,
+	.enable = stmmac_enable,
+};
+
+/**
+ * stmmac_ptp_register
+ * @priv: driver private structure
+ * Description: this function will register the ptp clock driver
+ * to kernel. It also does some house keeping work.
+ */
+void stmmac_ptp_register(struct stmmac_priv *priv)
+{
+	int i;
+
+	for (i = 0; i < priv->dma_cap.pps_out_num; i++) {
+		if (i >= STMMAC_PPS_MAX)
+			break;
+		priv->pps[i].available = true;
+	}
+
+	if (priv->plat->ptp_max_adj)
+		stmmac_ptp_clock_ops.max_adj = priv->plat->ptp_max_adj;
+
+	stmmac_ptp_clock_ops.n_per_out = priv->dma_cap.pps_out_num;
+
+	spin_lock_init(&priv->ptp_lock);
+	priv->ptp_clock_ops = stmmac_ptp_clock_ops;
+
+	priv->ptp_clock = ptp_clock_register(&priv->ptp_clock_ops,
+					     priv->device);
+	if (IS_ERR(priv->ptp_clock)) {
+		netdev_warn(priv->dummy, "ptp_clock_register failed\n");
+		priv->ptp_clock = NULL;
+	} else if (priv->ptp_clock)
+		netdev_info(priv->dummy, "registered PTP clock\n");
+}
+
+/**
+ * stmmac_ptp_unregister
+ * @priv: driver private structure
+ * Description: this function will remove/unregister the ptp clock driver
+ * from the kernel.
+ */
+void stmmac_ptp_unregister(struct stmmac_priv *priv)
+{
+	if (priv->ptp_clock) {
+		ptp_clock_unregister(priv->ptp_clock);
+		priv->ptp_clock = NULL;
+		pr_debug("Removed PTP HW clock successfully on %s\n",
+			 priv->dev->name);
+	}
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_ptp.h b/net/rtnet/drivers/orange-pi-one/stmmac_ptp.h
--- a/net/rtnet/drivers/orange-pi-one/stmmac_ptp.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_ptp.h	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/******************************************************************************
+  PTP Header file
+
+  Copyright (C) 2013  Vayavya Labs Pvt Ltd
+
+
+  Author: Rayagond Kokatanur <rayagond@vayavyalabs.com>
+******************************************************************************/
+
+#ifndef	__STMMAC_PTP_H__
+#define	__STMMAC_PTP_H__
+
+#define PTP_XGMAC_OFFSET	0xd00
+#define	PTP_GMAC4_OFFSET	0xb00
+#define	PTP_GMAC3_X_OFFSET	0x700
+
+/* IEEE 1588 PTP register offsets */
+#define	PTP_TCR		0x00	/* Timestamp Control Reg */
+#define	PTP_SSIR	0x04	/* Sub-Second Increment Reg */
+#define	PTP_STSR	0x08	/* System Time  Seconds Regr */
+#define	PTP_STNSR	0x0c	/* System Time  Nanoseconds Reg */
+#define	PTP_STSUR	0x10	/* System Time  Seconds Update Reg */
+#define	PTP_STNSUR	0x14	/* System Time  Nanoseconds Update Reg */
+#define	PTP_TAR		0x18	/* Timestamp Addend Reg */
+
+#define	PTP_STNSUR_ADDSUB_SHIFT	31
+#define	PTP_DIGITAL_ROLLOVER_MODE	0x3B9ACA00	/* 10e9-1 ns */
+#define	PTP_BINARY_ROLLOVER_MODE	0x80000000	/* ~0.466 ns */
+
+/* PTP Timestamp control register defines */
+#define	PTP_TCR_TSENA		BIT(0)	/* Timestamp Enable */
+#define	PTP_TCR_TSCFUPDT	BIT(1)	/* Timestamp Fine/Coarse Update */
+#define	PTP_TCR_TSINIT		BIT(2)	/* Timestamp Initialize */
+#define	PTP_TCR_TSUPDT		BIT(3)	/* Timestamp Update */
+#define	PTP_TCR_TSTRIG		BIT(4)	/* Timestamp Interrupt Trigger Enable */
+#define	PTP_TCR_TSADDREG	BIT(5)	/* Addend Reg Update */
+#define	PTP_TCR_TSENALL		BIT(8)	/* Enable Timestamp for All Frames */
+#define	PTP_TCR_TSCTRLSSR	BIT(9)	/* Digital or Binary Rollover Control */
+/* Enable PTP packet Processing for Version 2 Format */
+#define	PTP_TCR_TSVER2ENA	BIT(10)
+/* Enable Processing of PTP over Ethernet Frames */
+#define	PTP_TCR_TSIPENA		BIT(11)
+/* Enable Processing of PTP Frames Sent over IPv6-UDP */
+#define	PTP_TCR_TSIPV6ENA	BIT(12)
+/* Enable Processing of PTP Frames Sent over IPv4-UDP */
+#define	PTP_TCR_TSIPV4ENA	BIT(13)
+/* Enable Timestamp Snapshot for Event Messages */
+#define	PTP_TCR_TSEVNTENA	BIT(14)
+/* Enable Snapshot for Messages Relevant to Master */
+#define	PTP_TCR_TSMSTRENA	BIT(15)
+/* Select PTP packets for Taking Snapshots
+ * On gmac4 specifically:
+ * Enable SYNC, Pdelay_Req, Pdelay_Resp when TSEVNTENA is enabled.
+ * or
+ * Enable  SYNC, Follow_Up, Delay_Req, Delay_Resp, Pdelay_Req, Pdelay_Resp,
+ * Pdelay_Resp_Follow_Up if TSEVNTENA is disabled
+ */
+#define	PTP_TCR_SNAPTYPSEL_1	BIT(16)
+/* Enable MAC address for PTP Frame Filtering */
+#define	PTP_TCR_TSENMACADDR	BIT(18)
+
+/* SSIR defines */
+#define	PTP_SSIR_SSINC_MASK		0xff
+#define	GMAC4_PTP_SSIR_SSINC_SHIFT	16
+
+#endif	/* __STMMAC_PTP_H__ */
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_selftests.c b/net/rtnet/drivers/orange-pi-one/stmmac_selftests.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_selftests.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_selftests.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,2046 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2019 Synopsys, Inc. and/or its affiliates.
+ * stmmac Selftests Support
+ *
+ * Author: Jose Abreu <joabreu@synopsys.com>
+ */
+
+#include <linux/bitrev.h>
+#include <linux/completion.h>
+#include <linux/crc32.h>
+#include <linux/ethtool.h>
+#include <linux/ip.h>
+#include <linux/phy.h>
+#include <linux/udp.h>
+#include <net/pkt_cls.h>
+#include <net/pkt_sched.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/tc_act/tc_gact.h>
+#include "stmmac.h"
+
+struct stmmachdr {
+	__be32 version;
+	__be64 magic;
+	u8 id;
+} __packed;
+
+#define STMMAC_TEST_PKT_SIZE (sizeof(struct ethhdr) + sizeof(struct iphdr) + \
+			      sizeof(struct stmmachdr))
+#define STMMAC_TEST_PKT_MAGIC	0xdeadcafecafedeadULL
+#define STMMAC_LB_TIMEOUT	msecs_to_jiffies(200)
+
+struct stmmac_packet_attrs {
+	int vlan;
+	int vlan_id_in;
+	int vlan_id_out;
+	unsigned char *src;
+	unsigned char *dst;
+	u32 ip_src;
+	u32 ip_dst;
+	int tcp;
+	int sport;
+	int dport;
+	u32 exp_hash;
+	int dont_wait;
+	int timeout;
+	int size;
+	int max_size;
+	int remove_sa;
+	u8 id;
+	int sarc;
+	u16 queue_mapping;
+	u64 timestamp;
+};
+
+static u8 stmmac_test_next_id;
+
+static struct sk_buff *stmmac_test_get_udp_skb(struct stmmac_priv *priv,
+					       struct stmmac_packet_attrs *attr)
+{
+	struct sk_buff *skb = NULL;
+	struct udphdr *uhdr = NULL;
+	struct tcphdr *thdr = NULL;
+	struct stmmachdr *shdr;
+	struct ethhdr *ehdr;
+	struct iphdr *ihdr;
+	int iplen, size;
+
+	size = attr->size + STMMAC_TEST_PKT_SIZE;
+	if (attr->vlan) {
+		size += 4;
+		if (attr->vlan > 1)
+			size += 4;
+	}
+
+	if (attr->tcp)
+		size += sizeof(struct tcphdr);
+	else
+		size += sizeof(struct udphdr);
+
+	if (attr->max_size && (attr->max_size > size))
+		size = attr->max_size;
+
+	skb = netdev_alloc_skb(priv->dev, size);
+	if (!skb)
+		return NULL;
+
+	prefetchw(skb->data);
+
+	if (attr->vlan > 1)
+		ehdr = skb_push(skb, ETH_HLEN + 8);
+	else if (attr->vlan)
+		ehdr = skb_push(skb, ETH_HLEN + 4);
+	else if (attr->remove_sa)
+		ehdr = skb_push(skb, ETH_HLEN - 6);
+	else
+		ehdr = skb_push(skb, ETH_HLEN);
+	skb_reset_mac_header(skb);
+
+	skb_set_network_header(skb, skb->len);
+	ihdr = skb_put(skb, sizeof(*ihdr));
+
+	skb_set_transport_header(skb, skb->len);
+	if (attr->tcp)
+		thdr = skb_put(skb, sizeof(*thdr));
+	else
+		uhdr = skb_put(skb, sizeof(*uhdr));
+
+	if (!attr->remove_sa)
+		eth_zero_addr(ehdr->h_source);
+	eth_zero_addr(ehdr->h_dest);
+	if (attr->src && !attr->remove_sa)
+		ether_addr_copy(ehdr->h_source, attr->src);
+	if (attr->dst)
+		ether_addr_copy(ehdr->h_dest, attr->dst);
+
+	if (!attr->remove_sa) {
+		ehdr->h_proto = htons(ETH_P_IP);
+	} else {
+		__be16 *ptr = (__be16 *)ehdr;
+
+		/* HACK */
+		ptr[3] = htons(ETH_P_IP);
+	}
+
+	if (attr->vlan) {
+		__be16 *tag, *proto;
+
+		if (!attr->remove_sa) {
+			tag = (void *)ehdr + ETH_HLEN;
+			proto = (void *)ehdr + (2 * ETH_ALEN);
+		} else {
+			tag = (void *)ehdr + ETH_HLEN - 6;
+			proto = (void *)ehdr + ETH_ALEN;
+		}
+
+		proto[0] = htons(ETH_P_8021Q);
+		tag[0] = htons(attr->vlan_id_out);
+		tag[1] = htons(ETH_P_IP);
+		if (attr->vlan > 1) {
+			proto[0] = htons(ETH_P_8021AD);
+			tag[1] = htons(ETH_P_8021Q);
+			tag[2] = htons(attr->vlan_id_in);
+			tag[3] = htons(ETH_P_IP);
+		}
+	}
+
+	if (attr->tcp) {
+		thdr->source = htons(attr->sport);
+		thdr->dest = htons(attr->dport);
+		thdr->doff = sizeof(struct tcphdr) / 4;
+		thdr->check = 0;
+	} else {
+		uhdr->source = htons(attr->sport);
+		uhdr->dest = htons(attr->dport);
+		uhdr->len = htons(sizeof(*shdr) + sizeof(*uhdr) + attr->size);
+		if (attr->max_size)
+			uhdr->len = htons(attr->max_size -
+					  (sizeof(*ihdr) + sizeof(*ehdr)));
+		uhdr->check = 0;
+	}
+
+	ihdr->ihl = 5;
+	ihdr->ttl = 32;
+	ihdr->version = 4;
+	if (attr->tcp)
+		ihdr->protocol = IPPROTO_TCP;
+	else
+		ihdr->protocol = IPPROTO_UDP;
+	iplen = sizeof(*ihdr) + sizeof(*shdr) + attr->size;
+	if (attr->tcp)
+		iplen += sizeof(*thdr);
+	else
+		iplen += sizeof(*uhdr);
+
+	if (attr->max_size)
+		iplen = attr->max_size - sizeof(*ehdr);
+
+	ihdr->tot_len = htons(iplen);
+	ihdr->frag_off = 0;
+	ihdr->saddr = htonl(attr->ip_src);
+	ihdr->daddr = htonl(attr->ip_dst);
+	ihdr->tos = 0;
+	ihdr->id = 0;
+	ip_send_check(ihdr);
+
+	shdr = skb_put(skb, sizeof(*shdr));
+	shdr->version = 0;
+	shdr->magic = cpu_to_be64(STMMAC_TEST_PKT_MAGIC);
+	attr->id = stmmac_test_next_id;
+	shdr->id = stmmac_test_next_id++;
+
+	if (attr->size)
+		skb_put(skb, attr->size);
+	if (attr->max_size && (attr->max_size > skb->len))
+		skb_put(skb, attr->max_size - skb->len);
+
+	skb->csum = 0;
+	skb->ip_summed = CHECKSUM_PARTIAL;
+	if (attr->tcp) {
+		thdr->check = ~tcp_v4_check(skb->len, ihdr->saddr, ihdr->daddr, 0);
+		skb->csum_start = skb_transport_header(skb) - skb->head;
+		skb->csum_offset = offsetof(struct tcphdr, check);
+	} else {
+		udp4_hwcsum(skb, ihdr->saddr, ihdr->daddr);
+	}
+
+	skb->protocol = htons(ETH_P_IP);
+	skb->pkt_type = PACKET_HOST;
+	skb->dev = priv->dev;
+
+	if (attr->timestamp)
+		skb->tstamp = ns_to_ktime(attr->timestamp);
+
+	return skb;
+}
+
+static struct sk_buff *stmmac_test_get_arp_skb(struct stmmac_priv *priv,
+					       struct stmmac_packet_attrs *attr)
+{
+	__be32 ip_src = htonl(attr->ip_src);
+	__be32 ip_dst = htonl(attr->ip_dst);
+	struct sk_buff *skb = NULL;
+
+	skb = arp_create(ARPOP_REQUEST, ETH_P_ARP, ip_dst, priv->dev, ip_src,
+			 NULL, attr->src, attr->dst);
+	if (!skb)
+		return NULL;
+
+	skb->pkt_type = PACKET_HOST;
+	skb->dev = priv->dev;
+
+	return skb;
+}
+
+struct stmmac_test_priv {
+	struct stmmac_packet_attrs *packet;
+	struct packet_type pt;
+	struct completion comp;
+	int double_vlan;
+	int vlan_id;
+	int ok;
+};
+
+static int stmmac_test_loopback_validate(struct sk_buff *skb,
+					 struct net_device *ndev,
+					 struct packet_type *pt,
+					 struct net_device *orig_ndev)
+{
+	struct stmmac_test_priv *tpriv = pt->af_packet_priv;
+	unsigned char *src = tpriv->packet->src;
+	unsigned char *dst = tpriv->packet->dst;
+	struct stmmachdr *shdr;
+	struct ethhdr *ehdr;
+	struct udphdr *uhdr;
+	struct tcphdr *thdr;
+	struct iphdr *ihdr;
+
+	skb = skb_unshare(skb, GFP_ATOMIC);
+	if (!skb)
+		goto out;
+
+	if (skb_linearize(skb))
+		goto out;
+	if (skb_headlen(skb) < (STMMAC_TEST_PKT_SIZE - ETH_HLEN))
+		goto out;
+
+	ehdr = (struct ethhdr *)skb_mac_header(skb);
+	if (dst) {
+		if (!ether_addr_equal_unaligned(ehdr->h_dest, dst))
+			goto out;
+	}
+	if (tpriv->packet->sarc) {
+		if (!ether_addr_equal_unaligned(ehdr->h_source, ehdr->h_dest))
+			goto out;
+	} else if (src) {
+		if (!ether_addr_equal_unaligned(ehdr->h_source, src))
+			goto out;
+	}
+
+	ihdr = ip_hdr(skb);
+	if (tpriv->double_vlan)
+		ihdr = (struct iphdr *)(skb_network_header(skb) + 4);
+
+	if (tpriv->packet->tcp) {
+		if (ihdr->protocol != IPPROTO_TCP)
+			goto out;
+
+		thdr = (struct tcphdr *)((u8 *)ihdr + 4 * ihdr->ihl);
+		if (thdr->dest != htons(tpriv->packet->dport))
+			goto out;
+
+		shdr = (struct stmmachdr *)((u8 *)thdr + sizeof(*thdr));
+	} else {
+		if (ihdr->protocol != IPPROTO_UDP)
+			goto out;
+
+		uhdr = (struct udphdr *)((u8 *)ihdr + 4 * ihdr->ihl);
+		if (uhdr->dest != htons(tpriv->packet->dport))
+			goto out;
+
+		shdr = (struct stmmachdr *)((u8 *)uhdr + sizeof(*uhdr));
+	}
+
+	if (shdr->magic != cpu_to_be64(STMMAC_TEST_PKT_MAGIC))
+		goto out;
+	if (tpriv->packet->exp_hash && !skb->hash)
+		goto out;
+	if (tpriv->packet->id != shdr->id)
+		goto out;
+
+	tpriv->ok = true;
+	complete(&tpriv->comp);
+out:
+	kfree_skb(skb);
+	return 0;
+}
+
+static int __stmmac_test_loopback(struct stmmac_priv *priv,
+				  struct stmmac_packet_attrs *attr)
+{
+	struct stmmac_test_priv *tpriv;
+	struct sk_buff *skb = NULL;
+	int ret = 0;
+
+	tpriv = kzalloc(sizeof(*tpriv), GFP_KERNEL);
+	if (!tpriv)
+		return -ENOMEM;
+
+	tpriv->ok = false;
+	init_completion(&tpriv->comp);
+
+	tpriv->pt.type = htons(ETH_P_IP);
+	tpriv->pt.func = stmmac_test_loopback_validate;
+	tpriv->pt.dev = priv->dev;
+	tpriv->pt.af_packet_priv = tpriv;
+	tpriv->packet = attr;
+
+	if (!attr->dont_wait)
+		dev_add_pack(&tpriv->pt);
+
+	skb = stmmac_test_get_udp_skb(priv, attr);
+	if (!skb) {
+		ret = -ENOMEM;
+		goto cleanup;
+	}
+
+	ret = dev_direct_xmit(skb, attr->queue_mapping);
+	if (ret)
+		goto cleanup;
+
+	if (attr->dont_wait)
+		goto cleanup;
+
+	if (!attr->timeout)
+		attr->timeout = STMMAC_LB_TIMEOUT;
+
+	wait_for_completion_timeout(&tpriv->comp, attr->timeout);
+	ret = tpriv->ok ? 0 : -ETIMEDOUT;
+
+cleanup:
+	if (!attr->dont_wait)
+		dev_remove_pack(&tpriv->pt);
+	kfree(tpriv);
+	return ret;
+}
+
+static int stmmac_test_mac_loopback(struct stmmac_priv *priv)
+{
+	struct stmmac_packet_attrs attr = { };
+
+	attr.dst = priv->dev->dev_addr;
+	return __stmmac_test_loopback(priv, &attr);
+}
+
+static int stmmac_test_phy_loopback(struct stmmac_priv *priv)
+{
+	struct stmmac_packet_attrs attr = { };
+	int ret;
+
+	if (!priv->dev->phydev)
+		return -EOPNOTSUPP;
+
+	ret = phy_loopback(priv->dev->phydev, true);
+	if (ret)
+		return ret;
+
+	attr.dst = priv->dev->dev_addr;
+	ret = __stmmac_test_loopback(priv, &attr);
+
+	phy_loopback(priv->dev->phydev, false);
+	return ret;
+}
+
+static int stmmac_test_mmc(struct stmmac_priv *priv)
+{
+	struct stmmac_counters initial, final;
+	int ret;
+
+	memset(&initial, 0, sizeof(initial));
+	memset(&final, 0, sizeof(final));
+
+	if (!priv->dma_cap.rmon)
+		return -EOPNOTSUPP;
+
+	/* Save previous results into internal struct */
+	stmmac_mmc_read(priv, priv->mmcaddr, &priv->mmc);
+
+	ret = stmmac_test_mac_loopback(priv);
+	if (ret)
+		return ret;
+
+	/* These will be loopback results so no need to save them */
+	stmmac_mmc_read(priv, priv->mmcaddr, &final);
+
+	/*
+	 * The number of MMC counters available depends on HW configuration
+	 * so we just use this one to validate the feature. I hope there is
+	 * not a version without this counter.
+	 */
+	if (final.mmc_tx_framecount_g <= initial.mmc_tx_framecount_g)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int stmmac_test_eee(struct stmmac_priv *priv)
+{
+	struct stmmac_extra_stats *initial, *final;
+	int retries = 10;
+	int ret;
+
+	if (!priv->dma_cap.eee || !priv->eee_active)
+		return -EOPNOTSUPP;
+
+	initial = kzalloc(sizeof(*initial), GFP_KERNEL);
+	if (!initial)
+		return -ENOMEM;
+
+	final = kzalloc(sizeof(*final), GFP_KERNEL);
+	if (!final) {
+		ret = -ENOMEM;
+		goto out_free_initial;
+	}
+
+	memcpy(initial, &priv->xstats, sizeof(*initial));
+
+	ret = stmmac_test_mac_loopback(priv);
+	if (ret)
+		goto out_free_final;
+
+	/* We have no traffic in the line so, sooner or later it will go LPI */
+	while (--retries) {
+		memcpy(final, &priv->xstats, sizeof(*final));
+
+		if (final->irq_tx_path_in_lpi_mode_n >
+		    initial->irq_tx_path_in_lpi_mode_n)
+			break;
+		msleep(100);
+	}
+
+	if (!retries) {
+		ret = -ETIMEDOUT;
+		goto out_free_final;
+	}
+
+	if (final->irq_tx_path_in_lpi_mode_n <=
+	    initial->irq_tx_path_in_lpi_mode_n) {
+		ret = -EINVAL;
+		goto out_free_final;
+	}
+
+	if (final->irq_tx_path_exit_lpi_mode_n <=
+	    initial->irq_tx_path_exit_lpi_mode_n) {
+		ret = -EINVAL;
+		goto out_free_final;
+	}
+
+out_free_final:
+	kfree(final);
+out_free_initial:
+	kfree(initial);
+	return ret;
+}
+
+static int stmmac_filter_check(struct stmmac_priv *priv)
+{
+	if (!(priv->dev->flags & IFF_PROMISC))
+		return 0;
+
+	netdev_warn(priv->dev, "Test can't be run in promiscuous mode!\n");
+	return -EOPNOTSUPP;
+}
+
+static bool stmmac_hash_check(struct stmmac_priv *priv, unsigned char *addr)
+{
+	int mc_offset = 32 - priv->hw->mcast_bits_log2;
+	struct netdev_hw_addr *ha;
+	u32 hash, hash_nr;
+
+	/* First compute the hash for desired addr */
+	hash = bitrev32(~crc32_le(~0, addr, 6)) >> mc_offset;
+	hash_nr = hash >> 5;
+	hash = 1 << (hash & 0x1f);
+
+	/* Now, check if it collides with any existing one */
+	netdev_for_each_mc_addr(ha, priv->dev) {
+		u32 nr = bitrev32(~crc32_le(~0, ha->addr, ETH_ALEN)) >> mc_offset;
+		if (((nr >> 5) == hash_nr) && ((1 << (nr & 0x1f)) == hash))
+			return false;
+	}
+
+	/* No collisions, address is good to go */
+	return true;
+}
+
+static bool stmmac_perfect_check(struct stmmac_priv *priv, unsigned char *addr)
+{
+	struct netdev_hw_addr *ha;
+
+	/* Check if it collides with any existing one */
+	netdev_for_each_uc_addr(ha, priv->dev) {
+		if (!memcmp(ha->addr, addr, ETH_ALEN))
+			return false;
+	}
+
+	/* No collisions, address is good to go */
+	return true;
+}
+
+static int stmmac_test_hfilt(struct stmmac_priv *priv)
+{
+	unsigned char gd_addr[ETH_ALEN] = {0xf1, 0xee, 0xdd, 0xcc, 0xbb, 0xaa};
+	unsigned char bd_addr[ETH_ALEN] = {0xf1, 0xff, 0xff, 0xff, 0xff, 0xff};
+	struct stmmac_packet_attrs attr = { };
+	int ret, tries = 256;
+
+	ret = stmmac_filter_check(priv);
+	if (ret)
+		return ret;
+
+	if (netdev_mc_count(priv->dev) >= priv->hw->multicast_filter_bins)
+		return -EOPNOTSUPP;
+
+	while (--tries) {
+		/* We only need to check the bd_addr for collisions */
+		bd_addr[ETH_ALEN - 1] = tries;
+		if (stmmac_hash_check(priv, bd_addr))
+			break;
+	}
+
+	if (!tries)
+		return -EOPNOTSUPP;
+
+	ret = dev_mc_add(priv->dev, gd_addr);
+	if (ret)
+		return ret;
+
+	attr.dst = gd_addr;
+
+	/* Shall receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto cleanup;
+
+	attr.dst = bd_addr;
+
+	/* Shall NOT receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL;
+
+cleanup:
+	dev_mc_del(priv->dev, gd_addr);
+	return ret;
+}
+
+static int stmmac_test_pfilt(struct stmmac_priv *priv)
+{
+	unsigned char gd_addr[ETH_ALEN] = {0xf0, 0x01, 0x44, 0x55, 0x66, 0x77};
+	unsigned char bd_addr[ETH_ALEN] = {0xf0, 0xff, 0xff, 0xff, 0xff, 0xff};
+	struct stmmac_packet_attrs attr = { };
+	int ret, tries = 256;
+
+	if (stmmac_filter_check(priv))
+		return -EOPNOTSUPP;
+	if (netdev_uc_count(priv->dev) >= priv->hw->unicast_filter_entries)
+		return -EOPNOTSUPP;
+
+	while (--tries) {
+		/* We only need to check the bd_addr for collisions */
+		bd_addr[ETH_ALEN - 1] = tries;
+		if (stmmac_perfect_check(priv, bd_addr))
+			break;
+	}
+
+	if (!tries)
+		return -EOPNOTSUPP;
+
+	ret = dev_uc_add(priv->dev, gd_addr);
+	if (ret)
+		return ret;
+
+	attr.dst = gd_addr;
+
+	/* Shall receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto cleanup;
+
+	attr.dst = bd_addr;
+
+	/* Shall NOT receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL;
+
+cleanup:
+	dev_uc_del(priv->dev, gd_addr);
+	return ret;
+}
+
+static int stmmac_test_mcfilt(struct stmmac_priv *priv)
+{
+	unsigned char uc_addr[ETH_ALEN] = {0xf0, 0xff, 0xff, 0xff, 0xff, 0xff};
+	unsigned char mc_addr[ETH_ALEN] = {0xf1, 0xff, 0xff, 0xff, 0xff, 0xff};
+	struct stmmac_packet_attrs attr = { };
+	int ret, tries = 256;
+
+	if (stmmac_filter_check(priv))
+		return -EOPNOTSUPP;
+	if (netdev_uc_count(priv->dev) >= priv->hw->unicast_filter_entries)
+		return -EOPNOTSUPP;
+	if (netdev_mc_count(priv->dev) >= priv->hw->multicast_filter_bins)
+		return -EOPNOTSUPP;
+
+	while (--tries) {
+		/* We only need to check the mc_addr for collisions */
+		mc_addr[ETH_ALEN - 1] = tries;
+		if (stmmac_hash_check(priv, mc_addr))
+			break;
+	}
+
+	if (!tries)
+		return -EOPNOTSUPP;
+
+	ret = dev_uc_add(priv->dev, uc_addr);
+	if (ret)
+		return ret;
+
+	attr.dst = uc_addr;
+
+	/* Shall receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto cleanup;
+
+	attr.dst = mc_addr;
+
+	/* Shall NOT receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL;
+
+cleanup:
+	dev_uc_del(priv->dev, uc_addr);
+	return ret;
+}
+
+static int stmmac_test_ucfilt(struct stmmac_priv *priv)
+{
+	unsigned char uc_addr[ETH_ALEN] = {0xf0, 0xff, 0xff, 0xff, 0xff, 0xff};
+	unsigned char mc_addr[ETH_ALEN] = {0xf1, 0xff, 0xff, 0xff, 0xff, 0xff};
+	struct stmmac_packet_attrs attr = { };
+	int ret, tries = 256;
+
+	if (stmmac_filter_check(priv))
+		return -EOPNOTSUPP;
+	if (netdev_uc_count(priv->dev) >= priv->hw->unicast_filter_entries)
+		return -EOPNOTSUPP;
+	if (netdev_mc_count(priv->dev) >= priv->hw->multicast_filter_bins)
+		return -EOPNOTSUPP;
+
+	while (--tries) {
+		/* We only need to check the uc_addr for collisions */
+		uc_addr[ETH_ALEN - 1] = tries;
+		if (stmmac_perfect_check(priv, uc_addr))
+			break;
+	}
+
+	if (!tries)
+		return -EOPNOTSUPP;
+
+	ret = dev_mc_add(priv->dev, mc_addr);
+	if (ret)
+		return ret;
+
+	attr.dst = mc_addr;
+
+	/* Shall receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto cleanup;
+
+	attr.dst = uc_addr;
+
+	/* Shall NOT receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL;
+
+cleanup:
+	dev_mc_del(priv->dev, mc_addr);
+	return ret;
+}
+
+static int stmmac_test_flowctrl_validate(struct sk_buff *skb,
+					 struct net_device *ndev,
+					 struct packet_type *pt,
+					 struct net_device *orig_ndev)
+{
+	struct stmmac_test_priv *tpriv = pt->af_packet_priv;
+	struct ethhdr *ehdr;
+
+	ehdr = (struct ethhdr *)skb_mac_header(skb);
+	if (!ether_addr_equal_unaligned(ehdr->h_source, orig_ndev->dev_addr))
+		goto out;
+	if (ehdr->h_proto != htons(ETH_P_PAUSE))
+		goto out;
+
+	tpriv->ok = true;
+	complete(&tpriv->comp);
+out:
+	kfree_skb(skb);
+	return 0;
+}
+
+static int stmmac_test_flowctrl(struct stmmac_priv *priv)
+{
+	unsigned char paddr[ETH_ALEN] = {0x01, 0x80, 0xC2, 0x00, 0x00, 0x01};
+	struct phy_device *phydev = priv->dev->phydev;
+	u32 rx_cnt = priv->plat->rx_queues_to_use;
+	struct stmmac_test_priv *tpriv;
+	unsigned int pkt_count;
+	int i, ret = 0;
+
+	if (!phydev || (!phydev->pause && !phydev->asym_pause))
+		return -EOPNOTSUPP;
+
+	tpriv = kzalloc(sizeof(*tpriv), GFP_KERNEL);
+	if (!tpriv)
+		return -ENOMEM;
+
+	tpriv->ok = false;
+	init_completion(&tpriv->comp);
+	tpriv->pt.type = htons(ETH_P_PAUSE);
+	tpriv->pt.func = stmmac_test_flowctrl_validate;
+	tpriv->pt.dev = priv->dev;
+	tpriv->pt.af_packet_priv = tpriv;
+	dev_add_pack(&tpriv->pt);
+
+	/* Compute minimum number of packets to make FIFO full */
+	pkt_count = priv->plat->rx_fifo_size;
+	if (!pkt_count)
+		pkt_count = priv->dma_cap.rx_fifo_size;
+	pkt_count /= 1400;
+	pkt_count *= 2;
+
+	for (i = 0; i < rx_cnt; i++)
+		stmmac_stop_rx(priv, priv->ioaddr, i);
+
+	ret = dev_set_promiscuity(priv->dev, 1);
+	if (ret)
+		goto cleanup;
+
+	ret = dev_mc_add(priv->dev, paddr);
+	if (ret)
+		goto cleanup;
+
+	for (i = 0; i < pkt_count; i++) {
+		struct stmmac_packet_attrs attr = { };
+
+		attr.dst = priv->dev->dev_addr;
+		attr.dont_wait = true;
+		attr.size = 1400;
+
+		ret = __stmmac_test_loopback(priv, &attr);
+		if (ret)
+			goto cleanup;
+		if (tpriv->ok)
+			break;
+	}
+
+	/* Wait for some time in case RX Watchdog is enabled */
+	msleep(200);
+
+	for (i = 0; i < rx_cnt; i++) {
+		struct stmmac_channel *ch = &priv->channel[i];
+		u32 tail;
+
+		tail = priv->rx_queue[i].dma_rx_phy +
+			(DMA_RX_SIZE * sizeof(struct dma_desc));
+
+		stmmac_set_rx_tail_ptr(priv, priv->ioaddr, tail, i);
+		stmmac_start_rx(priv, priv->ioaddr, i);
+
+		local_bh_disable();
+		napi_reschedule(&ch->rx_napi);
+		local_bh_enable();
+	}
+
+	wait_for_completion_timeout(&tpriv->comp, STMMAC_LB_TIMEOUT);
+	ret = tpriv->ok ? 0 : -ETIMEDOUT;
+
+cleanup:
+	dev_mc_del(priv->dev, paddr);
+	dev_set_promiscuity(priv->dev, -1);
+	dev_remove_pack(&tpriv->pt);
+	kfree(tpriv);
+	return ret;
+}
+
+static int stmmac_test_rss(struct stmmac_priv *priv)
+{
+	struct stmmac_packet_attrs attr = { };
+
+	if (!priv->dma_cap.rssen || !priv->rss.enable)
+		return -EOPNOTSUPP;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.exp_hash = true;
+	attr.sport = 0x321;
+	attr.dport = 0x123;
+
+	return __stmmac_test_loopback(priv, &attr);
+}
+
+static int stmmac_test_vlan_validate(struct sk_buff *skb,
+				     struct net_device *ndev,
+				     struct packet_type *pt,
+				     struct net_device *orig_ndev)
+{
+	struct stmmac_test_priv *tpriv = pt->af_packet_priv;
+	struct stmmachdr *shdr;
+	struct ethhdr *ehdr;
+	struct udphdr *uhdr;
+	struct iphdr *ihdr;
+	u16 proto;
+
+	proto = tpriv->double_vlan ? ETH_P_8021AD : ETH_P_8021Q;
+
+	skb = skb_unshare(skb, GFP_ATOMIC);
+	if (!skb)
+		goto out;
+
+	if (skb_linearize(skb))
+		goto out;
+	if (skb_headlen(skb) < (STMMAC_TEST_PKT_SIZE - ETH_HLEN))
+		goto out;
+	if (tpriv->vlan_id) {
+		if (skb->vlan_proto != htons(proto))
+			goto out;
+		if (skb->vlan_tci != tpriv->vlan_id) {
+			/* Means filter did not work. */
+			tpriv->ok = false;
+			complete(&tpriv->comp);
+			goto out;
+		}
+	}
+
+	ehdr = (struct ethhdr *)skb_mac_header(skb);
+	if (!ether_addr_equal_unaligned(ehdr->h_dest, tpriv->packet->dst))
+		goto out;
+
+	ihdr = ip_hdr(skb);
+	if (tpriv->double_vlan)
+		ihdr = (struct iphdr *)(skb_network_header(skb) + 4);
+	if (ihdr->protocol != IPPROTO_UDP)
+		goto out;
+
+	uhdr = (struct udphdr *)((u8 *)ihdr + 4 * ihdr->ihl);
+	if (uhdr->dest != htons(tpriv->packet->dport))
+		goto out;
+
+	shdr = (struct stmmachdr *)((u8 *)uhdr + sizeof(*uhdr));
+	if (shdr->magic != cpu_to_be64(STMMAC_TEST_PKT_MAGIC))
+		goto out;
+
+	tpriv->ok = true;
+	complete(&tpriv->comp);
+
+out:
+	kfree_skb(skb);
+	return 0;
+}
+
+static int __stmmac_test_vlanfilt(struct stmmac_priv *priv)
+{
+	struct stmmac_packet_attrs attr = { };
+	struct stmmac_test_priv *tpriv;
+	struct sk_buff *skb = NULL;
+	int ret = 0, i;
+
+	tpriv = kzalloc(sizeof(*tpriv), GFP_KERNEL);
+	if (!tpriv)
+		return -ENOMEM;
+
+	tpriv->ok = false;
+	init_completion(&tpriv->comp);
+
+	tpriv->pt.type = htons(ETH_P_IP);
+	tpriv->pt.func = stmmac_test_vlan_validate;
+	tpriv->pt.dev = priv->dev;
+	tpriv->pt.af_packet_priv = tpriv;
+	tpriv->packet = &attr;
+
+	/*
+	 * As we use HASH filtering, false positives may appear. This is a
+	 * specially chosen ID so that adjacent IDs (+4) have different
+	 * HASH values.
+	 */
+	tpriv->vlan_id = 0x123;
+	dev_add_pack(&tpriv->pt);
+
+	ret = vlan_vid_add(priv->dev, htons(ETH_P_8021Q), tpriv->vlan_id);
+	if (ret)
+		goto cleanup;
+
+	for (i = 0; i < 4; i++) {
+		attr.vlan = 1;
+		attr.vlan_id_out = tpriv->vlan_id + i;
+		attr.dst = priv->dev->dev_addr;
+		attr.sport = 9;
+		attr.dport = 9;
+
+		skb = stmmac_test_get_udp_skb(priv, &attr);
+		if (!skb) {
+			ret = -ENOMEM;
+			goto vlan_del;
+		}
+
+		ret = dev_direct_xmit(skb, 0);
+		if (ret)
+			goto vlan_del;
+
+		wait_for_completion_timeout(&tpriv->comp, STMMAC_LB_TIMEOUT);
+		ret = tpriv->ok ? 0 : -ETIMEDOUT;
+		if (ret && !i) {
+			goto vlan_del;
+		} else if (!ret && i) {
+			ret = -EINVAL;
+			goto vlan_del;
+		} else {
+			ret = 0;
+		}
+
+		tpriv->ok = false;
+	}
+
+vlan_del:
+	vlan_vid_del(priv->dev, htons(ETH_P_8021Q), tpriv->vlan_id);
+cleanup:
+	dev_remove_pack(&tpriv->pt);
+	kfree(tpriv);
+	return ret;
+}
+
+static int stmmac_test_vlanfilt(struct stmmac_priv *priv)
+{
+	if (!priv->dma_cap.vlhash)
+		return -EOPNOTSUPP;
+
+	return __stmmac_test_vlanfilt(priv);
+}
+
+static int stmmac_test_vlanfilt_perfect(struct stmmac_priv *priv)
+{
+	int ret, prev_cap = priv->dma_cap.vlhash;
+
+	if (!(priv->dev->features & NETIF_F_HW_VLAN_CTAG_FILTER))
+		return -EOPNOTSUPP;
+
+	priv->dma_cap.vlhash = 0;
+	ret = __stmmac_test_vlanfilt(priv);
+	priv->dma_cap.vlhash = prev_cap;
+
+	return ret;
+}
+
+static int __stmmac_test_dvlanfilt(struct stmmac_priv *priv)
+{
+	struct stmmac_packet_attrs attr = { };
+	struct stmmac_test_priv *tpriv;
+	struct sk_buff *skb = NULL;
+	int ret = 0, i;
+
+	tpriv = kzalloc(sizeof(*tpriv), GFP_KERNEL);
+	if (!tpriv)
+		return -ENOMEM;
+
+	tpriv->ok = false;
+	tpriv->double_vlan = true;
+	init_completion(&tpriv->comp);
+
+	tpriv->pt.type = htons(ETH_P_8021Q);
+	tpriv->pt.func = stmmac_test_vlan_validate;
+	tpriv->pt.dev = priv->dev;
+	tpriv->pt.af_packet_priv = tpriv;
+	tpriv->packet = &attr;
+
+	/*
+	 * As we use HASH filtering, false positives may appear. This is a
+	 * specially chosen ID so that adjacent IDs (+4) have different
+	 * HASH values.
+	 */
+	tpriv->vlan_id = 0x123;
+	dev_add_pack(&tpriv->pt);
+
+	ret = vlan_vid_add(priv->dev, htons(ETH_P_8021AD), tpriv->vlan_id);
+	if (ret)
+		goto cleanup;
+
+	for (i = 0; i < 4; i++) {
+		attr.vlan = 2;
+		attr.vlan_id_out = tpriv->vlan_id + i;
+		attr.dst = priv->dev->dev_addr;
+		attr.sport = 9;
+		attr.dport = 9;
+
+		skb = stmmac_test_get_udp_skb(priv, &attr);
+		if (!skb) {
+			ret = -ENOMEM;
+			goto vlan_del;
+		}
+
+		ret = dev_direct_xmit(skb, 0);
+		if (ret)
+			goto vlan_del;
+
+		wait_for_completion_timeout(&tpriv->comp, STMMAC_LB_TIMEOUT);
+		ret = tpriv->ok ? 0 : -ETIMEDOUT;
+		if (ret && !i) {
+			goto vlan_del;
+		} else if (!ret && i) {
+			ret = -EINVAL;
+			goto vlan_del;
+		} else {
+			ret = 0;
+		}
+
+		tpriv->ok = false;
+	}
+
+vlan_del:
+	vlan_vid_del(priv->dev, htons(ETH_P_8021AD), tpriv->vlan_id);
+cleanup:
+	dev_remove_pack(&tpriv->pt);
+	kfree(tpriv);
+	return ret;
+}
+
+static int stmmac_test_dvlanfilt(struct stmmac_priv *priv)
+{
+	if (!priv->dma_cap.vlhash)
+		return -EOPNOTSUPP;
+
+	return __stmmac_test_dvlanfilt(priv);
+}
+
+static int stmmac_test_dvlanfilt_perfect(struct stmmac_priv *priv)
+{
+	int ret, prev_cap = priv->dma_cap.vlhash;
+
+	if (!(priv->dev->features & NETIF_F_HW_VLAN_STAG_FILTER))
+		return -EOPNOTSUPP;
+
+	priv->dma_cap.vlhash = 0;
+	ret = __stmmac_test_dvlanfilt(priv);
+	priv->dma_cap.vlhash = prev_cap;
+
+	return ret;
+}
+
+#ifdef CONFIG_NET_CLS_ACT
+static int stmmac_test_rxp(struct stmmac_priv *priv)
+{
+	unsigned char addr[ETH_ALEN] = {0xde, 0xad, 0xbe, 0xef, 0x00, 0x00};
+	struct tc_cls_u32_offload cls_u32 = { };
+	struct stmmac_packet_attrs attr = { };
+	struct tc_action **actions, *act;
+	struct tc_u32_sel *sel;
+	struct tcf_exts *exts;
+	int ret, i, nk = 1;
+
+	if (!tc_can_offload(priv->dev))
+		return -EOPNOTSUPP;
+	if (!priv->dma_cap.frpsel)
+		return -EOPNOTSUPP;
+
+	sel = kzalloc(struct_size(sel, keys, nk), GFP_KERNEL);
+	if (!sel)
+		return -ENOMEM;
+
+	exts = kzalloc(sizeof(*exts), GFP_KERNEL);
+	if (!exts) {
+		ret = -ENOMEM;
+		goto cleanup_sel;
+	}
+
+	actions = kzalloc(nk * sizeof(*actions), GFP_KERNEL);
+	if (!actions) {
+		ret = -ENOMEM;
+		goto cleanup_exts;
+	}
+
+	act = kzalloc(nk * sizeof(*act), GFP_KERNEL);
+	if (!act) {
+		ret = -ENOMEM;
+		goto cleanup_actions;
+	}
+
+	cls_u32.command = TC_CLSU32_NEW_KNODE;
+	cls_u32.common.chain_index = 0;
+	cls_u32.common.protocol = htons(ETH_P_ALL);
+	cls_u32.knode.exts = exts;
+	cls_u32.knode.sel = sel;
+	cls_u32.knode.handle = 0x123;
+
+	exts->nr_actions = nk;
+	exts->actions = actions;
+	for (i = 0; i < nk; i++) {
+		struct tcf_gact *gact = to_gact(&act[i]);
+
+		actions[i] = &act[i];
+		gact->tcf_action = TC_ACT_SHOT;
+	}
+
+	sel->nkeys = nk;
+	sel->offshift = 0;
+	sel->keys[0].off = 6;
+	sel->keys[0].val = htonl(0xdeadbeef);
+	sel->keys[0].mask = ~0x0;
+
+	ret = stmmac_tc_setup_cls_u32(priv, priv, &cls_u32);
+	if (ret)
+		goto cleanup_act;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.src = addr;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL; /* Shall NOT receive packet */
+
+	cls_u32.command = TC_CLSU32_DELETE_KNODE;
+	stmmac_tc_setup_cls_u32(priv, priv, &cls_u32);
+
+cleanup_act:
+	kfree(act);
+cleanup_actions:
+	kfree(actions);
+cleanup_exts:
+	kfree(exts);
+cleanup_sel:
+	kfree(sel);
+	return ret;
+}
+#else
+static int stmmac_test_rxp(struct stmmac_priv *priv)
+{
+	return -EOPNOTSUPP;
+}
+#endif
+
+static int stmmac_test_desc_sai(struct stmmac_priv *priv)
+{
+	unsigned char src[ETH_ALEN] = {0x00, 0x00, 0x00, 0x00, 0x00, 0x00};
+	struct stmmac_packet_attrs attr = { };
+	int ret;
+
+	if (!priv->dma_cap.vlins)
+		return -EOPNOTSUPP;
+
+	attr.remove_sa = true;
+	attr.sarc = true;
+	attr.src = src;
+	attr.dst = priv->dev->dev_addr;
+
+	priv->sarc_type = 0x1;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+
+	priv->sarc_type = 0x0;
+	return ret;
+}
+
+static int stmmac_test_desc_sar(struct stmmac_priv *priv)
+{
+	unsigned char src[ETH_ALEN] = {0x00, 0x00, 0x00, 0x00, 0x00, 0x00};
+	struct stmmac_packet_attrs attr = { };
+	int ret;
+
+	if (!priv->dma_cap.vlins)
+		return -EOPNOTSUPP;
+
+	attr.sarc = true;
+	attr.src = src;
+	attr.dst = priv->dev->dev_addr;
+
+	priv->sarc_type = 0x2;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+
+	priv->sarc_type = 0x0;
+	return ret;
+}
+
+static int stmmac_test_reg_sai(struct stmmac_priv *priv)
+{
+	unsigned char src[ETH_ALEN] = {0x00, 0x00, 0x00, 0x00, 0x00, 0x00};
+	struct stmmac_packet_attrs attr = { };
+	int ret;
+
+	if (!priv->dma_cap.vlins)
+		return -EOPNOTSUPP;
+
+	attr.remove_sa = true;
+	attr.sarc = true;
+	attr.src = src;
+	attr.dst = priv->dev->dev_addr;
+
+	if (stmmac_sarc_configure(priv, priv->ioaddr, 0x2))
+		return -EOPNOTSUPP;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+
+	stmmac_sarc_configure(priv, priv->ioaddr, 0x0);
+	return ret;
+}
+
+static int stmmac_test_reg_sar(struct stmmac_priv *priv)
+{
+	unsigned char src[ETH_ALEN] = {0x00, 0x00, 0x00, 0x00, 0x00, 0x00};
+	struct stmmac_packet_attrs attr = { };
+	int ret;
+
+	if (!priv->dma_cap.vlins)
+		return -EOPNOTSUPP;
+
+	attr.sarc = true;
+	attr.src = src;
+	attr.dst = priv->dev->dev_addr;
+
+	if (stmmac_sarc_configure(priv, priv->ioaddr, 0x3))
+		return -EOPNOTSUPP;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+
+	stmmac_sarc_configure(priv, priv->ioaddr, 0x0);
+	return ret;
+}
+
+static int stmmac_test_vlanoff_common(struct stmmac_priv *priv, bool svlan)
+{
+	struct stmmac_packet_attrs attr = { };
+	struct stmmac_test_priv *tpriv;
+	struct sk_buff *skb = NULL;
+	int ret = 0;
+	u16 proto;
+
+	if (!priv->dma_cap.vlins)
+		return -EOPNOTSUPP;
+
+	tpriv = kzalloc(sizeof(*tpriv), GFP_KERNEL);
+	if (!tpriv)
+		return -ENOMEM;
+
+	proto = svlan ? ETH_P_8021AD : ETH_P_8021Q;
+
+	tpriv->ok = false;
+	tpriv->double_vlan = svlan;
+	init_completion(&tpriv->comp);
+
+	tpriv->pt.type = svlan ? htons(ETH_P_8021Q) : htons(ETH_P_IP);
+	tpriv->pt.func = stmmac_test_vlan_validate;
+	tpriv->pt.dev = priv->dev;
+	tpriv->pt.af_packet_priv = tpriv;
+	tpriv->packet = &attr;
+	tpriv->vlan_id = 0x123;
+	dev_add_pack(&tpriv->pt);
+
+	ret = vlan_vid_add(priv->dev, htons(proto), tpriv->vlan_id);
+	if (ret)
+		goto cleanup;
+
+	attr.dst = priv->dev->dev_addr;
+
+	skb = stmmac_test_get_udp_skb(priv, &attr);
+	if (!skb) {
+		ret = -ENOMEM;
+		goto vlan_del;
+	}
+
+	__vlan_hwaccel_put_tag(skb, htons(proto), tpriv->vlan_id);
+	skb->protocol = htons(proto);
+
+	ret = dev_direct_xmit(skb, 0);
+	if (ret)
+		goto vlan_del;
+
+	wait_for_completion_timeout(&tpriv->comp, STMMAC_LB_TIMEOUT);
+	ret = tpriv->ok ? 0 : -ETIMEDOUT;
+
+vlan_del:
+	vlan_vid_del(priv->dev, htons(proto), tpriv->vlan_id);
+cleanup:
+	dev_remove_pack(&tpriv->pt);
+	kfree(tpriv);
+	return ret;
+}
+
+static int stmmac_test_vlanoff(struct stmmac_priv *priv)
+{
+	return stmmac_test_vlanoff_common(priv, false);
+}
+
+static int stmmac_test_svlanoff(struct stmmac_priv *priv)
+{
+	if (!priv->dma_cap.dvlan)
+		return -EOPNOTSUPP;
+	return stmmac_test_vlanoff_common(priv, true);
+}
+
+#ifdef CONFIG_NET_CLS_ACT
+static int __stmmac_test_l3filt(struct stmmac_priv *priv, u32 dst, u32 src,
+				u32 dst_mask, u32 src_mask)
+{
+	struct flow_dissector_key_ipv4_addrs key, mask;
+	unsigned long dummy_cookie = 0xdeadbeef;
+	struct stmmac_packet_attrs attr = { };
+	struct flow_dissector *dissector;
+	struct flow_cls_offload *cls;
+	int ret, old_enable = 0;
+	struct flow_rule *rule;
+
+	if (!tc_can_offload(priv->dev))
+		return -EOPNOTSUPP;
+	if (!priv->dma_cap.l3l4fnum)
+		return -EOPNOTSUPP;
+	if (priv->rss.enable) {
+		old_enable = priv->rss.enable;
+		priv->rss.enable = false;
+		stmmac_rss_configure(priv, priv->hw, NULL,
+				     priv->plat->rx_queues_to_use);
+	}
+
+	dissector = kzalloc(sizeof(*dissector), GFP_KERNEL);
+	if (!dissector) {
+		ret = -ENOMEM;
+		goto cleanup_rss;
+	}
+
+	dissector->used_keys |= (1 << FLOW_DISSECTOR_KEY_IPV4_ADDRS);
+	dissector->offset[FLOW_DISSECTOR_KEY_IPV4_ADDRS] = 0;
+
+	cls = kzalloc(sizeof(*cls), GFP_KERNEL);
+	if (!cls) {
+		ret = -ENOMEM;
+		goto cleanup_dissector;
+	}
+
+	cls->common.chain_index = 0;
+	cls->command = FLOW_CLS_REPLACE;
+	cls->cookie = dummy_cookie;
+
+	rule = kzalloc(struct_size(rule, action.entries, 1), GFP_KERNEL);
+	if (!rule) {
+		ret = -ENOMEM;
+		goto cleanup_cls;
+	}
+
+	rule->match.dissector = dissector;
+	rule->match.key = (void *)&key;
+	rule->match.mask = (void *)&mask;
+
+	key.src = htonl(src);
+	key.dst = htonl(dst);
+	mask.src = src_mask;
+	mask.dst = dst_mask;
+
+	cls->rule = rule;
+
+	rule->action.entries[0].id = FLOW_ACTION_DROP;
+	rule->action.entries[0].hw_stats = FLOW_ACTION_HW_STATS_ANY;
+	rule->action.num_entries = 1;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.ip_dst = dst;
+	attr.ip_src = src;
+
+	/* Shall receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto cleanup_rule;
+
+	ret = stmmac_tc_setup_cls(priv, priv, cls);
+	if (ret)
+		goto cleanup_rule;
+
+	/* Shall NOT receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL;
+
+	cls->command = FLOW_CLS_DESTROY;
+	stmmac_tc_setup_cls(priv, priv, cls);
+cleanup_rule:
+	kfree(rule);
+cleanup_cls:
+	kfree(cls);
+cleanup_dissector:
+	kfree(dissector);
+cleanup_rss:
+	if (old_enable) {
+		priv->rss.enable = old_enable;
+		stmmac_rss_configure(priv, priv->hw, &priv->rss,
+				     priv->plat->rx_queues_to_use);
+	}
+
+	return ret;
+}
+#else
+static int __stmmac_test_l3filt(struct stmmac_priv *priv, u32 dst, u32 src,
+				u32 dst_mask, u32 src_mask)
+{
+	return -EOPNOTSUPP;
+}
+#endif
+
+static int stmmac_test_l3filt_da(struct stmmac_priv *priv)
+{
+	u32 addr = 0x10203040;
+
+	return __stmmac_test_l3filt(priv, addr, 0, ~0, 0);
+}
+
+static int stmmac_test_l3filt_sa(struct stmmac_priv *priv)
+{
+	u32 addr = 0x10203040;
+
+	return __stmmac_test_l3filt(priv, 0, addr, 0, ~0);
+}
+
+#ifdef CONFIG_NET_CLS_ACT
+static int __stmmac_test_l4filt(struct stmmac_priv *priv, u32 dst, u32 src,
+				u32 dst_mask, u32 src_mask, bool udp)
+{
+	struct {
+		struct flow_dissector_key_basic bkey;
+		struct flow_dissector_key_ports key;
+	} __aligned(BITS_PER_LONG / 8) keys;
+	struct {
+		struct flow_dissector_key_basic bmask;
+		struct flow_dissector_key_ports mask;
+	} __aligned(BITS_PER_LONG / 8) masks;
+	unsigned long dummy_cookie = 0xdeadbeef;
+	struct stmmac_packet_attrs attr = { };
+	struct flow_dissector *dissector;
+	struct flow_cls_offload *cls;
+	int ret, old_enable = 0;
+	struct flow_rule *rule;
+
+	if (!tc_can_offload(priv->dev))
+		return -EOPNOTSUPP;
+	if (!priv->dma_cap.l3l4fnum)
+		return -EOPNOTSUPP;
+	if (priv->rss.enable) {
+		old_enable = priv->rss.enable;
+		priv->rss.enable = false;
+		stmmac_rss_configure(priv, priv->hw, NULL,
+				     priv->plat->rx_queues_to_use);
+	}
+
+	dissector = kzalloc(sizeof(*dissector), GFP_KERNEL);
+	if (!dissector) {
+		ret = -ENOMEM;
+		goto cleanup_rss;
+	}
+
+	dissector->used_keys |= (1 << FLOW_DISSECTOR_KEY_BASIC);
+	dissector->used_keys |= (1 << FLOW_DISSECTOR_KEY_PORTS);
+	dissector->offset[FLOW_DISSECTOR_KEY_BASIC] = 0;
+	dissector->offset[FLOW_DISSECTOR_KEY_PORTS] = offsetof(typeof(keys), key);
+
+	cls = kzalloc(sizeof(*cls), GFP_KERNEL);
+	if (!cls) {
+		ret = -ENOMEM;
+		goto cleanup_dissector;
+	}
+
+	cls->common.chain_index = 0;
+	cls->command = FLOW_CLS_REPLACE;
+	cls->cookie = dummy_cookie;
+
+	rule = kzalloc(struct_size(rule, action.entries, 1), GFP_KERNEL);
+	if (!rule) {
+		ret = -ENOMEM;
+		goto cleanup_cls;
+	}
+
+	rule->match.dissector = dissector;
+	rule->match.key = (void *)&keys;
+	rule->match.mask = (void *)&masks;
+
+	keys.bkey.ip_proto = udp ? IPPROTO_UDP : IPPROTO_TCP;
+	keys.key.src = htons(src);
+	keys.key.dst = htons(dst);
+	masks.mask.src = src_mask;
+	masks.mask.dst = dst_mask;
+
+	cls->rule = rule;
+
+	rule->action.entries[0].id = FLOW_ACTION_DROP;
+	rule->action.entries[0].hw_stats = FLOW_ACTION_HW_STATS_ANY;
+	rule->action.num_entries = 1;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.tcp = !udp;
+	attr.sport = src;
+	attr.dport = dst;
+	attr.ip_dst = 0;
+
+	/* Shall receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto cleanup_rule;
+
+	ret = stmmac_tc_setup_cls(priv, priv, cls);
+	if (ret)
+		goto cleanup_rule;
+
+	/* Shall NOT receive packet */
+	ret = __stmmac_test_loopback(priv, &attr);
+	ret = ret ? 0 : -EINVAL;
+
+	cls->command = FLOW_CLS_DESTROY;
+	stmmac_tc_setup_cls(priv, priv, cls);
+cleanup_rule:
+	kfree(rule);
+cleanup_cls:
+	kfree(cls);
+cleanup_dissector:
+	kfree(dissector);
+cleanup_rss:
+	if (old_enable) {
+		priv->rss.enable = old_enable;
+		stmmac_rss_configure(priv, priv->hw, &priv->rss,
+				     priv->plat->rx_queues_to_use);
+	}
+
+	return ret;
+}
+#else
+static int __stmmac_test_l4filt(struct stmmac_priv *priv, u32 dst, u32 src,
+				u32 dst_mask, u32 src_mask, bool udp)
+{
+	return -EOPNOTSUPP;
+}
+#endif
+
+static int stmmac_test_l4filt_da_tcp(struct stmmac_priv *priv)
+{
+	u16 dummy_port = 0x123;
+
+	return __stmmac_test_l4filt(priv, dummy_port, 0, ~0, 0, false);
+}
+
+static int stmmac_test_l4filt_sa_tcp(struct stmmac_priv *priv)
+{
+	u16 dummy_port = 0x123;
+
+	return __stmmac_test_l4filt(priv, 0, dummy_port, 0, ~0, false);
+}
+
+static int stmmac_test_l4filt_da_udp(struct stmmac_priv *priv)
+{
+	u16 dummy_port = 0x123;
+
+	return __stmmac_test_l4filt(priv, dummy_port, 0, ~0, 0, true);
+}
+
+static int stmmac_test_l4filt_sa_udp(struct stmmac_priv *priv)
+{
+	u16 dummy_port = 0x123;
+
+	return __stmmac_test_l4filt(priv, 0, dummy_port, 0, ~0, true);
+}
+
+static int stmmac_test_arp_validate(struct sk_buff *skb,
+				    struct net_device *ndev,
+				    struct packet_type *pt,
+				    struct net_device *orig_ndev)
+{
+	struct stmmac_test_priv *tpriv = pt->af_packet_priv;
+	struct ethhdr *ehdr;
+	struct arphdr *ahdr;
+
+	ehdr = (struct ethhdr *)skb_mac_header(skb);
+	if (!ether_addr_equal_unaligned(ehdr->h_dest, tpriv->packet->src))
+		goto out;
+
+	ahdr = arp_hdr(skb);
+	if (ahdr->ar_op != htons(ARPOP_REPLY))
+		goto out;
+
+	tpriv->ok = true;
+	complete(&tpriv->comp);
+out:
+	kfree_skb(skb);
+	return 0;
+}
+
+static int stmmac_test_arpoffload(struct stmmac_priv *priv)
+{
+	unsigned char src[ETH_ALEN] = {0x01, 0x02, 0x03, 0x04, 0x05, 0x06};
+	unsigned char dst[ETH_ALEN] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};
+	struct stmmac_packet_attrs attr = { };
+	struct stmmac_test_priv *tpriv;
+	struct sk_buff *skb = NULL;
+	u32 ip_addr = 0xdeadcafe;
+	u32 ip_src = 0xdeadbeef;
+	int ret;
+
+	if (!priv->dma_cap.arpoffsel)
+		return -EOPNOTSUPP;
+
+	tpriv = kzalloc(sizeof(*tpriv), GFP_KERNEL);
+	if (!tpriv)
+		return -ENOMEM;
+
+	tpriv->ok = false;
+	init_completion(&tpriv->comp);
+
+	tpriv->pt.type = htons(ETH_P_ARP);
+	tpriv->pt.func = stmmac_test_arp_validate;
+	tpriv->pt.dev = priv->dev;
+	tpriv->pt.af_packet_priv = tpriv;
+	tpriv->packet = &attr;
+	dev_add_pack(&tpriv->pt);
+
+	attr.src = src;
+	attr.ip_src = ip_src;
+	attr.dst = dst;
+	attr.ip_dst = ip_addr;
+
+	skb = stmmac_test_get_arp_skb(priv, &attr);
+	if (!skb) {
+		ret = -ENOMEM;
+		goto cleanup;
+	}
+
+	ret = stmmac_set_arp_offload(priv, priv->hw, true, ip_addr);
+	if (ret)
+		goto cleanup;
+
+	ret = dev_set_promiscuity(priv->dev, 1);
+	if (ret)
+		goto cleanup;
+
+	ret = dev_direct_xmit(skb, 0);
+	if (ret)
+		goto cleanup_promisc;
+
+	wait_for_completion_timeout(&tpriv->comp, STMMAC_LB_TIMEOUT);
+	ret = tpriv->ok ? 0 : -ETIMEDOUT;
+
+cleanup_promisc:
+	dev_set_promiscuity(priv->dev, -1);
+cleanup:
+	stmmac_set_arp_offload(priv, priv->hw, false, 0x0);
+	dev_remove_pack(&tpriv->pt);
+	kfree(tpriv);
+	return ret;
+}
+
+static int __stmmac_test_jumbo(struct stmmac_priv *priv, u16 queue)
+{
+	struct stmmac_packet_attrs attr = { };
+	int size = priv->dma_buf_sz;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.max_size = size - ETH_FCS_LEN;
+	attr.queue_mapping = queue;
+
+	return __stmmac_test_loopback(priv, &attr);
+}
+
+static int stmmac_test_jumbo(struct stmmac_priv *priv)
+{
+	return __stmmac_test_jumbo(priv, 0);
+}
+
+static int stmmac_test_mjumbo(struct stmmac_priv *priv)
+{
+	u32 chan, tx_cnt = priv->plat->tx_queues_to_use;
+	int ret;
+
+	if (tx_cnt <= 1)
+		return -EOPNOTSUPP;
+
+	for (chan = 0; chan < tx_cnt; chan++) {
+		ret = __stmmac_test_jumbo(priv, chan);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int stmmac_test_sph(struct stmmac_priv *priv)
+{
+	unsigned long cnt_end, cnt_start = priv->xstats.rx_split_hdr_pkt_n;
+	struct stmmac_packet_attrs attr = { };
+	int ret;
+
+	if (!priv->sph)
+		return -EOPNOTSUPP;
+
+	/* Check for UDP first */
+	attr.dst = priv->dev->dev_addr;
+	attr.tcp = false;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		return ret;
+
+	cnt_end = priv->xstats.rx_split_hdr_pkt_n;
+	if (cnt_end <= cnt_start)
+		return -EINVAL;
+
+	/* Check for TCP now */
+	cnt_start = cnt_end;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.tcp = true;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		return ret;
+
+	cnt_end = priv->xstats.rx_split_hdr_pkt_n;
+	if (cnt_end <= cnt_start)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int stmmac_test_tbs(struct stmmac_priv *priv)
+{
+#define STMMAC_TBS_LT_OFFSET		(500 * 1000 * 1000) /* 500 ms*/
+	struct stmmac_packet_attrs attr = { };
+	struct tc_etf_qopt_offload qopt;
+	u64 start_time, curr_time = 0;
+	unsigned long flags;
+	int ret, i;
+
+	if (!priv->hwts_tx_en)
+		return -EOPNOTSUPP;
+
+	/* Find first TBS enabled Queue, if any */
+	for (i = 0; i < priv->plat->tx_queues_to_use; i++)
+		if (priv->tx_queue[i].tbs & STMMAC_TBS_AVAIL)
+			break;
+
+	if (i >= priv->plat->tx_queues_to_use)
+		return -EOPNOTSUPP;
+
+	qopt.enable = true;
+	qopt.queue = i;
+
+	ret = stmmac_tc_setup_etf(priv, priv, &qopt);
+	if (ret)
+		return ret;
+
+	spin_lock_irqsave(&priv->ptp_lock, flags);
+	stmmac_get_systime(priv, priv->ptpaddr, &curr_time);
+	spin_unlock_irqrestore(&priv->ptp_lock, flags);
+
+	if (!curr_time) {
+		ret = -EOPNOTSUPP;
+		goto fail_disable;
+	}
+
+	start_time = curr_time;
+	curr_time += STMMAC_TBS_LT_OFFSET;
+
+	attr.dst = priv->dev->dev_addr;
+	attr.timestamp = curr_time;
+	attr.timeout = nsecs_to_jiffies(2 * STMMAC_TBS_LT_OFFSET);
+	attr.queue_mapping = i;
+
+	ret = __stmmac_test_loopback(priv, &attr);
+	if (ret)
+		goto fail_disable;
+
+	/* Check if expected time has elapsed */
+	spin_lock_irqsave(&priv->ptp_lock, flags);
+	stmmac_get_systime(priv, priv->ptpaddr, &curr_time);
+	spin_unlock_irqrestore(&priv->ptp_lock, flags);
+
+	if ((curr_time - start_time) < STMMAC_TBS_LT_OFFSET)
+		ret = -EINVAL;
+
+fail_disable:
+	qopt.enable = false;
+	stmmac_tc_setup_etf(priv, priv, &qopt);
+	return ret;
+}
+
+#define STMMAC_LOOPBACK_NONE	0
+#define STMMAC_LOOPBACK_MAC	1
+#define STMMAC_LOOPBACK_PHY	2
+
+static const struct stmmac_test {
+	char name[ETH_GSTRING_LEN];
+	int lb;
+	int (*fn)(struct stmmac_priv *priv);
+} stmmac_selftests[] = {
+	{
+		.name = "MAC Loopback               ",
+		.lb = STMMAC_LOOPBACK_MAC,
+		.fn = stmmac_test_mac_loopback,
+	}, {
+		.name = "PHY Loopback               ",
+		.lb = STMMAC_LOOPBACK_NONE, /* Test will handle it */
+		.fn = stmmac_test_phy_loopback,
+	}, {
+		.name = "MMC Counters               ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_mmc,
+	}, {
+		.name = "EEE                        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_eee,
+	}, {
+		.name = "Hash Filter MC             ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_hfilt,
+	}, {
+		.name = "Perfect Filter UC          ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_pfilt,
+	}, {
+		.name = "MC Filter                  ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_mcfilt,
+	}, {
+		.name = "UC Filter                  ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_ucfilt,
+	}, {
+		.name = "Flow Control               ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_flowctrl,
+	}, {
+		.name = "RSS                        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_rss,
+	}, {
+		.name = "VLAN Filtering             ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_vlanfilt,
+	}, {
+		.name = "VLAN Filtering (perf)      ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_vlanfilt_perfect,
+	}, {
+		.name = "Double VLAN Filter         ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_dvlanfilt,
+	}, {
+		.name = "Double VLAN Filter (perf)  ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_dvlanfilt_perfect,
+	}, {
+		.name = "Flexible RX Parser         ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_rxp,
+	}, {
+		.name = "SA Insertion (desc)        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_desc_sai,
+	}, {
+		.name = "SA Replacement (desc)      ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_desc_sar,
+	}, {
+		.name = "SA Insertion (reg)         ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_reg_sai,
+	}, {
+		.name = "SA Replacement (reg)       ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_reg_sar,
+	}, {
+		.name = "VLAN TX Insertion          ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_vlanoff,
+	}, {
+		.name = "SVLAN TX Insertion         ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_svlanoff,
+	}, {
+		.name = "L3 DA Filtering            ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_l3filt_da,
+	}, {
+		.name = "L3 SA Filtering            ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_l3filt_sa,
+	}, {
+		.name = "L4 DA TCP Filtering        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_l4filt_da_tcp,
+	}, {
+		.name = "L4 SA TCP Filtering        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_l4filt_sa_tcp,
+	}, {
+		.name = "L4 DA UDP Filtering        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_l4filt_da_udp,
+	}, {
+		.name = "L4 SA UDP Filtering        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_l4filt_sa_udp,
+	}, {
+		.name = "ARP Offload                ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_arpoffload,
+	}, {
+		.name = "Jumbo Frame                ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_jumbo,
+	}, {
+		.name = "Multichannel Jumbo         ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_mjumbo,
+	}, {
+		.name = "Split Header               ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_sph,
+	}, {
+		.name = "TBS (ETF Scheduler)        ",
+		.lb = STMMAC_LOOPBACK_PHY,
+		.fn = stmmac_test_tbs,
+	},
+};
+
+void stmmac_selftest_run(struct net_device *dev,
+			 struct ethtool_test *etest, u64 *buf)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int count = stmmac_selftest_get_count(priv);
+	int i, ret;
+
+	memset(buf, 0, sizeof(*buf) * count);
+	stmmac_test_next_id = 0;
+
+	if (etest->flags != ETH_TEST_FL_OFFLINE) {
+		netdev_err(priv->dev, "Only offline tests are supported\n");
+		etest->flags |= ETH_TEST_FL_FAILED;
+		return;
+	} else if (!netif_carrier_ok(dev)) {
+		netdev_err(priv->dev, "You need valid Link to execute tests\n");
+		etest->flags |= ETH_TEST_FL_FAILED;
+		return;
+	}
+
+	/* Wait for queues drain */
+	msleep(200);
+
+	for (i = 0; i < count; i++) {
+		ret = 0;
+
+		switch (stmmac_selftests[i].lb) {
+		case STMMAC_LOOPBACK_PHY:
+			ret = -EOPNOTSUPP;
+			if (dev->phydev)
+				ret = phy_loopback(dev->phydev, true);
+			if (!ret)
+				break;
+			/* Fallthrough */
+		case STMMAC_LOOPBACK_MAC:
+			ret = stmmac_set_mac_loopback(priv, priv->ioaddr, true);
+			break;
+		case STMMAC_LOOPBACK_NONE:
+			break;
+		default:
+			ret = -EOPNOTSUPP;
+			break;
+		}
+
+		/*
+		 * First tests will always be MAC / PHY loobpack. If any of
+		 * them is not supported we abort earlier.
+		 */
+		if (ret) {
+			netdev_err(priv->dev, "Loopback is not supported\n");
+			etest->flags |= ETH_TEST_FL_FAILED;
+			break;
+		}
+
+		ret = stmmac_selftests[i].fn(priv);
+		if (ret && (ret != -EOPNOTSUPP))
+			etest->flags |= ETH_TEST_FL_FAILED;
+		buf[i] = ret;
+
+		switch (stmmac_selftests[i].lb) {
+		case STMMAC_LOOPBACK_PHY:
+			ret = -EOPNOTSUPP;
+			if (dev->phydev)
+				ret = phy_loopback(dev->phydev, false);
+			if (!ret)
+				break;
+			/* Fallthrough */
+		case STMMAC_LOOPBACK_MAC:
+			stmmac_set_mac_loopback(priv, priv->ioaddr, false);
+			break;
+		default:
+			break;
+		}
+	}
+}
+
+void stmmac_selftest_get_strings(struct stmmac_priv *priv, u8 *data)
+{
+	u8 *p = data;
+	int i;
+
+	for (i = 0; i < stmmac_selftest_get_count(priv); i++) {
+		snprintf(p, ETH_GSTRING_LEN, "%2d. %s", i + 1,
+			 stmmac_selftests[i].name);
+		p += ETH_GSTRING_LEN;
+	}
+}
+
+int stmmac_selftest_get_count(struct stmmac_priv *priv)
+{
+	return ARRAY_SIZE(stmmac_selftests);
+}
diff -Naur a/net/rtnet/drivers/orange-pi-one/stmmac_tc.c b/net/rtnet/drivers/orange-pi-one/stmmac_tc.c
--- a/net/rtnet/drivers/orange-pi-one/stmmac_tc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/orange-pi-one/stmmac_tc.c	2021-07-14 15:39:13.310124970 +0300
@@ -0,0 +1,766 @@
+// SPDX-License-Identifier: (GPL-2.0 OR MIT)
+/*
+ * Copyright (c) 2018 Synopsys, Inc. and/or its affiliates.
+ * stmmac TC Handling (HW only)
+ */
+
+#include <net/pkt_cls.h>
+#include <net/tc_act/tc_gact.h>
+#include "common.h"
+#include "dwmac4.h"
+#include "dwmac5.h"
+#include "stmmac.h"
+
+static void tc_fill_all_pass_entry(struct stmmac_tc_entry *entry)
+{
+	memset(entry, 0, sizeof(*entry));
+	entry->in_use = true;
+	entry->is_last = true;
+	entry->is_frag = false;
+	entry->prio = ~0x0;
+	entry->handle = 0;
+	entry->val.match_data = 0x0;
+	entry->val.match_en = 0x0;
+	entry->val.af = 1;
+	entry->val.dma_ch_no = 0x0;
+}
+
+static struct stmmac_tc_entry *tc_find_entry(struct stmmac_priv *priv,
+					     struct tc_cls_u32_offload *cls,
+					     bool free)
+{
+	struct stmmac_tc_entry *entry, *first = NULL, *dup = NULL;
+	u32 loc = cls->knode.handle;
+	int i;
+
+	for (i = 0; i < priv->tc_entries_max; i++) {
+		entry = &priv->tc_entries[i];
+		if (!entry->in_use && !first && free)
+			first = entry;
+		if ((entry->handle == loc) && !free && !entry->is_frag)
+			dup = entry;
+	}
+
+	if (dup)
+		return dup;
+	if (first) {
+		first->handle = loc;
+		first->in_use = true;
+
+		/* Reset HW values */
+		memset(&first->val, 0, sizeof(first->val));
+	}
+
+	return first;
+}
+
+static int tc_fill_actions(struct stmmac_tc_entry *entry,
+			   struct stmmac_tc_entry *frag,
+			   struct tc_cls_u32_offload *cls)
+{
+	struct stmmac_tc_entry *action_entry = entry;
+	const struct tc_action *act;
+	struct tcf_exts *exts;
+	int i;
+
+	exts = cls->knode.exts;
+	if (!tcf_exts_has_actions(exts))
+		return -EINVAL;
+	if (frag)
+		action_entry = frag;
+
+	tcf_exts_for_each_action(i, act, exts) {
+		/* Accept */
+		if (is_tcf_gact_ok(act)) {
+			action_entry->val.af = 1;
+			break;
+		}
+		/* Drop */
+		if (is_tcf_gact_shot(act)) {
+			action_entry->val.rf = 1;
+			break;
+		}
+
+		/* Unsupported */
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int tc_fill_entry(struct stmmac_priv *priv,
+			 struct tc_cls_u32_offload *cls)
+{
+	struct stmmac_tc_entry *entry, *frag = NULL;
+	struct tc_u32_sel *sel = cls->knode.sel;
+	u32 off, data, mask, real_off, rem;
+	u32 prio = cls->common.prio << 16;
+	int ret;
+
+	/* Only 1 match per entry */
+	if (sel->nkeys <= 0 || sel->nkeys > 1)
+		return -EINVAL;
+
+	off = sel->keys[0].off << sel->offshift;
+	data = sel->keys[0].val;
+	mask = sel->keys[0].mask;
+
+	switch (ntohs(cls->common.protocol)) {
+	case ETH_P_ALL:
+		break;
+	case ETH_P_IP:
+		off += ETH_HLEN;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	if (off > priv->tc_off_max)
+		return -EINVAL;
+
+	real_off = off / 4;
+	rem = off % 4;
+
+	entry = tc_find_entry(priv, cls, true);
+	if (!entry)
+		return -EINVAL;
+
+	if (rem) {
+		frag = tc_find_entry(priv, cls, true);
+		if (!frag) {
+			ret = -EINVAL;
+			goto err_unuse;
+		}
+
+		entry->frag_ptr = frag;
+		entry->val.match_en = (mask << (rem * 8)) &
+			GENMASK(31, rem * 8);
+		entry->val.match_data = (data << (rem * 8)) &
+			GENMASK(31, rem * 8);
+		entry->val.frame_offset = real_off;
+		entry->prio = prio;
+
+		frag->val.match_en = (mask >> (rem * 8)) &
+			GENMASK(rem * 8 - 1, 0);
+		frag->val.match_data = (data >> (rem * 8)) &
+			GENMASK(rem * 8 - 1, 0);
+		frag->val.frame_offset = real_off + 1;
+		frag->prio = prio;
+		frag->is_frag = true;
+	} else {
+		entry->frag_ptr = NULL;
+		entry->val.match_en = mask;
+		entry->val.match_data = data;
+		entry->val.frame_offset = real_off;
+		entry->prio = prio;
+	}
+
+	ret = tc_fill_actions(entry, frag, cls);
+	if (ret)
+		goto err_unuse;
+
+	return 0;
+
+err_unuse:
+	if (frag)
+		frag->in_use = false;
+	entry->in_use = false;
+	return ret;
+}
+
+static void tc_unfill_entry(struct stmmac_priv *priv,
+			    struct tc_cls_u32_offload *cls)
+{
+	struct stmmac_tc_entry *entry;
+
+	entry = tc_find_entry(priv, cls, false);
+	if (!entry)
+		return;
+
+	entry->in_use = false;
+	if (entry->frag_ptr) {
+		entry = entry->frag_ptr;
+		entry->is_frag = false;
+		entry->in_use = false;
+	}
+}
+
+static int tc_config_knode(struct stmmac_priv *priv,
+			   struct tc_cls_u32_offload *cls)
+{
+	int ret;
+
+	ret = tc_fill_entry(priv, cls);
+	if (ret)
+		return ret;
+
+	ret = stmmac_rxp_config(priv, priv->hw->pcsr, priv->tc_entries,
+			priv->tc_entries_max);
+	if (ret)
+		goto err_unfill;
+
+	return 0;
+
+err_unfill:
+	tc_unfill_entry(priv, cls);
+	return ret;
+}
+
+static int tc_delete_knode(struct stmmac_priv *priv,
+			   struct tc_cls_u32_offload *cls)
+{
+	int ret;
+
+	/* Set entry and fragments as not used */
+	tc_unfill_entry(priv, cls);
+
+	ret = stmmac_rxp_config(priv, priv->hw->pcsr, priv->tc_entries,
+			priv->tc_entries_max);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int tc_setup_cls_u32(struct stmmac_priv *priv,
+			    struct tc_cls_u32_offload *cls)
+{
+	switch (cls->command) {
+	case TC_CLSU32_REPLACE_KNODE:
+		tc_unfill_entry(priv, cls);
+		/* Fall through */
+	case TC_CLSU32_NEW_KNODE:
+		return tc_config_knode(priv, cls);
+	case TC_CLSU32_DELETE_KNODE:
+		return tc_delete_knode(priv, cls);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int tc_init(struct stmmac_priv *priv)
+{
+	struct dma_features *dma_cap = &priv->dma_cap;
+	unsigned int count;
+	int i;
+
+	if (dma_cap->l3l4fnum) {
+		priv->flow_entries_max = dma_cap->l3l4fnum;
+		priv->flow_entries = devm_kcalloc(priv->device,
+						  dma_cap->l3l4fnum,
+						  sizeof(*priv->flow_entries),
+						  GFP_KERNEL);
+		if (!priv->flow_entries)
+			return -ENOMEM;
+
+		for (i = 0; i < priv->flow_entries_max; i++)
+			priv->flow_entries[i].idx = i;
+
+		dev_info(priv->device, "Enabled Flow TC (entries=%d)\n",
+			 priv->flow_entries_max);
+	}
+
+	/* Fail silently as we can still use remaining features, e.g. CBS */
+	if (!dma_cap->frpsel)
+		return 0;
+
+	switch (dma_cap->frpbs) {
+	case 0x0:
+		priv->tc_off_max = 64;
+		break;
+	case 0x1:
+		priv->tc_off_max = 128;
+		break;
+	case 0x2:
+		priv->tc_off_max = 256;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (dma_cap->frpes) {
+	case 0x0:
+		count = 64;
+		break;
+	case 0x1:
+		count = 128;
+		break;
+	case 0x2:
+		count = 256;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	/* Reserve one last filter which lets all pass */
+	priv->tc_entries_max = count;
+	priv->tc_entries = devm_kcalloc(priv->device,
+			count, sizeof(*priv->tc_entries), GFP_KERNEL);
+	if (!priv->tc_entries)
+		return -ENOMEM;
+
+	tc_fill_all_pass_entry(&priv->tc_entries[count - 1]);
+
+	dev_info(priv->device, "Enabling HW TC (entries=%d, max_off=%d)\n",
+			priv->tc_entries_max, priv->tc_off_max);
+	return 0;
+}
+
+static int tc_setup_cbs(struct stmmac_priv *priv,
+			struct tc_cbs_qopt_offload *qopt)
+{
+	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+	u32 queue = qopt->queue;
+	u32 ptr, speed_div;
+	u32 mode_to_use;
+	u64 value;
+	int ret;
+
+	/* Queue 0 is not AVB capable */
+	if (queue <= 0 || queue >= tx_queues_count)
+		return -EINVAL;
+	if (!priv->dma_cap.av)
+		return -EOPNOTSUPP;
+
+	mode_to_use = priv->plat->tx_queues_cfg[queue].mode_to_use;
+	if (mode_to_use == MTL_QUEUE_DCB && qopt->enable) {
+		ret = stmmac_dma_qmode(priv, priv->ioaddr, queue, MTL_QUEUE_AVB);
+		if (ret)
+			return ret;
+
+		priv->plat->tx_queues_cfg[queue].mode_to_use = MTL_QUEUE_AVB;
+	} else if (!qopt->enable) {
+		return stmmac_dma_qmode(priv, priv->ioaddr, queue, MTL_QUEUE_DCB);
+	}
+
+	/* Port Transmit Rate and Speed Divider */
+	ptr = (priv->speed == SPEED_100) ? 4 : 8;
+	speed_div = (priv->speed == SPEED_100) ? 100000 : 1000000;
+
+	/* Final adjustments for HW */
+	value = div_s64(qopt->idleslope * 1024ll * ptr, speed_div);
+	priv->plat->tx_queues_cfg[queue].idle_slope = value & GENMASK(31, 0);
+
+	value = div_s64(-qopt->sendslope * 1024ll * ptr, speed_div);
+	priv->plat->tx_queues_cfg[queue].send_slope = value & GENMASK(31, 0);
+
+	value = qopt->hicredit * 1024ll * 8;
+	priv->plat->tx_queues_cfg[queue].high_credit = value & GENMASK(31, 0);
+
+	value = qopt->locredit * 1024ll * 8;
+	priv->plat->tx_queues_cfg[queue].low_credit = value & GENMASK(31, 0);
+
+	ret = stmmac_config_cbs(priv, priv->hw,
+				priv->plat->tx_queues_cfg[queue].send_slope,
+				priv->plat->tx_queues_cfg[queue].idle_slope,
+				priv->plat->tx_queues_cfg[queue].high_credit,
+				priv->plat->tx_queues_cfg[queue].low_credit,
+				queue);
+	if (ret)
+		return ret;
+
+	dev_info(priv->device, "CBS queue %d: send %d, idle %d, hi %d, lo %d\n",
+			queue, qopt->sendslope, qopt->idleslope,
+			qopt->hicredit, qopt->locredit);
+	return 0;
+}
+
+static int tc_parse_flow_actions(struct stmmac_priv *priv,
+				 struct flow_action *action,
+				 struct stmmac_flow_entry *entry,
+				 struct netlink_ext_ack *extack)
+{
+	struct flow_action_entry *act;
+	int i;
+
+	if (!flow_action_has_entries(action))
+		return -EINVAL;
+
+	if (!flow_action_basic_hw_stats_check(action, extack))
+		return -EOPNOTSUPP;
+
+	flow_action_for_each(i, act, action) {
+		switch (act->id) {
+		case FLOW_ACTION_DROP:
+			entry->action |= STMMAC_FLOW_ACTION_DROP;
+			return 0;
+		default:
+			break;
+		}
+	}
+
+	/* Nothing to do, maybe inverse filter ? */
+	return 0;
+}
+
+static int tc_add_basic_flow(struct stmmac_priv *priv,
+			     struct flow_cls_offload *cls,
+			     struct stmmac_flow_entry *entry)
+{
+	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+	struct flow_dissector *dissector = rule->match.dissector;
+	struct flow_match_basic match;
+
+	/* Nothing to do here */
+	if (!dissector_uses_key(dissector, FLOW_DISSECTOR_KEY_BASIC))
+		return -EINVAL;
+
+	flow_rule_match_basic(rule, &match);
+	entry->ip_proto = match.key->ip_proto;
+	return 0;
+}
+
+static int tc_add_ip4_flow(struct stmmac_priv *priv,
+			   struct flow_cls_offload *cls,
+			   struct stmmac_flow_entry *entry)
+{
+	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+	struct flow_dissector *dissector = rule->match.dissector;
+	bool inv = entry->action & STMMAC_FLOW_ACTION_DROP;
+	struct flow_match_ipv4_addrs match;
+	u32 hw_match;
+	int ret;
+
+	/* Nothing to do here */
+	if (!dissector_uses_key(dissector, FLOW_DISSECTOR_KEY_IPV4_ADDRS))
+		return -EINVAL;
+
+	flow_rule_match_ipv4_addrs(rule, &match);
+	hw_match = ntohl(match.key->src) & ntohl(match.mask->src);
+	if (hw_match) {
+		ret = stmmac_config_l3_filter(priv, priv->hw, entry->idx, true,
+					      false, true, inv, hw_match);
+		if (ret)
+			return ret;
+	}
+
+	hw_match = ntohl(match.key->dst) & ntohl(match.mask->dst);
+	if (hw_match) {
+		ret = stmmac_config_l3_filter(priv, priv->hw, entry->idx, true,
+					      false, false, inv, hw_match);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int tc_add_ports_flow(struct stmmac_priv *priv,
+			     struct flow_cls_offload *cls,
+			     struct stmmac_flow_entry *entry)
+{
+	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+	struct flow_dissector *dissector = rule->match.dissector;
+	bool inv = entry->action & STMMAC_FLOW_ACTION_DROP;
+	struct flow_match_ports match;
+	u32 hw_match;
+	bool is_udp;
+	int ret;
+
+	/* Nothing to do here */
+	if (!dissector_uses_key(dissector, FLOW_DISSECTOR_KEY_PORTS))
+		return -EINVAL;
+
+	switch (entry->ip_proto) {
+	case IPPROTO_TCP:
+		is_udp = false;
+		break;
+	case IPPROTO_UDP:
+		is_udp = true;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	flow_rule_match_ports(rule, &match);
+
+	hw_match = ntohs(match.key->src) & ntohs(match.mask->src);
+	if (hw_match) {
+		ret = stmmac_config_l4_filter(priv, priv->hw, entry->idx, true,
+					      is_udp, true, inv, hw_match);
+		if (ret)
+			return ret;
+	}
+
+	hw_match = ntohs(match.key->dst) & ntohs(match.mask->dst);
+	if (hw_match) {
+		ret = stmmac_config_l4_filter(priv, priv->hw, entry->idx, true,
+					      is_udp, false, inv, hw_match);
+		if (ret)
+			return ret;
+	}
+
+	entry->is_l4 = true;
+	return 0;
+}
+
+static struct stmmac_flow_entry *tc_find_flow(struct stmmac_priv *priv,
+					      struct flow_cls_offload *cls,
+					      bool get_free)
+{
+	int i;
+
+	for (i = 0; i < priv->flow_entries_max; i++) {
+		struct stmmac_flow_entry *entry = &priv->flow_entries[i];
+
+		if (entry->cookie == cls->cookie)
+			return entry;
+		if (get_free && (entry->in_use == false))
+			return entry;
+	}
+
+	return NULL;
+}
+
+static struct {
+	int (*fn)(struct stmmac_priv *priv, struct flow_cls_offload *cls,
+		  struct stmmac_flow_entry *entry);
+} tc_flow_parsers[] = {
+	{ .fn = tc_add_basic_flow },
+	{ .fn = tc_add_ip4_flow },
+	{ .fn = tc_add_ports_flow },
+};
+
+static int tc_add_flow(struct stmmac_priv *priv,
+		       struct flow_cls_offload *cls)
+{
+	struct stmmac_flow_entry *entry = tc_find_flow(priv, cls, false);
+	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+	int i, ret;
+
+	if (!entry) {
+		entry = tc_find_flow(priv, cls, true);
+		if (!entry)
+			return -ENOENT;
+	}
+
+	ret = tc_parse_flow_actions(priv, &rule->action, entry,
+				    cls->common.extack);
+	if (ret)
+		return ret;
+
+	for (i = 0; i < ARRAY_SIZE(tc_flow_parsers); i++) {
+		ret = tc_flow_parsers[i].fn(priv, cls, entry);
+		if (!ret) {
+			entry->in_use = true;
+			continue;
+		}
+	}
+
+	if (!entry->in_use)
+		return -EINVAL;
+
+	entry->cookie = cls->cookie;
+	return 0;
+}
+
+static int tc_del_flow(struct stmmac_priv *priv,
+		       struct flow_cls_offload *cls)
+{
+	struct stmmac_flow_entry *entry = tc_find_flow(priv, cls, false);
+	int ret;
+
+	if (!entry || !entry->in_use)
+		return -ENOENT;
+
+	if (entry->is_l4) {
+		ret = stmmac_config_l4_filter(priv, priv->hw, entry->idx, false,
+					      false, false, false, 0);
+	} else {
+		ret = stmmac_config_l3_filter(priv, priv->hw, entry->idx, false,
+					      false, false, false, 0);
+	}
+
+	entry->in_use = false;
+	entry->cookie = 0;
+	entry->is_l4 = false;
+	return ret;
+}
+
+static int tc_setup_cls(struct stmmac_priv *priv,
+			struct flow_cls_offload *cls)
+{
+	int ret = 0;
+
+	/* When RSS is enabled, the filtering will be bypassed */
+	if (priv->rss.enable)
+		return -EBUSY;
+
+	switch (cls->command) {
+	case FLOW_CLS_REPLACE:
+		ret = tc_add_flow(priv, cls);
+		break;
+	case FLOW_CLS_DESTROY:
+		ret = tc_del_flow(priv, cls);
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return ret;
+}
+
+static int tc_setup_taprio(struct stmmac_priv *priv,
+			   struct tc_taprio_qopt_offload *qopt)
+{
+	u32 size, wid = priv->dma_cap.estwid, dep = priv->dma_cap.estdep;
+	struct plat_stmmacenet_data *plat = priv->plat;
+	struct timespec64 time;
+	bool fpe = false;
+	int i, ret = 0;
+	u64 ctr;
+
+	if (!priv->dma_cap.estsel)
+		return -EOPNOTSUPP;
+
+	switch (wid) {
+	case 0x1:
+		wid = 16;
+		break;
+	case 0x2:
+		wid = 20;
+		break;
+	case 0x3:
+		wid = 24;
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	switch (dep) {
+	case 0x1:
+		dep = 64;
+		break;
+	case 0x2:
+		dep = 128;
+		break;
+	case 0x3:
+		dep = 256;
+		break;
+	case 0x4:
+		dep = 512;
+		break;
+	case 0x5:
+		dep = 1024;
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	if (!qopt->enable)
+		goto disable;
+	if (qopt->num_entries >= dep)
+		return -EINVAL;
+	if (!qopt->base_time)
+		return -ERANGE;
+	if (!qopt->cycle_time)
+		return -ERANGE;
+
+	if (!plat->est) {
+		plat->est = devm_kzalloc(priv->device, sizeof(*plat->est),
+					 GFP_KERNEL);
+		if (!plat->est)
+			return -ENOMEM;
+	} else {
+		memset(plat->est, 0, sizeof(*plat->est));
+	}
+
+	size = qopt->num_entries;
+
+	priv->plat->est->gcl_size = size;
+	priv->plat->est->enable = qopt->enable;
+
+	for (i = 0; i < size; i++) {
+		s64 delta_ns = qopt->entries[i].interval;
+		u32 gates = qopt->entries[i].gate_mask;
+
+		if (delta_ns > GENMASK(wid, 0))
+			return -ERANGE;
+		if (gates > GENMASK(31 - wid, 0))
+			return -ERANGE;
+
+		switch (qopt->entries[i].command) {
+		case TC_TAPRIO_CMD_SET_GATES:
+			if (fpe)
+				return -EINVAL;
+			break;
+		case TC_TAPRIO_CMD_SET_AND_HOLD:
+			gates |= BIT(0);
+			fpe = true;
+			break;
+		case TC_TAPRIO_CMD_SET_AND_RELEASE:
+			gates &= ~BIT(0);
+			fpe = true;
+			break;
+		default:
+			return -EOPNOTSUPP;
+		}
+
+		priv->plat->est->gcl[i] = delta_ns | (gates << wid);
+	}
+
+	/* Adjust for real system time */
+	time = ktime_to_timespec64(qopt->base_time);
+	priv->plat->est->btr[0] = (u32)time.tv_nsec;
+	priv->plat->est->btr[1] = (u32)time.tv_sec;
+
+	ctr = qopt->cycle_time;
+	priv->plat->est->ctr[0] = do_div(ctr, NSEC_PER_SEC);
+	priv->plat->est->ctr[1] = (u32)ctr;
+
+	if (fpe && !priv->dma_cap.fpesel)
+		return -EOPNOTSUPP;
+
+	ret = stmmac_fpe_configure(priv, priv->ioaddr,
+				   priv->plat->tx_queues_to_use,
+				   priv->plat->rx_queues_to_use, fpe);
+	if (ret && fpe) {
+		netdev_err(priv->dummy, "failed to enable Frame Preemption\n");
+		return ret;
+	}
+
+	ret = stmmac_est_configure(priv, priv->ioaddr, priv->plat->est,
+				   priv->plat->clk_ptp_rate);
+	if (ret) {
+		netdev_err(priv->dummy, "failed to configure EST\n");
+		goto disable;
+	}
+
+	netdev_info(priv->dummy,  "configured EST\n");
+	return 0;
+
+disable:
+	priv->plat->est->enable = false;
+	stmmac_est_configure(priv, priv->ioaddr, priv->plat->est,
+			     priv->plat->clk_ptp_rate);
+	return ret;
+}
+
+static int tc_setup_etf(struct stmmac_priv *priv,
+			struct tc_etf_qopt_offload *qopt)
+{
+	if (!priv->dma_cap.tbssel)
+		return -EOPNOTSUPP;
+	if (qopt->queue >= priv->plat->tx_queues_to_use)
+		return -EINVAL;
+	if (!(priv->tx_queue[qopt->queue].tbs & STMMAC_TBS_AVAIL))
+		return -EINVAL;
+
+	if (qopt->enable)
+		priv->tx_queue[qopt->queue].tbs |= STMMAC_TBS_EN;
+	else
+		priv->tx_queue[qopt->queue].tbs &= ~STMMAC_TBS_EN;
+
+	netdev_info(priv->dummy,  "%s ETF for Queue %d\n",
+		    qopt->enable ? "enabled" : "disabled", qopt->queue);
+	return 0;
+}
+
+const struct stmmac_tc_ops dwmac510_tc_ops = {
+	.init = tc_init,
+	.setup_cls_u32 = tc_setup_cls_u32,
+	.setup_cbs = tc_setup_cbs,
+	.setup_cls = tc_setup_cls,
+	.setup_taprio = tc_setup_taprio,
+	.setup_etf = tc_setup_etf,
+};
diff -Naur a/net/rtnet/drivers/realtek/8139too.c b/net/rtnet/drivers/realtek/8139too.c
--- a/net/rtnet/drivers/realtek/8139too.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/realtek/8139too.c	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,1727 @@
+/***
+ * rt_8139too.c - Realtime driver for
+ * for more information, look to end of file or '8139too.c'
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+ /*
+  * This Version was modified by Fabian Koch
+  * It includes a different implementation of the 'cards' module parameter
+  * we are using an array of integers to determine which cards to use
+  * for RTnet (e.g. cards=0,1,0)
+  *
+  * Thanks to Jan Kiszka for this idea
+  */
+
+#define DRV_NAME            "rt_8139too"
+#define DRV_VERSION         "0.9.24-rt0.7"
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/compiler.h>
+#include <linux/pci.h>
+#include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if.h>
+#include <linux/ethtool.h>
+#include <linux/rtnetlink.h>
+#include <linux/delay.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/completion.h>
+#include <linux/crc32.h>
+#include <linux/uaccess.h>
+#include <asm/io.h>
+
+/* *** RTnet *** */
+#include <rtnet_port.h>
+#include <rtnet_multiple_queues.h>
+
+#define MAX_UNITS               8
+#define DEFAULT_RX_POOL_SIZE    16
+
+static int cards[MAX_UNITS] = { [0 ... (MAX_UNITS-1)] = 1 };
+static int media[MAX_UNITS] = { [0 ... (MAX_UNITS-1)] = -1 };
+static unsigned int rx_pool_size = DEFAULT_RX_POOL_SIZE;
+module_param_array(cards, int, NULL, 0444);
+module_param_array(media, int, NULL, 0444);
+module_param(rx_pool_size, uint, 0444);
+MODULE_PARM_DESC(cards, "array of cards to be supported (e.g. 1,0,1)");
+MODULE_PARM_DESC(media, "8139too: Bits 4+9: force full duplex, bit 5: 100Mbps");
+MODULE_PARM_DESC(rx_pool_size, "number of receive buffers");
+
+/* *** RTnet *** */
+
+
+#define RTL8139_DRIVER_NAME   DRV_NAME " Fast Ethernet driver " DRV_VERSION
+#define PFX DRV_NAME ": "
+
+/* enable PIO instead of MMIO, if CONFIG_8139TOO_PIO is selected */
+/* *** RTnet ***
+#ifdef CONFIG_8139TOO_PIO
+#define USE_IO_OPS 1
+#endif
+ *** RTnet *** */
+
+/* Size of the in-memory receive ring. */
+#define RX_BUF_LEN_IDX        2        /* 0==8K, 1==16K, 2==32K, 3==64K */
+#define RX_BUF_LEN        (8192 << RX_BUF_LEN_IDX)
+#define RX_BUF_PAD        16
+#define RX_BUF_WRAP_PAD 2048 /* spare padding to handle lack of packet wrap */
+#define RX_BUF_TOT_LEN        (RX_BUF_LEN + RX_BUF_PAD + RX_BUF_WRAP_PAD)
+
+/* Number of Tx descriptor registers. */
+#define NUM_TX_DESC        4
+
+/* max supported ethernet frame size -- must be at least (rtdev->mtu+14+4).*/
+#define MAX_ETH_FRAME_SIZE        1536
+
+/* Size of the Tx bounce buffers -- must be at least (rtdev->mtu+14+4). */
+#define TX_BUF_SIZE        MAX_ETH_FRAME_SIZE
+#define TX_BUF_TOT_LEN        (TX_BUF_SIZE * NUM_TX_DESC)
+
+/* PCI Tuning Parameters
+   Threshold is bytes transferred to chip before transmission starts. */
+#define TX_FIFO_THRESH 256        /* In bytes, rounded down to 32 byte units. */
+
+/* The following settings are log_2(bytes)-4:  0 == 16 bytes .. 6==1024, 7==end of packet. */
+#define RX_FIFO_THRESH        7        /* Rx buffer level before first PCI xfer.  */
+#define RX_DMA_BURST        7        /* Maximum PCI burst, '6' is 1024 */
+#define TX_DMA_BURST        6        /* Maximum PCI burst, '6' is 1024 */
+#define TX_RETRY        8        /* 0-15.  retries = 16 + (TX_RETRY * 16) */
+
+/* Operational parameters that usually are not changed. */
+/* Time in jiffies before concluding the transmitter is hung. */
+#define TX_TIMEOUT  (6*HZ)
+
+
+enum {
+	HAS_MII_XCVR = 0x010000,
+	HAS_CHIP_XCVR = 0x020000,
+	HAS_LNK_CHNG = 0x040000,
+};
+
+#define RTL_MIN_IO_SIZE 0x80
+#define RTL8139B_IO_SIZE 256
+
+#define RTL8129_CAPS        HAS_MII_XCVR
+#define RTL8139_CAPS        HAS_CHIP_XCVR|HAS_LNK_CHNG
+
+typedef enum {
+	RTL8139 = 0,
+	RTL8139_CB,
+	SMC1211TX,
+	/*MPX5030,*/
+	DELTA8139,
+	ADDTRON8139,
+	DFE538TX,
+	DFE690TXD,
+	FE2000VX,
+	ALLIED8139,
+	RTL8129,
+} board_t;
+
+
+/* indexed by board_t, above */
+static struct {
+	const char *name;
+	u32 hw_flags;
+} board_info[] = {
+	{ "RealTek RTL8139", RTL8139_CAPS },
+	{ "RealTek RTL8129", RTL8129_CAPS },
+};
+
+
+static struct pci_device_id rtl8139_pci_tbl[] = {
+	{0x10ec, 0x8139, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x10ec, 0x8138, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1113, 0x1211, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1500, 0x1360, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x4033, 0x1360, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1186, 0x1300, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1186, 0x1340, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x13d1, 0xab06, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1259, 0xa117, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1259, 0xa11e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x14ea, 0xab06, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x14ea, 0xab07, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x11db, 0x1234, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1432, 0x9130, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x02ac, 0x1012, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x018a, 0x0106, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x126c, 0x1211, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1743, 0x8139, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x021b, 0x8139, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+
+#ifdef CONFIG_SH_SECUREEDGE5410
+	/* Bogus 8139 silicon reports 8129 without external PROM :-( */
+	{0x10ec, 0x8129, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+#endif
+#ifdef CONFIG_8139TOO_8129
+	{0x10ec, 0x8129, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8129 },
+#endif
+
+	/* some crazy cards report invalid vendor ids like
+	 * 0x0001 here.  The other ids are valid and constant,
+	 * so we simply don't match on the main vendor id.
+	 */
+	{PCI_ANY_ID, 0x8139, 0x10ec, 0x8139, 0, 0, RTL8139 },
+	{PCI_ANY_ID, 0x8139, 0x1186, 0x1300, 0, 0, RTL8139 },
+	{PCI_ANY_ID, 0x8139, 0x13d1, 0xab06, 0, 0, RTL8139 },
+
+	{0,}
+};
+MODULE_DEVICE_TABLE (pci, rtl8139_pci_tbl);
+
+/* The rest of these values should never change. */
+
+/* Symbolic offsets to registers. */
+enum RTL8139_registers {
+	MAC0 = 0,                /* Ethernet hardware address. */
+	MAR0 = 8,                /* Multicast filter. */
+	TxStatus0 = 0x10,        /* Transmit status (Four 32bit registers). */
+	TxAddr0 = 0x20,                /* Tx descriptors (also four 32bit). */
+	RxBuf = 0x30,
+	ChipCmd = 0x37,
+	RxBufPtr = 0x38,
+	RxBufAddr = 0x3A,
+	IntrMask = 0x3C,
+	IntrStatus = 0x3E,
+	TxConfig = 0x40,
+	ChipVersion = 0x43,
+	RxConfig = 0x44,
+	Timer = 0x48,                /* A general-purpose counter. */
+	RxMissed = 0x4C,        /* 24 bits valid, write clears. */
+	Cfg9346 = 0x50,
+	Config0 = 0x51,
+	Config1 = 0x52,
+	FlashReg = 0x54,
+	MediaStatus = 0x58,
+	Config3 = 0x59,
+	Config4 = 0x5A,                /* absent on RTL-8139A */
+	HltClk = 0x5B,
+	MultiIntr = 0x5C,
+	TxSummary = 0x60,
+	BasicModeCtrl = 0x62,
+	BasicModeStatus = 0x64,
+	NWayAdvert = 0x66,
+	NWayLPAR = 0x68,
+	NWayExpansion = 0x6A,
+	/* Undocumented registers, but required for proper operation. */
+	FIFOTMS = 0x70,                /* FIFO Control and test. */
+	CSCR = 0x74,                /* Chip Status and Configuration Register. */
+	PARA78 = 0x78,
+	PARA7c = 0x7c,                /* Magic transceiver parameter register. */
+	Config5 = 0xD8,                /* absent on RTL-8139A */
+};
+
+enum ClearBitMasks {
+	MultiIntrClear = 0xF000,
+	ChipCmdClear = 0xE2,
+	Config1Clear = (1<<7)|(1<<6)|(1<<3)|(1<<2)|(1<<1),
+};
+
+enum ChipCmdBits {
+	CmdReset = 0x10,
+	CmdRxEnb = 0x08,
+	CmdTxEnb = 0x04,
+	RxBufEmpty = 0x01,
+};
+
+/* Interrupt register bits, using my own meaningful names. */
+enum IntrStatusBits {
+	PCIErr = 0x8000,
+	PCSTimeout = 0x4000,
+	RxFIFOOver = 0x40,
+	RxUnderrun = 0x20,
+	RxOverflow = 0x10,
+	TxErr = 0x08,
+	TxOK = 0x04,
+	RxErr = 0x02,
+	RxOK = 0x01,
+
+	RxAckBits = RxFIFOOver | RxOverflow | RxOK,
+};
+
+enum TxStatusBits {
+	TxHostOwns = 0x2000,
+	TxUnderrun = 0x4000,
+	TxStatOK = 0x8000,
+	TxOutOfWindow = 0x20000000,
+	TxAborted = 0x40000000,
+	TxCarrierLost = 0x80000000,
+};
+enum RxStatusBits {
+	RxMulticast = 0x8000,
+	RxPhysical = 0x4000,
+	RxBroadcast = 0x2000,
+	RxBadSymbol = 0x0020,
+	RxRunt = 0x0010,
+	RxTooLong = 0x0008,
+	RxCRCErr = 0x0004,
+	RxBadAlign = 0x0002,
+	RxStatusOK = 0x0001,
+};
+
+/* Bits in RxConfig. */
+enum rx_mode_bits {
+	AcceptErr = 0x20,
+	AcceptRunt = 0x10,
+	AcceptBroadcast = 0x08,
+	AcceptMulticast = 0x04,
+	AcceptMyPhys = 0x02,
+	AcceptAllPhys = 0x01,
+};
+
+/* Bits in TxConfig. */
+enum tx_config_bits {
+
+	/* Interframe Gap Time. Only TxIFG96 doesn't violate IEEE 802.3 */
+	TxIFGShift = 24,
+	TxIFG84 = (0 << TxIFGShift),    /* 8.4us / 840ns (10 / 100Mbps) */
+	TxIFG88 = (1 << TxIFGShift),    /* 8.8us / 880ns (10 / 100Mbps) */
+	TxIFG92 = (2 << TxIFGShift),    /* 9.2us / 920ns (10 / 100Mbps) */
+	TxIFG96 = (3 << TxIFGShift),    /* 9.6us / 960ns (10 / 100Mbps) */
+
+	TxLoopBack = (1 << 18) | (1 << 17), /* enable loopback test mode */
+	TxCRC = (1 << 16),        /* DISABLE appending CRC to end of Tx packets */
+	TxClearAbt = (1 << 0),        /* Clear abort (WO) */
+	TxDMAShift = 8,                /* DMA burst value (0-7) is shifted this many bits */
+	TxRetryShift = 4,        /* TXRR value (0-15) is shifted this many bits */
+
+	TxVersionMask = 0x7C800000, /* mask out version bits 30-26, 23 */
+};
+
+/* Bits in Config1 */
+enum Config1Bits {
+	Cfg1_PM_Enable = 0x01,
+	Cfg1_VPD_Enable = 0x02,
+	Cfg1_PIO = 0x04,
+	Cfg1_MMIO = 0x08,
+	LWAKE = 0x10,                /* not on 8139, 8139A */
+	Cfg1_Driver_Load = 0x20,
+	Cfg1_LED0 = 0x40,
+	Cfg1_LED1 = 0x80,
+	SLEEP = (1 << 1),        /* only on 8139, 8139A */
+	PWRDN = (1 << 0),        /* only on 8139, 8139A */
+};
+
+/* Bits in Config3 */
+enum Config3Bits {
+	Cfg3_FBtBEn    = (1 << 0), /* 1 = Fast Back to Back */
+	Cfg3_FuncRegEn = (1 << 1), /* 1 = enable CardBus Function registers */
+	Cfg3_CLKRUN_En = (1 << 2), /* 1 = enable CLKRUN */
+	Cfg3_CardB_En  = (1 << 3), /* 1 = enable CardBus registers */
+	Cfg3_LinkUp    = (1 << 4), /* 1 = wake up on link up */
+	Cfg3_Magic     = (1 << 5), /* 1 = wake up on Magic Packet (tm) */
+	Cfg3_PARM_En   = (1 << 6), /* 0 = software can set twister parameters */
+	Cfg3_GNTSel    = (1 << 7), /* 1 = delay 1 clock from PCI GNT signal */
+};
+
+/* Bits in Config4 */
+enum Config4Bits {
+	LWPTN = (1 << 2),        /* not on 8139, 8139A */
+};
+
+/* Bits in Config5 */
+enum Config5Bits {
+	Cfg5_PME_STS     = (1 << 0), /* 1 = PCI reset resets PME_Status */
+	Cfg5_LANWake     = (1 << 1), /* 1 = enable LANWake signal */
+	Cfg5_LDPS        = (1 << 2), /* 0 = save power when link is down */
+	Cfg5_FIFOAddrPtr = (1 << 3), /* Realtek internal SRAM testing */
+	Cfg5_UWF         = (1 << 4), /* 1 = accept unicast wakeup frame */
+	Cfg5_MWF         = (1 << 5), /* 1 = accept multicast wakeup frame */
+	Cfg5_BWF         = (1 << 6), /* 1 = accept broadcast wakeup frame */
+};
+
+enum RxConfigBits {
+	/* rx fifo threshold */
+	RxCfgFIFOShift = 13,
+	RxCfgFIFONone = (7 << RxCfgFIFOShift),
+
+	/* Max DMA burst */
+	RxCfgDMAShift = 8,
+	RxCfgDMAUnlimited = (7 << RxCfgDMAShift),
+
+	/* rx ring buffer length */
+	RxCfgRcv8K = 0,
+	RxCfgRcv16K = (1 << 11),
+	RxCfgRcv32K = (1 << 12),
+	RxCfgRcv64K = (1 << 11) | (1 << 12),
+
+	/* Disable packet wrap at end of Rx buffer */
+	RxNoWrap = (1 << 7),
+};
+
+
+/* Twister tuning parameters from RealTek.
+   Completely undocumented, but required to tune bad links. */
+enum CSCRBits {
+	CSCR_LinkOKBit = 0x0400,
+	CSCR_LinkChangeBit = 0x0800,
+	CSCR_LinkStatusBits = 0x0f000,
+	CSCR_LinkDownOffCmd = 0x003c0,
+	CSCR_LinkDownCmd = 0x0f3c0,
+};
+
+
+enum Cfg9346Bits {
+	Cfg9346_Lock = 0x00,
+	Cfg9346_Unlock = 0xC0,
+};
+
+
+#define PARA78_default        0x78fa8388
+#define PARA7c_default        0xcb38de43        /* param[0][3] */
+#define PARA7c_xxx                0xcb38de43
+/*static const unsigned long param[4][4] = {
+	{0xcb39de43, 0xcb39ce43, 0xfb38de03, 0xcb38de43},
+	{0xcb39de43, 0xcb39ce43, 0xcb39ce83, 0xcb39ce83},
+	{0xcb39de43, 0xcb39ce43, 0xcb39ce83, 0xcb39ce83},
+	{0xbb39de43, 0xbb39ce43, 0xbb39ce83, 0xbb39ce83}
+};*/
+
+typedef enum {
+	CH_8139 = 0,
+	CH_8139_K,
+	CH_8139A,
+	CH_8139B,
+	CH_8130,
+	CH_8139C,
+} chip_t;
+
+enum chip_flags {
+	HasHltClk = (1 << 0),
+	HasLWake = (1 << 1),
+};
+
+
+/* directly indexed by chip_t, above */
+const static struct {
+	const char *name;
+	u8 version; /* from RTL8139C docs */
+	u32 flags;
+} rtl_chip_info[] = {
+	{ "RTL-8139",
+	  0x40,
+	  HasHltClk,
+	},
+
+	{ "RTL-8139 rev K",
+	  0x60,
+	  HasHltClk,
+	},
+
+	{ "RTL-8139A",
+	  0x70,
+	  HasHltClk, /* XXX undocumented? */
+	},
+
+	{ "RTL-8139A rev G",
+	  0x72,
+	  HasHltClk, /* XXX undocumented? */
+	},
+
+	{ "RTL-8139B",
+	  0x78,
+	  HasLWake,
+	},
+
+	{ "RTL-8130",
+	  0x7C,
+	  HasLWake,
+	},
+
+	{ "RTL-8139C",
+	  0x74,
+	  HasLWake,
+	},
+
+	{ "RTL-8100",
+	  0x7A,
+	  HasLWake,
+	 },
+
+	{ "RTL-8100B/8139D",
+	  0x75,
+	  HasHltClk /* XXX undocumented? */
+	  | HasLWake,
+	},
+
+	{ "RTL-8101",
+	  0x77,
+	  HasLWake,
+	},
+};
+
+struct rtl_extra_stats {
+	unsigned long early_rx;
+	unsigned long tx_buf_mapped;
+	unsigned long tx_timeouts;
+	unsigned long rx_lost_in_ring;
+};
+
+struct rtl8139_private {
+	void *mmio_addr;
+	int drv_flags;
+	struct pci_dev *pci_dev;
+	struct net_device_stats stats;
+	unsigned char *rx_ring;
+	unsigned int cur_rx;        /* Index into the Rx buffer of next Rx pkt. */
+	unsigned int tx_flag;
+	unsigned long cur_tx;
+	unsigned long dirty_tx;
+	unsigned char *tx_buf[NUM_TX_DESC];        /* Tx bounce buffers */
+	unsigned char *tx_bufs;        /* Tx bounce buffer region. */
+	dma_addr_t rx_ring_dma;
+	dma_addr_t tx_bufs_dma;
+	signed char phys[4];                /* MII device addresses. */
+	char twistie, twist_row, twist_col;        /* Twister tune state. */
+	unsigned int default_port:4;        /* Last rtdev->if_port value. */
+	unsigned int medialock:1;        /* Don't sense media type. */
+	raw_spinlock_t lock;
+	chip_t chipset;
+	pid_t thr_pid;
+	u32 rx_config;
+	struct rtl_extra_stats xstats;
+	int time_to_die;
+	struct mii_if_info mii;
+};
+
+MODULE_AUTHOR ("Jeff Garzik <jgarzik@mandrakesoft.com>");
+MODULE_DESCRIPTION ("RealTek RTL-8139 Fast Ethernet driver");
+MODULE_LICENSE("GPL");
+
+static int read_eeprom (void *ioaddr, int location, int addr_len);
+static int mdio_read (struct rtnet_device *rtdev, int phy_id, int location);
+static void mdio_write (struct rtnet_device *rtdev, int phy_id, int location, int val);
+
+
+static int rtl8139_open (struct rtnet_device *rtdev);
+static int rtl8139_close (struct rtnet_device *rtdev);
+static irqreturn_t rtl8139_interrupt(int irq, void *data);
+static int rtl8139_start_xmit (struct rtskb *skb, struct rtnet_device *rtdev);
+
+static int rtl8139_ioctl(struct rtnet_device *, struct ifreq *rq, int cmd);
+static struct net_device_stats *rtl8139_get_stats(struct rtnet_device*rtdev);
+
+static void rtl8139_init_ring (struct rtnet_device *rtdev);
+static void rtl8139_set_rx_mode (struct rtnet_device *rtdev);
+static void __set_rx_mode (struct rtnet_device *rtdev);
+static void rtl8139_hw_start (struct rtnet_device *rtdev);
+
+#ifdef USE_IO_OPS
+
+#define RTL_R8(reg)                inb (((unsigned long)ioaddr) + (reg))
+#define RTL_R16(reg)                inw (((unsigned long)ioaddr) + (reg))
+#define RTL_R32(reg)                inl (((unsigned long)ioaddr) + (reg))
+#define RTL_W8(reg, val8)        outb ((val8), ((unsigned long)ioaddr) + (reg))
+#define RTL_W16(reg, val16)        outw ((val16), ((unsigned long)ioaddr) + (reg))
+#define RTL_W32(reg, val32)        outl ((val32), ((unsigned long)ioaddr) + (reg))
+#define RTL_W8_F                RTL_W8
+#define RTL_W16_F                RTL_W16
+#define RTL_W32_F                RTL_W32
+#undef readb
+#undef readw
+#undef readl
+#undef writeb
+#undef writew
+#undef writel
+#define readb(addr) inb((unsigned long)(addr))
+#define readw(addr) inw((unsigned long)(addr))
+#define readl(addr) inl((unsigned long)(addr))
+#define writeb(val,addr) outb((val),(unsigned long)(addr))
+#define writew(val,addr) outw((val),(unsigned long)(addr))
+#define writel(val,addr) outl((val),(unsigned long)(addr))
+
+#else
+
+/* write MMIO register, with flush */
+/* Flush avoids rtl8139 bug w/ posted MMIO writes */
+#define RTL_W8_F(reg, val8)        do { writeb ((val8), ioaddr + (reg)); readb (ioaddr + (reg)); } while (0)
+#define RTL_W16_F(reg, val16)        do { writew ((val16), ioaddr + (reg)); readw (ioaddr + (reg)); } while (0)
+#define RTL_W32_F(reg, val32)        do { writel ((val32), ioaddr + (reg)); readl (ioaddr + (reg)); } while (0)
+
+
+#define MMIO_FLUSH_AUDIT_COMPLETE 1
+#if MMIO_FLUSH_AUDIT_COMPLETE
+
+/* write MMIO register */
+#define RTL_W8(reg, val8)        writeb ((val8), ioaddr + (reg))
+#define RTL_W16(reg, val16)        writew ((val16), ioaddr + (reg))
+#define RTL_W32(reg, val32)        writel ((val32), ioaddr + (reg))
+
+#else
+
+/* write MMIO register, then flush */
+#define RTL_W8                RTL_W8_F
+#define RTL_W16                RTL_W16_F
+#define RTL_W32                RTL_W32_F
+
+#endif /* MMIO_FLUSH_AUDIT_COMPLETE */
+
+/* read MMIO register */
+#define RTL_R8(reg)                readb (ioaddr + (reg))
+#define RTL_R16(reg)                readw (ioaddr + (reg))
+#define RTL_R32(reg)                readl (ioaddr + (reg))
+
+#endif /* USE_IO_OPS */
+
+
+static const u16 rtl8139_intr_mask =
+	PCIErr | PCSTimeout | RxUnderrun | RxOverflow | RxFIFOOver |
+	TxErr | TxOK | RxErr | RxOK;
+
+static const unsigned int rtl8139_rx_config =
+	RxCfgRcv32K | RxNoWrap |
+	(RX_FIFO_THRESH << RxCfgFIFOShift) |
+	(RX_DMA_BURST << RxCfgDMAShift);
+
+static const unsigned int rtl8139_tx_config =
+	TxIFG96 | (TX_DMA_BURST << TxDMAShift) | (TX_RETRY << TxRetryShift);
+
+
+
+
+static void rtl8139_chip_reset (void *ioaddr)
+{
+	int i;
+
+	/* Soft reset the chip. */
+	RTL_W8 (ChipCmd, CmdReset);
+
+	/* Check that the chip has finished the reset. */
+	for (i = 1000; i > 0; i--) {
+		barrier();
+		if ((RTL_R8 (ChipCmd) & CmdReset) == 0)
+			break;
+		udelay (10);
+	}
+}
+
+
+static int rtl8139_init_board (struct pci_dev *pdev,
+					 struct rtnet_device **dev_out)
+{
+	void *ioaddr;
+	struct rtnet_device *rtdev;
+	struct rtl8139_private *tp;
+	u8 tmp8;
+	int rc;
+	unsigned int i;
+#ifdef USE_IO_OPS
+	u32 pio_start, pio_end, pio_flags, pio_len;
+#endif
+	unsigned long mmio_start, mmio_flags, mmio_len;
+	u32 tmp;
+
+
+	*dev_out = NULL;
+
+	/* dev and rtdev->priv zeroed in alloc_etherdev */
+	rtdev=rt_alloc_etherdev(sizeof (struct rtl8139_private),
+				rx_pool_size + NUM_TX_DESC);
+	if (rtdev==NULL) {
+		printk (KERN_ERR "%s: Unable to alloc new net device\n", pci_name(pdev));
+		return -ENOMEM;
+	}
+	rtdev_alloc_name(rtdev, "rteth%d");
+
+	rt_rtdev_connect(rtdev, &RTDEV_manager);
+
+	rtdev->vers = RTDEV_VERS_2_0;
+	tp = rtdev->priv;
+	tp->pci_dev = pdev;
+
+	/* enable device (incl. PCI PM wakeup and hotplug setup) */
+	rc = pci_enable_device (pdev);
+	if (rc)
+		goto err_out;
+
+	rc = pci_request_regions (pdev, "rtnet8139too");
+	if (rc)
+		goto err_out;
+
+	/* enable PCI bus-mastering */
+	pci_set_master (pdev);
+
+	mmio_start = pci_resource_start (pdev, 1);
+	mmio_flags = pci_resource_flags (pdev, 1);
+	mmio_len = pci_resource_len (pdev, 1);
+
+	/* set this immediately, we need to know before
+	 * we talk to the chip directly */
+#ifdef USE_IO_OPS
+	pio_start = pci_resource_start (pdev, 0);
+	pio_end = pci_resource_end (pdev, 0);
+	pio_flags = pci_resource_flags (pdev, 0);
+	pio_len = pci_resource_len (pdev, 0);
+
+	/* make sure PCI base addr 0 is PIO */
+	if (!(pio_flags & IORESOURCE_IO)) {
+		printk (KERN_ERR "%s: region #0 not a PIO resource, aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+	/* check for weird/broken PCI region reporting */
+	if (pio_len < RTL_MIN_IO_SIZE) {
+		printk (KERN_ERR "%s: Invalid PCI I/O region size(s), aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+#else
+	/* make sure PCI base addr 1 is MMIO */
+	if (!(mmio_flags & IORESOURCE_MEM)) {
+		printk(KERN_ERR "%s: region #1 not an MMIO resource, aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+	if (mmio_len < RTL_MIN_IO_SIZE) {
+		printk(KERN_ERR "%s: Invalid PCI mem region size(s), aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+#endif
+
+#ifdef USE_IO_OPS
+	ioaddr = (void *) pio_start;
+	rtdev->base_addr = pio_start;
+	tp->mmio_addr = ioaddr;
+#else
+	/* ioremap MMIO region */
+	ioaddr = ioremap (mmio_start, mmio_len);
+	if (ioaddr == NULL) {
+		printk(KERN_ERR "%s: cannot remap MMIO, aborting\n", pci_name(pdev));
+		rc = -EIO;
+		goto err_out;
+	}
+	rtdev->base_addr = (long) ioaddr;
+	tp->mmio_addr = ioaddr;
+#endif /* USE_IO_OPS */
+
+	/* Bring old chips out of low-power mode. */
+	RTL_W8 (HltClk, 'R');
+
+	/* check for missing/broken hardware */
+	if (RTL_R32 (TxConfig) == 0xFFFFFFFF) {
+		printk(KERN_ERR "%s: Chip not responding, ignoring board\n", pci_name(pdev));
+		rc = -EIO;
+		goto err_out;
+	}
+
+	/* identify chip attached to board */
+	tmp = RTL_R8 (ChipVersion);
+	for (i = 0; i < ARRAY_SIZE (rtl_chip_info); i++)
+		if (tmp == rtl_chip_info[i].version) {
+			tp->chipset = i;
+			goto match;
+		}
+
+	printk(KERN_WARNING "rt8139too: unknown chip version, assuming RTL-8139\n");
+	printk(KERN_WARNING "rt8139too: TxConfig = 0x%08x\n", RTL_R32 (TxConfig));
+
+	tp->chipset = 0;
+
+match:
+	if (tp->chipset >= CH_8139B) {
+		u8 new_tmp8 = tmp8 = RTL_R8 (Config1);
+		if ((rtl_chip_info[tp->chipset].flags & HasLWake) &&
+		    (tmp8 & LWAKE))
+			new_tmp8 &= ~LWAKE;
+		new_tmp8 |= Cfg1_PM_Enable;
+		if (new_tmp8 != tmp8) {
+			RTL_W8 (Cfg9346, Cfg9346_Unlock);
+			RTL_W8 (Config1, tmp8);
+			RTL_W8 (Cfg9346, Cfg9346_Lock);
+		}
+		if (rtl_chip_info[tp->chipset].flags & HasLWake) {
+			tmp8 = RTL_R8 (Config4);
+			if (tmp8 & LWPTN) {
+				RTL_W8 (Cfg9346, Cfg9346_Unlock);
+				RTL_W8 (Config4, tmp8 & ~LWPTN);
+				RTL_W8 (Cfg9346, Cfg9346_Lock);
+			}
+		}
+	} else {
+		tmp8 = RTL_R8 (Config1);
+		tmp8 &= ~(SLEEP | PWRDN);
+		RTL_W8 (Config1, tmp8);
+	}
+
+	rtl8139_chip_reset (ioaddr);
+
+	*dev_out = rtdev;
+	return 0;
+
+err_out:
+#ifndef USE_IO_OPS
+	if (tp->mmio_addr) iounmap (tp->mmio_addr);
+#endif /* !USE_IO_OPS */
+	/* it's ok to call this even if we have no regions to free */
+	pci_release_regions (pdev);
+	rtdev_free(rtdev);
+	pci_set_drvdata (pdev, NULL);
+
+	return rc;
+}
+
+
+
+
+static int rtl8139_init_one (struct pci_dev *pdev,
+				       const struct pci_device_id *ent)
+{
+	struct rtnet_device *rtdev = NULL;
+	struct rtl8139_private *tp;
+	int i, addr_len;
+	int option;
+	void *ioaddr;
+	static int board_idx = -1;
+
+	board_idx++;
+
+	if( cards[board_idx] == 0)
+		return -ENODEV;
+
+	/* when we're built into the kernel, the driver version message
+	 * is only printed if at least one 8139 board has been found
+	 */
+#ifndef MODULE
+	{
+		static int printed_version;
+		if (!printed_version++)
+			printk (KERN_INFO RTL8139_DRIVER_NAME "\n");
+	}
+#endif
+
+	if ((i=rtl8139_init_board (pdev, &rtdev)) < 0)
+		return i;
+
+
+	tp = rtdev->priv;
+	ioaddr = tp->mmio_addr;
+
+	addr_len = read_eeprom (ioaddr, 0, 8) == 0x8129 ? 8 : 6;
+	for (i = 0; i < 3; i++)
+		((u16 *) (rtdev->dev_addr))[i] =
+		    le16_to_cpu (read_eeprom (ioaddr, i + 7, addr_len));
+
+	/* The Rtl8139-specific entries in the device structure. */
+	rtdev->open = rtl8139_open;
+	rtdev->stop = rtl8139_close;
+	rtdev->hard_header = &rt_eth_header;
+	rtdev->hard_start_xmit = rtl8139_start_xmit;
+	rtdev->do_ioctl = rtl8139_ioctl;
+	rtdev->get_stats = rtl8139_get_stats;
+
+	/*rtdev->set_multicast_list = rtl8139_set_rx_mode; */
+	rtdev->features |= NETIF_F_SG|NETIF_F_HW_CSUM;
+
+	rtdev->irq = pdev->irq;
+
+	/* rtdev->priv/tp zeroed and aligned in init_etherdev */
+	tp = rtdev->priv;
+
+	/* note: tp->chipset set in rtl8139_init_board */
+	tp->drv_flags = board_info[ent->driver_data].hw_flags;
+	tp->mmio_addr = ioaddr;
+	raw_spin_lock_init(&tp->lock);
+
+	if ( (i=rt_register_rtnetdev(rtdev)) )
+		goto err_out;
+
+	pci_set_drvdata (pdev, rtdev);
+
+	tp->phys[0] = 32;
+
+	/* The lower four bits are the media type. */
+	option = (board_idx >= MAX_UNITS) ? 0 : media[board_idx];
+	if (option > 0) {
+		tp->mii.full_duplex = (option & 0x210) ? 1 : 0;
+		tp->default_port = option & 0xFF;
+		if (tp->default_port)
+			tp->medialock = 1;
+	}
+	if (tp->default_port) {
+		printk(KERN_INFO "  Forcing %dMbps %s-duplex operation.\n",
+			    (option & 0x20 ? 100 : 10),
+			    (option & 0x10 ? "full" : "half"));
+		mdio_write(rtdev, tp->phys[0], 0,
+				   ((option & 0x20) ? 0x2000 : 0) |         /* 100Mbps? */
+				   ((option & 0x10) ? 0x0100 : 0)); /* Full duplex? */
+	}
+
+
+	/* Put the chip into low-power mode. */
+	if (rtl_chip_info[tp->chipset].flags & HasHltClk)
+		RTL_W8 (HltClk, 'H');        /* 'R' would leave the clock running. */
+
+	return 0;
+
+
+err_out:
+#ifndef USE_IO_OPS
+	if (tp->mmio_addr) iounmap (tp->mmio_addr);
+#endif /* !USE_IO_OPS */
+	/* it's ok to call this even if we have no regions to free */
+	pci_release_regions (pdev);
+	rtdev_free(rtdev);
+	pci_set_drvdata (pdev, NULL);
+
+	return i;
+}
+
+
+static void rtl8139_remove_one (struct pci_dev *pdev)
+{
+	struct rtnet_device *rtdev = pci_get_drvdata(pdev);
+
+#ifndef USE_IO_OPS
+	struct rtl8139_private *tp = rtdev->priv;
+
+	if (tp->mmio_addr)
+		iounmap (tp->mmio_addr);
+#endif /* !USE_IO_OPS */
+
+	/* it's ok to call this even if we have no regions to free */
+	rt_unregister_rtnetdev(rtdev);
+	rt_rtdev_disconnect(rtdev);
+
+	pci_release_regions(pdev);
+	pci_set_drvdata(pdev, NULL);
+
+	rtdev_free(rtdev);
+}
+
+
+/* Serial EEPROM section. */
+
+/*  EEPROM_Ctrl bits. */
+#define EE_SHIFT_CLK        0x04        /* EEPROM shift clock. */
+#define EE_CS                        0x08        /* EEPROM chip select. */
+#define EE_DATA_WRITE        0x02        /* EEPROM chip data in. */
+#define EE_WRITE_0                0x00
+#define EE_WRITE_1                0x02
+#define EE_DATA_READ        0x01        /* EEPROM chip data out. */
+#define EE_ENB                        (0x80 | EE_CS)
+
+/* Delay between EEPROM clock transitions.
+   No extra delay is needed with 33Mhz PCI, but 66Mhz may change this.
+ */
+
+#define eeprom_delay()        readl(ee_addr)
+
+/* The EEPROM commands include the alway-set leading bit. */
+#define EE_WRITE_CMD        (5)
+#define EE_READ_CMD                (6)
+#define EE_ERASE_CMD        (7)
+
+static int read_eeprom (void *ioaddr, int location, int addr_len)
+{
+	int i;
+	unsigned retval = 0;
+	void *ee_addr = ioaddr + Cfg9346;
+	int read_cmd = location | (EE_READ_CMD << addr_len);
+
+	writeb (EE_ENB & ~EE_CS, ee_addr);
+	writeb (EE_ENB, ee_addr);
+	eeprom_delay ();
+
+	/* Shift the read command bits out. */
+	for (i = 4 + addr_len; i >= 0; i--) {
+		int dataval = (read_cmd & (1 << i)) ? EE_DATA_WRITE : 0;
+		writeb (EE_ENB | dataval, ee_addr);
+		eeprom_delay ();
+		writeb (EE_ENB | dataval | EE_SHIFT_CLK, ee_addr);
+		eeprom_delay ();
+	}
+	writeb (EE_ENB, ee_addr);
+	eeprom_delay ();
+
+	for (i = 16; i > 0; i--) {
+		writeb (EE_ENB | EE_SHIFT_CLK, ee_addr);
+		eeprom_delay ();
+		retval =
+		    (retval << 1) | ((readb (ee_addr) & EE_DATA_READ) ? 1 :
+				     0);
+		writeb (EE_ENB, ee_addr);
+		eeprom_delay ();
+	}
+
+	/* Terminate the EEPROM access. */
+	writeb (~EE_CS, ee_addr);
+	eeprom_delay ();
+
+	return retval;
+}
+
+/* MII serial management: mostly bogus for now. */
+/* Read and write the MII management registers using software-generated
+   serial MDIO protocol.
+   The maximum data clock rate is 2.5 Mhz.  The minimum timing is usually
+   met by back-to-back PCI I/O cycles, but we insert a delay to avoid
+   "overclocking" issues. */
+#define MDIO_DIR                0x80
+#define MDIO_DATA_OUT        0x04
+#define MDIO_DATA_IN        0x02
+#define MDIO_CLK                0x01
+#define MDIO_WRITE0 (MDIO_DIR)
+#define MDIO_WRITE1 (MDIO_DIR | MDIO_DATA_OUT)
+
+#define mdio_delay(mdio_addr)        readb(mdio_addr)
+
+
+
+static char mii_2_8139_map[8] = {
+	BasicModeCtrl,
+	BasicModeStatus,
+	0,
+	0,
+	NWayAdvert,
+	NWayLPAR,
+	NWayExpansion,
+	0
+};
+
+#ifdef CONFIG_8139TOO_8129
+/* Syncronize the MII management interface by shifting 32 one bits out. */
+static void mdio_sync (void *mdio_addr)
+{
+	int i;
+
+	for (i = 32; i >= 0; i--) {
+		writeb (MDIO_WRITE1, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (MDIO_WRITE1 | MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+}
+#endif
+
+
+static int mdio_read (struct rtnet_device *rtdev, int phy_id, int location)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	int retval = 0;
+#ifdef CONFIG_8139TOO_8129
+	void *mdio_addr = tp->mmio_addr + Config4;
+	int mii_cmd = (0xf6 << 10) | (phy_id << 5) | location;
+	int i;
+#endif
+
+	if (phy_id > 31) {        /* Really a 8139.  Use internal registers. */
+		return location < 8 && mii_2_8139_map[location] ?
+		    readw (tp->mmio_addr + mii_2_8139_map[location]) : 0;
+	}
+
+#ifdef CONFIG_8139TOO_8129
+	mdio_sync (mdio_addr);
+	/* Shift the read command bits out. */
+	for (i = 15; i >= 0; i--) {
+		int dataval = (mii_cmd & (1 << i)) ? MDIO_DATA_OUT : 0;
+
+		writeb (MDIO_DIR | dataval, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (MDIO_DIR | dataval | MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+
+	/* Read the two transition, 16 data, and wire-idle bits. */
+	for (i = 19; i > 0; i--) {
+		writeb (0, mdio_addr);
+		mdio_delay (mdio_addr);
+		retval = (retval << 1) | ((readb (mdio_addr) & MDIO_DATA_IN) ? 1 : 0);
+		writeb (MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+#endif
+
+	return (retval >> 1) & 0xffff;
+}
+
+
+static void mdio_write (struct rtnet_device *rtdev, int phy_id, int location,
+			int value)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+#ifdef CONFIG_8139TOO_8129
+	void *mdio_addr = tp->mmio_addr + Config4;
+	int mii_cmd = (0x5002 << 16) | (phy_id << 23) | (location << 18) | value;
+	int i;
+#endif
+
+	if (phy_id > 31) {        /* Really a 8139.  Use internal registers. */
+		void *ioaddr = tp->mmio_addr;
+		if (location == 0) {
+			RTL_W8 (Cfg9346, Cfg9346_Unlock);
+			RTL_W16 (BasicModeCtrl, value);
+			RTL_W8 (Cfg9346, Cfg9346_Lock);
+		} else if (location < 8 && mii_2_8139_map[location])
+			RTL_W16 (mii_2_8139_map[location], value);
+		return;
+	}
+
+#ifdef CONFIG_8139TOO_8129
+	mdio_sync (mdio_addr);
+
+	/* Shift the command bits out. */
+	for (i = 31; i >= 0; i--) {
+		int dataval =
+		    (mii_cmd & (1 << i)) ? MDIO_WRITE1 : MDIO_WRITE0;
+		writeb (dataval, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (dataval | MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+	/* Clear out extra bits. */
+	for (i = 2; i > 0; i--) {
+		writeb (0, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+#endif
+}
+
+static int rtl8139_open (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	int retval;
+
+	rt_stack_connect(rtdev, &STACK_manager);
+
+	retval = request_irq(rtdev->irq, rtl8139_interrupt,
+		IRQF_NO_THREAD | IRQF_SHARED, rtdev->name, rtdev);
+	if (retval)
+		return retval;
+
+	tp->tx_bufs = pci_alloc_consistent(tp->pci_dev, TX_BUF_TOT_LEN, &tp->tx_bufs_dma);
+	tp->rx_ring = pci_alloc_consistent(tp->pci_dev, RX_BUF_TOT_LEN, &tp->rx_ring_dma);
+
+	if (tp->tx_bufs == NULL || tp->rx_ring == NULL) {
+		free_irq(rtdev->irq, rtdev);
+		if (tp->tx_bufs)
+			pci_free_consistent(tp->pci_dev, TX_BUF_TOT_LEN, tp->tx_bufs, tp->tx_bufs_dma);
+		if (tp->rx_ring)
+			pci_free_consistent(tp->pci_dev, RX_BUF_TOT_LEN, tp->rx_ring, tp->rx_ring_dma);
+
+		return -ENOMEM;
+	}
+	/* FIXME: create wrapper for duplex_lock vs. force_media
+	   tp->mii.full_duplex = tp->mii.duplex_lock; */
+	tp->tx_flag = (TX_FIFO_THRESH << 11) & 0x003f0000;
+	tp->twistie = 1;
+	tp->time_to_die = 0;
+
+	rtl8139_init_ring (rtdev);
+	rtl8139_hw_start (rtdev);
+
+	return 0;
+}
+
+
+static void rtl_check_media (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	u16 mii_lpa;
+
+	if (tp->phys[0] < 0)
+		return;
+
+	mii_lpa = mdio_read(rtdev, tp->phys[0], MII_LPA);
+	if (mii_lpa == 0xffff)
+		return;
+
+	tp->mii.full_duplex = (mii_lpa & LPA_100FULL) == LPA_100FULL ||
+		(mii_lpa & 0x00C0) == LPA_10FULL;
+}
+
+
+/* Start the hardware at open or resume. */
+static void rtl8139_hw_start (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	u32 i;
+	u8 tmp;
+
+	/* Bring old chips out of low-power mode. */
+	if (rtl_chip_info[tp->chipset].flags & HasHltClk)
+		RTL_W8 (HltClk, 'R');
+
+	rtl8139_chip_reset(ioaddr);
+
+	/* unlock Config[01234] and BMCR register writes */
+	RTL_W8_F (Cfg9346, Cfg9346_Unlock);
+	/* Restore our idea of the MAC address. */
+	RTL_W32_F (MAC0 + 0, cpu_to_le32 (*(u32 *) (rtdev->dev_addr + 0)));
+	RTL_W32_F (MAC0 + 4, cpu_to_le32 (*(u32 *) (rtdev->dev_addr + 4)));
+
+	tp->cur_rx = 0;
+
+	/* init Rx ring buffer DMA address */
+	RTL_W32_F (RxBuf, tp->rx_ring_dma);
+
+	/* Must enable Tx/Rx before setting transfer thresholds! */
+	RTL_W8 (ChipCmd, CmdRxEnb | CmdTxEnb);
+
+	tp->rx_config = rtl8139_rx_config | AcceptBroadcast | AcceptMyPhys;
+	RTL_W32 (RxConfig, tp->rx_config);
+
+	/* Check this value: the documentation for IFG contradicts ifself. */
+	RTL_W32 (TxConfig, rtl8139_tx_config);
+
+	rtl_check_media (rtdev);
+
+	if (tp->chipset >= CH_8139B) {
+		/* Disable magic packet scanning, which is enabled
+		 * when PM is enabled in Config1.  It can be reenabled
+		 * via ETHTOOL_SWOL if desired.  */
+		RTL_W8 (Config3, RTL_R8 (Config3) & ~Cfg3_Magic);
+	}
+
+	/* Lock Config[01234] and BMCR register writes */
+	RTL_W8 (Cfg9346, Cfg9346_Lock);
+
+	/* init Tx buffer DMA addresses */
+	for (i = 0; i < NUM_TX_DESC; i++)
+		RTL_W32_F (TxAddr0 + (i * 4), tp->tx_bufs_dma + (tp->tx_buf[i] - tp->tx_bufs));
+
+	RTL_W32 (RxMissed, 0);
+
+	rtl8139_set_rx_mode (rtdev);
+
+	/* no early-rx interrupts */
+	RTL_W16 (MultiIntr, RTL_R16 (MultiIntr) & MultiIntrClear);
+
+	/* make sure RxTx has started */
+	tmp = RTL_R8 (ChipCmd);
+	if ((!(tmp & CmdRxEnb)) || (!(tmp & CmdTxEnb)))
+		RTL_W8 (ChipCmd, CmdRxEnb | CmdTxEnb);
+
+	/* Enable all known interrupts by setting the interrupt mask. */
+	RTL_W16 (IntrMask, rtl8139_intr_mask);
+
+	rtnetif_start_queue (rtdev);
+}
+
+
+/* Initialize the Rx and Tx rings, along with various 'dev' bits. */
+static void rtl8139_init_ring (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	int i;
+
+	tp->cur_rx = 0;
+	tp->cur_tx = 0;
+	tp->dirty_tx = 0;
+
+	for (i = 0; i < NUM_TX_DESC; i++)
+		tp->tx_buf[i] = &tp->tx_bufs[i * TX_BUF_SIZE];
+}
+
+
+static void rtl8139_tx_clear (struct rtl8139_private *tp)
+{
+	tp->cur_tx = 0;
+	tp->dirty_tx = 0;
+
+	/* XXX account for unsent Tx packets in tp->stats.tx_dropped */
+}
+
+
+
+static int rtl8139_start_xmit (struct rtskb *skb, struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+
+	void *ioaddr = tp->mmio_addr;
+	unsigned int entry;
+	unsigned int len = skb->len;
+	unsigned long context;
+
+	raw_spin_lock_irqsave(&tp->lock, context);
+	/* Calculate the next Tx descriptor entry. */
+	entry = tp->cur_tx % NUM_TX_DESC;
+
+	if (likely(len < TX_BUF_SIZE)) {
+		if (unlikely(skb->xmit_stamp != NULL)) {
+			*skb->xmit_stamp = cpu_to_be64(ktime_get() +
+						       *skb->xmit_stamp);
+			/* typically, we are only copying a few bytes here */
+			rtskb_copy_and_csum_dev(skb, tp->tx_buf[entry]);
+		} else {
+			/* copy larger packets outside the lock */
+			rtskb_copy_and_csum_dev(skb, tp->tx_buf[entry]);
+		}
+	} else {
+		dev_kfree_rtskb(skb);
+		tp->stats.tx_dropped++;
+		return 0;
+	}
+
+
+	/* Note: the chip doesn't have auto-pad! */
+	RTL_W32_F (TxStatus0 + (entry * sizeof (u32)), tp->tx_flag | max(len, (unsigned int)ETH_ZLEN));
+	tp->cur_tx++;
+	wmb();
+	if ((tp->cur_tx - NUM_TX_DESC) == tp->dirty_tx)
+		rtnetif_stop_queue (rtdev);
+	raw_spin_unlock_irqrestore(&tp->lock, context);
+
+	dev_kfree_rtskb(skb);
+
+#if 0
+	trace_printk (KERN_INFO "%s: Queued Tx packet size %u to slot %d.\n", rtdev->name, len, entry);
+#endif
+	return 0;
+}
+
+static int rtl8139_ioctl(struct rtnet_device *rtdev, struct ifreq *ifr, int cmd)
+{
+    struct rtl8139_private *tp = rtdev->priv;
+    void *ioaddr = tp->mmio_addr;
+    int nReturn = 0;
+    struct ethtool_value *value;
+
+    switch (cmd) {
+	case SIOCETHTOOL:
+	    /* TODO: user-safe parameter access, most probably one layer higher */
+	    value = (struct ethtool_value *)ifr->ifr_data;
+	    if (value->cmd == ETHTOOL_GLINK)
+	    {
+		if (RTL_R16(CSCR) & CSCR_LinkOKBit)
+		    value->data = 1;
+		else
+		    value->data = 0;
+	    }
+	    break;
+
+	default:
+	    nReturn = -EOPNOTSUPP;
+	    break;
+    }
+    return nReturn;
+}
+
+static struct net_device_stats *rtl8139_get_stats(struct rtnet_device*rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	return &tp->stats;
+}
+
+static void rtl8139_tx_interrupt (struct rtnet_device *rtdev,
+				  struct rtl8139_private *tp,
+				  void *ioaddr)
+{
+	unsigned long dirty_tx, tx_left;
+
+	dirty_tx = tp->dirty_tx;
+	tx_left = tp->cur_tx - dirty_tx;
+
+	while (tx_left > 0) {
+		int entry = dirty_tx % NUM_TX_DESC;
+		int txstatus;
+
+		txstatus = RTL_R32 (TxStatus0 + (entry * sizeof (u32)));
+
+		if (!(txstatus & (TxStatOK | TxUnderrun | TxAborted)))
+			break;        /* It still hasn't been Txed */
+
+		/* Note: TxCarrierLost is always asserted at 100mbps. */
+		if (txstatus & (TxOutOfWindow | TxAborted)) {
+			/* There was an major error, log it. */
+			trace_printk("%s: Transmit error, Tx status %8.8x.\n",
+				    rtdev->name, txstatus);
+			tp->stats.tx_errors++;
+			if (txstatus & TxAborted) {
+				tp->stats.tx_aborted_errors++;
+				RTL_W32 (TxConfig, TxClearAbt);
+				RTL_W16 (IntrStatus, TxErr);
+				wmb();
+			}
+			if (txstatus & TxCarrierLost)
+				tp->stats.tx_carrier_errors++;
+			if (txstatus & TxOutOfWindow)
+				tp->stats.tx_window_errors++;
+#ifdef ETHER_STATS
+			if ((txstatus & 0x0f000000) == 0x0f000000)
+				tp->stats.collisions16++;
+#endif
+		} else {
+			if (txstatus & TxUnderrun) {
+				/* Add 64 to the Tx FIFO threshold. */
+				if (tp->tx_flag < 0x00300000)
+					tp->tx_flag += 0x00020000;
+				tp->stats.tx_fifo_errors++;
+			}
+			tp->stats.collisions += (txstatus >> 24) & 15;
+			tp->stats.tx_bytes += txstatus & 0x7ff;
+			tp->stats.tx_packets++;
+		}
+
+		dirty_tx++;
+		tx_left--;
+	}
+
+	/* only wake the queue if we did work, and the queue is stopped */
+	if (tp->dirty_tx != dirty_tx) {
+		tp->dirty_tx = dirty_tx;
+		mb();
+		if (rtnetif_queue_stopped (rtdev))
+			rtnetif_wake_queue (rtdev);
+	}
+}
+
+
+/* TODO: clean this up!  Rx reset need not be this intensive */
+static void rtl8139_rx_err
+(u32 rx_status, struct rtnet_device *rtdev, struct rtl8139_private *tp, void *ioaddr)
+{
+/*        u8 tmp8;
+#ifndef CONFIG_8139_NEW_RX_RESET
+	int tmp_work;
+#endif */
+
+	/* RTnet-TODO: We really need an error manager to handle such issues... */
+	trace_printk("%s: FATAL - Ethernet frame had errors, status %8.8x.\n",
+		    rtdev->name, rx_status);
+}
+
+
+static void rtl8139_rx_interrupt (struct rtnet_device *rtdev,
+				  struct rtl8139_private *tp, void *ioaddr,
+				  ktime_t *time_stamp)
+{
+	unsigned char *rx_ring;
+	u16 cur_rx;
+
+	rx_ring = tp->rx_ring;
+	cur_rx = tp->cur_rx;
+
+	while ((RTL_R8 (ChipCmd) & RxBufEmpty) == 0) {
+		int ring_offset = cur_rx % RX_BUF_LEN;
+		u32 rx_status;
+		unsigned int rx_size;
+		unsigned int pkt_size;
+		struct rtskb *skb;
+
+		rmb();
+
+		/* read size+status of next frame from DMA ring buffer */
+		rx_status = le32_to_cpu (*(u32 *) (rx_ring + ring_offset));
+		rx_size = rx_status >> 16;
+		pkt_size = rx_size - 4;
+
+		/* Packet copy from FIFO still in progress.
+		 * Theoretically, this should never happen
+		 * since EarlyRx is disabled.
+		 */
+		if (rx_size == 0xfff0) {
+			tp->xstats.early_rx++;
+			break;
+		}
+
+		/* If Rx err or invalid rx_size/rx_status received
+		 * (which happens if we get lost in the ring),
+		 * Rx process gets reset, so we abort any further
+		 * Rx processing.
+		 */
+		if ((rx_size > (MAX_ETH_FRAME_SIZE+4)) ||
+		    (rx_size < 8) ||
+		    (!(rx_status & RxStatusOK))) {
+			rtl8139_rx_err (rx_status, rtdev, tp, ioaddr);
+			return;
+		}
+
+		/* Malloc up new buffer, compatible with net-2e. */
+		/* Omit the four octet CRC from the length. */
+
+		/* TODO: consider allocating skb's outside of
+		 * interrupt context, both to speed interrupt processing,
+		 * and also to reduce the chances of having to
+		 * drop packets here under memory pressure.
+		 */
+
+		skb = rtnetdev_alloc_rtskb(rtdev, pkt_size + 2);
+		if (skb) {
+			skb->time_stamp = *time_stamp;
+			rtskb_reserve (skb, 2);        /* 16 byte align the IP fields. */
+
+
+			/* eth_copy_and_sum (skb, &rx_ring[ring_offset + 4], pkt_size, 0); */
+			memcpy (skb->data, &rx_ring[ring_offset + 4], pkt_size);
+			rtskb_put (skb, pkt_size);
+			skb->protocol = rt_eth_type_trans (skb, rtdev);
+			rtnetif_rx (skb);
+			tp->stats.rx_bytes += pkt_size;
+			tp->stats.rx_packets++;
+		} else {
+			printk (KERN_WARNING "%s: Memory squeeze, dropping packet.\n", rtdev->name);
+			tp->stats.rx_dropped++;
+		}
+
+		cur_rx = (cur_rx + rx_size + 4 + 3) & ~3;
+		RTL_W16 (RxBufPtr, cur_rx - 16);
+
+		if (RTL_R16 (IntrStatus) & RxAckBits)
+			RTL_W16_F (IntrStatus, RxAckBits);
+	}
+
+	tp->cur_rx = cur_rx;
+}
+
+
+static void rtl8139_weird_interrupt (struct rtnet_device *rtdev,
+				     struct rtl8139_private *tp,
+				     void *ioaddr,
+				     int status, int link_changed)
+{
+	printk (KERN_ERR "%s: Abnormal interrupt, status %8.8x.\n",
+		      rtdev->name, status);
+
+	/* Update the error count. */
+	tp->stats.rx_missed_errors += RTL_R32 (RxMissed);
+	RTL_W32 (RxMissed, 0);
+
+	if ((status & RxUnderrun) && link_changed && (tp->drv_flags & HAS_LNK_CHNG)) {
+		/* Really link-change on new chips. */
+		status &= ~RxUnderrun;
+	}
+
+	/* XXX along with rtl8139_rx_err, are we double-counting errors? */
+	if (status &
+	    (RxUnderrun | RxOverflow | RxErr | RxFIFOOver))
+		tp->stats.rx_errors++;
+
+	if (status & PCSTimeout)
+		tp->stats.rx_length_errors++;
+
+	if (status & (RxUnderrun | RxFIFOOver))
+		tp->stats.rx_fifo_errors++;
+
+	if (status & PCIErr) {
+		u16 pci_cmd_status;
+		pci_read_config_word (tp->pci_dev, PCI_STATUS, &pci_cmd_status);
+		pci_write_config_word (tp->pci_dev, PCI_STATUS, pci_cmd_status);
+
+		printk (KERN_ERR "%s: PCI Bus error %4.4x.\n", rtdev->name, pci_cmd_status);
+	}
+}
+
+/* The interrupt handler does all of the Rx thread work and cleans up
+   after the Tx thread. */
+static irqreturn_t rtl8139_interrupt(int irq, void *data)
+{
+	ktime_t time_stamp = ktime_get();
+	struct rtnet_device *rtdev = (struct rtnet_device *)data;
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	int ackstat;
+	int status;
+	int link_changed = 0; /* avoid bogus "uninit" warning */
+	int saved_status = 0;
+	int ret = IRQ_NONE;
+
+	raw_spin_lock(&tp->lock);
+
+	status = RTL_R16(IntrStatus);
+
+	/* h/w no longer present (hotplug?) or major error, bail */
+	if (unlikely(status == 0xFFFF) || unlikely(!(status & rtl8139_intr_mask)))
+		goto out;
+
+	ret = IRQ_HANDLED;
+
+	/* close possible race with dev_close */
+	if (unlikely(!rtnetif_running(rtdev))) {
+		RTL_W16(IntrMask, 0);
+		goto out;
+	}
+
+	/* Acknowledge all of the current interrupt sources ASAP, but
+	   first get an additional status bit from CSCR. */
+	if (unlikely(status & RxUnderrun))
+		link_changed = RTL_R16(CSCR) & CSCR_LinkChangeBit;
+
+	/* The chip takes special action when we clear RxAckBits,
+	 * so we clear them later in rtl8139_rx_interrupt
+	 */
+	ackstat = status & ~(RxAckBits | TxErr);
+	if (ackstat)
+		RTL_W16(IntrStatus, ackstat);
+
+	if (status & RxAckBits) {
+		saved_status |= RxAckBits;
+		rtl8139_rx_interrupt(rtdev, tp, ioaddr, &time_stamp);
+	}
+
+	/* Check uncommon events with one test. */
+	if (unlikely(status & (PCIErr | PCSTimeout | RxUnderrun | RxErr)))
+		rtl8139_weird_interrupt(rtdev, tp, ioaddr, status, link_changed);
+
+	if (status & (TxOK |TxErr)) {
+		rtl8139_tx_interrupt(rtdev, tp, ioaddr);
+		if (status & TxErr) {
+			RTL_W16(IntrStatus, TxErr);
+			saved_status |= TxErr;
+		}
+	}
+ out:
+	raw_spin_unlock(&tp->lock);
+
+	if (saved_status & RxAckBits)
+		rt_mark_stack_mgr(rtdev);
+
+	if (saved_status & TxErr)
+		rtnetif_err_tx(rtdev);
+
+	return ret;
+}
+
+
+static int rtl8139_close (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	unsigned long context;
+
+	printk (KERN_INFO "%s: Shutting down ethercard, status was 0x%4.4x.\n", rtdev->name, RTL_R16 (IntrStatus));
+
+	rtnetif_stop_queue (rtdev);
+
+	raw_spin_lock_irqsave (&tp->lock, context);
+	/* Stop the chip's Tx and Rx DMA processes. */
+	RTL_W8 (ChipCmd, 0);
+	/* Disable interrupts by clearing the interrupt mask. */
+	RTL_W16 (IntrMask, 0);
+	/* Update the error counts. */
+	tp->stats.rx_missed_errors += RTL_R32 (RxMissed);
+	RTL_W32 (RxMissed, 0);
+	raw_spin_unlock_irqrestore (&tp->lock, context);
+
+	free_irq(rtdev->irq, rtdev);
+
+	rt_stack_disconnect(rtdev);
+
+	rtl8139_tx_clear (tp);
+
+	pci_free_consistent(tp->pci_dev, RX_BUF_TOT_LEN, tp->rx_ring, tp->rx_ring_dma);
+	pci_free_consistent(tp->pci_dev, TX_BUF_TOT_LEN, tp->tx_bufs, tp->tx_bufs_dma);
+	tp->rx_ring = NULL;
+	tp->tx_bufs = NULL;
+
+	/* Green! Put the chip in low-power mode. */
+	RTL_W8 (Cfg9346, Cfg9346_Unlock);
+
+	if (rtl_chip_info[tp->chipset].flags & HasHltClk)
+		RTL_W8 (HltClk, 'H');        /* 'R' would leave the clock running. */
+
+	return 0;
+}
+
+
+
+/* Set or clear the multicast filter for this adaptor.
+   This routine is not state sensitive and need not be SMP locked. */
+static void __set_rx_mode (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	u32 mc_filter[2];        /* Multicast hash filter */
+	int rx_mode;
+	u32 tmp;
+
+#ifdef DEBUG
+	printk (KERN_INFO "%s:   rtl8139_set_rx_mode(%4.4x) done -- Rx config %8.8lx.\n",
+			rtdev->name, rtdev->flags, RTL_R32 (RxConfig));
+#endif
+
+	/* Note: do not reorder, GCC is clever about common statements. */
+	if (rtdev->flags & IFF_PROMISC) {
+		/* Unconditionally log net taps. */
+		/*printk (KERN_NOTICE "%s: Promiscuous mode enabled.\n", rtdev->name);*/
+		rx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys | AcceptAllPhys;
+		mc_filter[1] = mc_filter[0] = 0xffffffff;
+	} else if (rtdev->flags & IFF_ALLMULTI) {
+		/* Too many to filter perfectly -- accept all multicasts. */
+		rx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys;
+		mc_filter[1] = mc_filter[0] = 0xffffffff;
+	} else {
+		rx_mode = AcceptBroadcast | AcceptMyPhys;
+		mc_filter[1] = mc_filter[0] = 0;
+	}
+
+	/* We can safely update without stopping the chip. */
+	tmp = rtl8139_rx_config | rx_mode;
+	if (tp->rx_config != tmp) {
+		RTL_W32_F (RxConfig, tmp);
+		tp->rx_config = tmp;
+	}
+	RTL_W32_F (MAR0 + 0, mc_filter[0]);
+	RTL_W32_F (MAR0 + 4, mc_filter[1]);
+}
+
+static void rtl8139_set_rx_mode (struct rtnet_device *rtdev)
+{
+	unsigned long context;
+	struct rtl8139_private *tp = rtdev->priv;
+
+	raw_spin_lock_irqsave (&tp->lock, context);
+	__set_rx_mode(rtdev);
+	raw_spin_unlock_irqrestore (&tp->lock, context);
+}
+
+static struct pci_driver rtl8139_pci_driver = {
+	name:                   DRV_NAME,
+	id_table:               rtl8139_pci_tbl,
+	probe:                  rtl8139_init_one,
+	remove:                 rtl8139_remove_one,
+	suspend:                NULL,
+	resume:                 NULL,
+};
+
+
+static int __init rtl8139_init_module (void)
+{
+	/* when we're a module, we always print a version message,
+	 * even if no 8139 board is found.
+	 */
+
+#ifdef MODULE
+	printk (KERN_INFO RTL8139_DRIVER_NAME "\n");
+#endif
+
+	return pci_register_driver (&rtl8139_pci_driver);
+}
+
+
+static void __exit rtl8139_cleanup_module (void)
+{
+	pci_unregister_driver (&rtl8139_pci_driver);
+}
+
+
+module_init(rtl8139_init_module);
+module_exit(rtl8139_cleanup_module);
diff -Naur a/net/rtnet/drivers/realtek/Kconfig b/net/rtnet/drivers/realtek/Kconfig
--- a/net/rtnet/drivers/realtek/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/realtek/Kconfig	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,37 @@
+config RTNET_DRV_REALTEK
+	depends on RTNET
+        bool "Realtek devices"
+        default n
+        help
+          If you have a network (Ethernet) card belonging to this class, say Y.
+
+          Note that the answer to this question doesn't directly affect the
+          kernel: saying N will just cause the configurator to skip all
+          the questions about Realtek cards. If you say Y, you will be asked
+          for your specific card in the following questions.
+
+if RTNET_DRV_REALTEK
+
+config RTNET_DRV_8139
+        depends on RTNET && PCI
+        bool "Realtek 8139"
+        help
+          Realtek 8139too real-time network driver
+
+#config RTNET_DRV_R8169
+#        bool "Realtek 8169/8168/8101/8125 ethernet support"
+#        depends on RTNET && PCI
+#        select FW_LOADER
+#        select CRC32
+#        select PHYLIB
+#        select REALTEK_PHY
+#        help
+#          Say Y here if you have a Realtek Ethernet adapter belonging to
+#          the following families:
+#          RTL8169 Gigabit Ethernet
+#          RTL8168 Gigabit Ethernet
+#          RTL8101 Fast Ethernet
+#          RTL8125 2.5GBit Ethernet
+#          Note that the driver is not RT yet
+
+endif # RTNET_DRV_REALTEK
diff -Naur a/net/rtnet/drivers/realtek/Makefile b/net/rtnet/drivers/realtek/Makefile
--- a/net/rtnet/drivers/realtek/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/realtek/Makefile	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,9 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_DRV_8139) += rt_8139too.o
+rt_8139too-y := 8139too.o
+
+obj-$(CONFIG_RTNET_DRV_R8169) += r8169.o
+r8169-objs += r8169_main.o r8169_firmware.o
+
+
diff -Naur a/net/rtnet/drivers/realtek/r8169_firmware.c b/net/rtnet/drivers/realtek/r8169_firmware.c
--- a/net/rtnet/drivers/realtek/r8169_firmware.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/realtek/r8169_firmware.c	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,231 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* r8169_firmware.c: RealTek 8169/8168/8101 ethernet driver.
+ *
+ * Copyright (c) 2002 ShuChen <shuchen@realtek.com.tw>
+ * Copyright (c) 2003 - 2007 Francois Romieu <romieu@fr.zoreil.com>
+ * Copyright (c) a lot of people too. Please respect their work.
+ *
+ * See MAINTAINERS file for support contact information.
+ */
+
+#include <linux/delay.h>
+#include <linux/firmware.h>
+
+#include "r8169_firmware.h"
+
+enum rtl_fw_opcode {
+	PHY_READ		= 0x0,
+	PHY_DATA_OR		= 0x1,
+	PHY_DATA_AND		= 0x2,
+	PHY_BJMPN		= 0x3,
+	PHY_MDIO_CHG		= 0x4,
+	PHY_CLEAR_READCOUNT	= 0x7,
+	PHY_WRITE		= 0x8,
+	PHY_READCOUNT_EQ_SKIP	= 0x9,
+	PHY_COMP_EQ_SKIPN	= 0xa,
+	PHY_COMP_NEQ_SKIPN	= 0xb,
+	PHY_WRITE_PREVIOUS	= 0xc,
+	PHY_SKIPN		= 0xd,
+	PHY_DELAY_MS		= 0xe,
+};
+
+struct fw_info {
+	u32	magic;
+	char	version[RTL_VER_SIZE];
+	__le32	fw_start;
+	__le32	fw_len;
+	u8	chksum;
+} __packed;
+
+#define FW_OPCODE_SIZE	sizeof(typeof(*((struct rtl_fw_phy_action *)0)->code))
+
+static bool rtl_fw_format_ok(struct rtl_fw *rtl_fw)
+{
+	const struct firmware *fw = rtl_fw->fw;
+	struct fw_info *fw_info = (struct fw_info *)fw->data;
+	struct rtl_fw_phy_action *pa = &rtl_fw->phy_action;
+
+	if (fw->size < FW_OPCODE_SIZE)
+		return false;
+
+	if (!fw_info->magic) {
+		size_t i, size, start;
+		u8 checksum = 0;
+
+		if (fw->size < sizeof(*fw_info))
+			return false;
+
+		for (i = 0; i < fw->size; i++)
+			checksum += fw->data[i];
+		if (checksum != 0)
+			return false;
+
+		start = le32_to_cpu(fw_info->fw_start);
+		if (start > fw->size)
+			return false;
+
+		size = le32_to_cpu(fw_info->fw_len);
+		if (size > (fw->size - start) / FW_OPCODE_SIZE)
+			return false;
+
+		strscpy(rtl_fw->version, fw_info->version, RTL_VER_SIZE);
+
+		pa->code = (__le32 *)(fw->data + start);
+		pa->size = size;
+	} else {
+		if (fw->size % FW_OPCODE_SIZE)
+			return false;
+
+		strscpy(rtl_fw->version, rtl_fw->fw_name, RTL_VER_SIZE);
+
+		pa->code = (__le32 *)fw->data;
+		pa->size = fw->size / FW_OPCODE_SIZE;
+	}
+
+	return true;
+}
+
+static bool rtl_fw_data_ok(struct rtl_fw *rtl_fw)
+{
+	struct rtl_fw_phy_action *pa = &rtl_fw->phy_action;
+	size_t index;
+
+	for (index = 0; index < pa->size; index++) {
+		u32 action = le32_to_cpu(pa->code[index]);
+		u32 regno = (action & 0x0fff0000) >> 16;
+
+		switch (action >> 28) {
+		case PHY_READ:
+		case PHY_DATA_OR:
+		case PHY_DATA_AND:
+		case PHY_MDIO_CHG:
+		case PHY_CLEAR_READCOUNT:
+		case PHY_WRITE:
+		case PHY_WRITE_PREVIOUS:
+		case PHY_DELAY_MS:
+			break;
+
+		case PHY_BJMPN:
+			if (regno > index)
+				goto out;
+			break;
+		case PHY_READCOUNT_EQ_SKIP:
+			if (index + 2 >= pa->size)
+				goto out;
+			break;
+		case PHY_COMP_EQ_SKIPN:
+		case PHY_COMP_NEQ_SKIPN:
+		case PHY_SKIPN:
+			if (index + 1 + regno >= pa->size)
+				goto out;
+			break;
+
+		default:
+			dev_err(rtl_fw->dev, "Invalid action 0x%08x\n", action);
+			return false;
+		}
+	}
+
+	return true;
+out:
+	dev_err(rtl_fw->dev, "Out of range of firmware\n");
+	return false;
+}
+
+void rtl_fw_write_firmware(struct rtl8169_private *tp, struct rtl_fw *rtl_fw)
+{
+	struct rtl_fw_phy_action *pa = &rtl_fw->phy_action;
+	rtl_fw_write_t fw_write = rtl_fw->phy_write;
+	rtl_fw_read_t fw_read = rtl_fw->phy_read;
+	int predata = 0, count = 0;
+	size_t index;
+
+	for (index = 0; index < pa->size; index++) {
+		u32 action = le32_to_cpu(pa->code[index]);
+		u32 data = action & 0x0000ffff;
+		u32 regno = (action & 0x0fff0000) >> 16;
+		enum rtl_fw_opcode opcode = action >> 28;
+
+		if (!action)
+			break;
+
+		switch (opcode) {
+		case PHY_READ:
+			predata = fw_read(tp, regno);
+			count++;
+			break;
+		case PHY_DATA_OR:
+			predata |= data;
+			break;
+		case PHY_DATA_AND:
+			predata &= data;
+			break;
+		case PHY_BJMPN:
+			index -= (regno + 1);
+			break;
+		case PHY_MDIO_CHG:
+			if (data == 0) {
+				fw_write = rtl_fw->phy_write;
+				fw_read = rtl_fw->phy_read;
+			} else if (data == 1) {
+				fw_write = rtl_fw->mac_mcu_write;
+				fw_read = rtl_fw->mac_mcu_read;
+			}
+
+			break;
+		case PHY_CLEAR_READCOUNT:
+			count = 0;
+			break;
+		case PHY_WRITE:
+			fw_write(tp, regno, data);
+			break;
+		case PHY_READCOUNT_EQ_SKIP:
+			if (count == data)
+				index++;
+			break;
+		case PHY_COMP_EQ_SKIPN:
+			if (predata == data)
+				index += regno;
+			break;
+		case PHY_COMP_NEQ_SKIPN:
+			if (predata != data)
+				index += regno;
+			break;
+		case PHY_WRITE_PREVIOUS:
+			fw_write(tp, regno, predata);
+			break;
+		case PHY_SKIPN:
+			index += regno;
+			break;
+		case PHY_DELAY_MS:
+			mdelay(data);
+			break;
+		}
+	}
+}
+
+void rtl_fw_release_firmware(struct rtl_fw *rtl_fw)
+{
+	release_firmware(rtl_fw->fw);
+}
+
+int rtl_fw_request_firmware(struct rtl_fw *rtl_fw)
+{
+	int rc;
+
+	rc = request_firmware(&rtl_fw->fw, rtl_fw->fw_name, rtl_fw->dev);
+	if (rc < 0)
+		goto out;
+
+	if (!rtl_fw_format_ok(rtl_fw) || !rtl_fw_data_ok(rtl_fw)) {
+		release_firmware(rtl_fw->fw);
+		rc = -EINVAL;
+		goto out;
+	}
+
+	return 0;
+out:
+	dev_err(rtl_fw->dev, "Unable to load firmware %s (%d)\n",
+		rtl_fw->fw_name, rc);
+	return rc;
+}
diff -Naur a/net/rtnet/drivers/realtek/r8169_firmware.h b/net/rtnet/drivers/realtek/r8169_firmware.h
--- a/net/rtnet/drivers/realtek/r8169_firmware.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/realtek/r8169_firmware.h	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/* r8169_firmware.h: RealTek 8169/8168/8101 ethernet driver.
+ *
+ * Copyright (c) 2002 ShuChen <shuchen@realtek.com.tw>
+ * Copyright (c) 2003 - 2007 Francois Romieu <romieu@fr.zoreil.com>
+ * Copyright (c) a lot of people too. Please respect their work.
+ *
+ * See MAINTAINERS file for support contact information.
+ */
+
+#include <linux/device.h>
+#include <linux/firmware.h>
+
+struct rtl8169_private;
+typedef void (*rtl_fw_write_t)(struct rtl8169_private *tp, int reg, int val);
+typedef int (*rtl_fw_read_t)(struct rtl8169_private *tp, int reg);
+
+#define RTL_VER_SIZE		32
+
+struct rtl_fw {
+	rtl_fw_write_t phy_write;
+	rtl_fw_read_t phy_read;
+	rtl_fw_write_t mac_mcu_write;
+	rtl_fw_read_t mac_mcu_read;
+	const struct firmware *fw;
+	const char *fw_name;
+	struct device *dev;
+
+	char version[RTL_VER_SIZE];
+
+	struct rtl_fw_phy_action {
+		__le32 *code;
+		size_t size;
+	} phy_action;
+};
+
+int rtl_fw_request_firmware(struct rtl_fw *rtl_fw);
+void rtl_fw_release_firmware(struct rtl_fw *rtl_fw);
+void rtl_fw_write_firmware(struct rtl8169_private *tp, struct rtl_fw *rtl_fw);
diff -Naur a/net/rtnet/drivers/realtek/r8169_main.c b/net/rtnet/drivers/realtek/r8169_main.c
--- a/net/rtnet/drivers/realtek/r8169_main.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/realtek/r8169_main.c	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,7257 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * r8169.c: RealTek 8169/8168/8101 ethernet driver.
+ *
+ * Copyright (c) 2002 ShuChen <shuchen@realtek.com.tw>
+ * Copyright (c) 2003 - 2007 Francois Romieu <romieu@fr.zoreil.com>
+ * Copyright (c) a lot of people too. Please respect their work.
+ *
+ * See MAINTAINERS file for support contact information.
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/if_vlan.h>
+#include <linux/crc32.h>
+#include <linux/in.h>
+#include <linux/io.h>
+#include <linux/ip.h>
+#include <linux/tcp.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/pm_runtime.h>
+#include <linux/prefetch.h>
+#include <linux/ipv6.h>
+#include <net/ip6_checksum.h>
+
+#include "r8169_firmware.h"
+
+#define MODULENAME "r8169"
+
+#define FIRMWARE_8168D_1	"rtl_nic/rtl8168d-1.fw"
+#define FIRMWARE_8168D_2	"rtl_nic/rtl8168d-2.fw"
+#define FIRMWARE_8168E_1	"rtl_nic/rtl8168e-1.fw"
+#define FIRMWARE_8168E_2	"rtl_nic/rtl8168e-2.fw"
+#define FIRMWARE_8168E_3	"rtl_nic/rtl8168e-3.fw"
+#define FIRMWARE_8168F_1	"rtl_nic/rtl8168f-1.fw"
+#define FIRMWARE_8168F_2	"rtl_nic/rtl8168f-2.fw"
+#define FIRMWARE_8105E_1	"rtl_nic/rtl8105e-1.fw"
+#define FIRMWARE_8402_1		"rtl_nic/rtl8402-1.fw"
+#define FIRMWARE_8411_1		"rtl_nic/rtl8411-1.fw"
+#define FIRMWARE_8411_2		"rtl_nic/rtl8411-2.fw"
+#define FIRMWARE_8106E_1	"rtl_nic/rtl8106e-1.fw"
+#define FIRMWARE_8106E_2	"rtl_nic/rtl8106e-2.fw"
+#define FIRMWARE_8168G_2	"rtl_nic/rtl8168g-2.fw"
+#define FIRMWARE_8168G_3	"rtl_nic/rtl8168g-3.fw"
+#define FIRMWARE_8168H_1	"rtl_nic/rtl8168h-1.fw"
+#define FIRMWARE_8168H_2	"rtl_nic/rtl8168h-2.fw"
+#define FIRMWARE_8107E_1	"rtl_nic/rtl8107e-1.fw"
+#define FIRMWARE_8107E_2	"rtl_nic/rtl8107e-2.fw"
+#define FIRMWARE_8125A_3	"rtl_nic/rtl8125a-3.fw"
+
+#define R8169_MSG_DEFAULT \
+	(NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_IFUP | NETIF_MSG_IFDOWN)
+
+/* Maximum number of multicast addresses to filter (vs. Rx-all-multicast).
+   The RTL chips use a 64 element hash table based on the Ethernet CRC. */
+#define	MC_FILTER_LIMIT	32
+
+#define TX_DMA_BURST	7	/* Maximum PCI burst, '7' is unlimited */
+#define InterFrameGap	0x03	/* 3 means InterFrameGap = the shortest one */
+
+#define R8169_REGS_SIZE		256
+#define R8169_RX_BUF_SIZE	(SZ_16K - 1)
+#define NUM_TX_DESC	64	/* Number of Tx descriptor registers */
+#define NUM_RX_DESC	256U	/* Number of Rx descriptor registers */
+#define R8169_TX_RING_BYTES	(NUM_TX_DESC * sizeof(struct TxDesc))
+#define R8169_RX_RING_BYTES	(NUM_RX_DESC * sizeof(struct RxDesc))
+
+#define RTL_CFG_NO_GBIT	1
+
+/* write/read MMIO register */
+#define RTL_W8(tp, reg, val8)	writeb((val8), tp->mmio_addr + (reg))
+#define RTL_W16(tp, reg, val16)	writew((val16), tp->mmio_addr + (reg))
+#define RTL_W32(tp, reg, val32)	writel((val32), tp->mmio_addr + (reg))
+#define RTL_R8(tp, reg)		readb(tp->mmio_addr + (reg))
+#define RTL_R16(tp, reg)		readw(tp->mmio_addr + (reg))
+#define RTL_R32(tp, reg)		readl(tp->mmio_addr + (reg))
+
+enum mac_version {
+	/* support for ancient RTL_GIGA_MAC_VER_01 has been removed */
+	RTL_GIGA_MAC_VER_02,
+	RTL_GIGA_MAC_VER_03,
+	RTL_GIGA_MAC_VER_04,
+	RTL_GIGA_MAC_VER_05,
+	RTL_GIGA_MAC_VER_06,
+	RTL_GIGA_MAC_VER_07,
+	RTL_GIGA_MAC_VER_08,
+	RTL_GIGA_MAC_VER_09,
+	RTL_GIGA_MAC_VER_10,
+	RTL_GIGA_MAC_VER_11,
+	RTL_GIGA_MAC_VER_12,
+	RTL_GIGA_MAC_VER_13,
+	RTL_GIGA_MAC_VER_14,
+	RTL_GIGA_MAC_VER_15,
+	RTL_GIGA_MAC_VER_16,
+	RTL_GIGA_MAC_VER_17,
+	RTL_GIGA_MAC_VER_18,
+	RTL_GIGA_MAC_VER_19,
+	RTL_GIGA_MAC_VER_20,
+	RTL_GIGA_MAC_VER_21,
+	RTL_GIGA_MAC_VER_22,
+	RTL_GIGA_MAC_VER_23,
+	RTL_GIGA_MAC_VER_24,
+	RTL_GIGA_MAC_VER_25,
+	RTL_GIGA_MAC_VER_26,
+	RTL_GIGA_MAC_VER_27,
+	RTL_GIGA_MAC_VER_28,
+	RTL_GIGA_MAC_VER_29,
+	RTL_GIGA_MAC_VER_30,
+	RTL_GIGA_MAC_VER_31,
+	RTL_GIGA_MAC_VER_32,
+	RTL_GIGA_MAC_VER_33,
+	RTL_GIGA_MAC_VER_34,
+	RTL_GIGA_MAC_VER_35,
+	RTL_GIGA_MAC_VER_36,
+	RTL_GIGA_MAC_VER_37,
+	RTL_GIGA_MAC_VER_38,
+	RTL_GIGA_MAC_VER_39,
+	RTL_GIGA_MAC_VER_40,
+	RTL_GIGA_MAC_VER_41,
+	RTL_GIGA_MAC_VER_42,
+	RTL_GIGA_MAC_VER_43,
+	RTL_GIGA_MAC_VER_44,
+	RTL_GIGA_MAC_VER_45,
+	RTL_GIGA_MAC_VER_46,
+	RTL_GIGA_MAC_VER_47,
+	RTL_GIGA_MAC_VER_48,
+	RTL_GIGA_MAC_VER_49,
+	RTL_GIGA_MAC_VER_50,
+	RTL_GIGA_MAC_VER_51,
+	RTL_GIGA_MAC_VER_60,
+	RTL_GIGA_MAC_VER_61,
+	RTL_GIGA_MAC_NONE
+};
+
+#define JUMBO_1K	ETH_DATA_LEN
+#define JUMBO_4K	(4*1024 - ETH_HLEN - 2)
+#define JUMBO_6K	(6*1024 - ETH_HLEN - 2)
+#define JUMBO_7K	(7*1024 - ETH_HLEN - 2)
+#define JUMBO_9K	(9*1024 - ETH_HLEN - 2)
+
+static const struct {
+	const char *name;
+	const char *fw_name;
+} rtl_chip_infos[] = {
+	/* PCI devices. */
+	[RTL_GIGA_MAC_VER_02] = {"RTL8169s"				},
+	[RTL_GIGA_MAC_VER_03] = {"RTL8110s"				},
+	[RTL_GIGA_MAC_VER_04] = {"RTL8169sb/8110sb"			},
+	[RTL_GIGA_MAC_VER_05] = {"RTL8169sc/8110sc"			},
+	[RTL_GIGA_MAC_VER_06] = {"RTL8169sc/8110sc"			},
+	/* PCI-E devices. */
+	[RTL_GIGA_MAC_VER_07] = {"RTL8102e"				},
+	[RTL_GIGA_MAC_VER_08] = {"RTL8102e"				},
+	[RTL_GIGA_MAC_VER_09] = {"RTL8102e/RTL8103e"			},
+	[RTL_GIGA_MAC_VER_10] = {"RTL8101e"				},
+	[RTL_GIGA_MAC_VER_11] = {"RTL8168b/8111b"			},
+	[RTL_GIGA_MAC_VER_12] = {"RTL8168b/8111b"			},
+	[RTL_GIGA_MAC_VER_13] = {"RTL8101e"				},
+	[RTL_GIGA_MAC_VER_14] = {"RTL8100e"				},
+	[RTL_GIGA_MAC_VER_15] = {"RTL8100e"				},
+	[RTL_GIGA_MAC_VER_16] = {"RTL8101e"				},
+	[RTL_GIGA_MAC_VER_17] = {"RTL8168b/8111b"			},
+	[RTL_GIGA_MAC_VER_18] = {"RTL8168cp/8111cp"			},
+	[RTL_GIGA_MAC_VER_19] = {"RTL8168c/8111c"			},
+	[RTL_GIGA_MAC_VER_20] = {"RTL8168c/8111c"			},
+	[RTL_GIGA_MAC_VER_21] = {"RTL8168c/8111c"			},
+	[RTL_GIGA_MAC_VER_22] = {"RTL8168c/8111c"			},
+	[RTL_GIGA_MAC_VER_23] = {"RTL8168cp/8111cp"			},
+	[RTL_GIGA_MAC_VER_24] = {"RTL8168cp/8111cp"			},
+	[RTL_GIGA_MAC_VER_25] = {"RTL8168d/8111d",	FIRMWARE_8168D_1},
+	[RTL_GIGA_MAC_VER_26] = {"RTL8168d/8111d",	FIRMWARE_8168D_2},
+	[RTL_GIGA_MAC_VER_27] = {"RTL8168dp/8111dp"			},
+	[RTL_GIGA_MAC_VER_28] = {"RTL8168dp/8111dp"			},
+	[RTL_GIGA_MAC_VER_29] = {"RTL8105e",		FIRMWARE_8105E_1},
+	[RTL_GIGA_MAC_VER_30] = {"RTL8105e",		FIRMWARE_8105E_1},
+	[RTL_GIGA_MAC_VER_31] = {"RTL8168dp/8111dp"			},
+	[RTL_GIGA_MAC_VER_32] = {"RTL8168e/8111e",	FIRMWARE_8168E_1},
+	[RTL_GIGA_MAC_VER_33] = {"RTL8168e/8111e",	FIRMWARE_8168E_2},
+	[RTL_GIGA_MAC_VER_34] = {"RTL8168evl/8111evl",	FIRMWARE_8168E_3},
+	[RTL_GIGA_MAC_VER_35] = {"RTL8168f/8111f",	FIRMWARE_8168F_1},
+	[RTL_GIGA_MAC_VER_36] = {"RTL8168f/8111f",	FIRMWARE_8168F_2},
+	[RTL_GIGA_MAC_VER_37] = {"RTL8402",		FIRMWARE_8402_1 },
+	[RTL_GIGA_MAC_VER_38] = {"RTL8411",		FIRMWARE_8411_1 },
+	[RTL_GIGA_MAC_VER_39] = {"RTL8106e",		FIRMWARE_8106E_1},
+	[RTL_GIGA_MAC_VER_40] = {"RTL8168g/8111g",	FIRMWARE_8168G_2},
+	[RTL_GIGA_MAC_VER_41] = {"RTL8168g/8111g"			},
+	[RTL_GIGA_MAC_VER_42] = {"RTL8168gu/8111gu",	FIRMWARE_8168G_3},
+	[RTL_GIGA_MAC_VER_43] = {"RTL8106eus",		FIRMWARE_8106E_2},
+	[RTL_GIGA_MAC_VER_44] = {"RTL8411b",		FIRMWARE_8411_2 },
+	[RTL_GIGA_MAC_VER_45] = {"RTL8168h/8111h",	FIRMWARE_8168H_1},
+	[RTL_GIGA_MAC_VER_46] = {"RTL8168h/8111h",	FIRMWARE_8168H_2},
+	[RTL_GIGA_MAC_VER_47] = {"RTL8107e",		FIRMWARE_8107E_1},
+	[RTL_GIGA_MAC_VER_48] = {"RTL8107e",		FIRMWARE_8107E_2},
+	[RTL_GIGA_MAC_VER_49] = {"RTL8168ep/8111ep"			},
+	[RTL_GIGA_MAC_VER_50] = {"RTL8168ep/8111ep"			},
+	[RTL_GIGA_MAC_VER_51] = {"RTL8168ep/8111ep"			},
+	[RTL_GIGA_MAC_VER_60] = {"RTL8125"				},
+	[RTL_GIGA_MAC_VER_61] = {"RTL8125",		FIRMWARE_8125A_3},
+};
+
+static const struct pci_device_id rtl8169_pci_tbl[] = {
+	{ PCI_VDEVICE(REALTEK,	0x2502) },
+	{ PCI_VDEVICE(REALTEK,	0x2600) },
+	{ PCI_VDEVICE(REALTEK,	0x8129) },
+	{ PCI_VDEVICE(REALTEK,	0x8136), RTL_CFG_NO_GBIT },
+	{ PCI_VDEVICE(REALTEK,	0x8161) },
+	{ PCI_VDEVICE(REALTEK,	0x8167) },
+	{ PCI_VDEVICE(REALTEK,	0x8168) },
+	{ PCI_VDEVICE(NCUBE,	0x8168) },
+	{ PCI_VDEVICE(REALTEK,	0x8169) },
+	{ PCI_VENDOR_ID_DLINK,	0x4300,
+		PCI_VENDOR_ID_DLINK, 0x4b10, 0, 0 },
+	{ PCI_VDEVICE(DLINK,	0x4300) },
+	{ PCI_VDEVICE(DLINK,	0x4302) },
+	{ PCI_VDEVICE(AT,	0xc107) },
+	{ PCI_VDEVICE(USR,	0x0116) },
+	{ PCI_VENDOR_ID_LINKSYS, 0x1032, PCI_ANY_ID, 0x0024 },
+	{ 0x0001, 0x8168, PCI_ANY_ID, 0x2410 },
+	{ PCI_VDEVICE(REALTEK,	0x8125) },
+	{ PCI_VDEVICE(REALTEK,	0x3000) },
+	{}
+};
+
+MODULE_DEVICE_TABLE(pci, rtl8169_pci_tbl);
+
+static struct {
+	u32 msg_enable;
+} debug = { -1 };
+
+enum rtl_registers {
+	MAC0		= 0,	/* Ethernet hardware address. */
+	MAC4		= 4,
+	MAR0		= 8,	/* Multicast filter. */
+	CounterAddrLow		= 0x10,
+	CounterAddrHigh		= 0x14,
+	TxDescStartAddrLow	= 0x20,
+	TxDescStartAddrHigh	= 0x24,
+	TxHDescStartAddrLow	= 0x28,
+	TxHDescStartAddrHigh	= 0x2c,
+	FLASH		= 0x30,
+	ERSR		= 0x36,
+	ChipCmd		= 0x37,
+	TxPoll		= 0x38,
+	IntrMask	= 0x3c,
+	IntrStatus	= 0x3e,
+
+	TxConfig	= 0x40,
+#define	TXCFG_AUTO_FIFO			(1 << 7)	/* 8111e-vl */
+#define	TXCFG_EMPTY			(1 << 11)	/* 8111e-vl */
+
+	RxConfig	= 0x44,
+#define	RX128_INT_EN			(1 << 15)	/* 8111c and later */
+#define	RX_MULTI_EN			(1 << 14)	/* 8111c only */
+#define	RXCFG_FIFO_SHIFT		13
+					/* No threshold before first PCI xfer */
+#define	RX_FIFO_THRESH			(7 << RXCFG_FIFO_SHIFT)
+#define	RX_EARLY_OFF			(1 << 11)
+#define	RXCFG_DMA_SHIFT			8
+					/* Unlimited maximum PCI burst. */
+#define	RX_DMA_BURST			(7 << RXCFG_DMA_SHIFT)
+
+	RxMissed	= 0x4c,
+	Cfg9346		= 0x50,
+	Config0		= 0x51,
+	Config1		= 0x52,
+	Config2		= 0x53,
+#define PME_SIGNAL			(1 << 5)	/* 8168c and later */
+
+	Config3		= 0x54,
+	Config4		= 0x55,
+	Config5		= 0x56,
+	PHYAR		= 0x60,
+	PHYstatus	= 0x6c,
+	RxMaxSize	= 0xda,
+	CPlusCmd	= 0xe0,
+	IntrMitigate	= 0xe2,
+
+#define RTL_COALESCE_MASK	0x0f
+#define RTL_COALESCE_SHIFT	4
+#define RTL_COALESCE_T_MAX	(RTL_COALESCE_MASK)
+#define RTL_COALESCE_FRAME_MAX	(RTL_COALESCE_MASK << 2)
+
+	RxDescAddrLow	= 0xe4,
+	RxDescAddrHigh	= 0xe8,
+	EarlyTxThres	= 0xec,	/* 8169. Unit of 32 bytes. */
+
+#define NoEarlyTx	0x3f	/* Max value : no early transmit. */
+
+	MaxTxPacketSize	= 0xec,	/* 8101/8168. Unit of 128 bytes. */
+
+#define TxPacketMax	(8064 >> 7)
+#define EarlySize	0x27
+
+	FuncEvent	= 0xf0,
+	FuncEventMask	= 0xf4,
+	FuncPresetState	= 0xf8,
+	IBCR0           = 0xf8,
+	IBCR2           = 0xf9,
+	IBIMR0          = 0xfa,
+	IBISR0          = 0xfb,
+	FuncForceEvent	= 0xfc,
+};
+
+enum rtl8168_8101_registers {
+	CSIDR			= 0x64,
+	CSIAR			= 0x68,
+#define	CSIAR_FLAG			0x80000000
+#define	CSIAR_WRITE_CMD			0x80000000
+#define	CSIAR_BYTE_ENABLE		0x0000f000
+#define	CSIAR_ADDR_MASK			0x00000fff
+	PMCH			= 0x6f,
+	EPHYAR			= 0x80,
+#define	EPHYAR_FLAG			0x80000000
+#define	EPHYAR_WRITE_CMD		0x80000000
+#define	EPHYAR_REG_MASK			0x1f
+#define	EPHYAR_REG_SHIFT		16
+#define	EPHYAR_DATA_MASK		0xffff
+	DLLPR			= 0xd0,
+#define	PFM_EN				(1 << 6)
+#define	TX_10M_PS_EN			(1 << 7)
+	DBG_REG			= 0xd1,
+#define	FIX_NAK_1			(1 << 4)
+#define	FIX_NAK_2			(1 << 3)
+	TWSI			= 0xd2,
+	MCU			= 0xd3,
+#define	NOW_IS_OOB			(1 << 7)
+#define	TX_EMPTY			(1 << 5)
+#define	RX_EMPTY			(1 << 4)
+#define	RXTX_EMPTY			(TX_EMPTY | RX_EMPTY)
+#define	EN_NDP				(1 << 3)
+#define	EN_OOB_RESET			(1 << 2)
+#define	LINK_LIST_RDY			(1 << 1)
+	EFUSEAR			= 0xdc,
+#define	EFUSEAR_FLAG			0x80000000
+#define	EFUSEAR_WRITE_CMD		0x80000000
+#define	EFUSEAR_READ_CMD		0x00000000
+#define	EFUSEAR_REG_MASK		0x03ff
+#define	EFUSEAR_REG_SHIFT		8
+#define	EFUSEAR_DATA_MASK		0xff
+	MISC_1			= 0xf2,
+#define	PFM_D3COLD_EN			(1 << 6)
+};
+
+enum rtl8168_registers {
+	LED_FREQ		= 0x1a,
+	EEE_LED			= 0x1b,
+	ERIDR			= 0x70,
+	ERIAR			= 0x74,
+#define ERIAR_FLAG			0x80000000
+#define ERIAR_WRITE_CMD			0x80000000
+#define ERIAR_READ_CMD			0x00000000
+#define ERIAR_ADDR_BYTE_ALIGN		4
+#define ERIAR_TYPE_SHIFT		16
+#define ERIAR_EXGMAC			(0x00 << ERIAR_TYPE_SHIFT)
+#define ERIAR_MSIX			(0x01 << ERIAR_TYPE_SHIFT)
+#define ERIAR_ASF			(0x02 << ERIAR_TYPE_SHIFT)
+#define ERIAR_OOB			(0x02 << ERIAR_TYPE_SHIFT)
+#define ERIAR_MASK_SHIFT		12
+#define ERIAR_MASK_0001			(0x1 << ERIAR_MASK_SHIFT)
+#define ERIAR_MASK_0011			(0x3 << ERIAR_MASK_SHIFT)
+#define ERIAR_MASK_0100			(0x4 << ERIAR_MASK_SHIFT)
+#define ERIAR_MASK_0101			(0x5 << ERIAR_MASK_SHIFT)
+#define ERIAR_MASK_1111			(0xf << ERIAR_MASK_SHIFT)
+	EPHY_RXER_NUM		= 0x7c,
+	OCPDR			= 0xb0,	/* OCP GPHY access */
+#define OCPDR_WRITE_CMD			0x80000000
+#define OCPDR_READ_CMD			0x00000000
+#define OCPDR_REG_MASK			0x7f
+#define OCPDR_GPHY_REG_SHIFT		16
+#define OCPDR_DATA_MASK			0xffff
+	OCPAR			= 0xb4,
+#define OCPAR_FLAG			0x80000000
+#define OCPAR_GPHY_WRITE_CMD		0x8000f060
+#define OCPAR_GPHY_READ_CMD		0x0000f060
+	GPHY_OCP		= 0xb8,
+	RDSAR1			= 0xd0,	/* 8168c only. Undocumented on 8168dp */
+	MISC			= 0xf0,	/* 8168e only. */
+#define TXPLA_RST			(1 << 29)
+#define DISABLE_LAN_EN			(1 << 23) /* Enable GPIO pin */
+#define PWM_EN				(1 << 22)
+#define RXDV_GATED_EN			(1 << 19)
+#define EARLY_TALLY_EN			(1 << 16)
+};
+
+enum rtl8125_registers {
+	IntrMask_8125		= 0x38,
+	IntrStatus_8125		= 0x3c,
+	TxPoll_8125		= 0x90,
+	MAC0_BKP		= 0x19e0,
+};
+
+#define RX_VLAN_INNER_8125	BIT(22)
+#define RX_VLAN_OUTER_8125	BIT(23)
+#define RX_VLAN_8125		(RX_VLAN_INNER_8125 | RX_VLAN_OUTER_8125)
+
+#define RX_FETCH_DFLT_8125	(8 << 27)
+
+enum rtl_register_content {
+	/* InterruptStatusBits */
+	SYSErr		= 0x8000,
+	PCSTimeout	= 0x4000,
+	SWInt		= 0x0100,
+	TxDescUnavail	= 0x0080,
+	RxFIFOOver	= 0x0040,
+	LinkChg		= 0x0020,
+	RxOverflow	= 0x0010,
+	TxErr		= 0x0008,
+	TxOK		= 0x0004,
+	RxErr		= 0x0002,
+	RxOK		= 0x0001,
+
+	/* RxStatusDesc */
+	RxRWT	= (1 << 22),
+	RxRES	= (1 << 21),
+	RxRUNT	= (1 << 20),
+	RxCRC	= (1 << 19),
+
+	/* ChipCmdBits */
+	StopReq		= 0x80,
+	CmdReset	= 0x10,
+	CmdRxEnb	= 0x08,
+	CmdTxEnb	= 0x04,
+	RxBufEmpty	= 0x01,
+
+	/* TXPoll register p.5 */
+	HPQ		= 0x80,		/* Poll cmd on the high prio queue */
+	NPQ		= 0x40,		/* Poll cmd on the low prio queue */
+	FSWInt		= 0x01,		/* Forced software interrupt */
+
+	/* Cfg9346Bits */
+	Cfg9346_Lock	= 0x00,
+	Cfg9346_Unlock	= 0xc0,
+
+	/* rx_mode_bits */
+	AcceptErr	= 0x20,
+	AcceptRunt	= 0x10,
+	AcceptBroadcast	= 0x08,
+	AcceptMulticast	= 0x04,
+	AcceptMyPhys	= 0x02,
+	AcceptAllPhys	= 0x01,
+#define RX_CONFIG_ACCEPT_MASK		0x3f
+
+	/* TxConfigBits */
+	TxInterFrameGapShift = 24,
+	TxDMAShift = 8,	/* DMA burst value (0-7) is shift this many bits */
+
+	/* Config1 register p.24 */
+	LEDS1		= (1 << 7),
+	LEDS0		= (1 << 6),
+	Speed_down	= (1 << 4),
+	MEMMAP		= (1 << 3),
+	IOMAP		= (1 << 2),
+	VPD		= (1 << 1),
+	PMEnable	= (1 << 0),	/* Power Management Enable */
+
+	/* Config2 register p. 25 */
+	ClkReqEn	= (1 << 7),	/* Clock Request Enable */
+	MSIEnable	= (1 << 5),	/* 8169 only. Reserved in the 8168. */
+	PCI_Clock_66MHz = 0x01,
+	PCI_Clock_33MHz = 0x00,
+
+	/* Config3 register p.25 */
+	MagicPacket	= (1 << 5),	/* Wake up when receives a Magic Packet */
+	LinkUp		= (1 << 4),	/* Wake up when the cable connection is re-established */
+	Jumbo_En0	= (1 << 2),	/* 8168 only. Reserved in the 8168b */
+	Rdy_to_L23	= (1 << 1),	/* L23 Enable */
+	Beacon_en	= (1 << 0),	/* 8168 only. Reserved in the 8168b */
+
+	/* Config4 register */
+	Jumbo_En1	= (1 << 1),	/* 8168 only. Reserved in the 8168b */
+
+	/* Config5 register p.27 */
+	BWF		= (1 << 6),	/* Accept Broadcast wakeup frame */
+	MWF		= (1 << 5),	/* Accept Multicast wakeup frame */
+	UWF		= (1 << 4),	/* Accept Unicast wakeup frame */
+	Spi_en		= (1 << 3),
+	LanWake		= (1 << 1),	/* LanWake enable/disable */
+	PMEStatus	= (1 << 0),	/* PME status can be reset by PCI RST# */
+	ASPM_en		= (1 << 0),	/* ASPM enable */
+
+	/* CPlusCmd p.31 */
+	EnableBist	= (1 << 15),	// 8168 8101
+	Mac_dbgo_oe	= (1 << 14),	// 8168 8101
+	Normal_mode	= (1 << 13),	// unused
+	Force_half_dup	= (1 << 12),	// 8168 8101
+	Force_rxflow_en	= (1 << 11),	// 8168 8101
+	Force_txflow_en	= (1 << 10),	// 8168 8101
+	Cxpl_dbg_sel	= (1 << 9),	// 8168 8101
+	ASF		= (1 << 8),	// 8168 8101
+	PktCntrDisable	= (1 << 7),	// 8168 8101
+	Mac_dbgo_sel	= 0x001c,	// 8168
+	RxVlan		= (1 << 6),
+	RxChkSum	= (1 << 5),
+	PCIDAC		= (1 << 4),
+	PCIMulRW	= (1 << 3),
+#define INTT_MASK	GENMASK(1, 0)
+#define CPCMD_MASK	(Normal_mode | RxVlan | RxChkSum | INTT_MASK)
+
+	/* rtl8169_PHYstatus */
+	TBI_Enable	= 0x80,
+	TxFlowCtrl	= 0x40,
+	RxFlowCtrl	= 0x20,
+	_1000bpsF	= 0x10,
+	_100bps		= 0x08,
+	_10bps		= 0x04,
+	LinkStatus	= 0x02,
+	FullDup		= 0x01,
+
+	/* ResetCounterCommand */
+	CounterReset	= 0x1,
+
+	/* DumpCounterCommand */
+	CounterDump	= 0x8,
+
+	/* magic enable v2 */
+	MagicPacket_v2	= (1 << 16),	/* Wake up when receives a Magic Packet */
+};
+
+enum rtl_desc_bit {
+	/* First doubleword. */
+	DescOwn		= (1 << 31), /* Descriptor is owned by NIC */
+	RingEnd		= (1 << 30), /* End of descriptor ring */
+	FirstFrag	= (1 << 29), /* First segment of a packet */
+	LastFrag	= (1 << 28), /* Final segment of a packet */
+};
+
+/* Generic case. */
+enum rtl_tx_desc_bit {
+	/* First doubleword. */
+	TD_LSO		= (1 << 27),		/* Large Send Offload */
+#define TD_MSS_MAX			0x07ffu	/* MSS value */
+
+	/* Second doubleword. */
+	TxVlanTag	= (1 << 17),		/* Add VLAN tag */
+};
+
+/* 8169, 8168b and 810x except 8102e. */
+enum rtl_tx_desc_bit_0 {
+	/* First doubleword. */
+#define TD0_MSS_SHIFT			16	/* MSS position (11 bits) */
+	TD0_TCP_CS	= (1 << 16),		/* Calculate TCP/IP checksum */
+	TD0_UDP_CS	= (1 << 17),		/* Calculate UDP/IP checksum */
+	TD0_IP_CS	= (1 << 18),		/* Calculate IP checksum */
+};
+
+/* 8102e, 8168c and beyond. */
+enum rtl_tx_desc_bit_1 {
+	/* First doubleword. */
+	TD1_GTSENV4	= (1 << 26),		/* Giant Send for IPv4 */
+	TD1_GTSENV6	= (1 << 25),		/* Giant Send for IPv6 */
+#define GTTCPHO_SHIFT			18
+#define GTTCPHO_MAX			0x7f
+
+	/* Second doubleword. */
+#define TCPHO_SHIFT			18
+#define TCPHO_MAX			0x3ff
+#define TD1_MSS_SHIFT			18	/* MSS position (11 bits) */
+	TD1_IPv6_CS	= (1 << 28),		/* Calculate IPv6 checksum */
+	TD1_IPv4_CS	= (1 << 29),		/* Calculate IPv4 checksum */
+	TD1_TCP_CS	= (1 << 30),		/* Calculate TCP/IP checksum */
+	TD1_UDP_CS	= (1 << 31),		/* Calculate UDP/IP checksum */
+};
+
+enum rtl_rx_desc_bit {
+	/* Rx private */
+	PID1		= (1 << 18), /* Protocol ID bit 1/2 */
+	PID0		= (1 << 17), /* Protocol ID bit 0/2 */
+
+#define RxProtoUDP	(PID1)
+#define RxProtoTCP	(PID0)
+#define RxProtoIP	(PID1 | PID0)
+#define RxProtoMask	RxProtoIP
+
+	IPFail		= (1 << 16), /* IP checksum failed */
+	UDPFail		= (1 << 15), /* UDP/IP checksum failed */
+	TCPFail		= (1 << 14), /* TCP/IP checksum failed */
+	RxVlanTag	= (1 << 16), /* VLAN tag available */
+};
+
+#define RsvdMask	0x3fffc000
+
+#define RTL_GSO_MAX_SIZE_V1	32000
+#define RTL_GSO_MAX_SEGS_V1	24
+#define RTL_GSO_MAX_SIZE_V2	64000
+#define RTL_GSO_MAX_SEGS_V2	64
+
+struct TxDesc {
+	__le32 opts1;
+	__le32 opts2;
+	__le64 addr;
+};
+
+struct RxDesc {
+	__le32 opts1;
+	__le32 opts2;
+	__le64 addr;
+};
+
+struct ring_info {
+	struct sk_buff	*skb;
+	u32		len;
+};
+
+struct rtl8169_counters {
+	__le64	tx_packets;
+	__le64	rx_packets;
+	__le64	tx_errors;
+	__le32	rx_errors;
+	__le16	rx_missed;
+	__le16	align_errors;
+	__le32	tx_one_collision;
+	__le32	tx_multi_collision;
+	__le64	rx_unicast;
+	__le64	rx_broadcast;
+	__le32	rx_multicast;
+	__le16	tx_aborted;
+	__le16	tx_underun;
+};
+
+struct rtl8169_tc_offsets {
+	bool	inited;
+	__le64	tx_errors;
+	__le32	tx_multi_collision;
+	__le16	tx_aborted;
+};
+
+enum rtl_flag {
+	RTL_FLAG_TASK_ENABLED = 0,
+	RTL_FLAG_TASK_RESET_PENDING,
+	RTL_FLAG_MAX
+};
+
+struct rtl8169_stats {
+	u64			packets;
+	u64			bytes;
+	struct u64_stats_sync	syncp;
+};
+
+struct rtl8169_private {
+	void __iomem *mmio_addr;	/* memory map physical address */
+	struct pci_dev *pci_dev;
+	struct net_device *dev;
+	struct phy_device *phydev;
+	struct napi_struct napi;
+	u32 msg_enable;
+	enum mac_version mac_version;
+	u32 cur_rx; /* Index into the Rx descriptor buffer of next Rx pkt. */
+	u32 cur_tx; /* Index into the Tx descriptor buffer of next Rx pkt. */
+	u32 dirty_tx;
+	struct rtl8169_stats rx_stats;
+	struct rtl8169_stats tx_stats;
+	struct TxDesc *TxDescArray;	/* 256-aligned Tx descriptor ring */
+	struct RxDesc *RxDescArray;	/* 256-aligned Rx descriptor ring */
+	dma_addr_t TxPhyAddr;
+	dma_addr_t RxPhyAddr;
+	struct page *Rx_databuff[NUM_RX_DESC];	/* Rx data buffers */
+	struct ring_info tx_skb[NUM_TX_DESC];	/* Tx data buffers */
+	u16 cp_cmd;
+	u32 irq_mask;
+	struct clk *clk;
+
+	struct {
+		DECLARE_BITMAP(flags, RTL_FLAG_MAX);
+		struct mutex mutex;
+		struct work_struct work;
+	} wk;
+
+	unsigned irq_enabled:1;
+	unsigned supports_gmii:1;
+	unsigned aspm_manageable:1;
+	dma_addr_t counters_phys_addr;
+	struct rtl8169_counters *counters;
+	struct rtl8169_tc_offsets tc_offset;
+	u32 saved_wolopts;
+
+	const char *fw_name;
+	struct rtl_fw *rtl_fw;
+
+	u32 ocp_base;
+};
+
+typedef void (*rtl_generic_fct)(struct rtl8169_private *tp);
+
+MODULE_AUTHOR("Realtek and the Linux r8169 crew <netdev@vger.kernel.org>");
+MODULE_DESCRIPTION("RealTek RTL-8169 Gigabit Ethernet driver");
+module_param_named(debug, debug.msg_enable, int, 0);
+MODULE_PARM_DESC(debug, "Debug verbosity level (0=none, ..., 16=all)");
+MODULE_SOFTDEP("pre: realtek");
+MODULE_LICENSE("GPL");
+MODULE_FIRMWARE(FIRMWARE_8168D_1);
+MODULE_FIRMWARE(FIRMWARE_8168D_2);
+MODULE_FIRMWARE(FIRMWARE_8168E_1);
+MODULE_FIRMWARE(FIRMWARE_8168E_2);
+MODULE_FIRMWARE(FIRMWARE_8168E_3);
+MODULE_FIRMWARE(FIRMWARE_8105E_1);
+MODULE_FIRMWARE(FIRMWARE_8168F_1);
+MODULE_FIRMWARE(FIRMWARE_8168F_2);
+MODULE_FIRMWARE(FIRMWARE_8402_1);
+MODULE_FIRMWARE(FIRMWARE_8411_1);
+MODULE_FIRMWARE(FIRMWARE_8411_2);
+MODULE_FIRMWARE(FIRMWARE_8106E_1);
+MODULE_FIRMWARE(FIRMWARE_8106E_2);
+MODULE_FIRMWARE(FIRMWARE_8168G_2);
+MODULE_FIRMWARE(FIRMWARE_8168G_3);
+MODULE_FIRMWARE(FIRMWARE_8168H_1);
+MODULE_FIRMWARE(FIRMWARE_8168H_2);
+MODULE_FIRMWARE(FIRMWARE_8107E_1);
+MODULE_FIRMWARE(FIRMWARE_8107E_2);
+MODULE_FIRMWARE(FIRMWARE_8125A_3);
+
+static inline struct device *tp_to_dev(struct rtl8169_private *tp)
+{
+	return &tp->pci_dev->dev;
+}
+
+static void rtl_lock_work(struct rtl8169_private *tp)
+{
+	mutex_lock(&tp->wk.mutex);
+}
+
+static void rtl_unlock_work(struct rtl8169_private *tp)
+{
+	mutex_unlock(&tp->wk.mutex);
+}
+
+static void rtl_lock_config_regs(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Cfg9346, Cfg9346_Lock);
+}
+
+static void rtl_unlock_config_regs(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Cfg9346, Cfg9346_Unlock);
+}
+
+static void rtl_tx_performance_tweak(struct rtl8169_private *tp, u16 force)
+{
+	pcie_capability_clear_and_set_word(tp->pci_dev, PCI_EXP_DEVCTL,
+					   PCI_EXP_DEVCTL_READRQ, force);
+}
+
+static bool rtl_is_8125(struct rtl8169_private *tp)
+{
+	return tp->mac_version >= RTL_GIGA_MAC_VER_60;
+}
+
+static bool rtl_is_8168evl_up(struct rtl8169_private *tp)
+{
+	return tp->mac_version >= RTL_GIGA_MAC_VER_34 &&
+	       tp->mac_version != RTL_GIGA_MAC_VER_39 &&
+	       tp->mac_version <= RTL_GIGA_MAC_VER_51;
+}
+
+static bool rtl_supports_eee(struct rtl8169_private *tp)
+{
+	return tp->mac_version >= RTL_GIGA_MAC_VER_34 &&
+	       tp->mac_version != RTL_GIGA_MAC_VER_37 &&
+	       tp->mac_version != RTL_GIGA_MAC_VER_39;
+}
+
+static void rtl_read_mac_from_reg(struct rtl8169_private *tp, u8 *mac, int reg)
+{
+	int i;
+
+	for (i = 0; i < ETH_ALEN; i++)
+		mac[i] = RTL_R8(tp, reg + i);
+}
+
+struct rtl_cond {
+	bool (*check)(struct rtl8169_private *);
+	const char *msg;
+};
+
+static void rtl_udelay(unsigned int d)
+{
+	udelay(d);
+}
+
+static bool rtl_loop_wait(struct rtl8169_private *tp, const struct rtl_cond *c,
+			  void (*delay)(unsigned int), unsigned int d, int n,
+			  bool high)
+{
+	int i;
+
+	for (i = 0; i < n; i++) {
+		if (c->check(tp) == high)
+			return true;
+		delay(d);
+	}
+	netif_err(tp, drv, tp->dev, "%s == %d (loop: %d, delay: %d).\n",
+		  c->msg, !high, n, d);
+	return false;
+}
+
+static bool rtl_udelay_loop_wait_high(struct rtl8169_private *tp,
+				      const struct rtl_cond *c,
+				      unsigned int d, int n)
+{
+	return rtl_loop_wait(tp, c, rtl_udelay, d, n, true);
+}
+
+static bool rtl_udelay_loop_wait_low(struct rtl8169_private *tp,
+				     const struct rtl_cond *c,
+				     unsigned int d, int n)
+{
+	return rtl_loop_wait(tp, c, rtl_udelay, d, n, false);
+}
+
+static bool rtl_msleep_loop_wait_high(struct rtl8169_private *tp,
+				      const struct rtl_cond *c,
+				      unsigned int d, int n)
+{
+	return rtl_loop_wait(tp, c, msleep, d, n, true);
+}
+
+static bool rtl_msleep_loop_wait_low(struct rtl8169_private *tp,
+				     const struct rtl_cond *c,
+				     unsigned int d, int n)
+{
+	return rtl_loop_wait(tp, c, msleep, d, n, false);
+}
+
+#define DECLARE_RTL_COND(name)				\
+static bool name ## _check(struct rtl8169_private *);	\
+							\
+static const struct rtl_cond name = {			\
+	.check	= name ## _check,			\
+	.msg	= #name					\
+};							\
+							\
+static bool name ## _check(struct rtl8169_private *tp)
+
+static bool rtl_ocp_reg_failure(struct rtl8169_private *tp, u32 reg)
+{
+	if (reg & 0xffff0001) {
+		netif_err(tp, drv, tp->dev, "Invalid ocp reg %x!\n", reg);
+		return true;
+	}
+	return false;
+}
+
+DECLARE_RTL_COND(rtl_ocp_gphy_cond)
+{
+	return RTL_R32(tp, GPHY_OCP) & OCPAR_FLAG;
+}
+
+static void r8168_phy_ocp_write(struct rtl8169_private *tp, u32 reg, u32 data)
+{
+	if (rtl_ocp_reg_failure(tp, reg))
+		return;
+
+	RTL_W32(tp, GPHY_OCP, OCPAR_FLAG | (reg << 15) | data);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_ocp_gphy_cond, 25, 10);
+}
+
+static int r8168_phy_ocp_read(struct rtl8169_private *tp, u32 reg)
+{
+	if (rtl_ocp_reg_failure(tp, reg))
+		return 0;
+
+	RTL_W32(tp, GPHY_OCP, reg << 15);
+
+	return rtl_udelay_loop_wait_high(tp, &rtl_ocp_gphy_cond, 25, 10) ?
+		(RTL_R32(tp, GPHY_OCP) & 0xffff) : -ETIMEDOUT;
+}
+
+static void r8168_mac_ocp_write(struct rtl8169_private *tp, u32 reg, u32 data)
+{
+	if (rtl_ocp_reg_failure(tp, reg))
+		return;
+
+	RTL_W32(tp, OCPDR, OCPAR_FLAG | (reg << 15) | data);
+}
+
+static u16 r8168_mac_ocp_read(struct rtl8169_private *tp, u32 reg)
+{
+	if (rtl_ocp_reg_failure(tp, reg))
+		return 0;
+
+	RTL_W32(tp, OCPDR, reg << 15);
+
+	return RTL_R32(tp, OCPDR);
+}
+
+static void r8168_mac_ocp_modify(struct rtl8169_private *tp, u32 reg, u16 mask,
+				 u16 set)
+{
+	u16 data = r8168_mac_ocp_read(tp, reg);
+
+	r8168_mac_ocp_write(tp, reg, (data & ~mask) | set);
+}
+
+#define OCP_STD_PHY_BASE	0xa400
+
+static void r8168g_mdio_write(struct rtl8169_private *tp, int reg, int value)
+{
+	if (reg == 0x1f) {
+		tp->ocp_base = value ? value << 4 : OCP_STD_PHY_BASE;
+		return;
+	}
+
+	if (tp->ocp_base != OCP_STD_PHY_BASE)
+		reg -= 0x10;
+
+	r8168_phy_ocp_write(tp, tp->ocp_base + reg * 2, value);
+}
+
+static int r8168g_mdio_read(struct rtl8169_private *tp, int reg)
+{
+	if (reg == 0x1f)
+		return tp->ocp_base == OCP_STD_PHY_BASE ? 0 : tp->ocp_base >> 4;
+
+	if (tp->ocp_base != OCP_STD_PHY_BASE)
+		reg -= 0x10;
+
+	return r8168_phy_ocp_read(tp, tp->ocp_base + reg * 2);
+}
+
+static void mac_mcu_write(struct rtl8169_private *tp, int reg, int value)
+{
+	if (reg == 0x1f) {
+		tp->ocp_base = value << 4;
+		return;
+	}
+
+	r8168_mac_ocp_write(tp, tp->ocp_base + reg, value);
+}
+
+static int mac_mcu_read(struct rtl8169_private *tp, int reg)
+{
+	return r8168_mac_ocp_read(tp, tp->ocp_base + reg);
+}
+
+DECLARE_RTL_COND(rtl_phyar_cond)
+{
+	return RTL_R32(tp, PHYAR) & 0x80000000;
+}
+
+static void r8169_mdio_write(struct rtl8169_private *tp, int reg, int value)
+{
+	RTL_W32(tp, PHYAR, 0x80000000 | (reg & 0x1f) << 16 | (value & 0xffff));
+
+	rtl_udelay_loop_wait_low(tp, &rtl_phyar_cond, 25, 20);
+	/*
+	 * According to hardware specs a 20us delay is required after write
+	 * complete indication, but before sending next command.
+	 */
+	udelay(20);
+}
+
+static int r8169_mdio_read(struct rtl8169_private *tp, int reg)
+{
+	int value;
+
+	RTL_W32(tp, PHYAR, 0x0 | (reg & 0x1f) << 16);
+
+	value = rtl_udelay_loop_wait_high(tp, &rtl_phyar_cond, 25, 20) ?
+		RTL_R32(tp, PHYAR) & 0xffff : -ETIMEDOUT;
+
+	/*
+	 * According to hardware specs a 20us delay is required after read
+	 * complete indication, but before sending next command.
+	 */
+	udelay(20);
+
+	return value;
+}
+
+DECLARE_RTL_COND(rtl_ocpar_cond)
+{
+	return RTL_R32(tp, OCPAR) & OCPAR_FLAG;
+}
+
+static void r8168dp_1_mdio_access(struct rtl8169_private *tp, int reg, u32 data)
+{
+	RTL_W32(tp, OCPDR, data | ((reg & OCPDR_REG_MASK) << OCPDR_GPHY_REG_SHIFT));
+	RTL_W32(tp, OCPAR, OCPAR_GPHY_WRITE_CMD);
+	RTL_W32(tp, EPHY_RXER_NUM, 0);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_ocpar_cond, 1000, 100);
+}
+
+static void r8168dp_1_mdio_write(struct rtl8169_private *tp, int reg, int value)
+{
+	r8168dp_1_mdio_access(tp, reg,
+			      OCPDR_WRITE_CMD | (value & OCPDR_DATA_MASK));
+}
+
+static int r8168dp_1_mdio_read(struct rtl8169_private *tp, int reg)
+{
+	r8168dp_1_mdio_access(tp, reg, OCPDR_READ_CMD);
+
+	mdelay(1);
+	RTL_W32(tp, OCPAR, OCPAR_GPHY_READ_CMD);
+	RTL_W32(tp, EPHY_RXER_NUM, 0);
+
+	return rtl_udelay_loop_wait_high(tp, &rtl_ocpar_cond, 1000, 100) ?
+		RTL_R32(tp, OCPDR) & OCPDR_DATA_MASK : -ETIMEDOUT;
+}
+
+#define R8168DP_1_MDIO_ACCESS_BIT	0x00020000
+
+static void r8168dp_2_mdio_start(struct rtl8169_private *tp)
+{
+	RTL_W32(tp, 0xd0, RTL_R32(tp, 0xd0) & ~R8168DP_1_MDIO_ACCESS_BIT);
+}
+
+static void r8168dp_2_mdio_stop(struct rtl8169_private *tp)
+{
+	RTL_W32(tp, 0xd0, RTL_R32(tp, 0xd0) | R8168DP_1_MDIO_ACCESS_BIT);
+}
+
+static void r8168dp_2_mdio_write(struct rtl8169_private *tp, int reg, int value)
+{
+	r8168dp_2_mdio_start(tp);
+
+	r8169_mdio_write(tp, reg, value);
+
+	r8168dp_2_mdio_stop(tp);
+}
+
+static int r8168dp_2_mdio_read(struct rtl8169_private *tp, int reg)
+{
+	int value;
+
+	/* Work around issue with chip reporting wrong PHY ID */
+	if (reg == MII_PHYSID2)
+		return 0xc912;
+
+	r8168dp_2_mdio_start(tp);
+
+	value = r8169_mdio_read(tp, reg);
+
+	r8168dp_2_mdio_stop(tp);
+
+	return value;
+}
+
+static void rtl_writephy(struct rtl8169_private *tp, int location, int val)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_27:
+		r8168dp_1_mdio_write(tp, location, val);
+		break;
+	case RTL_GIGA_MAC_VER_28:
+	case RTL_GIGA_MAC_VER_31:
+		r8168dp_2_mdio_write(tp, location, val);
+		break;
+	case RTL_GIGA_MAC_VER_40 ... RTL_GIGA_MAC_VER_61:
+		r8168g_mdio_write(tp, location, val);
+		break;
+	default:
+		r8169_mdio_write(tp, location, val);
+		break;
+	}
+}
+
+static int rtl_readphy(struct rtl8169_private *tp, int location)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_27:
+		return r8168dp_1_mdio_read(tp, location);
+	case RTL_GIGA_MAC_VER_28:
+	case RTL_GIGA_MAC_VER_31:
+		return r8168dp_2_mdio_read(tp, location);
+	case RTL_GIGA_MAC_VER_40 ... RTL_GIGA_MAC_VER_61:
+		return r8168g_mdio_read(tp, location);
+	default:
+		return r8169_mdio_read(tp, location);
+	}
+}
+
+static void rtl_patchphy(struct rtl8169_private *tp, int reg_addr, int value)
+{
+	rtl_writephy(tp, reg_addr, rtl_readphy(tp, reg_addr) | value);
+}
+
+static void rtl_w0w1_phy(struct rtl8169_private *tp, int reg_addr, int p, int m)
+{
+	int val;
+
+	val = rtl_readphy(tp, reg_addr);
+	rtl_writephy(tp, reg_addr, (val & ~m) | p);
+}
+
+DECLARE_RTL_COND(rtl_ephyar_cond)
+{
+	return RTL_R32(tp, EPHYAR) & EPHYAR_FLAG;
+}
+
+static void rtl_ephy_write(struct rtl8169_private *tp, int reg_addr, int value)
+{
+	RTL_W32(tp, EPHYAR, EPHYAR_WRITE_CMD | (value & EPHYAR_DATA_MASK) |
+		(reg_addr & EPHYAR_REG_MASK) << EPHYAR_REG_SHIFT);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_ephyar_cond, 10, 100);
+
+	udelay(10);
+}
+
+static u16 rtl_ephy_read(struct rtl8169_private *tp, int reg_addr)
+{
+	RTL_W32(tp, EPHYAR, (reg_addr & EPHYAR_REG_MASK) << EPHYAR_REG_SHIFT);
+
+	return rtl_udelay_loop_wait_high(tp, &rtl_ephyar_cond, 10, 100) ?
+		RTL_R32(tp, EPHYAR) & EPHYAR_DATA_MASK : ~0;
+}
+
+DECLARE_RTL_COND(rtl_eriar_cond)
+{
+	return RTL_R32(tp, ERIAR) & ERIAR_FLAG;
+}
+
+static void _rtl_eri_write(struct rtl8169_private *tp, int addr, u32 mask,
+			   u32 val, int type)
+{
+	BUG_ON((addr & 3) || (mask == 0));
+	RTL_W32(tp, ERIDR, val);
+	RTL_W32(tp, ERIAR, ERIAR_WRITE_CMD | type | mask | addr);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_eriar_cond, 100, 100);
+}
+
+static void rtl_eri_write(struct rtl8169_private *tp, int addr, u32 mask,
+			  u32 val)
+{
+	_rtl_eri_write(tp, addr, mask, val, ERIAR_EXGMAC);
+}
+
+static u32 _rtl_eri_read(struct rtl8169_private *tp, int addr, int type)
+{
+	RTL_W32(tp, ERIAR, ERIAR_READ_CMD | type | ERIAR_MASK_1111 | addr);
+
+	return rtl_udelay_loop_wait_high(tp, &rtl_eriar_cond, 100, 100) ?
+		RTL_R32(tp, ERIDR) : ~0;
+}
+
+static u32 rtl_eri_read(struct rtl8169_private *tp, int addr)
+{
+	return _rtl_eri_read(tp, addr, ERIAR_EXGMAC);
+}
+
+static void rtl_w0w1_eri(struct rtl8169_private *tp, int addr, u32 mask, u32 p,
+			 u32 m)
+{
+	u32 val;
+
+	val = rtl_eri_read(tp, addr);
+	rtl_eri_write(tp, addr, mask, (val & ~m) | p);
+}
+
+static void rtl_eri_set_bits(struct rtl8169_private *tp, int addr, u32 mask,
+			     u32 p)
+{
+	rtl_w0w1_eri(tp, addr, mask, p, 0);
+}
+
+static void rtl_eri_clear_bits(struct rtl8169_private *tp, int addr, u32 mask,
+			       u32 m)
+{
+	rtl_w0w1_eri(tp, addr, mask, 0, m);
+}
+
+static u32 r8168dp_ocp_read(struct rtl8169_private *tp, u8 mask, u16 reg)
+{
+	RTL_W32(tp, OCPAR, ((u32)mask & 0x0f) << 12 | (reg & 0x0fff));
+	return rtl_udelay_loop_wait_high(tp, &rtl_ocpar_cond, 100, 20) ?
+		RTL_R32(tp, OCPDR) : ~0;
+}
+
+static u32 r8168ep_ocp_read(struct rtl8169_private *tp, u8 mask, u16 reg)
+{
+	return _rtl_eri_read(tp, reg, ERIAR_OOB);
+}
+
+static void r8168dp_ocp_write(struct rtl8169_private *tp, u8 mask, u16 reg,
+			      u32 data)
+{
+	RTL_W32(tp, OCPDR, data);
+	RTL_W32(tp, OCPAR, OCPAR_FLAG | ((u32)mask & 0x0f) << 12 | (reg & 0x0fff));
+	rtl_udelay_loop_wait_low(tp, &rtl_ocpar_cond, 100, 20);
+}
+
+static void r8168ep_ocp_write(struct rtl8169_private *tp, u8 mask, u16 reg,
+			      u32 data)
+{
+	_rtl_eri_write(tp, reg, ((u32)mask & 0x0f) << ERIAR_MASK_SHIFT,
+		       data, ERIAR_OOB);
+}
+
+static void r8168dp_oob_notify(struct rtl8169_private *tp, u8 cmd)
+{
+	rtl_eri_write(tp, 0xe8, ERIAR_MASK_0001, cmd);
+
+	r8168dp_ocp_write(tp, 0x1, 0x30, 0x00000001);
+}
+
+#define OOB_CMD_RESET		0x00
+#define OOB_CMD_DRIVER_START	0x05
+#define OOB_CMD_DRIVER_STOP	0x06
+
+static u16 rtl8168_get_ocp_reg(struct rtl8169_private *tp)
+{
+	return (tp->mac_version == RTL_GIGA_MAC_VER_31) ? 0xb8 : 0x10;
+}
+
+DECLARE_RTL_COND(rtl_dp_ocp_read_cond)
+{
+	u16 reg;
+
+	reg = rtl8168_get_ocp_reg(tp);
+
+	return r8168dp_ocp_read(tp, 0x0f, reg) & 0x00000800;
+}
+
+DECLARE_RTL_COND(rtl_ep_ocp_read_cond)
+{
+	return r8168ep_ocp_read(tp, 0x0f, 0x124) & 0x00000001;
+}
+
+DECLARE_RTL_COND(rtl_ocp_tx_cond)
+{
+	return RTL_R8(tp, IBISR0) & 0x20;
+}
+
+static void rtl8168ep_stop_cmac(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, IBCR2, RTL_R8(tp, IBCR2) & ~0x01);
+	rtl_msleep_loop_wait_high(tp, &rtl_ocp_tx_cond, 50, 2000);
+	RTL_W8(tp, IBISR0, RTL_R8(tp, IBISR0) | 0x20);
+	RTL_W8(tp, IBCR0, RTL_R8(tp, IBCR0) & ~0x01);
+}
+
+static void rtl8168dp_driver_start(struct rtl8169_private *tp)
+{
+	r8168dp_oob_notify(tp, OOB_CMD_DRIVER_START);
+	rtl_msleep_loop_wait_high(tp, &rtl_dp_ocp_read_cond, 10, 10);
+}
+
+static void rtl8168ep_driver_start(struct rtl8169_private *tp)
+{
+	r8168ep_ocp_write(tp, 0x01, 0x180, OOB_CMD_DRIVER_START);
+	r8168ep_ocp_write(tp, 0x01, 0x30,
+			  r8168ep_ocp_read(tp, 0x01, 0x30) | 0x01);
+	rtl_msleep_loop_wait_high(tp, &rtl_ep_ocp_read_cond, 10, 10);
+}
+
+static void rtl8168_driver_start(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_27:
+	case RTL_GIGA_MAC_VER_28:
+	case RTL_GIGA_MAC_VER_31:
+		rtl8168dp_driver_start(tp);
+		break;
+	case RTL_GIGA_MAC_VER_49:
+	case RTL_GIGA_MAC_VER_50:
+	case RTL_GIGA_MAC_VER_51:
+		rtl8168ep_driver_start(tp);
+		break;
+	default:
+		BUG();
+		break;
+	}
+}
+
+static void rtl8168dp_driver_stop(struct rtl8169_private *tp)
+{
+	r8168dp_oob_notify(tp, OOB_CMD_DRIVER_STOP);
+	rtl_msleep_loop_wait_low(tp, &rtl_dp_ocp_read_cond, 10, 10);
+}
+
+static void rtl8168ep_driver_stop(struct rtl8169_private *tp)
+{
+	rtl8168ep_stop_cmac(tp);
+	r8168ep_ocp_write(tp, 0x01, 0x180, OOB_CMD_DRIVER_STOP);
+	r8168ep_ocp_write(tp, 0x01, 0x30,
+			  r8168ep_ocp_read(tp, 0x01, 0x30) | 0x01);
+	rtl_msleep_loop_wait_low(tp, &rtl_ep_ocp_read_cond, 10, 10);
+}
+
+static void rtl8168_driver_stop(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_27:
+	case RTL_GIGA_MAC_VER_28:
+	case RTL_GIGA_MAC_VER_31:
+		rtl8168dp_driver_stop(tp);
+		break;
+	case RTL_GIGA_MAC_VER_49:
+	case RTL_GIGA_MAC_VER_50:
+	case RTL_GIGA_MAC_VER_51:
+		rtl8168ep_driver_stop(tp);
+		break;
+	default:
+		BUG();
+		break;
+	}
+}
+
+static bool r8168dp_check_dash(struct rtl8169_private *tp)
+{
+	u16 reg = rtl8168_get_ocp_reg(tp);
+
+	return !!(r8168dp_ocp_read(tp, 0x0f, reg) & 0x00008000);
+}
+
+static bool r8168ep_check_dash(struct rtl8169_private *tp)
+{
+	return !!(r8168ep_ocp_read(tp, 0x0f, 0x128) & 0x00000001);
+}
+
+static bool r8168_check_dash(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_27:
+	case RTL_GIGA_MAC_VER_28:
+	case RTL_GIGA_MAC_VER_31:
+		return r8168dp_check_dash(tp);
+	case RTL_GIGA_MAC_VER_49:
+	case RTL_GIGA_MAC_VER_50:
+	case RTL_GIGA_MAC_VER_51:
+		return r8168ep_check_dash(tp);
+	default:
+		return false;
+	}
+}
+
+static void rtl_reset_packet_filter(struct rtl8169_private *tp)
+{
+	rtl_eri_clear_bits(tp, 0xdc, ERIAR_MASK_0001, BIT(0));
+	rtl_eri_set_bits(tp, 0xdc, ERIAR_MASK_0001, BIT(0));
+}
+
+DECLARE_RTL_COND(rtl_efusear_cond)
+{
+	return RTL_R32(tp, EFUSEAR) & EFUSEAR_FLAG;
+}
+
+static u8 rtl8168d_efuse_read(struct rtl8169_private *tp, int reg_addr)
+{
+	RTL_W32(tp, EFUSEAR, (reg_addr & EFUSEAR_REG_MASK) << EFUSEAR_REG_SHIFT);
+
+	return rtl_udelay_loop_wait_high(tp, &rtl_efusear_cond, 100, 300) ?
+		RTL_R32(tp, EFUSEAR) & EFUSEAR_DATA_MASK : ~0;
+}
+
+static u32 rtl_get_events(struct rtl8169_private *tp)
+{
+	if (rtl_is_8125(tp))
+		return RTL_R32(tp, IntrStatus_8125);
+	else
+		return RTL_R16(tp, IntrStatus);
+}
+
+static void rtl_ack_events(struct rtl8169_private *tp, u32 bits)
+{
+	if (rtl_is_8125(tp))
+		RTL_W32(tp, IntrStatus_8125, bits);
+	else
+		RTL_W16(tp, IntrStatus, bits);
+}
+
+static void rtl_irq_disable(struct rtl8169_private *tp)
+{
+	if (rtl_is_8125(tp))
+		RTL_W32(tp, IntrMask_8125, 0);
+	else
+		RTL_W16(tp, IntrMask, 0);
+	tp->irq_enabled = 0;
+}
+
+#define RTL_EVENT_NAPI_RX	(RxOK | RxErr)
+#define RTL_EVENT_NAPI_TX	(TxOK | TxErr)
+#define RTL_EVENT_NAPI		(RTL_EVENT_NAPI_RX | RTL_EVENT_NAPI_TX)
+
+static void rtl_irq_enable(struct rtl8169_private *tp)
+{
+	tp->irq_enabled = 1;
+	if (rtl_is_8125(tp))
+		RTL_W32(tp, IntrMask_8125, tp->irq_mask);
+	else
+		RTL_W16(tp, IntrMask, tp->irq_mask);
+}
+
+static void rtl8169_irq_mask_and_ack(struct rtl8169_private *tp)
+{
+	rtl_irq_disable(tp);
+	rtl_ack_events(tp, 0xffffffff);
+	/* PCI commit */
+	RTL_R8(tp, ChipCmd);
+}
+
+static void rtl_link_chg_patch(struct rtl8169_private *tp)
+{
+	struct net_device *dev = tp->dev;
+	struct phy_device *phydev = tp->phydev;
+
+	if (!netif_running(dev))
+		return;
+
+	if (tp->mac_version == RTL_GIGA_MAC_VER_34 ||
+	    tp->mac_version == RTL_GIGA_MAC_VER_38) {
+		if (phydev->speed == SPEED_1000) {
+			rtl_eri_write(tp, 0x1bc, ERIAR_MASK_1111, 0x00000011);
+			rtl_eri_write(tp, 0x1dc, ERIAR_MASK_1111, 0x00000005);
+		} else if (phydev->speed == SPEED_100) {
+			rtl_eri_write(tp, 0x1bc, ERIAR_MASK_1111, 0x0000001f);
+			rtl_eri_write(tp, 0x1dc, ERIAR_MASK_1111, 0x00000005);
+		} else {
+			rtl_eri_write(tp, 0x1bc, ERIAR_MASK_1111, 0x0000001f);
+			rtl_eri_write(tp, 0x1dc, ERIAR_MASK_1111, 0x0000003f);
+		}
+		rtl_reset_packet_filter(tp);
+	} else if (tp->mac_version == RTL_GIGA_MAC_VER_35 ||
+		   tp->mac_version == RTL_GIGA_MAC_VER_36) {
+		if (phydev->speed == SPEED_1000) {
+			rtl_eri_write(tp, 0x1bc, ERIAR_MASK_1111, 0x00000011);
+			rtl_eri_write(tp, 0x1dc, ERIAR_MASK_1111, 0x00000005);
+		} else {
+			rtl_eri_write(tp, 0x1bc, ERIAR_MASK_1111, 0x0000001f);
+			rtl_eri_write(tp, 0x1dc, ERIAR_MASK_1111, 0x0000003f);
+		}
+	} else if (tp->mac_version == RTL_GIGA_MAC_VER_37) {
+		if (phydev->speed == SPEED_10) {
+			rtl_eri_write(tp, 0x1d0, ERIAR_MASK_0011, 0x4d02);
+			rtl_eri_write(tp, 0x1dc, ERIAR_MASK_0011, 0x0060a);
+		} else {
+			rtl_eri_write(tp, 0x1d0, ERIAR_MASK_0011, 0x0000);
+		}
+	}
+}
+
+#define WAKE_ANY (WAKE_PHY | WAKE_MAGIC | WAKE_UCAST | WAKE_BCAST | WAKE_MCAST)
+
+static void rtl8169_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl_lock_work(tp);
+	wol->supported = WAKE_ANY;
+	wol->wolopts = tp->saved_wolopts;
+	rtl_unlock_work(tp);
+}
+
+static void __rtl8169_set_wol(struct rtl8169_private *tp, u32 wolopts)
+{
+	static const struct {
+		u32 opt;
+		u16 reg;
+		u8  mask;
+	} cfg[] = {
+		{ WAKE_PHY,   Config3, LinkUp },
+		{ WAKE_UCAST, Config5, UWF },
+		{ WAKE_BCAST, Config5, BWF },
+		{ WAKE_MCAST, Config5, MWF },
+		{ WAKE_ANY,   Config5, LanWake },
+		{ WAKE_MAGIC, Config3, MagicPacket }
+	};
+	unsigned int i, tmp = ARRAY_SIZE(cfg);
+	u8 options;
+
+	rtl_unlock_config_regs(tp);
+
+	if (rtl_is_8168evl_up(tp)) {
+		tmp--;
+		if (wolopts & WAKE_MAGIC)
+			rtl_eri_set_bits(tp, 0x0dc, ERIAR_MASK_0100,
+					 MagicPacket_v2);
+		else
+			rtl_eri_clear_bits(tp, 0x0dc, ERIAR_MASK_0100,
+					   MagicPacket_v2);
+	} else if (rtl_is_8125(tp)) {
+		tmp--;
+		if (wolopts & WAKE_MAGIC)
+			r8168_mac_ocp_modify(tp, 0xc0b6, 0, BIT(0));
+		else
+			r8168_mac_ocp_modify(tp, 0xc0b6, BIT(0), 0);
+	}
+
+	for (i = 0; i < tmp; i++) {
+		options = RTL_R8(tp, cfg[i].reg) & ~cfg[i].mask;
+		if (wolopts & cfg[i].opt)
+			options |= cfg[i].mask;
+		RTL_W8(tp, cfg[i].reg, options);
+	}
+
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_02 ... RTL_GIGA_MAC_VER_06:
+		options = RTL_R8(tp, Config1) & ~PMEnable;
+		if (wolopts)
+			options |= PMEnable;
+		RTL_W8(tp, Config1, options);
+		break;
+	case RTL_GIGA_MAC_VER_34:
+	case RTL_GIGA_MAC_VER_37:
+	case RTL_GIGA_MAC_VER_39 ... RTL_GIGA_MAC_VER_51:
+		options = RTL_R8(tp, Config2) & ~PME_SIGNAL;
+		if (wolopts)
+			options |= PME_SIGNAL;
+		RTL_W8(tp, Config2, options);
+		break;
+	default:
+		break;
+	}
+
+	rtl_lock_config_regs(tp);
+
+	device_set_wakeup_enable(tp_to_dev(tp), wolopts);
+	tp->dev->wol_enabled = wolopts ? 1 : 0;
+}
+
+static int rtl8169_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct device *d = tp_to_dev(tp);
+
+	if (wol->wolopts & ~WAKE_ANY)
+		return -EINVAL;
+
+	pm_runtime_get_noresume(d);
+
+	rtl_lock_work(tp);
+
+	tp->saved_wolopts = wol->wolopts;
+
+	if (pm_runtime_active(d))
+		__rtl8169_set_wol(tp, tp->saved_wolopts);
+
+	rtl_unlock_work(tp);
+
+	pm_runtime_put_noidle(d);
+
+	return 0;
+}
+
+static void rtl8169_get_drvinfo(struct net_device *dev,
+				struct ethtool_drvinfo *info)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct rtl_fw *rtl_fw = tp->rtl_fw;
+
+	strlcpy(info->driver, MODULENAME, sizeof(info->driver));
+	strlcpy(info->bus_info, pci_name(tp->pci_dev), sizeof(info->bus_info));
+	BUILD_BUG_ON(sizeof(info->fw_version) < sizeof(rtl_fw->version));
+	if (rtl_fw)
+		strlcpy(info->fw_version, rtl_fw->version,
+			sizeof(info->fw_version));
+}
+
+static int rtl8169_get_regs_len(struct net_device *dev)
+{
+	return R8169_REGS_SIZE;
+}
+
+static netdev_features_t rtl8169_fix_features(struct net_device *dev,
+	netdev_features_t features)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (dev->mtu > TD_MSS_MAX)
+		features &= ~NETIF_F_ALL_TSO;
+
+	if (dev->mtu > JUMBO_1K &&
+	    tp->mac_version > RTL_GIGA_MAC_VER_06)
+		features &= ~NETIF_F_IP_CSUM;
+
+	return features;
+}
+
+static int rtl8169_set_features(struct net_device *dev,
+				netdev_features_t features)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	u32 rx_config;
+
+	rtl_lock_work(tp);
+
+	rx_config = RTL_R32(tp, RxConfig);
+	if (features & NETIF_F_RXALL)
+		rx_config |= (AcceptErr | AcceptRunt);
+	else
+		rx_config &= ~(AcceptErr | AcceptRunt);
+
+	if (rtl_is_8125(tp)) {
+		if (features & NETIF_F_HW_VLAN_CTAG_RX)
+			rx_config |= RX_VLAN_8125;
+		else
+			rx_config &= ~RX_VLAN_8125;
+	}
+
+	RTL_W32(tp, RxConfig, rx_config);
+
+	if (features & NETIF_F_RXCSUM)
+		tp->cp_cmd |= RxChkSum;
+	else
+		tp->cp_cmd &= ~RxChkSum;
+
+	if (!rtl_is_8125(tp)) {
+		if (features & NETIF_F_HW_VLAN_CTAG_RX)
+			tp->cp_cmd |= RxVlan;
+		else
+			tp->cp_cmd &= ~RxVlan;
+	}
+
+	RTL_W16(tp, CPlusCmd, tp->cp_cmd);
+	RTL_R16(tp, CPlusCmd);
+
+	rtl_unlock_work(tp);
+
+	return 0;
+}
+
+static inline u32 rtl8169_tx_vlan_tag(struct sk_buff *skb)
+{
+	return (skb_vlan_tag_present(skb)) ?
+		TxVlanTag | swab16(skb_vlan_tag_get(skb)) : 0x00;
+}
+
+static void rtl8169_rx_vlan_tag(struct RxDesc *desc, struct sk_buff *skb)
+{
+	u32 opts2 = le32_to_cpu(desc->opts2);
+
+	if (opts2 & RxVlanTag)
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), swab16(opts2 & 0xffff));
+}
+
+static void rtl8169_get_regs(struct net_device *dev, struct ethtool_regs *regs,
+			     void *p)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	u32 __iomem *data = tp->mmio_addr;
+	u32 *dw = p;
+	int i;
+
+	rtl_lock_work(tp);
+	for (i = 0; i < R8169_REGS_SIZE; i += 4)
+		memcpy_fromio(dw++, data++, 4);
+	rtl_unlock_work(tp);
+}
+
+static u32 rtl8169_get_msglevel(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	return tp->msg_enable;
+}
+
+static void rtl8169_set_msglevel(struct net_device *dev, u32 value)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	tp->msg_enable = value;
+}
+
+static const char rtl8169_gstrings[][ETH_GSTRING_LEN] = {
+	"tx_packets",
+	"rx_packets",
+	"tx_errors",
+	"rx_errors",
+	"rx_missed",
+	"align_errors",
+	"tx_single_collisions",
+	"tx_multi_collisions",
+	"unicast",
+	"broadcast",
+	"multicast",
+	"tx_aborted",
+	"tx_underrun",
+};
+
+static int rtl8169_get_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return ARRAY_SIZE(rtl8169_gstrings);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+DECLARE_RTL_COND(rtl_counters_cond)
+{
+	return RTL_R32(tp, CounterAddrLow) & (CounterReset | CounterDump);
+}
+
+static bool rtl8169_do_counters(struct rtl8169_private *tp, u32 counter_cmd)
+{
+	dma_addr_t paddr = tp->counters_phys_addr;
+	u32 cmd;
+
+	RTL_W32(tp, CounterAddrHigh, (u64)paddr >> 32);
+	RTL_R32(tp, CounterAddrHigh);
+	cmd = (u64)paddr & DMA_BIT_MASK(32);
+	RTL_W32(tp, CounterAddrLow, cmd);
+	RTL_W32(tp, CounterAddrLow, cmd | counter_cmd);
+
+	return rtl_udelay_loop_wait_low(tp, &rtl_counters_cond, 10, 1000);
+}
+
+static bool rtl8169_reset_counters(struct rtl8169_private *tp)
+{
+	/*
+	 * Versions prior to RTL_GIGA_MAC_VER_19 don't support resetting the
+	 * tally counters.
+	 */
+	if (tp->mac_version < RTL_GIGA_MAC_VER_19)
+		return true;
+
+	return rtl8169_do_counters(tp, CounterReset);
+}
+
+static bool rtl8169_update_counters(struct rtl8169_private *tp)
+{
+	u8 val = RTL_R8(tp, ChipCmd);
+
+	/*
+	 * Some chips are unable to dump tally counters when the receiver
+	 * is disabled. If 0xff chip may be in a PCI power-save state.
+	 */
+	if (!(val & CmdRxEnb) || val == 0xff)
+		return true;
+
+	return rtl8169_do_counters(tp, CounterDump);
+}
+
+static bool rtl8169_init_counter_offsets(struct rtl8169_private *tp)
+{
+	struct rtl8169_counters *counters = tp->counters;
+	bool ret = false;
+
+	/*
+	 * rtl8169_init_counter_offsets is called from rtl_open.  On chip
+	 * versions prior to RTL_GIGA_MAC_VER_19 the tally counters are only
+	 * reset by a power cycle, while the counter values collected by the
+	 * driver are reset at every driver unload/load cycle.
+	 *
+	 * To make sure the HW values returned by @get_stats64 match the SW
+	 * values, we collect the initial values at first open(*) and use them
+	 * as offsets to normalize the values returned by @get_stats64.
+	 *
+	 * (*) We can't call rtl8169_init_counter_offsets from rtl_init_one
+	 * for the reason stated in rtl8169_update_counters; CmdRxEnb is only
+	 * set at open time by rtl_hw_start.
+	 */
+
+	if (tp->tc_offset.inited)
+		return true;
+
+	/* If both, reset and update fail, propagate to caller. */
+	if (rtl8169_reset_counters(tp))
+		ret = true;
+
+	if (rtl8169_update_counters(tp))
+		ret = true;
+
+	tp->tc_offset.tx_errors = counters->tx_errors;
+	tp->tc_offset.tx_multi_collision = counters->tx_multi_collision;
+	tp->tc_offset.tx_aborted = counters->tx_aborted;
+	tp->tc_offset.inited = true;
+
+	return ret;
+}
+
+static void rtl8169_get_ethtool_stats(struct net_device *dev,
+				      struct ethtool_stats *stats, u64 *data)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct device *d = tp_to_dev(tp);
+	struct rtl8169_counters *counters = tp->counters;
+
+	ASSERT_RTNL();
+
+	pm_runtime_get_noresume(d);
+
+	if (pm_runtime_active(d))
+		rtl8169_update_counters(tp);
+
+	pm_runtime_put_noidle(d);
+
+	data[0] = le64_to_cpu(counters->tx_packets);
+	data[1] = le64_to_cpu(counters->rx_packets);
+	data[2] = le64_to_cpu(counters->tx_errors);
+	data[3] = le32_to_cpu(counters->rx_errors);
+	data[4] = le16_to_cpu(counters->rx_missed);
+	data[5] = le16_to_cpu(counters->align_errors);
+	data[6] = le32_to_cpu(counters->tx_one_collision);
+	data[7] = le32_to_cpu(counters->tx_multi_collision);
+	data[8] = le64_to_cpu(counters->rx_unicast);
+	data[9] = le64_to_cpu(counters->rx_broadcast);
+	data[10] = le32_to_cpu(counters->rx_multicast);
+	data[11] = le16_to_cpu(counters->tx_aborted);
+	data[12] = le16_to_cpu(counters->tx_underun);
+}
+
+static void rtl8169_get_strings(struct net_device *dev, u32 stringset, u8 *data)
+{
+	switch(stringset) {
+	case ETH_SS_STATS:
+		memcpy(data, *rtl8169_gstrings, sizeof(rtl8169_gstrings));
+		break;
+	}
+}
+
+/*
+ * Interrupt coalescing
+ *
+ * > 1 - the availability of the IntrMitigate (0xe2) register through the
+ * >     8169, 8168 and 810x line of chipsets
+ *
+ * 8169, 8168, and 8136(810x) serial chipsets support it.
+ *
+ * > 2 - the Tx timer unit at gigabit speed
+ *
+ * The unit of the timer depends on both the speed and the setting of CPlusCmd
+ * (0xe0) bit 1 and bit 0.
+ *
+ * For 8169
+ * bit[1:0] \ speed        1000M           100M            10M
+ * 0 0                     320ns           2.56us          40.96us
+ * 0 1                     2.56us          20.48us         327.7us
+ * 1 0                     5.12us          40.96us         655.4us
+ * 1 1                     10.24us         81.92us         1.31ms
+ *
+ * For the other
+ * bit[1:0] \ speed        1000M           100M            10M
+ * 0 0                     5us             2.56us          40.96us
+ * 0 1                     40us            20.48us         327.7us
+ * 1 0                     80us            40.96us         655.4us
+ * 1 1                     160us           81.92us         1.31ms
+ */
+
+/* rx/tx scale factors for one particular CPlusCmd[0:1] value */
+struct rtl_coalesce_scale {
+	/* Rx / Tx */
+	u32 nsecs[2];
+};
+
+/* rx/tx scale factors for all CPlusCmd[0:1] cases */
+struct rtl_coalesce_info {
+	u32 speed;
+	struct rtl_coalesce_scale scalev[4];	/* each CPlusCmd[0:1] case */
+};
+
+/* produce (r,t) pairs with each being in series of *1, *8, *8*2, *8*2*2 */
+#define rxtx_x1822(r, t) {		\
+	{{(r),		(t)}},		\
+	{{(r)*8,	(t)*8}},	\
+	{{(r)*8*2,	(t)*8*2}},	\
+	{{(r)*8*2*2,	(t)*8*2*2}},	\
+}
+static const struct rtl_coalesce_info rtl_coalesce_info_8169[] = {
+	/* speed	delays:     rx00   tx00	*/
+	{ SPEED_10,	rxtx_x1822(40960, 40960)	},
+	{ SPEED_100,	rxtx_x1822( 2560,  2560)	},
+	{ SPEED_1000,	rxtx_x1822(  320,   320)	},
+	{ 0 },
+};
+
+static const struct rtl_coalesce_info rtl_coalesce_info_8168_8136[] = {
+	/* speed	delays:     rx00   tx00	*/
+	{ SPEED_10,	rxtx_x1822(40960, 40960)	},
+	{ SPEED_100,	rxtx_x1822( 2560,  2560)	},
+	{ SPEED_1000,	rxtx_x1822( 5000,  5000)	},
+	{ 0 },
+};
+#undef rxtx_x1822
+
+/* get rx/tx scale vector corresponding to current speed */
+static const struct rtl_coalesce_info *rtl_coalesce_info(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	const struct rtl_coalesce_info *ci;
+
+	if (tp->mac_version <= RTL_GIGA_MAC_VER_06)
+		ci = rtl_coalesce_info_8169;
+	else
+		ci = rtl_coalesce_info_8168_8136;
+
+	for (; ci->speed; ci++) {
+		if (tp->phydev->speed == ci->speed)
+			return ci;
+	}
+
+	return ERR_PTR(-ELNRNG);
+}
+
+static int rtl_get_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	const struct rtl_coalesce_info *ci;
+	const struct rtl_coalesce_scale *scale;
+	struct {
+		u32 *max_frames;
+		u32 *usecs;
+	} coal_settings [] = {
+		{ &ec->rx_max_coalesced_frames, &ec->rx_coalesce_usecs },
+		{ &ec->tx_max_coalesced_frames, &ec->tx_coalesce_usecs }
+	}, *p = coal_settings;
+	int i;
+	u16 w;
+
+	if (rtl_is_8125(tp))
+		return -EOPNOTSUPP;
+
+	memset(ec, 0, sizeof(*ec));
+
+	/* get rx/tx scale corresponding to current speed and CPlusCmd[0:1] */
+	ci = rtl_coalesce_info(dev);
+	if (IS_ERR(ci))
+		return PTR_ERR(ci);
+
+	scale = &ci->scalev[tp->cp_cmd & INTT_MASK];
+
+	/* read IntrMitigate and adjust according to scale */
+	for (w = RTL_R16(tp, IntrMitigate); w; w >>= RTL_COALESCE_SHIFT, p++) {
+		*p->max_frames = (w & RTL_COALESCE_MASK) << 2;
+		w >>= RTL_COALESCE_SHIFT;
+		*p->usecs = w & RTL_COALESCE_MASK;
+	}
+
+	for (i = 0; i < 2; i++) {
+		p = coal_settings + i;
+		*p->usecs = (*p->usecs * scale->nsecs[i]) / 1000;
+
+		/*
+		 * ethtool_coalesce says it is illegal to set both usecs and
+		 * max_frames to 0.
+		 */
+		if (!*p->usecs && !*p->max_frames)
+			*p->max_frames = 1;
+	}
+
+	return 0;
+}
+
+/* choose appropriate scale factor and CPlusCmd[0:1] for (speed, nsec) */
+static const struct rtl_coalesce_scale *rtl_coalesce_choose_scale(
+			struct net_device *dev, u32 nsec, u16 *cp01)
+{
+	const struct rtl_coalesce_info *ci;
+	u16 i;
+
+	ci = rtl_coalesce_info(dev);
+	if (IS_ERR(ci))
+		return ERR_CAST(ci);
+
+	for (i = 0; i < 4; i++) {
+		u32 rxtx_maxscale = max(ci->scalev[i].nsecs[0],
+					ci->scalev[i].nsecs[1]);
+		if (nsec <= rxtx_maxscale * RTL_COALESCE_T_MAX) {
+			*cp01 = i;
+			return &ci->scalev[i];
+		}
+	}
+
+	return ERR_PTR(-EINVAL);
+}
+
+static int rtl_set_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	const struct rtl_coalesce_scale *scale;
+	struct {
+		u32 frames;
+		u32 usecs;
+	} coal_settings [] = {
+		{ ec->rx_max_coalesced_frames, ec->rx_coalesce_usecs },
+		{ ec->tx_max_coalesced_frames, ec->tx_coalesce_usecs }
+	}, *p = coal_settings;
+	u16 w = 0, cp01;
+	int i;
+
+	if (rtl_is_8125(tp))
+		return -EOPNOTSUPP;
+
+	scale = rtl_coalesce_choose_scale(dev,
+			max(p[0].usecs, p[1].usecs) * 1000, &cp01);
+	if (IS_ERR(scale))
+		return PTR_ERR(scale);
+
+	for (i = 0; i < 2; i++, p++) {
+		u32 units;
+
+		/*
+		 * accept max_frames=1 we returned in rtl_get_coalesce.
+		 * accept it not only when usecs=0 because of e.g. the following scenario:
+		 *
+		 * - both rx_usecs=0 & rx_frames=0 in hardware (no delay on RX)
+		 * - rtl_get_coalesce returns rx_usecs=0, rx_frames=1
+		 * - then user does `ethtool -C eth0 rx-usecs 100`
+		 *
+		 * since ethtool sends to kernel whole ethtool_coalesce
+		 * settings, if we do not handle rx_usecs=!0, rx_frames=1
+		 * we'll reject it below in `frames % 4 != 0`.
+		 */
+		if (p->frames == 1) {
+			p->frames = 0;
+		}
+
+		units = p->usecs * 1000 / scale->nsecs[i];
+		if (p->frames > RTL_COALESCE_FRAME_MAX || p->frames % 4)
+			return -EINVAL;
+
+		w <<= RTL_COALESCE_SHIFT;
+		w |= units;
+		w <<= RTL_COALESCE_SHIFT;
+		w |= p->frames >> 2;
+	}
+
+	rtl_lock_work(tp);
+
+	RTL_W16(tp, IntrMitigate, swab16(w));
+
+	tp->cp_cmd = (tp->cp_cmd & ~INTT_MASK) | cp01;
+	RTL_W16(tp, CPlusCmd, tp->cp_cmd);
+	RTL_R16(tp, CPlusCmd);
+
+	rtl_unlock_work(tp);
+
+	return 0;
+}
+
+static int rtl8169_get_eee(struct net_device *dev, struct ethtool_eee *data)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct device *d = tp_to_dev(tp);
+	int ret;
+
+	if (!rtl_supports_eee(tp))
+		return -EOPNOTSUPP;
+
+	pm_runtime_get_noresume(d);
+
+	if (!pm_runtime_active(d)) {
+		ret = -EOPNOTSUPP;
+	} else {
+		ret = phy_ethtool_get_eee(tp->phydev, data);
+	}
+
+	pm_runtime_put_noidle(d);
+
+	return ret;
+}
+
+static int rtl8169_set_eee(struct net_device *dev, struct ethtool_eee *data)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct device *d = tp_to_dev(tp);
+	int ret;
+
+	if (!rtl_supports_eee(tp))
+		return -EOPNOTSUPP;
+
+	pm_runtime_get_noresume(d);
+
+	if (!pm_runtime_active(d)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+	if (dev->phydev->autoneg == AUTONEG_DISABLE ||
+	    dev->phydev->duplex != DUPLEX_FULL) {
+		ret = -EPROTONOSUPPORT;
+		goto out;
+	}
+
+	ret = phy_ethtool_set_eee(tp->phydev, data);
+out:
+	pm_runtime_put_noidle(d);
+	return ret;
+}
+
+static const struct ethtool_ops rtl8169_ethtool_ops = {
+	.get_drvinfo		= rtl8169_get_drvinfo,
+	.get_regs_len		= rtl8169_get_regs_len,
+	.get_link		= ethtool_op_get_link,
+	.get_coalesce		= rtl_get_coalesce,
+	.set_coalesce		= rtl_set_coalesce,
+	.get_msglevel		= rtl8169_get_msglevel,
+	.set_msglevel		= rtl8169_set_msglevel,
+	.get_regs		= rtl8169_get_regs,
+	.get_wol		= rtl8169_get_wol,
+	.set_wol		= rtl8169_set_wol,
+	.get_strings		= rtl8169_get_strings,
+	.get_sset_count		= rtl8169_get_sset_count,
+	.get_ethtool_stats	= rtl8169_get_ethtool_stats,
+	.get_ts_info		= ethtool_op_get_ts_info,
+	.nway_reset		= phy_ethtool_nway_reset,
+	.get_eee		= rtl8169_get_eee,
+	.set_eee		= rtl8169_set_eee,
+	.get_link_ksettings	= phy_ethtool_get_link_ksettings,
+	.set_link_ksettings	= phy_ethtool_set_link_ksettings,
+};
+
+static void rtl_enable_eee(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+	int supported = phy_read_mmd(phydev, MDIO_MMD_PCS, MDIO_PCS_EEE_ABLE);
+
+	if (supported > 0)
+		phy_write_mmd(phydev, MDIO_MMD_AN, MDIO_AN_EEE_ADV, supported);
+}
+
+static void rtl8169_get_mac_version(struct rtl8169_private *tp)
+{
+	/*
+	 * The driver currently handles the 8168Bf and the 8168Be identically
+	 * but they can be identified more specifically through the test below
+	 * if needed:
+	 *
+	 * (RTL_R32(tp, TxConfig) & 0x700000) == 0x500000 ? 8168Bf : 8168Be
+	 *
+	 * Same thing for the 8101Eb and the 8101Ec:
+	 *
+	 * (RTL_R32(tp, TxConfig) & 0x700000) == 0x200000 ? 8101Eb : 8101Ec
+	 */
+	static const struct rtl_mac_info {
+		u16 mask;
+		u16 val;
+		u16 mac_version;
+	} mac_info[] = {
+		/* 8125 family. */
+		{ 0x7cf, 0x608,	RTL_GIGA_MAC_VER_60 },
+		{ 0x7c8, 0x608,	RTL_GIGA_MAC_VER_61 },
+
+		/* 8168EP family. */
+		{ 0x7cf, 0x502,	RTL_GIGA_MAC_VER_51 },
+		{ 0x7cf, 0x501,	RTL_GIGA_MAC_VER_50 },
+		{ 0x7cf, 0x500,	RTL_GIGA_MAC_VER_49 },
+
+		/* 8168H family. */
+		{ 0x7cf, 0x541,	RTL_GIGA_MAC_VER_46 },
+		{ 0x7cf, 0x540,	RTL_GIGA_MAC_VER_45 },
+
+		/* 8168G family. */
+		{ 0x7cf, 0x5c8,	RTL_GIGA_MAC_VER_44 },
+		{ 0x7cf, 0x509,	RTL_GIGA_MAC_VER_42 },
+		{ 0x7cf, 0x4c1,	RTL_GIGA_MAC_VER_41 },
+		{ 0x7cf, 0x4c0,	RTL_GIGA_MAC_VER_40 },
+
+		/* 8168F family. */
+		{ 0x7c8, 0x488,	RTL_GIGA_MAC_VER_38 },
+		{ 0x7cf, 0x481,	RTL_GIGA_MAC_VER_36 },
+		{ 0x7cf, 0x480,	RTL_GIGA_MAC_VER_35 },
+
+		/* 8168E family. */
+		{ 0x7c8, 0x2c8,	RTL_GIGA_MAC_VER_34 },
+		{ 0x7cf, 0x2c1,	RTL_GIGA_MAC_VER_32 },
+		{ 0x7c8, 0x2c0,	RTL_GIGA_MAC_VER_33 },
+
+		/* 8168D family. */
+		{ 0x7cf, 0x281,	RTL_GIGA_MAC_VER_25 },
+		{ 0x7c8, 0x280,	RTL_GIGA_MAC_VER_26 },
+
+		/* 8168DP family. */
+		{ 0x7cf, 0x288,	RTL_GIGA_MAC_VER_27 },
+		{ 0x7cf, 0x28a,	RTL_GIGA_MAC_VER_28 },
+		{ 0x7cf, 0x28b,	RTL_GIGA_MAC_VER_31 },
+
+		/* 8168C family. */
+		{ 0x7cf, 0x3c9,	RTL_GIGA_MAC_VER_23 },
+		{ 0x7cf, 0x3c8,	RTL_GIGA_MAC_VER_18 },
+		{ 0x7c8, 0x3c8,	RTL_GIGA_MAC_VER_24 },
+		{ 0x7cf, 0x3c0,	RTL_GIGA_MAC_VER_19 },
+		{ 0x7cf, 0x3c2,	RTL_GIGA_MAC_VER_20 },
+		{ 0x7cf, 0x3c3,	RTL_GIGA_MAC_VER_21 },
+		{ 0x7c8, 0x3c0,	RTL_GIGA_MAC_VER_22 },
+
+		/* 8168B family. */
+		{ 0x7cf, 0x380,	RTL_GIGA_MAC_VER_12 },
+		{ 0x7c8, 0x380,	RTL_GIGA_MAC_VER_17 },
+		{ 0x7c8, 0x300,	RTL_GIGA_MAC_VER_11 },
+
+		/* 8101 family. */
+		{ 0x7c8, 0x448,	RTL_GIGA_MAC_VER_39 },
+		{ 0x7c8, 0x440,	RTL_GIGA_MAC_VER_37 },
+		{ 0x7cf, 0x409,	RTL_GIGA_MAC_VER_29 },
+		{ 0x7c8, 0x408,	RTL_GIGA_MAC_VER_30 },
+		{ 0x7cf, 0x349,	RTL_GIGA_MAC_VER_08 },
+		{ 0x7cf, 0x249,	RTL_GIGA_MAC_VER_08 },
+		{ 0x7cf, 0x348,	RTL_GIGA_MAC_VER_07 },
+		{ 0x7cf, 0x248,	RTL_GIGA_MAC_VER_07 },
+		{ 0x7cf, 0x340,	RTL_GIGA_MAC_VER_13 },
+		{ 0x7cf, 0x343,	RTL_GIGA_MAC_VER_10 },
+		{ 0x7cf, 0x342,	RTL_GIGA_MAC_VER_16 },
+		{ 0x7c8, 0x348,	RTL_GIGA_MAC_VER_09 },
+		{ 0x7c8, 0x248,	RTL_GIGA_MAC_VER_09 },
+		{ 0x7c8, 0x340,	RTL_GIGA_MAC_VER_16 },
+		/* FIXME: where did these entries come from ? -- FR */
+		{ 0xfc8, 0x388,	RTL_GIGA_MAC_VER_15 },
+		{ 0xfc8, 0x308,	RTL_GIGA_MAC_VER_14 },
+
+		/* 8110 family. */
+		{ 0xfc8, 0x980,	RTL_GIGA_MAC_VER_06 },
+		{ 0xfc8, 0x180,	RTL_GIGA_MAC_VER_05 },
+		{ 0xfc8, 0x100,	RTL_GIGA_MAC_VER_04 },
+		{ 0xfc8, 0x040,	RTL_GIGA_MAC_VER_03 },
+		{ 0xfc8, 0x008,	RTL_GIGA_MAC_VER_02 },
+
+		/* Catch-all */
+		{ 0x000, 0x000,	RTL_GIGA_MAC_NONE   }
+	};
+	const struct rtl_mac_info *p = mac_info;
+	u16 reg = RTL_R32(tp, TxConfig) >> 20;
+
+	while ((reg & p->mask) != p->val)
+		p++;
+	tp->mac_version = p->mac_version;
+
+	if (tp->mac_version == RTL_GIGA_MAC_NONE) {
+		dev_err(tp_to_dev(tp), "unknown chip XID %03x\n", reg & 0xfcf);
+	} else if (!tp->supports_gmii) {
+		if (tp->mac_version == RTL_GIGA_MAC_VER_42)
+			tp->mac_version = RTL_GIGA_MAC_VER_43;
+		else if (tp->mac_version == RTL_GIGA_MAC_VER_45)
+			tp->mac_version = RTL_GIGA_MAC_VER_47;
+		else if (tp->mac_version == RTL_GIGA_MAC_VER_46)
+			tp->mac_version = RTL_GIGA_MAC_VER_48;
+	}
+}
+
+struct phy_reg {
+	u16 reg;
+	u16 val;
+};
+
+static void __rtl_writephy_batch(struct rtl8169_private *tp,
+				 const struct phy_reg *regs, int len)
+{
+	while (len-- > 0) {
+		rtl_writephy(tp, regs->reg, regs->val);
+		regs++;
+	}
+}
+
+#define rtl_writephy_batch(tp, a) __rtl_writephy_batch(tp, a, ARRAY_SIZE(a))
+
+static void rtl_release_firmware(struct rtl8169_private *tp)
+{
+	if (tp->rtl_fw) {
+		rtl_fw_release_firmware(tp->rtl_fw);
+		kfree(tp->rtl_fw);
+		tp->rtl_fw = NULL;
+	}
+}
+
+static void rtl_apply_firmware(struct rtl8169_private *tp)
+{
+	/* TODO: release firmware if rtl_fw_write_firmware signals failure. */
+	if (tp->rtl_fw)
+		rtl_fw_write_firmware(tp, tp->rtl_fw);
+}
+
+static void rtl_apply_firmware_cond(struct rtl8169_private *tp, u8 reg, u16 val)
+{
+	if (rtl_readphy(tp, reg) != val)
+		netif_warn(tp, hw, tp->dev, "chipset not ready for firmware\n");
+	else
+		rtl_apply_firmware(tp);
+}
+
+static void rtl8168_config_eee_mac(struct rtl8169_private *tp)
+{
+	/* Adjust EEE LED frequency */
+	if (tp->mac_version != RTL_GIGA_MAC_VER_38)
+		RTL_W8(tp, EEE_LED, RTL_R8(tp, EEE_LED) & ~0x07);
+
+	rtl_eri_set_bits(tp, 0x1b0, ERIAR_MASK_1111, 0x0003);
+}
+
+static void rtl8125_config_eee_mac(struct rtl8169_private *tp)
+{
+	r8168_mac_ocp_modify(tp, 0xe040, 0, BIT(1) | BIT(0));
+	r8168_mac_ocp_modify(tp, 0xeb62, 0, BIT(2) | BIT(1));
+}
+
+static void rtl8168f_config_eee_phy(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+
+	phy_write(phydev, 0x1f, 0x0007);
+	phy_write(phydev, 0x1e, 0x0020);
+	phy_set_bits(phydev, 0x15, BIT(8));
+
+	phy_write(phydev, 0x1f, 0x0005);
+	phy_write(phydev, 0x05, 0x8b85);
+	phy_set_bits(phydev, 0x06, BIT(13));
+
+	phy_write(phydev, 0x1f, 0x0000);
+}
+
+static void rtl8168g_config_eee_phy(struct rtl8169_private *tp)
+{
+	phy_modify_paged(tp->phydev, 0x0a43, 0x11, 0, BIT(4));
+}
+
+static void rtl8168h_config_eee_phy(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+
+	rtl8168g_config_eee_phy(tp);
+
+	phy_modify_paged(phydev, 0xa4a, 0x11, 0x0000, 0x0200);
+	phy_modify_paged(phydev, 0xa42, 0x14, 0x0000, 0x0080);
+}
+
+static void rtl8125_config_eee_phy(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+
+	rtl8168h_config_eee_phy(tp);
+
+	phy_modify_paged(phydev, 0xa6d, 0x12, 0x0001, 0x0000);
+	phy_modify_paged(phydev, 0xa6d, 0x14, 0x0010, 0x0000);
+}
+
+static void rtl8169s_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x06, 0x006e },
+		{ 0x08, 0x0708 },
+		{ 0x15, 0x4000 },
+		{ 0x18, 0x65c7 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x03, 0x00a1 },
+		{ 0x02, 0x0008 },
+		{ 0x01, 0x0120 },
+		{ 0x00, 0x1000 },
+		{ 0x04, 0x0800 },
+		{ 0x04, 0x0000 },
+
+		{ 0x03, 0xff41 },
+		{ 0x02, 0xdf60 },
+		{ 0x01, 0x0140 },
+		{ 0x00, 0x0077 },
+		{ 0x04, 0x7800 },
+		{ 0x04, 0x7000 },
+
+		{ 0x03, 0x802f },
+		{ 0x02, 0x4f02 },
+		{ 0x01, 0x0409 },
+		{ 0x00, 0xf0f9 },
+		{ 0x04, 0x9800 },
+		{ 0x04, 0x9000 },
+
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0xff95 },
+		{ 0x00, 0xba00 },
+		{ 0x04, 0xa800 },
+		{ 0x04, 0xa000 },
+
+		{ 0x03, 0xff41 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0x0140 },
+		{ 0x00, 0x00bb },
+		{ 0x04, 0xb800 },
+		{ 0x04, 0xb000 },
+
+		{ 0x03, 0xdf41 },
+		{ 0x02, 0xdc60 },
+		{ 0x01, 0x6340 },
+		{ 0x00, 0x007d },
+		{ 0x04, 0xd800 },
+		{ 0x04, 0xd000 },
+
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0x100a },
+		{ 0x00, 0xa0ff },
+		{ 0x04, 0xf800 },
+		{ 0x04, 0xf000 },
+
+		{ 0x1f, 0x0000 },
+		{ 0x0b, 0x0000 },
+		{ 0x00, 0x9200 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8169sb_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0002 },
+		{ 0x01, 0x90d0 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8169scd_hw_phy_config_quirk(struct rtl8169_private *tp)
+{
+	struct pci_dev *pdev = tp->pci_dev;
+
+	if ((pdev->subsystem_vendor != PCI_VENDOR_ID_GIGABYTE) ||
+	    (pdev->subsystem_device != 0xe000))
+		return;
+
+	rtl_writephy(tp, 0x1f, 0x0001);
+	rtl_writephy(tp, 0x10, 0xf01b);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8169scd_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x04, 0x0000 },
+		{ 0x03, 0x00a1 },
+		{ 0x02, 0x0008 },
+		{ 0x01, 0x0120 },
+		{ 0x00, 0x1000 },
+		{ 0x04, 0x0800 },
+		{ 0x04, 0x9000 },
+		{ 0x03, 0x802f },
+		{ 0x02, 0x4f02 },
+		{ 0x01, 0x0409 },
+		{ 0x00, 0xf099 },
+		{ 0x04, 0x9800 },
+		{ 0x04, 0xa000 },
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0xff95 },
+		{ 0x00, 0xba00 },
+		{ 0x04, 0xa800 },
+		{ 0x04, 0xf000 },
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0x101a },
+		{ 0x00, 0xa0ff },
+		{ 0x04, 0xf800 },
+		{ 0x04, 0x0000 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x10, 0xf41b },
+		{ 0x14, 0xfb54 },
+		{ 0x18, 0xf5c7 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x17, 0x0cc0 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	rtl8169scd_hw_phy_config_quirk(tp);
+}
+
+static void rtl8169sce_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x04, 0x0000 },
+		{ 0x03, 0x00a1 },
+		{ 0x02, 0x0008 },
+		{ 0x01, 0x0120 },
+		{ 0x00, 0x1000 },
+		{ 0x04, 0x0800 },
+		{ 0x04, 0x9000 },
+		{ 0x03, 0x802f },
+		{ 0x02, 0x4f02 },
+		{ 0x01, 0x0409 },
+		{ 0x00, 0xf099 },
+		{ 0x04, 0x9800 },
+		{ 0x04, 0xa000 },
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0xff95 },
+		{ 0x00, 0xba00 },
+		{ 0x04, 0xa800 },
+		{ 0x04, 0xf000 },
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0x101a },
+		{ 0x00, 0xa0ff },
+		{ 0x04, 0xf800 },
+		{ 0x04, 0x0000 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x0b, 0x8480 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x18, 0x67c7 },
+		{ 0x04, 0x2000 },
+		{ 0x03, 0x002f },
+		{ 0x02, 0x4360 },
+		{ 0x01, 0x0109 },
+		{ 0x00, 0x3022 },
+		{ 0x04, 0x2800 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x17, 0x0cc0 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8168bb_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x10, 0xf41b },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy(tp, 0x1f, 0x0001);
+	rtl_patchphy(tp, 0x16, 1 << 0);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8168bef_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x10, 0xf41b },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8168cp_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0000 },
+		{ 0x1d, 0x0f00 },
+		{ 0x1f, 0x0002 },
+		{ 0x0c, 0x1ec8 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8168cp_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x1d, 0x3d98 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_patchphy(tp, 0x14, 1 << 5);
+	rtl_patchphy(tp, 0x0d, 1 << 5);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8168c_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x12, 0x2300 },
+		{ 0x1f, 0x0002 },
+		{ 0x00, 0x88d4 },
+		{ 0x01, 0x82b1 },
+		{ 0x03, 0x7002 },
+		{ 0x08, 0x9e30 },
+		{ 0x09, 0x01f0 },
+		{ 0x0a, 0x5500 },
+		{ 0x0c, 0x00c8 },
+		{ 0x1f, 0x0003 },
+		{ 0x12, 0xc096 },
+		{ 0x16, 0x000a },
+		{ 0x1f, 0x0000 },
+		{ 0x1f, 0x0000 },
+		{ 0x09, 0x2000 },
+		{ 0x09, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	rtl_patchphy(tp, 0x14, 1 << 5);
+	rtl_patchphy(tp, 0x0d, 1 << 5);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168c_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x12, 0x2300 },
+		{ 0x03, 0x802f },
+		{ 0x02, 0x4f02 },
+		{ 0x01, 0x0409 },
+		{ 0x00, 0xf099 },
+		{ 0x04, 0x9800 },
+		{ 0x04, 0x9000 },
+		{ 0x1d, 0x3d98 },
+		{ 0x1f, 0x0002 },
+		{ 0x0c, 0x7eb8 },
+		{ 0x06, 0x0761 },
+		{ 0x1f, 0x0003 },
+		{ 0x16, 0x0f0a },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	rtl_patchphy(tp, 0x16, 1 << 0);
+	rtl_patchphy(tp, 0x14, 1 << 5);
+	rtl_patchphy(tp, 0x0d, 1 << 5);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168c_3_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x12, 0x2300 },
+		{ 0x1d, 0x3d98 },
+		{ 0x1f, 0x0002 },
+		{ 0x0c, 0x7eb8 },
+		{ 0x06, 0x5461 },
+		{ 0x1f, 0x0003 },
+		{ 0x16, 0x0f0a },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	rtl_patchphy(tp, 0x16, 1 << 0);
+	rtl_patchphy(tp, 0x14, 1 << 5);
+	rtl_patchphy(tp, 0x0d, 1 << 5);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168c_4_hw_phy_config(struct rtl8169_private *tp)
+{
+	rtl8168c_3_hw_phy_config(tp);
+}
+
+static const struct phy_reg rtl8168d_1_phy_reg_init_0[] = {
+	/* Channel Estimation */
+	{ 0x1f, 0x0001 },
+	{ 0x06, 0x4064 },
+	{ 0x07, 0x2863 },
+	{ 0x08, 0x059c },
+	{ 0x09, 0x26b4 },
+	{ 0x0a, 0x6a19 },
+	{ 0x0b, 0xdcc8 },
+	{ 0x10, 0xf06d },
+	{ 0x14, 0x7f68 },
+	{ 0x18, 0x7fd9 },
+	{ 0x1c, 0xf0ff },
+	{ 0x1d, 0x3d9c },
+	{ 0x1f, 0x0003 },
+	{ 0x12, 0xf49f },
+	{ 0x13, 0x070b },
+	{ 0x1a, 0x05ad },
+	{ 0x14, 0x94c0 },
+
+	/*
+	 * Tx Error Issue
+	 * Enhance line driver power
+	 */
+	{ 0x1f, 0x0002 },
+	{ 0x06, 0x5561 },
+	{ 0x1f, 0x0005 },
+	{ 0x05, 0x8332 },
+	{ 0x06, 0x5561 },
+
+	/*
+	 * Can not link to 1Gbps with bad cable
+	 * Decrease SNR threshold form 21.07dB to 19.04dB
+	 */
+	{ 0x1f, 0x0001 },
+	{ 0x17, 0x0cc0 },
+
+	{ 0x1f, 0x0000 },
+	{ 0x0d, 0xf880 }
+};
+
+static const struct phy_reg rtl8168d_1_phy_reg_init_1[] = {
+	{ 0x1f, 0x0002 },
+	{ 0x05, 0x669a },
+	{ 0x1f, 0x0005 },
+	{ 0x05, 0x8330 },
+	{ 0x06, 0x669a },
+	{ 0x1f, 0x0002 }
+};
+
+static void rtl8168d_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	rtl_writephy_batch(tp, rtl8168d_1_phy_reg_init_0);
+
+	/*
+	 * Rx Error Issue
+	 * Fine Tune Switching regulator parameter
+	 */
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_w0w1_phy(tp, 0x0b, 0x0010, 0x00ef);
+	rtl_w0w1_phy(tp, 0x0c, 0xa200, 0x5d00);
+
+	if (rtl8168d_efuse_read(tp, 0x01) == 0xb1) {
+		int val;
+
+		rtl_writephy_batch(tp, rtl8168d_1_phy_reg_init_1);
+
+		val = rtl_readphy(tp, 0x0d);
+
+		if ((val & 0x00ff) != 0x006c) {
+			static const u32 set[] = {
+				0x0065, 0x0066, 0x0067, 0x0068,
+				0x0069, 0x006a, 0x006b, 0x006c
+			};
+			int i;
+
+			rtl_writephy(tp, 0x1f, 0x0002);
+
+			val &= 0xff00;
+			for (i = 0; i < ARRAY_SIZE(set); i++)
+				rtl_writephy(tp, 0x0d, val | set[i]);
+		}
+	} else {
+		static const struct phy_reg phy_reg_init[] = {
+			{ 0x1f, 0x0002 },
+			{ 0x05, 0x6662 },
+			{ 0x1f, 0x0005 },
+			{ 0x05, 0x8330 },
+			{ 0x06, 0x6662 }
+		};
+
+		rtl_writephy_batch(tp, phy_reg_init);
+	}
+
+	/* RSET couple improve */
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_patchphy(tp, 0x0d, 0x0300);
+	rtl_patchphy(tp, 0x0f, 0x0010);
+
+	/* Fine tune PLL performance */
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_w0w1_phy(tp, 0x02, 0x0100, 0x0600);
+	rtl_w0w1_phy(tp, 0x03, 0x0000, 0xe000);
+
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x001b);
+
+	rtl_apply_firmware_cond(tp, MII_EXPANSION, 0xbf00);
+
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168d_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	rtl_writephy_batch(tp, rtl8168d_1_phy_reg_init_0);
+
+	if (rtl8168d_efuse_read(tp, 0x01) == 0xb1) {
+		int val;
+
+		rtl_writephy_batch(tp, rtl8168d_1_phy_reg_init_1);
+
+		val = rtl_readphy(tp, 0x0d);
+		if ((val & 0x00ff) != 0x006c) {
+			static const u32 set[] = {
+				0x0065, 0x0066, 0x0067, 0x0068,
+				0x0069, 0x006a, 0x006b, 0x006c
+			};
+			int i;
+
+			rtl_writephy(tp, 0x1f, 0x0002);
+
+			val &= 0xff00;
+			for (i = 0; i < ARRAY_SIZE(set); i++)
+				rtl_writephy(tp, 0x0d, val | set[i]);
+		}
+	} else {
+		static const struct phy_reg phy_reg_init[] = {
+			{ 0x1f, 0x0002 },
+			{ 0x05, 0x2642 },
+			{ 0x1f, 0x0005 },
+			{ 0x05, 0x8330 },
+			{ 0x06, 0x2642 }
+		};
+
+		rtl_writephy_batch(tp, phy_reg_init);
+	}
+
+	/* Fine tune PLL performance */
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_w0w1_phy(tp, 0x02, 0x0100, 0x0600);
+	rtl_w0w1_phy(tp, 0x03, 0x0000, 0xe000);
+
+	/* Switching regulator Slew rate */
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_patchphy(tp, 0x0f, 0x0017);
+
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x001b);
+
+	rtl_apply_firmware_cond(tp, MII_EXPANSION, 0xb300);
+
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168d_3_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0002 },
+		{ 0x10, 0x0008 },
+		{ 0x0d, 0x006c },
+
+		{ 0x1f, 0x0000 },
+		{ 0x0d, 0xf880 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x17, 0x0cc0 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x0b, 0xa4d8 },
+		{ 0x09, 0x281c },
+		{ 0x07, 0x2883 },
+		{ 0x0a, 0x6b35 },
+		{ 0x1d, 0x3da4 },
+		{ 0x1c, 0xeffd },
+		{ 0x14, 0x7f52 },
+		{ 0x18, 0x7fc6 },
+		{ 0x08, 0x0601 },
+		{ 0x06, 0x4063 },
+		{ 0x10, 0xf074 },
+		{ 0x1f, 0x0003 },
+		{ 0x13, 0x0789 },
+		{ 0x12, 0xf4bd },
+		{ 0x1a, 0x04fd },
+		{ 0x14, 0x84b0 },
+		{ 0x1f, 0x0000 },
+		{ 0x00, 0x9200 },
+
+		{ 0x1f, 0x0005 },
+		{ 0x01, 0x0340 },
+		{ 0x1f, 0x0001 },
+		{ 0x04, 0x4000 },
+		{ 0x03, 0x1d21 },
+		{ 0x02, 0x0c32 },
+		{ 0x01, 0x0200 },
+		{ 0x00, 0x5554 },
+		{ 0x04, 0x4800 },
+		{ 0x04, 0x4000 },
+		{ 0x04, 0xf000 },
+		{ 0x03, 0xdf01 },
+		{ 0x02, 0xdf20 },
+		{ 0x01, 0x101a },
+		{ 0x00, 0xa0ff },
+		{ 0x04, 0xf800 },
+		{ 0x04, 0xf000 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x0023 },
+		{ 0x16, 0x0000 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8168d_4_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0001 },
+		{ 0x17, 0x0cc0 },
+
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x002d },
+		{ 0x18, 0x0040 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy_batch(tp, phy_reg_init);
+	rtl_patchphy(tp, 0x0d, 1 << 5);
+}
+
+static void rtl8168e_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		/* Enable Delay cap */
+		{ 0x1f, 0x0005 },
+		{ 0x05, 0x8b80 },
+		{ 0x06, 0xc896 },
+		{ 0x1f, 0x0000 },
+
+		/* Channel estimation fine tune */
+		{ 0x1f, 0x0001 },
+		{ 0x0b, 0x6c20 },
+		{ 0x07, 0x2872 },
+		{ 0x1c, 0xefff },
+		{ 0x1f, 0x0003 },
+		{ 0x14, 0x6420 },
+		{ 0x1f, 0x0000 },
+
+		/* Update PFM & 10M TX idle timer */
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x002f },
+		{ 0x15, 0x1919 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x00ac },
+		{ 0x18, 0x0006 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_apply_firmware(tp);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	/* DCO enable for 10M IDLE Power */
+	rtl_writephy(tp, 0x1f, 0x0007);
+	rtl_writephy(tp, 0x1e, 0x0023);
+	rtl_w0w1_phy(tp, 0x17, 0x0006, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* For impedance matching */
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_w0w1_phy(tp, 0x08, 0x8000, 0x7f00);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* PHY auto speed down */
+	rtl_writephy(tp, 0x1f, 0x0007);
+	rtl_writephy(tp, 0x1e, 0x002d);
+	rtl_w0w1_phy(tp, 0x18, 0x0050, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0x0000);
+
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b86);
+	rtl_w0w1_phy(tp, 0x06, 0x0001, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b85);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x2000);
+	rtl_writephy(tp, 0x1f, 0x0007);
+	rtl_writephy(tp, 0x1e, 0x0020);
+	rtl_w0w1_phy(tp, 0x15, 0x0000, 0x1100);
+	rtl_writephy(tp, 0x1f, 0x0006);
+	rtl_writephy(tp, 0x00, 0x5a00);
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_writephy(tp, 0x0d, 0x0007);
+	rtl_writephy(tp, 0x0e, 0x003c);
+	rtl_writephy(tp, 0x0d, 0x4007);
+	rtl_writephy(tp, 0x0e, 0x0000);
+	rtl_writephy(tp, 0x0d, 0x0000);
+}
+
+static void rtl_rar_exgmac_set(struct rtl8169_private *tp, u8 *addr)
+{
+	const u16 w[] = {
+		addr[0] | (addr[1] << 8),
+		addr[2] | (addr[3] << 8),
+		addr[4] | (addr[5] << 8)
+	};
+
+	rtl_eri_write(tp, 0xe0, ERIAR_MASK_1111, w[0] | (w[1] << 16));
+	rtl_eri_write(tp, 0xe4, ERIAR_MASK_1111, w[2]);
+	rtl_eri_write(tp, 0xf0, ERIAR_MASK_1111, w[0] << 16);
+	rtl_eri_write(tp, 0xf4, ERIAR_MASK_1111, w[1] | (w[2] << 16));
+}
+
+static void rtl8168e_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		/* Enable Delay cap */
+		{ 0x1f, 0x0004 },
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x00ac },
+		{ 0x18, 0x0006 },
+		{ 0x1f, 0x0002 },
+		{ 0x1f, 0x0000 },
+		{ 0x1f, 0x0000 },
+
+		/* Channel estimation fine tune */
+		{ 0x1f, 0x0003 },
+		{ 0x09, 0xa20f },
+		{ 0x1f, 0x0000 },
+		{ 0x1f, 0x0000 },
+
+		/* Green Setting */
+		{ 0x1f, 0x0005 },
+		{ 0x05, 0x8b5b },
+		{ 0x06, 0x9222 },
+		{ 0x05, 0x8b6d },
+		{ 0x06, 0x8000 },
+		{ 0x05, 0x8b76 },
+		{ 0x06, 0x8000 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_apply_firmware(tp);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	/* For 4-corner performance improve */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b80);
+	rtl_w0w1_phy(tp, 0x17, 0x0006, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* PHY auto speed down */
+	rtl_writephy(tp, 0x1f, 0x0004);
+	rtl_writephy(tp, 0x1f, 0x0007);
+	rtl_writephy(tp, 0x1e, 0x002d);
+	rtl_w0w1_phy(tp, 0x18, 0x0010, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0002);
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0x0000);
+
+	/* improve 10M EEE waveform */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b86);
+	rtl_w0w1_phy(tp, 0x06, 0x0001, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* Improve 2-pair detection performance */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b85);
+	rtl_w0w1_phy(tp, 0x06, 0x4000, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	rtl8168f_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+
+	/* Green feature */
+	rtl_writephy(tp, 0x1f, 0x0003);
+	rtl_w0w1_phy(tp, 0x19, 0x0001, 0x0000);
+	rtl_w0w1_phy(tp, 0x10, 0x0400, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_w0w1_phy(tp, 0x01, 0x0100, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* Broken BIOS workaround: feed GigaMAC registers with MAC address. */
+	rtl_rar_exgmac_set(tp, tp->dev->dev_addr);
+}
+
+static void rtl8168f_hw_phy_config(struct rtl8169_private *tp)
+{
+	/* For 4-corner performance improve */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b80);
+	rtl_w0w1_phy(tp, 0x06, 0x0006, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* PHY auto speed down */
+	rtl_writephy(tp, 0x1f, 0x0007);
+	rtl_writephy(tp, 0x1e, 0x002d);
+	rtl_w0w1_phy(tp, 0x18, 0x0010, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0x0000);
+
+	/* Improve 10M EEE waveform */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b86);
+	rtl_w0w1_phy(tp, 0x06, 0x0001, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	rtl8168f_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8168f_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		/* Channel estimation fine tune */
+		{ 0x1f, 0x0003 },
+		{ 0x09, 0xa20f },
+		{ 0x1f, 0x0000 },
+
+		/* Modify green table for giga & fnet */
+		{ 0x1f, 0x0005 },
+		{ 0x05, 0x8b55 },
+		{ 0x06, 0x0000 },
+		{ 0x05, 0x8b5e },
+		{ 0x06, 0x0000 },
+		{ 0x05, 0x8b67 },
+		{ 0x06, 0x0000 },
+		{ 0x05, 0x8b70 },
+		{ 0x06, 0x0000 },
+		{ 0x1f, 0x0000 },
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x0078 },
+		{ 0x17, 0x0000 },
+		{ 0x19, 0x00fb },
+		{ 0x1f, 0x0000 },
+
+		/* Modify green table for 10M */
+		{ 0x1f, 0x0005 },
+		{ 0x05, 0x8b79 },
+		{ 0x06, 0xaa00 },
+		{ 0x1f, 0x0000 },
+
+		/* Disable hiimpedance detection (RTCT) */
+		{ 0x1f, 0x0003 },
+		{ 0x01, 0x328a },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_apply_firmware(tp);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	rtl8168f_hw_phy_config(tp);
+
+	/* Improve 2-pair detection performance */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b85);
+	rtl_w0w1_phy(tp, 0x06, 0x4000, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168f_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	rtl_apply_firmware(tp);
+
+	rtl8168f_hw_phy_config(tp);
+}
+
+static void rtl8411_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		/* Channel estimation fine tune */
+		{ 0x1f, 0x0003 },
+		{ 0x09, 0xa20f },
+		{ 0x1f, 0x0000 },
+
+		/* Modify green table for giga & fnet */
+		{ 0x1f, 0x0005 },
+		{ 0x05, 0x8b55 },
+		{ 0x06, 0x0000 },
+		{ 0x05, 0x8b5e },
+		{ 0x06, 0x0000 },
+		{ 0x05, 0x8b67 },
+		{ 0x06, 0x0000 },
+		{ 0x05, 0x8b70 },
+		{ 0x06, 0x0000 },
+		{ 0x1f, 0x0000 },
+		{ 0x1f, 0x0007 },
+		{ 0x1e, 0x0078 },
+		{ 0x17, 0x0000 },
+		{ 0x19, 0x00aa },
+		{ 0x1f, 0x0000 },
+
+		/* Modify green table for 10M */
+		{ 0x1f, 0x0005 },
+		{ 0x05, 0x8b79 },
+		{ 0x06, 0xaa00 },
+		{ 0x1f, 0x0000 },
+
+		/* Disable hiimpedance detection (RTCT) */
+		{ 0x1f, 0x0003 },
+		{ 0x01, 0x328a },
+		{ 0x1f, 0x0000 }
+	};
+
+
+	rtl_apply_firmware(tp);
+
+	rtl8168f_hw_phy_config(tp);
+
+	/* Improve 2-pair detection performance */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b85);
+	rtl_w0w1_phy(tp, 0x06, 0x4000, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	/* Modify green table for giga */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b54);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x0800);
+	rtl_writephy(tp, 0x05, 0x8b5d);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x0800);
+	rtl_writephy(tp, 0x05, 0x8a7c);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x0100);
+	rtl_writephy(tp, 0x05, 0x8a7f);
+	rtl_w0w1_phy(tp, 0x06, 0x0100, 0x0000);
+	rtl_writephy(tp, 0x05, 0x8a82);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x0100);
+	rtl_writephy(tp, 0x05, 0x8a85);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x0100);
+	rtl_writephy(tp, 0x05, 0x8a88);
+	rtl_w0w1_phy(tp, 0x06, 0x0000, 0x0100);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* uc same-seed solution */
+	rtl_writephy(tp, 0x1f, 0x0005);
+	rtl_writephy(tp, 0x05, 0x8b85);
+	rtl_w0w1_phy(tp, 0x06, 0x8000, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* Green feature */
+	rtl_writephy(tp, 0x1f, 0x0003);
+	rtl_w0w1_phy(tp, 0x19, 0x0000, 0x0001);
+	rtl_w0w1_phy(tp, 0x10, 0x0000, 0x0400);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8168g_disable_aldps(struct rtl8169_private *tp)
+{
+	phy_modify_paged(tp->phydev, 0x0a43, 0x10, BIT(2), 0);
+}
+
+static void rtl8168g_phy_adjust_10m_aldps(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+
+	phy_modify_paged(phydev, 0x0bcc, 0x14, BIT(8), 0);
+	phy_modify_paged(phydev, 0x0a44, 0x11, 0, BIT(7) | BIT(6));
+	phy_write(phydev, 0x1f, 0x0a43);
+	phy_write(phydev, 0x13, 0x8084);
+	phy_clear_bits(phydev, 0x14, BIT(14) | BIT(13));
+	phy_set_bits(phydev, 0x10, BIT(12) | BIT(1) | BIT(0));
+
+	phy_write(phydev, 0x1f, 0x0000);
+}
+
+static void rtl8168g_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	int ret;
+
+	rtl_apply_firmware(tp);
+
+	ret = phy_read_paged(tp->phydev, 0x0a46, 0x10);
+	if (ret & BIT(8))
+		phy_modify_paged(tp->phydev, 0x0bcc, 0x12, BIT(15), 0);
+	else
+		phy_modify_paged(tp->phydev, 0x0bcc, 0x12, 0, BIT(15));
+
+	ret = phy_read_paged(tp->phydev, 0x0a46, 0x13);
+	if (ret & BIT(8))
+		phy_modify_paged(tp->phydev, 0x0c41, 0x15, 0, BIT(1));
+	else
+		phy_modify_paged(tp->phydev, 0x0c41, 0x15, BIT(1), 0);
+
+	/* Enable PHY auto speed down */
+	phy_modify_paged(tp->phydev, 0x0a44, 0x11, 0, BIT(3) | BIT(2));
+
+	rtl8168g_phy_adjust_10m_aldps(tp);
+
+	/* EEE auto-fallback function */
+	phy_modify_paged(tp->phydev, 0x0a4b, 0x11, 0, BIT(2));
+
+	/* Enable UC LPF tune function */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x8012);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0x0000);
+
+	phy_modify_paged(tp->phydev, 0x0c42, 0x11, BIT(13), BIT(14));
+
+	/* Improve SWR Efficiency */
+	rtl_writephy(tp, 0x1f, 0x0bcd);
+	rtl_writephy(tp, 0x14, 0x5065);
+	rtl_writephy(tp, 0x14, 0xd065);
+	rtl_writephy(tp, 0x1f, 0x0bc8);
+	rtl_writephy(tp, 0x11, 0x5655);
+	rtl_writephy(tp, 0x1f, 0x0bcd);
+	rtl_writephy(tp, 0x14, 0x1065);
+	rtl_writephy(tp, 0x14, 0x9065);
+	rtl_writephy(tp, 0x14, 0x1065);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	rtl8168g_disable_aldps(tp);
+	rtl8168g_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8168g_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	rtl_apply_firmware(tp);
+	rtl8168g_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8168h_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	u16 dout_tapbin;
+	u32 data;
+
+	rtl_apply_firmware(tp);
+
+	/* CHN EST parameters adjust - giga master */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x809b);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0xf800);
+	rtl_writephy(tp, 0x13, 0x80a2);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0xff00);
+	rtl_writephy(tp, 0x13, 0x80a4);
+	rtl_w0w1_phy(tp, 0x14, 0x8500, 0xff00);
+	rtl_writephy(tp, 0x13, 0x809c);
+	rtl_w0w1_phy(tp, 0x14, 0xbd00, 0xff00);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* CHN EST parameters adjust - giga slave */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x80ad);
+	rtl_w0w1_phy(tp, 0x14, 0x7000, 0xf800);
+	rtl_writephy(tp, 0x13, 0x80b4);
+	rtl_w0w1_phy(tp, 0x14, 0x5000, 0xff00);
+	rtl_writephy(tp, 0x13, 0x80ac);
+	rtl_w0w1_phy(tp, 0x14, 0x4000, 0xff00);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* CHN EST parameters adjust - fnet */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x808e);
+	rtl_w0w1_phy(tp, 0x14, 0x1200, 0xff00);
+	rtl_writephy(tp, 0x13, 0x8090);
+	rtl_w0w1_phy(tp, 0x14, 0xe500, 0xff00);
+	rtl_writephy(tp, 0x13, 0x8092);
+	rtl_w0w1_phy(tp, 0x14, 0x9f00, 0xff00);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* enable R-tune & PGA-retune function */
+	dout_tapbin = 0;
+	rtl_writephy(tp, 0x1f, 0x0a46);
+	data = rtl_readphy(tp, 0x13);
+	data &= 3;
+	data <<= 2;
+	dout_tapbin |= data;
+	data = rtl_readphy(tp, 0x12);
+	data &= 0xc000;
+	data >>= 14;
+	dout_tapbin |= data;
+	dout_tapbin = ~(dout_tapbin^0x08);
+	dout_tapbin <<= 12;
+	dout_tapbin &= 0xf000;
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x827a);
+	rtl_w0w1_phy(tp, 0x14, dout_tapbin, 0xf000);
+	rtl_writephy(tp, 0x13, 0x827b);
+	rtl_w0w1_phy(tp, 0x14, dout_tapbin, 0xf000);
+	rtl_writephy(tp, 0x13, 0x827c);
+	rtl_w0w1_phy(tp, 0x14, dout_tapbin, 0xf000);
+	rtl_writephy(tp, 0x13, 0x827d);
+	rtl_w0w1_phy(tp, 0x14, dout_tapbin, 0xf000);
+
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x0811);
+	rtl_w0w1_phy(tp, 0x14, 0x0800, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0a42);
+	rtl_w0w1_phy(tp, 0x16, 0x0002, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* enable GPHY 10M */
+	phy_modify_paged(tp->phydev, 0x0a44, 0x11, 0, BIT(11));
+
+	/* SAR ADC performance */
+	phy_modify_paged(tp->phydev, 0x0bca, 0x17, BIT(12) | BIT(13), BIT(14));
+
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x803f);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x13, 0x8047);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x13, 0x804f);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x13, 0x8057);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x13, 0x805f);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x13, 0x8067);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x13, 0x806f);
+	rtl_w0w1_phy(tp, 0x14, 0x0000, 0x3000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* disable phy pfm mode */
+	phy_modify_paged(tp->phydev, 0x0a44, 0x11, BIT(7), 0);
+
+	rtl8168g_disable_aldps(tp);
+	rtl8168h_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8168h_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	u16 ioffset_p3, ioffset_p2, ioffset_p1, ioffset_p0;
+	u16 rlen;
+	u32 data;
+
+	rtl_apply_firmware(tp);
+
+	/* CHIN EST parameter update */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x808a);
+	rtl_w0w1_phy(tp, 0x14, 0x000a, 0x003f);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* enable R-tune & PGA-retune function */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x0811);
+	rtl_w0w1_phy(tp, 0x14, 0x0800, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0a42);
+	rtl_w0w1_phy(tp, 0x16, 0x0002, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* enable GPHY 10M */
+	phy_modify_paged(tp->phydev, 0x0a44, 0x11, 0, BIT(11));
+
+	r8168_mac_ocp_write(tp, 0xdd02, 0x807d);
+	data = r8168_mac_ocp_read(tp, 0xdd02);
+	ioffset_p3 = ((data & 0x80)>>7);
+	ioffset_p3 <<= 3;
+
+	data = r8168_mac_ocp_read(tp, 0xdd00);
+	ioffset_p3 |= ((data & (0xe000))>>13);
+	ioffset_p2 = ((data & (0x1e00))>>9);
+	ioffset_p1 = ((data & (0x01e0))>>5);
+	ioffset_p0 = ((data & 0x0010)>>4);
+	ioffset_p0 <<= 3;
+	ioffset_p0 |= (data & (0x07));
+	data = (ioffset_p3<<12)|(ioffset_p2<<8)|(ioffset_p1<<4)|(ioffset_p0);
+
+	if ((ioffset_p3 != 0x0f) || (ioffset_p2 != 0x0f) ||
+	    (ioffset_p1 != 0x0f) || (ioffset_p0 != 0x0f)) {
+		rtl_writephy(tp, 0x1f, 0x0bcf);
+		rtl_writephy(tp, 0x16, data);
+		rtl_writephy(tp, 0x1f, 0x0000);
+	}
+
+	/* Modify rlen (TX LPF corner frequency) level */
+	rtl_writephy(tp, 0x1f, 0x0bcd);
+	data = rtl_readphy(tp, 0x16);
+	data &= 0x000f;
+	rlen = 0;
+	if (data > 3)
+		rlen = data - 3;
+	data = rlen | (rlen<<4) | (rlen<<8) | (rlen<<12);
+	rtl_writephy(tp, 0x17, data);
+	rtl_writephy(tp, 0x1f, 0x0bcd);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* disable phy pfm mode */
+	phy_modify_paged(tp->phydev, 0x0a44, 0x11, BIT(7), 0);
+
+	rtl8168g_disable_aldps(tp);
+	rtl8168g_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8168ep_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	/* Enable PHY auto speed down */
+	phy_modify_paged(tp->phydev, 0x0a44, 0x11, 0, BIT(3) | BIT(2));
+
+	rtl8168g_phy_adjust_10m_aldps(tp);
+
+	/* Enable EEE auto-fallback function */
+	phy_modify_paged(tp->phydev, 0x0a4b, 0x11, 0, BIT(2));
+
+	/* Enable UC LPF tune function */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x8012);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* set rg_sel_sdm_rate */
+	phy_modify_paged(tp->phydev, 0x0c42, 0x11, BIT(13), BIT(14));
+
+	rtl8168g_disable_aldps(tp);
+	rtl8168g_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8168ep_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	rtl8168g_phy_adjust_10m_aldps(tp);
+
+	/* Enable UC LPF tune function */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x8012);
+	rtl_w0w1_phy(tp, 0x14, 0x8000, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	/* Set rg_sel_sdm_rate */
+	phy_modify_paged(tp->phydev, 0x0c42, 0x11, BIT(13), BIT(14));
+
+	/* Channel estimation parameters */
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x80f3);
+	rtl_w0w1_phy(tp, 0x14, 0x8b00, ~0x8bff);
+	rtl_writephy(tp, 0x13, 0x80f0);
+	rtl_w0w1_phy(tp, 0x14, 0x3a00, ~0x3aff);
+	rtl_writephy(tp, 0x13, 0x80ef);
+	rtl_w0w1_phy(tp, 0x14, 0x0500, ~0x05ff);
+	rtl_writephy(tp, 0x13, 0x80f6);
+	rtl_w0w1_phy(tp, 0x14, 0x6e00, ~0x6eff);
+	rtl_writephy(tp, 0x13, 0x80ec);
+	rtl_w0w1_phy(tp, 0x14, 0x6800, ~0x68ff);
+	rtl_writephy(tp, 0x13, 0x80ed);
+	rtl_w0w1_phy(tp, 0x14, 0x7c00, ~0x7cff);
+	rtl_writephy(tp, 0x13, 0x80f2);
+	rtl_w0w1_phy(tp, 0x14, 0xf400, ~0xf4ff);
+	rtl_writephy(tp, 0x13, 0x80f4);
+	rtl_w0w1_phy(tp, 0x14, 0x8500, ~0x85ff);
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x8110);
+	rtl_w0w1_phy(tp, 0x14, 0xa800, ~0xa8ff);
+	rtl_writephy(tp, 0x13, 0x810f);
+	rtl_w0w1_phy(tp, 0x14, 0x1d00, ~0x1dff);
+	rtl_writephy(tp, 0x13, 0x8111);
+	rtl_w0w1_phy(tp, 0x14, 0xf500, ~0xf5ff);
+	rtl_writephy(tp, 0x13, 0x8113);
+	rtl_w0w1_phy(tp, 0x14, 0x6100, ~0x61ff);
+	rtl_writephy(tp, 0x13, 0x8115);
+	rtl_w0w1_phy(tp, 0x14, 0x9200, ~0x92ff);
+	rtl_writephy(tp, 0x13, 0x810e);
+	rtl_w0w1_phy(tp, 0x14, 0x0400, ~0x04ff);
+	rtl_writephy(tp, 0x13, 0x810c);
+	rtl_w0w1_phy(tp, 0x14, 0x7c00, ~0x7cff);
+	rtl_writephy(tp, 0x13, 0x810b);
+	rtl_w0w1_phy(tp, 0x14, 0x5a00, ~0x5aff);
+	rtl_writephy(tp, 0x1f, 0x0a43);
+	rtl_writephy(tp, 0x13, 0x80d1);
+	rtl_w0w1_phy(tp, 0x14, 0xff00, ~0xffff);
+	rtl_writephy(tp, 0x13, 0x80cd);
+	rtl_w0w1_phy(tp, 0x14, 0x9e00, ~0x9eff);
+	rtl_writephy(tp, 0x13, 0x80d3);
+	rtl_w0w1_phy(tp, 0x14, 0x0e00, ~0x0eff);
+	rtl_writephy(tp, 0x13, 0x80d5);
+	rtl_w0w1_phy(tp, 0x14, 0xca00, ~0xcaff);
+	rtl_writephy(tp, 0x13, 0x80d7);
+	rtl_w0w1_phy(tp, 0x14, 0x8400, ~0x84ff);
+
+	/* Force PWM-mode */
+	rtl_writephy(tp, 0x1f, 0x0bcd);
+	rtl_writephy(tp, 0x14, 0x5065);
+	rtl_writephy(tp, 0x14, 0xd065);
+	rtl_writephy(tp, 0x1f, 0x0bc8);
+	rtl_writephy(tp, 0x12, 0x00ed);
+	rtl_writephy(tp, 0x1f, 0x0bcd);
+	rtl_writephy(tp, 0x14, 0x1065);
+	rtl_writephy(tp, 0x14, 0x9065);
+	rtl_writephy(tp, 0x14, 0x1065);
+	rtl_writephy(tp, 0x1f, 0x0000);
+
+	rtl8168g_disable_aldps(tp);
+	rtl8168g_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8102e_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0003 },
+		{ 0x08, 0x441d },
+		{ 0x01, 0x9100 },
+		{ 0x1f, 0x0000 }
+	};
+
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_patchphy(tp, 0x11, 1 << 12);
+	rtl_patchphy(tp, 0x19, 1 << 13);
+	rtl_patchphy(tp, 0x10, 1 << 15);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8105e_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0005 },
+		{ 0x1a, 0x0000 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0004 },
+		{ 0x1c, 0x0000 },
+		{ 0x1f, 0x0000 },
+
+		{ 0x1f, 0x0001 },
+		{ 0x15, 0x7701 },
+		{ 0x1f, 0x0000 }
+	};
+
+	/* Disable ALDPS before ram code */
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_writephy(tp, 0x18, 0x0310);
+	msleep(100);
+
+	rtl_apply_firmware(tp);
+
+	rtl_writephy_batch(tp, phy_reg_init);
+}
+
+static void rtl8402_hw_phy_config(struct rtl8169_private *tp)
+{
+	/* Disable ALDPS before setting firmware */
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_writephy(tp, 0x18, 0x0310);
+	msleep(20);
+
+	rtl_apply_firmware(tp);
+
+	/* EEE setting */
+	rtl_eri_write(tp, 0x1b0, ERIAR_MASK_0011, 0x0000);
+	rtl_writephy(tp, 0x1f, 0x0004);
+	rtl_writephy(tp, 0x10, 0x401f);
+	rtl_writephy(tp, 0x19, 0x7030);
+	rtl_writephy(tp, 0x1f, 0x0000);
+}
+
+static void rtl8106e_hw_phy_config(struct rtl8169_private *tp)
+{
+	static const struct phy_reg phy_reg_init[] = {
+		{ 0x1f, 0x0004 },
+		{ 0x10, 0xc07f },
+		{ 0x19, 0x7030 },
+		{ 0x1f, 0x0000 }
+	};
+
+	/* Disable ALDPS before ram code */
+	rtl_writephy(tp, 0x1f, 0x0000);
+	rtl_writephy(tp, 0x18, 0x0310);
+	msleep(100);
+
+	rtl_apply_firmware(tp);
+
+	rtl_eri_write(tp, 0x1b0, ERIAR_MASK_0011, 0x0000);
+	rtl_writephy_batch(tp, phy_reg_init);
+
+	rtl_eri_write(tp, 0x1d0, ERIAR_MASK_0011, 0x0000);
+}
+
+static void rtl8125_1_hw_phy_config(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+
+	phy_modify_paged(phydev, 0xad4, 0x10, 0x03ff, 0x0084);
+	phy_modify_paged(phydev, 0xad4, 0x17, 0x0000, 0x0010);
+	phy_modify_paged(phydev, 0xad1, 0x13, 0x03ff, 0x0006);
+	phy_modify_paged(phydev, 0xad3, 0x11, 0x003f, 0x0006);
+	phy_modify_paged(phydev, 0xac0, 0x14, 0x0000, 0x1100);
+	phy_modify_paged(phydev, 0xac8, 0x15, 0xf000, 0x7000);
+	phy_modify_paged(phydev, 0xad1, 0x14, 0x0000, 0x0400);
+	phy_modify_paged(phydev, 0xad1, 0x15, 0x0000, 0x03ff);
+	phy_modify_paged(phydev, 0xad1, 0x16, 0x0000, 0x03ff);
+
+	phy_write(phydev, 0x1f, 0x0a43);
+	phy_write(phydev, 0x13, 0x80ea);
+	phy_modify(phydev, 0x14, 0xff00, 0xc400);
+	phy_write(phydev, 0x13, 0x80eb);
+	phy_modify(phydev, 0x14, 0x0700, 0x0300);
+	phy_write(phydev, 0x13, 0x80f8);
+	phy_modify(phydev, 0x14, 0xff00, 0x1c00);
+	phy_write(phydev, 0x13, 0x80f1);
+	phy_modify(phydev, 0x14, 0xff00, 0x3000);
+	phy_write(phydev, 0x13, 0x80fe);
+	phy_modify(phydev, 0x14, 0xff00, 0xa500);
+	phy_write(phydev, 0x13, 0x8102);
+	phy_modify(phydev, 0x14, 0xff00, 0x5000);
+	phy_write(phydev, 0x13, 0x8105);
+	phy_modify(phydev, 0x14, 0xff00, 0x3300);
+	phy_write(phydev, 0x13, 0x8100);
+	phy_modify(phydev, 0x14, 0xff00, 0x7000);
+	phy_write(phydev, 0x13, 0x8104);
+	phy_modify(phydev, 0x14, 0xff00, 0xf000);
+	phy_write(phydev, 0x13, 0x8106);
+	phy_modify(phydev, 0x14, 0xff00, 0x6500);
+	phy_write(phydev, 0x13, 0x80dc);
+	phy_modify(phydev, 0x14, 0xff00, 0xed00);
+	phy_write(phydev, 0x13, 0x80df);
+	phy_set_bits(phydev, 0x14, BIT(8));
+	phy_write(phydev, 0x13, 0x80e1);
+	phy_clear_bits(phydev, 0x14, BIT(8));
+	phy_write(phydev, 0x1f, 0x0000);
+
+	phy_modify_paged(phydev, 0xbf0, 0x13, 0x003f, 0x0038);
+	phy_write_paged(phydev, 0xa43, 0x13, 0x819f);
+	phy_write_paged(phydev, 0xa43, 0x14, 0xd0b6);
+
+	phy_write_paged(phydev, 0xbc3, 0x12, 0x5555);
+	phy_modify_paged(phydev, 0xbf0, 0x15, 0x0e00, 0x0a00);
+	phy_modify_paged(phydev, 0xa5c, 0x10, 0x0400, 0x0000);
+	phy_modify_paged(phydev, 0xa44, 0x11, 0x0000, 0x0800);
+
+	rtl8125_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl8125_2_hw_phy_config(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+	int i;
+
+	phy_modify_paged(phydev, 0xad4, 0x17, 0x0000, 0x0010);
+	phy_modify_paged(phydev, 0xad1, 0x13, 0x03ff, 0x03ff);
+	phy_modify_paged(phydev, 0xad3, 0x11, 0x003f, 0x0006);
+	phy_modify_paged(phydev, 0xac0, 0x14, 0x1100, 0x0000);
+	phy_modify_paged(phydev, 0xacc, 0x10, 0x0003, 0x0002);
+	phy_modify_paged(phydev, 0xad4, 0x10, 0x00e7, 0x0044);
+	phy_modify_paged(phydev, 0xac1, 0x12, 0x0080, 0x0000);
+	phy_modify_paged(phydev, 0xac8, 0x10, 0x0300, 0x0000);
+	phy_modify_paged(phydev, 0xac5, 0x17, 0x0007, 0x0002);
+	phy_write_paged(phydev, 0xad4, 0x16, 0x00a8);
+	phy_write_paged(phydev, 0xac5, 0x16, 0x01ff);
+	phy_modify_paged(phydev, 0xac8, 0x15, 0x00f0, 0x0030);
+
+	phy_write(phydev, 0x1f, 0x0b87);
+	phy_write(phydev, 0x16, 0x80a2);
+	phy_write(phydev, 0x17, 0x0153);
+	phy_write(phydev, 0x16, 0x809c);
+	phy_write(phydev, 0x17, 0x0153);
+	phy_write(phydev, 0x1f, 0x0000);
+
+	phy_write(phydev, 0x1f, 0x0a43);
+	phy_write(phydev, 0x13, 0x81B3);
+	phy_write(phydev, 0x14, 0x0043);
+	phy_write(phydev, 0x14, 0x00A7);
+	phy_write(phydev, 0x14, 0x00D6);
+	phy_write(phydev, 0x14, 0x00EC);
+	phy_write(phydev, 0x14, 0x00F6);
+	phy_write(phydev, 0x14, 0x00FB);
+	phy_write(phydev, 0x14, 0x00FD);
+	phy_write(phydev, 0x14, 0x00FF);
+	phy_write(phydev, 0x14, 0x00BB);
+	phy_write(phydev, 0x14, 0x0058);
+	phy_write(phydev, 0x14, 0x0029);
+	phy_write(phydev, 0x14, 0x0013);
+	phy_write(phydev, 0x14, 0x0009);
+	phy_write(phydev, 0x14, 0x0004);
+	phy_write(phydev, 0x14, 0x0002);
+	for (i = 0; i < 25; i++)
+		phy_write(phydev, 0x14, 0x0000);
+
+	phy_write(phydev, 0x13, 0x8257);
+	phy_write(phydev, 0x14, 0x020F);
+
+	phy_write(phydev, 0x13, 0x80EA);
+	phy_write(phydev, 0x14, 0x7843);
+	phy_write(phydev, 0x1f, 0x0000);
+
+	rtl_apply_firmware(tp);
+
+	phy_modify_paged(phydev, 0xd06, 0x14, 0x0000, 0x2000);
+
+	phy_write(phydev, 0x1f, 0x0a43);
+	phy_write(phydev, 0x13, 0x81a2);
+	phy_set_bits(phydev, 0x14, BIT(8));
+	phy_write(phydev, 0x1f, 0x0000);
+
+	phy_modify_paged(phydev, 0xb54, 0x16, 0xff00, 0xdb00);
+	phy_modify_paged(phydev, 0xa45, 0x12, 0x0001, 0x0000);
+	phy_modify_paged(phydev, 0xa5d, 0x12, 0x0000, 0x0020);
+	phy_modify_paged(phydev, 0xad4, 0x17, 0x0010, 0x0000);
+	phy_modify_paged(phydev, 0xa86, 0x15, 0x0001, 0x0000);
+	phy_modify_paged(phydev, 0xa44, 0x11, 0x0000, 0x0800);
+
+	rtl8125_config_eee_phy(tp);
+	rtl_enable_eee(tp);
+}
+
+static void rtl_hw_phy_config(struct net_device *dev)
+{
+	static const rtl_generic_fct phy_configs[] = {
+		/* PCI devices. */
+		[RTL_GIGA_MAC_VER_02] = rtl8169s_hw_phy_config,
+		[RTL_GIGA_MAC_VER_03] = rtl8169s_hw_phy_config,
+		[RTL_GIGA_MAC_VER_04] = rtl8169sb_hw_phy_config,
+		[RTL_GIGA_MAC_VER_05] = rtl8169scd_hw_phy_config,
+		[RTL_GIGA_MAC_VER_06] = rtl8169sce_hw_phy_config,
+		/* PCI-E devices. */
+		[RTL_GIGA_MAC_VER_07] = rtl8102e_hw_phy_config,
+		[RTL_GIGA_MAC_VER_08] = rtl8102e_hw_phy_config,
+		[RTL_GIGA_MAC_VER_09] = rtl8102e_hw_phy_config,
+		[RTL_GIGA_MAC_VER_10] = NULL,
+		[RTL_GIGA_MAC_VER_11] = rtl8168bb_hw_phy_config,
+		[RTL_GIGA_MAC_VER_12] = rtl8168bef_hw_phy_config,
+		[RTL_GIGA_MAC_VER_13] = NULL,
+		[RTL_GIGA_MAC_VER_14] = NULL,
+		[RTL_GIGA_MAC_VER_15] = NULL,
+		[RTL_GIGA_MAC_VER_16] = NULL,
+		[RTL_GIGA_MAC_VER_17] = rtl8168bef_hw_phy_config,
+		[RTL_GIGA_MAC_VER_18] = rtl8168cp_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_19] = rtl8168c_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_20] = rtl8168c_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_21] = rtl8168c_3_hw_phy_config,
+		[RTL_GIGA_MAC_VER_22] = rtl8168c_4_hw_phy_config,
+		[RTL_GIGA_MAC_VER_23] = rtl8168cp_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_24] = rtl8168cp_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_25] = rtl8168d_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_26] = rtl8168d_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_27] = rtl8168d_3_hw_phy_config,
+		[RTL_GIGA_MAC_VER_28] = rtl8168d_4_hw_phy_config,
+		[RTL_GIGA_MAC_VER_29] = rtl8105e_hw_phy_config,
+		[RTL_GIGA_MAC_VER_30] = rtl8105e_hw_phy_config,
+		[RTL_GIGA_MAC_VER_31] = NULL,
+		[RTL_GIGA_MAC_VER_32] = rtl8168e_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_33] = rtl8168e_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_34] = rtl8168e_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_35] = rtl8168f_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_36] = rtl8168f_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_37] = rtl8402_hw_phy_config,
+		[RTL_GIGA_MAC_VER_38] = rtl8411_hw_phy_config,
+		[RTL_GIGA_MAC_VER_39] = rtl8106e_hw_phy_config,
+		[RTL_GIGA_MAC_VER_40] = rtl8168g_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_41] = NULL,
+		[RTL_GIGA_MAC_VER_42] = rtl8168g_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_43] = rtl8168g_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_44] = rtl8168g_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_45] = rtl8168h_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_46] = rtl8168h_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_47] = rtl8168h_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_48] = rtl8168h_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_49] = rtl8168ep_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_50] = rtl8168ep_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_51] = rtl8168ep_2_hw_phy_config,
+		[RTL_GIGA_MAC_VER_60] = rtl8125_1_hw_phy_config,
+		[RTL_GIGA_MAC_VER_61] = rtl8125_2_hw_phy_config,
+	};
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (phy_configs[tp->mac_version])
+		phy_configs[tp->mac_version](tp);
+}
+
+static void rtl_schedule_task(struct rtl8169_private *tp, enum rtl_flag flag)
+{
+	if (!test_and_set_bit(flag, tp->wk.flags))
+		schedule_work(&tp->wk.work);
+}
+
+static void rtl8169_init_phy(struct net_device *dev, struct rtl8169_private *tp)
+{
+	rtl_hw_phy_config(dev);
+
+	if (tp->mac_version <= RTL_GIGA_MAC_VER_06) {
+		pci_write_config_byte(tp->pci_dev, PCI_LATENCY_TIMER, 0x40);
+		pci_write_config_byte(tp->pci_dev, PCI_CACHE_LINE_SIZE, 0x08);
+		netif_dbg(tp, drv, dev,
+			  "Set MAC Reg C+CR Offset 0x82h = 0x01h\n");
+		RTL_W8(tp, 0x82, 0x01);
+	}
+
+	/* We may have called phy_speed_down before */
+	phy_speed_up(tp->phydev);
+
+	genphy_soft_reset(tp->phydev);
+}
+
+static void rtl_rar_set(struct rtl8169_private *tp, u8 *addr)
+{
+	rtl_lock_work(tp);
+
+	rtl_unlock_config_regs(tp);
+
+	RTL_W32(tp, MAC4, addr[4] | addr[5] << 8);
+	RTL_R32(tp, MAC4);
+
+	RTL_W32(tp, MAC0, addr[0] | addr[1] << 8 | addr[2] << 16 | addr[3] << 24);
+	RTL_R32(tp, MAC0);
+
+	if (tp->mac_version == RTL_GIGA_MAC_VER_34)
+		rtl_rar_exgmac_set(tp, addr);
+
+	rtl_lock_config_regs(tp);
+
+	rtl_unlock_work(tp);
+}
+
+static int rtl_set_mac_address(struct net_device *dev, void *p)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct device *d = tp_to_dev(tp);
+	int ret;
+
+	ret = eth_mac_addr(dev, p);
+	if (ret)
+		return ret;
+
+	pm_runtime_get_noresume(d);
+
+	if (pm_runtime_active(d))
+		rtl_rar_set(tp, dev->dev_addr);
+
+	pm_runtime_put_noidle(d);
+
+	return 0;
+}
+
+static int rtl8169_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (!netif_running(dev))
+		return -ENODEV;
+
+	return phy_mii_ioctl(tp->phydev, ifr, cmd);
+}
+
+static void rtl_wol_suspend_quirk(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_25:
+	case RTL_GIGA_MAC_VER_26:
+	case RTL_GIGA_MAC_VER_29:
+	case RTL_GIGA_MAC_VER_30:
+	case RTL_GIGA_MAC_VER_32:
+	case RTL_GIGA_MAC_VER_33:
+	case RTL_GIGA_MAC_VER_34:
+	case RTL_GIGA_MAC_VER_37 ... RTL_GIGA_MAC_VER_61:
+		RTL_W32(tp, RxConfig, RTL_R32(tp, RxConfig) |
+			AcceptBroadcast | AcceptMulticast | AcceptMyPhys);
+		break;
+	default:
+		break;
+	}
+}
+
+static void rtl_pll_power_down(struct rtl8169_private *tp)
+{
+	if (r8168_check_dash(tp))
+		return;
+
+	if (tp->mac_version == RTL_GIGA_MAC_VER_32 ||
+	    tp->mac_version == RTL_GIGA_MAC_VER_33)
+		rtl_ephy_write(tp, 0x19, 0xff64);
+
+	if (device_may_wakeup(tp_to_dev(tp))) {
+		phy_speed_down(tp->phydev, false);
+		rtl_wol_suspend_quirk(tp);
+		return;
+	}
+
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_25 ... RTL_GIGA_MAC_VER_33:
+	case RTL_GIGA_MAC_VER_37:
+	case RTL_GIGA_MAC_VER_39:
+	case RTL_GIGA_MAC_VER_43:
+	case RTL_GIGA_MAC_VER_44:
+	case RTL_GIGA_MAC_VER_45:
+	case RTL_GIGA_MAC_VER_46:
+	case RTL_GIGA_MAC_VER_47:
+	case RTL_GIGA_MAC_VER_48:
+	case RTL_GIGA_MAC_VER_50:
+	case RTL_GIGA_MAC_VER_51:
+	case RTL_GIGA_MAC_VER_60:
+	case RTL_GIGA_MAC_VER_61:
+		RTL_W8(tp, PMCH, RTL_R8(tp, PMCH) & ~0x80);
+		break;
+	case RTL_GIGA_MAC_VER_40:
+	case RTL_GIGA_MAC_VER_41:
+	case RTL_GIGA_MAC_VER_49:
+		rtl_eri_clear_bits(tp, 0x1a8, ERIAR_MASK_1111, 0xfc000000);
+		RTL_W8(tp, PMCH, RTL_R8(tp, PMCH) & ~0x80);
+		break;
+	default:
+		break;
+	}
+}
+
+static void rtl_pll_power_up(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_25 ... RTL_GIGA_MAC_VER_33:
+	case RTL_GIGA_MAC_VER_37:
+	case RTL_GIGA_MAC_VER_39:
+	case RTL_GIGA_MAC_VER_43:
+		RTL_W8(tp, PMCH, RTL_R8(tp, PMCH) | 0x80);
+		break;
+	case RTL_GIGA_MAC_VER_44:
+	case RTL_GIGA_MAC_VER_45:
+	case RTL_GIGA_MAC_VER_46:
+	case RTL_GIGA_MAC_VER_47:
+	case RTL_GIGA_MAC_VER_48:
+	case RTL_GIGA_MAC_VER_50:
+	case RTL_GIGA_MAC_VER_51:
+	case RTL_GIGA_MAC_VER_60:
+	case RTL_GIGA_MAC_VER_61:
+		RTL_W8(tp, PMCH, RTL_R8(tp, PMCH) | 0xc0);
+		break;
+	case RTL_GIGA_MAC_VER_40:
+	case RTL_GIGA_MAC_VER_41:
+	case RTL_GIGA_MAC_VER_49:
+		RTL_W8(tp, PMCH, RTL_R8(tp, PMCH) | 0xc0);
+		rtl_eri_set_bits(tp, 0x1a8, ERIAR_MASK_1111, 0xfc000000);
+		break;
+	default:
+		break;
+	}
+
+	phy_resume(tp->phydev);
+	/* give MAC/PHY some time to resume */
+	msleep(20);
+}
+
+static void rtl_init_rxcfg(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_02 ... RTL_GIGA_MAC_VER_06:
+	case RTL_GIGA_MAC_VER_10 ... RTL_GIGA_MAC_VER_17:
+		RTL_W32(tp, RxConfig, RX_FIFO_THRESH | RX_DMA_BURST);
+		break;
+	case RTL_GIGA_MAC_VER_18 ... RTL_GIGA_MAC_VER_24:
+	case RTL_GIGA_MAC_VER_34 ... RTL_GIGA_MAC_VER_36:
+	case RTL_GIGA_MAC_VER_38:
+		RTL_W32(tp, RxConfig, RX128_INT_EN | RX_MULTI_EN | RX_DMA_BURST);
+		break;
+	case RTL_GIGA_MAC_VER_40 ... RTL_GIGA_MAC_VER_51:
+		RTL_W32(tp, RxConfig, RX128_INT_EN | RX_MULTI_EN | RX_DMA_BURST | RX_EARLY_OFF);
+		break;
+	case RTL_GIGA_MAC_VER_60 ... RTL_GIGA_MAC_VER_61:
+		RTL_W32(tp, RxConfig, RX_FETCH_DFLT_8125 | RX_VLAN_8125 |
+				      RX_DMA_BURST);
+		break;
+	default:
+		RTL_W32(tp, RxConfig, RX128_INT_EN | RX_DMA_BURST);
+		break;
+	}
+}
+
+static void rtl8169_init_ring_indexes(struct rtl8169_private *tp)
+{
+	tp->dirty_tx = tp->cur_tx = tp->cur_rx = 0;
+}
+
+static void r8168c_hw_jumbo_enable(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) | Jumbo_En0);
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) | Jumbo_En1);
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_512B);
+}
+
+static void r8168c_hw_jumbo_disable(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Jumbo_En0);
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) & ~Jumbo_En1);
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+}
+
+static void r8168dp_hw_jumbo_enable(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) | Jumbo_En0);
+}
+
+static void r8168dp_hw_jumbo_disable(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Jumbo_En0);
+}
+
+static void r8168e_hw_jumbo_enable(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, MaxTxPacketSize, 0x3f);
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) | Jumbo_En0);
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) | 0x01);
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_512B);
+}
+
+static void r8168e_hw_jumbo_disable(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, MaxTxPacketSize, 0x0c);
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Jumbo_En0);
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) & ~0x01);
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+}
+
+static void r8168b_0_hw_jumbo_enable(struct rtl8169_private *tp)
+{
+	rtl_tx_performance_tweak(tp,
+		PCI_EXP_DEVCTL_READRQ_512B | PCI_EXP_DEVCTL_NOSNOOP_EN);
+}
+
+static void r8168b_0_hw_jumbo_disable(struct rtl8169_private *tp)
+{
+	rtl_tx_performance_tweak(tp,
+		PCI_EXP_DEVCTL_READRQ_4096B | PCI_EXP_DEVCTL_NOSNOOP_EN);
+}
+
+static void r8168b_1_hw_jumbo_enable(struct rtl8169_private *tp)
+{
+	r8168b_0_hw_jumbo_enable(tp);
+
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) | (1 << 0));
+}
+
+static void r8168b_1_hw_jumbo_disable(struct rtl8169_private *tp)
+{
+	r8168b_0_hw_jumbo_disable(tp);
+
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) & ~(1 << 0));
+}
+
+static void rtl_hw_jumbo_enable(struct rtl8169_private *tp)
+{
+	rtl_unlock_config_regs(tp);
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_11:
+		r8168b_0_hw_jumbo_enable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_12:
+	case RTL_GIGA_MAC_VER_17:
+		r8168b_1_hw_jumbo_enable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_18 ... RTL_GIGA_MAC_VER_26:
+		r8168c_hw_jumbo_enable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_27 ... RTL_GIGA_MAC_VER_28:
+		r8168dp_hw_jumbo_enable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_31 ... RTL_GIGA_MAC_VER_33:
+		r8168e_hw_jumbo_enable(tp);
+		break;
+	default:
+		break;
+	}
+	rtl_lock_config_regs(tp);
+}
+
+static void rtl_hw_jumbo_disable(struct rtl8169_private *tp)
+{
+	rtl_unlock_config_regs(tp);
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_11:
+		r8168b_0_hw_jumbo_disable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_12:
+	case RTL_GIGA_MAC_VER_17:
+		r8168b_1_hw_jumbo_disable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_18 ... RTL_GIGA_MAC_VER_26:
+		r8168c_hw_jumbo_disable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_27 ... RTL_GIGA_MAC_VER_28:
+		r8168dp_hw_jumbo_disable(tp);
+		break;
+	case RTL_GIGA_MAC_VER_31 ... RTL_GIGA_MAC_VER_33:
+		r8168e_hw_jumbo_disable(tp);
+		break;
+	default:
+		break;
+	}
+	rtl_lock_config_regs(tp);
+}
+
+static void rtl_jumbo_config(struct rtl8169_private *tp, int mtu)
+{
+	if (mtu > ETH_DATA_LEN)
+		rtl_hw_jumbo_enable(tp);
+	else
+		rtl_hw_jumbo_disable(tp);
+}
+
+DECLARE_RTL_COND(rtl_chipcmd_cond)
+{
+	return RTL_R8(tp, ChipCmd) & CmdReset;
+}
+
+static void rtl_hw_reset(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, ChipCmd, CmdReset);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_chipcmd_cond, 100, 100);
+}
+
+static void rtl_request_firmware(struct rtl8169_private *tp)
+{
+	struct rtl_fw *rtl_fw;
+
+	/* firmware loaded already or no firmware available */
+	if (tp->rtl_fw || !tp->fw_name)
+		return;
+
+	rtl_fw = kzalloc(sizeof(*rtl_fw), GFP_KERNEL);
+	if (!rtl_fw) {
+		netif_warn(tp, ifup, tp->dev, "Unable to load firmware, out of memory\n");
+		return;
+	}
+
+	rtl_fw->phy_write = rtl_writephy;
+	rtl_fw->phy_read = rtl_readphy;
+	rtl_fw->mac_mcu_write = mac_mcu_write;
+	rtl_fw->mac_mcu_read = mac_mcu_read;
+	rtl_fw->fw_name = tp->fw_name;
+	rtl_fw->dev = tp_to_dev(tp);
+
+	if (rtl_fw_request_firmware(rtl_fw))
+		kfree(rtl_fw);
+	else
+		tp->rtl_fw = rtl_fw;
+}
+
+static void rtl_rx_close(struct rtl8169_private *tp)
+{
+	RTL_W32(tp, RxConfig, RTL_R32(tp, RxConfig) & ~RX_CONFIG_ACCEPT_MASK);
+}
+
+DECLARE_RTL_COND(rtl_npq_cond)
+{
+	return RTL_R8(tp, TxPoll) & NPQ;
+}
+
+DECLARE_RTL_COND(rtl_txcfg_empty_cond)
+{
+	return RTL_R32(tp, TxConfig) & TXCFG_EMPTY;
+}
+
+static void rtl8169_hw_reset(struct rtl8169_private *tp)
+{
+	/* Disable interrupts */
+	rtl8169_irq_mask_and_ack(tp);
+
+	rtl_rx_close(tp);
+
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_27:
+	case RTL_GIGA_MAC_VER_28:
+	case RTL_GIGA_MAC_VER_31:
+		rtl_udelay_loop_wait_low(tp, &rtl_npq_cond, 20, 42*42);
+		break;
+	case RTL_GIGA_MAC_VER_34 ... RTL_GIGA_MAC_VER_38:
+	case RTL_GIGA_MAC_VER_40 ... RTL_GIGA_MAC_VER_51:
+		RTL_W8(tp, ChipCmd, RTL_R8(tp, ChipCmd) | StopReq);
+		rtl_udelay_loop_wait_high(tp, &rtl_txcfg_empty_cond, 100, 666);
+		break;
+	default:
+		RTL_W8(tp, ChipCmd, RTL_R8(tp, ChipCmd) | StopReq);
+		udelay(100);
+		break;
+	}
+
+	rtl_hw_reset(tp);
+}
+
+static void rtl_set_tx_config_registers(struct rtl8169_private *tp)
+{
+	u32 val = TX_DMA_BURST << TxDMAShift |
+		  InterFrameGap << TxInterFrameGapShift;
+
+	if (rtl_is_8168evl_up(tp))
+		val |= TXCFG_AUTO_FIFO;
+
+	RTL_W32(tp, TxConfig, val);
+}
+
+static void rtl_set_rx_max_size(struct rtl8169_private *tp)
+{
+	/* Low hurts. Let's disable the filtering. */
+	RTL_W16(tp, RxMaxSize, R8169_RX_BUF_SIZE + 1);
+}
+
+static void rtl_set_rx_tx_desc_registers(struct rtl8169_private *tp)
+{
+	/*
+	 * Magic spell: some iop3xx ARM board needs the TxDescAddrHigh
+	 * register to be written before TxDescAddrLow to work.
+	 * Switching from MMIO to I/O access fixes the issue as well.
+	 */
+	RTL_W32(tp, TxDescStartAddrHigh, ((u64) tp->TxPhyAddr) >> 32);
+	RTL_W32(tp, TxDescStartAddrLow, ((u64) tp->TxPhyAddr) & DMA_BIT_MASK(32));
+	RTL_W32(tp, RxDescAddrHigh, ((u64) tp->RxPhyAddr) >> 32);
+	RTL_W32(tp, RxDescAddrLow, ((u64) tp->RxPhyAddr) & DMA_BIT_MASK(32));
+}
+
+static void rtl8169_set_magic_reg(struct rtl8169_private *tp, unsigned mac_version)
+{
+	u32 val;
+
+	if (tp->mac_version == RTL_GIGA_MAC_VER_05)
+		val = 0x000fff00;
+	else if (tp->mac_version == RTL_GIGA_MAC_VER_06)
+		val = 0x00ffff00;
+	else
+		return;
+
+	if (RTL_R8(tp, Config2) & PCI_Clock_66MHz)
+		val |= 0xff;
+
+	RTL_W32(tp, 0x7c, val);
+}
+
+static void rtl_set_rx_mode(struct net_device *dev)
+{
+	u32 rx_mode = AcceptBroadcast | AcceptMyPhys | AcceptMulticast;
+	/* Multicast hash filter */
+	u32 mc_filter[2] = { 0xffffffff, 0xffffffff };
+	struct rtl8169_private *tp = netdev_priv(dev);
+	u32 tmp;
+
+	if (dev->flags & IFF_PROMISC) {
+		/* Unconditionally log net taps. */
+		netif_notice(tp, link, dev, "Promiscuous mode enabled\n");
+		rx_mode |= AcceptAllPhys;
+	} else if (netdev_mc_count(dev) > MC_FILTER_LIMIT ||
+		   dev->flags & IFF_ALLMULTI ||
+		   tp->mac_version == RTL_GIGA_MAC_VER_35) {
+		/* accept all multicasts */
+	} else if (netdev_mc_empty(dev)) {
+		rx_mode &= ~AcceptMulticast;
+	} else {
+		struct netdev_hw_addr *ha;
+
+		mc_filter[1] = mc_filter[0] = 0;
+		netdev_for_each_mc_addr(ha, dev) {
+			u32 bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;
+			mc_filter[bit_nr >> 5] |= BIT(bit_nr & 31);
+		}
+
+		if (tp->mac_version > RTL_GIGA_MAC_VER_06) {
+			tmp = mc_filter[0];
+			mc_filter[0] = swab32(mc_filter[1]);
+			mc_filter[1] = swab32(tmp);
+		}
+	}
+
+	if (dev->features & NETIF_F_RXALL)
+		rx_mode |= (AcceptErr | AcceptRunt);
+
+	RTL_W32(tp, MAR0 + 4, mc_filter[1]);
+	RTL_W32(tp, MAR0 + 0, mc_filter[0]);
+
+	tmp = RTL_R32(tp, RxConfig);
+	RTL_W32(tp, RxConfig, (tmp & ~RX_CONFIG_ACCEPT_MASK) | rx_mode);
+}
+
+DECLARE_RTL_COND(rtl_csiar_cond)
+{
+	return RTL_R32(tp, CSIAR) & CSIAR_FLAG;
+}
+
+static void rtl_csi_write(struct rtl8169_private *tp, int addr, int value)
+{
+	u32 func = PCI_FUNC(tp->pci_dev->devfn);
+
+	RTL_W32(tp, CSIDR, value);
+	RTL_W32(tp, CSIAR, CSIAR_WRITE_CMD | (addr & CSIAR_ADDR_MASK) |
+		CSIAR_BYTE_ENABLE | func << 16);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_csiar_cond, 10, 100);
+}
+
+static u32 rtl_csi_read(struct rtl8169_private *tp, int addr)
+{
+	u32 func = PCI_FUNC(tp->pci_dev->devfn);
+
+	RTL_W32(tp, CSIAR, (addr & CSIAR_ADDR_MASK) | func << 16 |
+		CSIAR_BYTE_ENABLE);
+
+	return rtl_udelay_loop_wait_high(tp, &rtl_csiar_cond, 10, 100) ?
+		RTL_R32(tp, CSIDR) : ~0;
+}
+
+static void rtl_csi_access_enable(struct rtl8169_private *tp, u8 val)
+{
+	struct pci_dev *pdev = tp->pci_dev;
+	u32 csi;
+
+	/* According to Realtek the value at config space address 0x070f
+	 * controls the L0s/L1 entrance latency. We try standard ECAM access
+	 * first and if it fails fall back to CSI.
+	 */
+	if (pdev->cfg_size > 0x070f &&
+	    pci_write_config_byte(pdev, 0x070f, val) == PCIBIOS_SUCCESSFUL)
+		return;
+
+	netdev_notice_once(tp->dev,
+		"No native access to PCI extended config space, falling back to CSI\n");
+	csi = rtl_csi_read(tp, 0x070c) & 0x00ffffff;
+	rtl_csi_write(tp, 0x070c, csi | val << 24);
+}
+
+static void rtl_set_def_aspm_entry_latency(struct rtl8169_private *tp)
+{
+	rtl_csi_access_enable(tp, 0x27);
+}
+
+struct ephy_info {
+	unsigned int offset;
+	u16 mask;
+	u16 bits;
+};
+
+static void __rtl_ephy_init(struct rtl8169_private *tp,
+			    const struct ephy_info *e, int len)
+{
+	u16 w;
+
+	while (len-- > 0) {
+		w = (rtl_ephy_read(tp, e->offset) & ~e->mask) | e->bits;
+		rtl_ephy_write(tp, e->offset, w);
+		e++;
+	}
+}
+
+#define rtl_ephy_init(tp, a) __rtl_ephy_init(tp, a, ARRAY_SIZE(a))
+
+static void rtl_disable_clock_request(struct rtl8169_private *tp)
+{
+	pcie_capability_clear_word(tp->pci_dev, PCI_EXP_LNKCTL,
+				   PCI_EXP_LNKCTL_CLKREQ_EN);
+}
+
+static void rtl_enable_clock_request(struct rtl8169_private *tp)
+{
+	pcie_capability_set_word(tp->pci_dev, PCI_EXP_LNKCTL,
+				 PCI_EXP_LNKCTL_CLKREQ_EN);
+}
+
+static void rtl_pcie_state_l2l3_disable(struct rtl8169_private *tp)
+{
+	/* work around an issue when PCI reset occurs during L2/L3 state */
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Rdy_to_L23);
+}
+
+static void rtl_hw_aspm_clkreq_enable(struct rtl8169_private *tp, bool enable)
+{
+	/* Don't enable ASPM in the chip if OS can't control ASPM */
+	if (enable && tp->aspm_manageable) {
+		RTL_W8(tp, Config5, RTL_R8(tp, Config5) | ASPM_en);
+		RTL_W8(tp, Config2, RTL_R8(tp, Config2) | ClkReqEn);
+	} else {
+		RTL_W8(tp, Config2, RTL_R8(tp, Config2) & ~ClkReqEn);
+		RTL_W8(tp, Config5, RTL_R8(tp, Config5) & ~ASPM_en);
+	}
+
+	udelay(10);
+}
+
+static void rtl_set_fifo_size(struct rtl8169_private *tp, u16 rx_stat,
+			      u16 tx_stat, u16 rx_dyn, u16 tx_dyn)
+{
+	/* Usage of dynamic vs. static FIFO is controlled by bit
+	 * TXCFG_AUTO_FIFO. Exact meaning of FIFO values isn't known.
+	 */
+	rtl_eri_write(tp, 0xc8, ERIAR_MASK_1111, (rx_stat << 16) | rx_dyn);
+	rtl_eri_write(tp, 0xe8, ERIAR_MASK_1111, (tx_stat << 16) | tx_dyn);
+}
+
+static void rtl8168g_set_pause_thresholds(struct rtl8169_private *tp,
+					  u8 low, u8 high)
+{
+	/* FIFO thresholds for pause flow control */
+	rtl_eri_write(tp, 0xcc, ERIAR_MASK_0001, low);
+	rtl_eri_write(tp, 0xd0, ERIAR_MASK_0001, high);
+}
+
+static void rtl_hw_start_8168bb(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Beacon_en);
+}
+
+static void rtl_hw_start_8168bef(struct rtl8169_private *tp)
+{
+	rtl_hw_start_8168bb(tp);
+
+	RTL_W8(tp, Config4, RTL_R8(tp, Config4) & ~(1 << 0));
+}
+
+static void __rtl_hw_start_8168cp(struct rtl8169_private *tp)
+{
+	RTL_W8(tp, Config1, RTL_R8(tp, Config1) | Speed_down);
+
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Beacon_en);
+
+	rtl_disable_clock_request(tp);
+}
+
+static void rtl_hw_start_8168cp_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168cp[] = {
+		{ 0x01, 0,	0x0001 },
+		{ 0x02, 0x0800,	0x1000 },
+		{ 0x03, 0,	0x0042 },
+		{ 0x06, 0x0080,	0x0000 },
+		{ 0x07, 0,	0x2000 }
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_ephy_init(tp, e_info_8168cp);
+
+	__rtl_hw_start_8168cp(tp);
+}
+
+static void rtl_hw_start_8168cp_2(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Beacon_en);
+}
+
+static void rtl_hw_start_8168cp_3(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Beacon_en);
+
+	/* Magic. */
+	RTL_W8(tp, DBG_REG, 0x20);
+}
+
+static void rtl_hw_start_8168c_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168c_1[] = {
+		{ 0x02, 0x0800,	0x1000 },
+		{ 0x03, 0,	0x0002 },
+		{ 0x06, 0x0080,	0x0000 }
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	RTL_W8(tp, DBG_REG, 0x06 | FIX_NAK_1 | FIX_NAK_2);
+
+	rtl_ephy_init(tp, e_info_8168c_1);
+
+	__rtl_hw_start_8168cp(tp);
+}
+
+static void rtl_hw_start_8168c_2(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168c_2[] = {
+		{ 0x01, 0,	0x0001 },
+		{ 0x03, 0x0400,	0x0020 }
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_ephy_init(tp, e_info_8168c_2);
+
+	__rtl_hw_start_8168cp(tp);
+}
+
+static void rtl_hw_start_8168c_3(struct rtl8169_private *tp)
+{
+	rtl_hw_start_8168c_2(tp);
+}
+
+static void rtl_hw_start_8168c_4(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	__rtl_hw_start_8168cp(tp);
+}
+
+static void rtl_hw_start_8168d(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_disable_clock_request(tp);
+
+	if (tp->dev->mtu <= ETH_DATA_LEN)
+		rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+}
+
+static void rtl_hw_start_8168dp(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	if (tp->dev->mtu <= ETH_DATA_LEN)
+		rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_disable_clock_request(tp);
+}
+
+static void rtl_hw_start_8168d_4(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168d_4[] = {
+		{ 0x0b, 0x0000,	0x0048 },
+		{ 0x19, 0x0020,	0x0050 },
+		{ 0x0c, 0x0100,	0x0020 },
+		{ 0x10, 0x0004,	0x0000 },
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_ephy_init(tp, e_info_8168d_4);
+
+	rtl_enable_clock_request(tp);
+}
+
+static void rtl_hw_start_8168e_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168e_1[] = {
+		{ 0x00, 0x0200,	0x0100 },
+		{ 0x00, 0x0000,	0x0004 },
+		{ 0x06, 0x0002,	0x0001 },
+		{ 0x06, 0x0000,	0x0030 },
+		{ 0x07, 0x0000,	0x2000 },
+		{ 0x00, 0x0000,	0x0020 },
+		{ 0x03, 0x5800,	0x2000 },
+		{ 0x03, 0x0000,	0x0001 },
+		{ 0x01, 0x0800,	0x1000 },
+		{ 0x07, 0x0000,	0x4000 },
+		{ 0x1e, 0x0000,	0x2000 },
+		{ 0x19, 0xffff,	0xfe6c },
+		{ 0x0a, 0x0000,	0x0040 }
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_ephy_init(tp, e_info_8168e_1);
+
+	rtl_disable_clock_request(tp);
+
+	/* Reset tx FIFO pointer */
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) | TXPLA_RST);
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) & ~TXPLA_RST);
+
+	RTL_W8(tp, Config5, RTL_R8(tp, Config5) & ~Spi_en);
+}
+
+static void rtl_hw_start_8168e_2(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168e_2[] = {
+		{ 0x09, 0x0000,	0x0080 },
+		{ 0x19, 0x0000,	0x0224 },
+		{ 0x00, 0x0000,	0x0004 },
+		{ 0x0c, 0x3df0,	0x0200 },
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_ephy_init(tp, e_info_8168e_2);
+
+	rtl_eri_write(tp, 0xc0, ERIAR_MASK_0011, 0x0000);
+	rtl_eri_write(tp, 0xb8, ERIAR_MASK_0011, 0x0000);
+	rtl_set_fifo_size(tp, 0x10, 0x10, 0x02, 0x06);
+	rtl_eri_write(tp, 0xcc, ERIAR_MASK_1111, 0x00000050);
+	rtl_eri_write(tp, 0xd0, ERIAR_MASK_1111, 0x07ff0060);
+	rtl_eri_set_bits(tp, 0x1b0, ERIAR_MASK_0001, BIT(4));
+	rtl_w0w1_eri(tp, 0x0d4, ERIAR_MASK_0011, 0x0c00, 0xff00);
+
+	rtl_disable_clock_request(tp);
+
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) & ~NOW_IS_OOB);
+
+	rtl8168_config_eee_mac(tp);
+
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) | PFM_EN);
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) | PWM_EN);
+	RTL_W8(tp, Config5, RTL_R8(tp, Config5) & ~Spi_en);
+
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8168f(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_eri_write(tp, 0xc0, ERIAR_MASK_0011, 0x0000);
+	rtl_eri_write(tp, 0xb8, ERIAR_MASK_0011, 0x0000);
+	rtl_set_fifo_size(tp, 0x10, 0x10, 0x02, 0x06);
+	rtl_reset_packet_filter(tp);
+	rtl_eri_set_bits(tp, 0x1b0, ERIAR_MASK_0001, BIT(4));
+	rtl_eri_set_bits(tp, 0x1d0, ERIAR_MASK_0001, BIT(4));
+	rtl_eri_write(tp, 0xcc, ERIAR_MASK_1111, 0x00000050);
+	rtl_eri_write(tp, 0xd0, ERIAR_MASK_1111, 0x00000060);
+
+	rtl_disable_clock_request(tp);
+
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) & ~NOW_IS_OOB);
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) | PFM_EN);
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) | PWM_EN);
+	RTL_W8(tp, Config5, RTL_R8(tp, Config5) & ~Spi_en);
+
+	rtl8168_config_eee_mac(tp);
+}
+
+static void rtl_hw_start_8168f_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168f_1[] = {
+		{ 0x06, 0x00c0,	0x0020 },
+		{ 0x08, 0x0001,	0x0002 },
+		{ 0x09, 0x0000,	0x0080 },
+		{ 0x19, 0x0000,	0x0224 },
+		{ 0x00, 0x0000,	0x0004 },
+		{ 0x0c, 0x3df0,	0x0200 },
+	};
+
+	rtl_hw_start_8168f(tp);
+
+	rtl_ephy_init(tp, e_info_8168f_1);
+
+	rtl_w0w1_eri(tp, 0x0d4, ERIAR_MASK_0011, 0x0c00, 0xff00);
+}
+
+static void rtl_hw_start_8411(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168f_1[] = {
+		{ 0x06, 0x00c0,	0x0020 },
+		{ 0x0f, 0xffff,	0x5200 },
+		{ 0x19, 0x0000,	0x0224 },
+		{ 0x00, 0x0000,	0x0004 },
+		{ 0x0c, 0x3df0,	0x0200 },
+	};
+
+	rtl_hw_start_8168f(tp);
+	rtl_pcie_state_l2l3_disable(tp);
+
+	rtl_ephy_init(tp, e_info_8168f_1);
+
+	rtl_eri_set_bits(tp, 0x0d4, ERIAR_MASK_0011, 0x0c00);
+}
+
+static void rtl_hw_start_8168g(struct rtl8169_private *tp)
+{
+	rtl_set_fifo_size(tp, 0x08, 0x10, 0x02, 0x06);
+	rtl8168g_set_pause_thresholds(tp, 0x38, 0x48);
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_reset_packet_filter(tp);
+	rtl_eri_write(tp, 0x2f8, ERIAR_MASK_0011, 0x1d8f);
+
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) & ~RXDV_GATED_EN);
+
+	rtl_eri_write(tp, 0xc0, ERIAR_MASK_0011, 0x0000);
+	rtl_eri_write(tp, 0xb8, ERIAR_MASK_0011, 0x0000);
+
+	rtl8168_config_eee_mac(tp);
+
+	rtl_w0w1_eri(tp, 0x2fc, ERIAR_MASK_0001, 0x01, 0x06);
+	rtl_eri_clear_bits(tp, 0x1b0, ERIAR_MASK_0011, BIT(12));
+
+	rtl_pcie_state_l2l3_disable(tp);
+}
+
+static void rtl_hw_start_8168g_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168g_1[] = {
+		{ 0x00, 0x0008,	0x0000 },
+		{ 0x0c, 0x3ff0,	0x0820 },
+		{ 0x1e, 0x0000,	0x0001 },
+		{ 0x19, 0x8000,	0x0000 }
+	};
+
+	rtl_hw_start_8168g(tp);
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8168g_1);
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8168g_2(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168g_2[] = {
+		{ 0x00, 0x0008,	0x0000 },
+		{ 0x0c, 0x3ff0,	0x0820 },
+		{ 0x19, 0xffff,	0x7c00 },
+		{ 0x1e, 0xffff,	0x20eb },
+		{ 0x0d, 0xffff,	0x1666 },
+		{ 0x00, 0xffff,	0x10a3 },
+		{ 0x06, 0xffff,	0xf050 },
+		{ 0x04, 0x0000,	0x0010 },
+		{ 0x1d, 0x4000,	0x0000 },
+	};
+
+	rtl_hw_start_8168g(tp);
+
+	/* disable aspm and clock request before access ephy */
+	RTL_W8(tp, Config2, RTL_R8(tp, Config2) & ~ClkReqEn);
+	RTL_W8(tp, Config5, RTL_R8(tp, Config5) & ~ASPM_en);
+	rtl_ephy_init(tp, e_info_8168g_2);
+}
+
+static void rtl_hw_start_8411_2(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8411_2[] = {
+		{ 0x00, 0x0008,	0x0000 },
+		{ 0x0c, 0x37d0,	0x0820 },
+		{ 0x1e, 0x0000,	0x0001 },
+		{ 0x19, 0x8021,	0x0000 },
+		{ 0x1e, 0x0000,	0x2000 },
+		{ 0x0d, 0x0100,	0x0200 },
+		{ 0x00, 0x0000,	0x0080 },
+		{ 0x06, 0x0000,	0x0010 },
+		{ 0x04, 0x0000,	0x0010 },
+		{ 0x1d, 0x0000,	0x4000 },
+	};
+
+	rtl_hw_start_8168g(tp);
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8411_2);
+
+	/* The following Realtek-provided magic fixes an issue with the RX unit
+	 * getting confused after the PHY having been powered-down.
+	 */
+	r8168_mac_ocp_write(tp, 0xFC28, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC2A, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC2C, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC2E, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC30, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC32, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC34, 0x0000);
+	r8168_mac_ocp_write(tp, 0xFC36, 0x0000);
+	mdelay(3);
+	r8168_mac_ocp_write(tp, 0xFC26, 0x0000);
+
+	r8168_mac_ocp_write(tp, 0xF800, 0xE008);
+	r8168_mac_ocp_write(tp, 0xF802, 0xE00A);
+	r8168_mac_ocp_write(tp, 0xF804, 0xE00C);
+	r8168_mac_ocp_write(tp, 0xF806, 0xE00E);
+	r8168_mac_ocp_write(tp, 0xF808, 0xE027);
+	r8168_mac_ocp_write(tp, 0xF80A, 0xE04F);
+	r8168_mac_ocp_write(tp, 0xF80C, 0xE05E);
+	r8168_mac_ocp_write(tp, 0xF80E, 0xE065);
+	r8168_mac_ocp_write(tp, 0xF810, 0xC602);
+	r8168_mac_ocp_write(tp, 0xF812, 0xBE00);
+	r8168_mac_ocp_write(tp, 0xF814, 0x0000);
+	r8168_mac_ocp_write(tp, 0xF816, 0xC502);
+	r8168_mac_ocp_write(tp, 0xF818, 0xBD00);
+	r8168_mac_ocp_write(tp, 0xF81A, 0x074C);
+	r8168_mac_ocp_write(tp, 0xF81C, 0xC302);
+	r8168_mac_ocp_write(tp, 0xF81E, 0xBB00);
+	r8168_mac_ocp_write(tp, 0xF820, 0x080A);
+	r8168_mac_ocp_write(tp, 0xF822, 0x6420);
+	r8168_mac_ocp_write(tp, 0xF824, 0x48C2);
+	r8168_mac_ocp_write(tp, 0xF826, 0x8C20);
+	r8168_mac_ocp_write(tp, 0xF828, 0xC516);
+	r8168_mac_ocp_write(tp, 0xF82A, 0x64A4);
+	r8168_mac_ocp_write(tp, 0xF82C, 0x49C0);
+	r8168_mac_ocp_write(tp, 0xF82E, 0xF009);
+	r8168_mac_ocp_write(tp, 0xF830, 0x74A2);
+	r8168_mac_ocp_write(tp, 0xF832, 0x8CA5);
+	r8168_mac_ocp_write(tp, 0xF834, 0x74A0);
+	r8168_mac_ocp_write(tp, 0xF836, 0xC50E);
+	r8168_mac_ocp_write(tp, 0xF838, 0x9CA2);
+	r8168_mac_ocp_write(tp, 0xF83A, 0x1C11);
+	r8168_mac_ocp_write(tp, 0xF83C, 0x9CA0);
+	r8168_mac_ocp_write(tp, 0xF83E, 0xE006);
+	r8168_mac_ocp_write(tp, 0xF840, 0x74F8);
+	r8168_mac_ocp_write(tp, 0xF842, 0x48C4);
+	r8168_mac_ocp_write(tp, 0xF844, 0x8CF8);
+	r8168_mac_ocp_write(tp, 0xF846, 0xC404);
+	r8168_mac_ocp_write(tp, 0xF848, 0xBC00);
+	r8168_mac_ocp_write(tp, 0xF84A, 0xC403);
+	r8168_mac_ocp_write(tp, 0xF84C, 0xBC00);
+	r8168_mac_ocp_write(tp, 0xF84E, 0x0BF2);
+	r8168_mac_ocp_write(tp, 0xF850, 0x0C0A);
+	r8168_mac_ocp_write(tp, 0xF852, 0xE434);
+	r8168_mac_ocp_write(tp, 0xF854, 0xD3C0);
+	r8168_mac_ocp_write(tp, 0xF856, 0x49D9);
+	r8168_mac_ocp_write(tp, 0xF858, 0xF01F);
+	r8168_mac_ocp_write(tp, 0xF85A, 0xC526);
+	r8168_mac_ocp_write(tp, 0xF85C, 0x64A5);
+	r8168_mac_ocp_write(tp, 0xF85E, 0x1400);
+	r8168_mac_ocp_write(tp, 0xF860, 0xF007);
+	r8168_mac_ocp_write(tp, 0xF862, 0x0C01);
+	r8168_mac_ocp_write(tp, 0xF864, 0x8CA5);
+	r8168_mac_ocp_write(tp, 0xF866, 0x1C15);
+	r8168_mac_ocp_write(tp, 0xF868, 0xC51B);
+	r8168_mac_ocp_write(tp, 0xF86A, 0x9CA0);
+	r8168_mac_ocp_write(tp, 0xF86C, 0xE013);
+	r8168_mac_ocp_write(tp, 0xF86E, 0xC519);
+	r8168_mac_ocp_write(tp, 0xF870, 0x74A0);
+	r8168_mac_ocp_write(tp, 0xF872, 0x48C4);
+	r8168_mac_ocp_write(tp, 0xF874, 0x8CA0);
+	r8168_mac_ocp_write(tp, 0xF876, 0xC516);
+	r8168_mac_ocp_write(tp, 0xF878, 0x74A4);
+	r8168_mac_ocp_write(tp, 0xF87A, 0x48C8);
+	r8168_mac_ocp_write(tp, 0xF87C, 0x48CA);
+	r8168_mac_ocp_write(tp, 0xF87E, 0x9CA4);
+	r8168_mac_ocp_write(tp, 0xF880, 0xC512);
+	r8168_mac_ocp_write(tp, 0xF882, 0x1B00);
+	r8168_mac_ocp_write(tp, 0xF884, 0x9BA0);
+	r8168_mac_ocp_write(tp, 0xF886, 0x1B1C);
+	r8168_mac_ocp_write(tp, 0xF888, 0x483F);
+	r8168_mac_ocp_write(tp, 0xF88A, 0x9BA2);
+	r8168_mac_ocp_write(tp, 0xF88C, 0x1B04);
+	r8168_mac_ocp_write(tp, 0xF88E, 0xC508);
+	r8168_mac_ocp_write(tp, 0xF890, 0x9BA0);
+	r8168_mac_ocp_write(tp, 0xF892, 0xC505);
+	r8168_mac_ocp_write(tp, 0xF894, 0xBD00);
+	r8168_mac_ocp_write(tp, 0xF896, 0xC502);
+	r8168_mac_ocp_write(tp, 0xF898, 0xBD00);
+	r8168_mac_ocp_write(tp, 0xF89A, 0x0300);
+	r8168_mac_ocp_write(tp, 0xF89C, 0x051E);
+	r8168_mac_ocp_write(tp, 0xF89E, 0xE434);
+	r8168_mac_ocp_write(tp, 0xF8A0, 0xE018);
+	r8168_mac_ocp_write(tp, 0xF8A2, 0xE092);
+	r8168_mac_ocp_write(tp, 0xF8A4, 0xDE20);
+	r8168_mac_ocp_write(tp, 0xF8A6, 0xD3C0);
+	r8168_mac_ocp_write(tp, 0xF8A8, 0xC50F);
+	r8168_mac_ocp_write(tp, 0xF8AA, 0x76A4);
+	r8168_mac_ocp_write(tp, 0xF8AC, 0x49E3);
+	r8168_mac_ocp_write(tp, 0xF8AE, 0xF007);
+	r8168_mac_ocp_write(tp, 0xF8B0, 0x49C0);
+	r8168_mac_ocp_write(tp, 0xF8B2, 0xF103);
+	r8168_mac_ocp_write(tp, 0xF8B4, 0xC607);
+	r8168_mac_ocp_write(tp, 0xF8B6, 0xBE00);
+	r8168_mac_ocp_write(tp, 0xF8B8, 0xC606);
+	r8168_mac_ocp_write(tp, 0xF8BA, 0xBE00);
+	r8168_mac_ocp_write(tp, 0xF8BC, 0xC602);
+	r8168_mac_ocp_write(tp, 0xF8BE, 0xBE00);
+	r8168_mac_ocp_write(tp, 0xF8C0, 0x0C4C);
+	r8168_mac_ocp_write(tp, 0xF8C2, 0x0C28);
+	r8168_mac_ocp_write(tp, 0xF8C4, 0x0C2C);
+	r8168_mac_ocp_write(tp, 0xF8C6, 0xDC00);
+	r8168_mac_ocp_write(tp, 0xF8C8, 0xC707);
+	r8168_mac_ocp_write(tp, 0xF8CA, 0x1D00);
+	r8168_mac_ocp_write(tp, 0xF8CC, 0x8DE2);
+	r8168_mac_ocp_write(tp, 0xF8CE, 0x48C1);
+	r8168_mac_ocp_write(tp, 0xF8D0, 0xC502);
+	r8168_mac_ocp_write(tp, 0xF8D2, 0xBD00);
+	r8168_mac_ocp_write(tp, 0xF8D4, 0x00AA);
+	r8168_mac_ocp_write(tp, 0xF8D6, 0xE0C0);
+	r8168_mac_ocp_write(tp, 0xF8D8, 0xC502);
+	r8168_mac_ocp_write(tp, 0xF8DA, 0xBD00);
+	r8168_mac_ocp_write(tp, 0xF8DC, 0x0132);
+
+	r8168_mac_ocp_write(tp, 0xFC26, 0x8000);
+
+	r8168_mac_ocp_write(tp, 0xFC2A, 0x0743);
+	r8168_mac_ocp_write(tp, 0xFC2C, 0x0801);
+	r8168_mac_ocp_write(tp, 0xFC2E, 0x0BE9);
+	r8168_mac_ocp_write(tp, 0xFC30, 0x02FD);
+	r8168_mac_ocp_write(tp, 0xFC32, 0x0C25);
+	r8168_mac_ocp_write(tp, 0xFC34, 0x00A9);
+	r8168_mac_ocp_write(tp, 0xFC36, 0x012D);
+
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8168h_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168h_1[] = {
+		{ 0x1e, 0x0800,	0x0001 },
+		{ 0x1d, 0x0000,	0x0800 },
+		{ 0x05, 0xffff,	0x2089 },
+		{ 0x06, 0xffff,	0x5881 },
+		{ 0x04, 0xffff,	0x854a },
+		{ 0x01, 0xffff,	0x068b }
+	};
+	int rg_saw_cnt;
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8168h_1);
+
+	rtl_set_fifo_size(tp, 0x08, 0x10, 0x02, 0x06);
+	rtl8168g_set_pause_thresholds(tp, 0x38, 0x48);
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_reset_packet_filter(tp);
+
+	rtl_eri_set_bits(tp, 0xdc, ERIAR_MASK_1111, BIT(4));
+
+	rtl_eri_set_bits(tp, 0xd4, ERIAR_MASK_1111, 0x1f00);
+
+	rtl_eri_write(tp, 0x5f0, ERIAR_MASK_0011, 0x4f87);
+
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) & ~RXDV_GATED_EN);
+
+	rtl_eri_write(tp, 0xc0, ERIAR_MASK_0011, 0x0000);
+	rtl_eri_write(tp, 0xb8, ERIAR_MASK_0011, 0x0000);
+
+	rtl8168_config_eee_mac(tp);
+
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) & ~PFM_EN);
+	RTL_W8(tp, MISC_1, RTL_R8(tp, MISC_1) & ~PFM_D3COLD_EN);
+
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) & ~TX_10M_PS_EN);
+
+	rtl_eri_clear_bits(tp, 0x1b0, ERIAR_MASK_0011, BIT(12));
+
+	rtl_pcie_state_l2l3_disable(tp);
+
+	rtl_writephy(tp, 0x1f, 0x0c42);
+	rg_saw_cnt = (rtl_readphy(tp, 0x13) & 0x3fff);
+	rtl_writephy(tp, 0x1f, 0x0000);
+	if (rg_saw_cnt > 0) {
+		u16 sw_cnt_1ms_ini;
+
+		sw_cnt_1ms_ini = 16000000/rg_saw_cnt;
+		sw_cnt_1ms_ini &= 0x0fff;
+		r8168_mac_ocp_modify(tp, 0xd412, 0x0fff, sw_cnt_1ms_ini);
+	}
+
+	r8168_mac_ocp_modify(tp, 0xe056, 0x00f0, 0x0070);
+	r8168_mac_ocp_modify(tp, 0xe052, 0x6000, 0x8008);
+	r8168_mac_ocp_modify(tp, 0xe0d6, 0x01ff, 0x017f);
+	r8168_mac_ocp_modify(tp, 0xd420, 0x0fff, 0x047f);
+
+	r8168_mac_ocp_write(tp, 0xe63e, 0x0001);
+	r8168_mac_ocp_write(tp, 0xe63e, 0x0000);
+	r8168_mac_ocp_write(tp, 0xc094, 0x0000);
+	r8168_mac_ocp_write(tp, 0xc09e, 0x0000);
+
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8168ep(struct rtl8169_private *tp)
+{
+	rtl8168ep_stop_cmac(tp);
+
+	rtl_set_fifo_size(tp, 0x08, 0x10, 0x02, 0x06);
+	rtl8168g_set_pause_thresholds(tp, 0x2f, 0x5f);
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_reset_packet_filter(tp);
+
+	rtl_eri_set_bits(tp, 0xd4, ERIAR_MASK_1111, 0x1f80);
+
+	rtl_eri_write(tp, 0x5f0, ERIAR_MASK_0011, 0x4f87);
+
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) & ~RXDV_GATED_EN);
+
+	rtl_eri_write(tp, 0xc0, ERIAR_MASK_0011, 0x0000);
+	rtl_eri_write(tp, 0xb8, ERIAR_MASK_0011, 0x0000);
+
+	rtl8168_config_eee_mac(tp);
+
+	rtl_w0w1_eri(tp, 0x2fc, ERIAR_MASK_0001, 0x01, 0x06);
+
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) & ~TX_10M_PS_EN);
+
+	rtl_pcie_state_l2l3_disable(tp);
+}
+
+static void rtl_hw_start_8168ep_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168ep_1[] = {
+		{ 0x00, 0xffff,	0x10ab },
+		{ 0x06, 0xffff,	0xf030 },
+		{ 0x08, 0xffff,	0x2006 },
+		{ 0x0d, 0xffff,	0x1666 },
+		{ 0x0c, 0x3ff0,	0x0000 }
+	};
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8168ep_1);
+
+	rtl_hw_start_8168ep(tp);
+
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8168ep_2(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168ep_2[] = {
+		{ 0x00, 0xffff,	0x10a3 },
+		{ 0x19, 0xffff,	0xfc00 },
+		{ 0x1e, 0xffff,	0x20ea }
+	};
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8168ep_2);
+
+	rtl_hw_start_8168ep(tp);
+
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) & ~PFM_EN);
+	RTL_W8(tp, MISC_1, RTL_R8(tp, MISC_1) & ~PFM_D3COLD_EN);
+
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8168ep_3(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8168ep_3[] = {
+		{ 0x00, 0x0000,	0x0080 },
+		{ 0x0d, 0x0100,	0x0200 },
+		{ 0x19, 0x8021,	0x0000 },
+		{ 0x1e, 0x0000,	0x2000 },
+	};
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8168ep_3);
+
+	rtl_hw_start_8168ep(tp);
+
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) & ~PFM_EN);
+	RTL_W8(tp, MISC_1, RTL_R8(tp, MISC_1) & ~PFM_D3COLD_EN);
+
+	r8168_mac_ocp_modify(tp, 0xd3e2, 0x0fff, 0x0271);
+	r8168_mac_ocp_modify(tp, 0xd3e4, 0x00ff, 0x0000);
+	r8168_mac_ocp_modify(tp, 0xe860, 0x0000, 0x0080);
+
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+static void rtl_hw_start_8102e_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8102e_1[] = {
+		{ 0x01,	0, 0x6e65 },
+		{ 0x02,	0, 0x091f },
+		{ 0x03,	0, 0xc2f9 },
+		{ 0x06,	0, 0xafb5 },
+		{ 0x07,	0, 0x0e00 },
+		{ 0x19,	0, 0xec80 },
+		{ 0x01,	0, 0x2e65 },
+		{ 0x01,	0, 0x6e65 }
+	};
+	u8 cfg1;
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	RTL_W8(tp, DBG_REG, FIX_NAK_1);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	RTL_W8(tp, Config1,
+	       LEDS1 | LEDS0 | Speed_down | MEMMAP | IOMAP | VPD | PMEnable);
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Beacon_en);
+
+	cfg1 = RTL_R8(tp, Config1);
+	if ((cfg1 & LEDS0) && (cfg1 & LEDS1))
+		RTL_W8(tp, Config1, cfg1 & ~LEDS0);
+
+	rtl_ephy_init(tp, e_info_8102e_1);
+}
+
+static void rtl_hw_start_8102e_2(struct rtl8169_private *tp)
+{
+	rtl_set_def_aspm_entry_latency(tp);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	RTL_W8(tp, Config1, MEMMAP | IOMAP | VPD | PMEnable);
+	RTL_W8(tp, Config3, RTL_R8(tp, Config3) & ~Beacon_en);
+}
+
+static void rtl_hw_start_8102e_3(struct rtl8169_private *tp)
+{
+	rtl_hw_start_8102e_2(tp);
+
+	rtl_ephy_write(tp, 0x03, 0xc2f9);
+}
+
+static void rtl_hw_start_8105e_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8105e_1[] = {
+		{ 0x07,	0, 0x4000 },
+		{ 0x19,	0, 0x0200 },
+		{ 0x19,	0, 0x0020 },
+		{ 0x1e,	0, 0x2000 },
+		{ 0x03,	0, 0x0001 },
+		{ 0x19,	0, 0x0100 },
+		{ 0x19,	0, 0x0004 },
+		{ 0x0a,	0, 0x0020 }
+	};
+
+	/* Force LAN exit from ASPM if Rx/Tx are not idle */
+	RTL_W32(tp, FuncEvent, RTL_R32(tp, FuncEvent) | 0x002800);
+
+	/* Disable Early Tally Counter */
+	RTL_W32(tp, FuncEvent, RTL_R32(tp, FuncEvent) & ~0x010000);
+
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) | EN_NDP | EN_OOB_RESET);
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) | PFM_EN);
+
+	rtl_ephy_init(tp, e_info_8105e_1);
+
+	rtl_pcie_state_l2l3_disable(tp);
+}
+
+static void rtl_hw_start_8105e_2(struct rtl8169_private *tp)
+{
+	rtl_hw_start_8105e_1(tp);
+	rtl_ephy_write(tp, 0x1e, rtl_ephy_read(tp, 0x1e) | 0x8000);
+}
+
+static void rtl_hw_start_8402(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8402[] = {
+		{ 0x19,	0xffff, 0xff64 },
+		{ 0x1e,	0, 0x4000 }
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	/* Force LAN exit from ASPM if Rx/Tx are not idle */
+	RTL_W32(tp, FuncEvent, RTL_R32(tp, FuncEvent) | 0x002800);
+
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) & ~NOW_IS_OOB);
+
+	rtl_ephy_init(tp, e_info_8402);
+
+	rtl_tx_performance_tweak(tp, PCI_EXP_DEVCTL_READRQ_4096B);
+
+	rtl_set_fifo_size(tp, 0x00, 0x00, 0x02, 0x06);
+	rtl_reset_packet_filter(tp);
+	rtl_eri_write(tp, 0xc0, ERIAR_MASK_0011, 0x0000);
+	rtl_eri_write(tp, 0xb8, ERIAR_MASK_0011, 0x0000);
+	rtl_w0w1_eri(tp, 0x0d4, ERIAR_MASK_0011, 0x0e00, 0xff00);
+
+	rtl_pcie_state_l2l3_disable(tp);
+}
+
+static void rtl_hw_start_8106(struct rtl8169_private *tp)
+{
+	rtl_hw_aspm_clkreq_enable(tp, false);
+
+	/* Force LAN exit from ASPM if Rx/Tx are not idle */
+	RTL_W32(tp, FuncEvent, RTL_R32(tp, FuncEvent) | 0x002800);
+
+	RTL_W32(tp, MISC, (RTL_R32(tp, MISC) | DISABLE_LAN_EN) & ~EARLY_TALLY_EN);
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) | EN_NDP | EN_OOB_RESET);
+	RTL_W8(tp, DLLPR, RTL_R8(tp, DLLPR) & ~PFM_EN);
+
+	rtl_pcie_state_l2l3_disable(tp);
+	rtl_hw_aspm_clkreq_enable(tp, true);
+}
+
+DECLARE_RTL_COND(rtl_mac_ocp_e00e_cond)
+{
+	return r8168_mac_ocp_read(tp, 0xe00e) & BIT(13);
+}
+
+static void rtl_hw_start_8125_common(struct rtl8169_private *tp)
+{
+	rtl_pcie_state_l2l3_disable(tp);
+
+	RTL_W16(tp, 0x382, 0x221b);
+	RTL_W8(tp, 0x4500, 0);
+	RTL_W16(tp, 0x4800, 0);
+
+	/* disable UPS */
+	r8168_mac_ocp_modify(tp, 0xd40a, 0x0010, 0x0000);
+
+	RTL_W8(tp, Config1, RTL_R8(tp, Config1) & ~0x10);
+
+	r8168_mac_ocp_write(tp, 0xc140, 0xffff);
+	r8168_mac_ocp_write(tp, 0xc142, 0xffff);
+
+	r8168_mac_ocp_modify(tp, 0xd3e2, 0x0fff, 0x03a9);
+	r8168_mac_ocp_modify(tp, 0xd3e4, 0x00ff, 0x0000);
+	r8168_mac_ocp_modify(tp, 0xe860, 0x0000, 0x0080);
+
+	/* disable new tx descriptor format */
+	r8168_mac_ocp_modify(tp, 0xeb58, 0x0001, 0x0000);
+
+	r8168_mac_ocp_modify(tp, 0xe614, 0x0700, 0x0400);
+	r8168_mac_ocp_modify(tp, 0xe63e, 0x0c30, 0x0020);
+	r8168_mac_ocp_modify(tp, 0xc0b4, 0x0000, 0x000c);
+	r8168_mac_ocp_modify(tp, 0xeb6a, 0x00ff, 0x0033);
+	r8168_mac_ocp_modify(tp, 0xeb50, 0x03e0, 0x0040);
+	r8168_mac_ocp_modify(tp, 0xe056, 0x00f0, 0x0030);
+	r8168_mac_ocp_modify(tp, 0xe040, 0x1000, 0x0000);
+	r8168_mac_ocp_modify(tp, 0xe0c0, 0x4f0f, 0x4403);
+	r8168_mac_ocp_modify(tp, 0xe052, 0x0080, 0x0067);
+	r8168_mac_ocp_modify(tp, 0xc0ac, 0x0080, 0x1f00);
+	r8168_mac_ocp_modify(tp, 0xd430, 0x0fff, 0x047f);
+	r8168_mac_ocp_modify(tp, 0xe84c, 0x0000, 0x00c0);
+	r8168_mac_ocp_modify(tp, 0xea1c, 0x0004, 0x0000);
+	r8168_mac_ocp_modify(tp, 0xeb54, 0x0000, 0x0001);
+	udelay(1);
+	r8168_mac_ocp_modify(tp, 0xeb54, 0x0001, 0x0000);
+	RTL_W16(tp, 0x1880, RTL_R16(tp, 0x1880) & ~0x0030);
+
+	r8168_mac_ocp_write(tp, 0xe098, 0xc302);
+
+	rtl_udelay_loop_wait_low(tp, &rtl_mac_ocp_e00e_cond, 1000, 10);
+
+	rtl8125_config_eee_mac(tp);
+
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) & ~RXDV_GATED_EN);
+	udelay(10);
+}
+
+static void rtl_hw_start_8125_1(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8125_1[] = {
+		{ 0x01, 0xffff, 0xa812 },
+		{ 0x09, 0xffff, 0x520c },
+		{ 0x04, 0xffff, 0xd000 },
+		{ 0x0d, 0xffff, 0xf702 },
+		{ 0x0a, 0xffff, 0x8653 },
+		{ 0x06, 0xffff, 0x001e },
+		{ 0x08, 0xffff, 0x3595 },
+		{ 0x20, 0xffff, 0x9455 },
+		{ 0x21, 0xffff, 0x99ff },
+		{ 0x02, 0xffff, 0x6046 },
+		{ 0x29, 0xffff, 0xfe00 },
+		{ 0x23, 0xffff, 0xab62 },
+
+		{ 0x41, 0xffff, 0xa80c },
+		{ 0x49, 0xffff, 0x520c },
+		{ 0x44, 0xffff, 0xd000 },
+		{ 0x4d, 0xffff, 0xf702 },
+		{ 0x4a, 0xffff, 0x8653 },
+		{ 0x46, 0xffff, 0x001e },
+		{ 0x48, 0xffff, 0x3595 },
+		{ 0x60, 0xffff, 0x9455 },
+		{ 0x61, 0xffff, 0x99ff },
+		{ 0x42, 0xffff, 0x6046 },
+		{ 0x69, 0xffff, 0xfe00 },
+		{ 0x63, 0xffff, 0xab62 },
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8125_1);
+
+	rtl_hw_start_8125_common(tp);
+}
+
+static void rtl_hw_start_8125_2(struct rtl8169_private *tp)
+{
+	static const struct ephy_info e_info_8125_2[] = {
+		{ 0x04, 0xffff, 0xd000 },
+		{ 0x0a, 0xffff, 0x8653 },
+		{ 0x23, 0xffff, 0xab66 },
+		{ 0x20, 0xffff, 0x9455 },
+		{ 0x21, 0xffff, 0x99ff },
+		{ 0x29, 0xffff, 0xfe04 },
+
+		{ 0x44, 0xffff, 0xd000 },
+		{ 0x4a, 0xffff, 0x8653 },
+		{ 0x63, 0xffff, 0xab66 },
+		{ 0x60, 0xffff, 0x9455 },
+		{ 0x61, 0xffff, 0x99ff },
+		{ 0x69, 0xffff, 0xfe04 },
+	};
+
+	rtl_set_def_aspm_entry_latency(tp);
+
+	/* disable aspm and clock request before access ephy */
+	rtl_hw_aspm_clkreq_enable(tp, false);
+	rtl_ephy_init(tp, e_info_8125_2);
+
+	rtl_hw_start_8125_common(tp);
+}
+
+static void rtl_hw_config(struct rtl8169_private *tp)
+{
+	static const rtl_generic_fct hw_configs[] = {
+		[RTL_GIGA_MAC_VER_07] = rtl_hw_start_8102e_1,
+		[RTL_GIGA_MAC_VER_08] = rtl_hw_start_8102e_3,
+		[RTL_GIGA_MAC_VER_09] = rtl_hw_start_8102e_2,
+		[RTL_GIGA_MAC_VER_10] = NULL,
+		[RTL_GIGA_MAC_VER_11] = rtl_hw_start_8168bb,
+		[RTL_GIGA_MAC_VER_12] = rtl_hw_start_8168bef,
+		[RTL_GIGA_MAC_VER_13] = NULL,
+		[RTL_GIGA_MAC_VER_14] = NULL,
+		[RTL_GIGA_MAC_VER_15] = NULL,
+		[RTL_GIGA_MAC_VER_16] = NULL,
+		[RTL_GIGA_MAC_VER_17] = rtl_hw_start_8168bef,
+		[RTL_GIGA_MAC_VER_18] = rtl_hw_start_8168cp_1,
+		[RTL_GIGA_MAC_VER_19] = rtl_hw_start_8168c_1,
+		[RTL_GIGA_MAC_VER_20] = rtl_hw_start_8168c_2,
+		[RTL_GIGA_MAC_VER_21] = rtl_hw_start_8168c_3,
+		[RTL_GIGA_MAC_VER_22] = rtl_hw_start_8168c_4,
+		[RTL_GIGA_MAC_VER_23] = rtl_hw_start_8168cp_2,
+		[RTL_GIGA_MAC_VER_24] = rtl_hw_start_8168cp_3,
+		[RTL_GIGA_MAC_VER_25] = rtl_hw_start_8168d,
+		[RTL_GIGA_MAC_VER_26] = rtl_hw_start_8168d,
+		[RTL_GIGA_MAC_VER_27] = rtl_hw_start_8168d,
+		[RTL_GIGA_MAC_VER_28] = rtl_hw_start_8168d_4,
+		[RTL_GIGA_MAC_VER_29] = rtl_hw_start_8105e_1,
+		[RTL_GIGA_MAC_VER_30] = rtl_hw_start_8105e_2,
+		[RTL_GIGA_MAC_VER_31] = rtl_hw_start_8168dp,
+		[RTL_GIGA_MAC_VER_32] = rtl_hw_start_8168e_1,
+		[RTL_GIGA_MAC_VER_33] = rtl_hw_start_8168e_1,
+		[RTL_GIGA_MAC_VER_34] = rtl_hw_start_8168e_2,
+		[RTL_GIGA_MAC_VER_35] = rtl_hw_start_8168f_1,
+		[RTL_GIGA_MAC_VER_36] = rtl_hw_start_8168f_1,
+		[RTL_GIGA_MAC_VER_37] = rtl_hw_start_8402,
+		[RTL_GIGA_MAC_VER_38] = rtl_hw_start_8411,
+		[RTL_GIGA_MAC_VER_39] = rtl_hw_start_8106,
+		[RTL_GIGA_MAC_VER_40] = rtl_hw_start_8168g_1,
+		[RTL_GIGA_MAC_VER_41] = rtl_hw_start_8168g_1,
+		[RTL_GIGA_MAC_VER_42] = rtl_hw_start_8168g_2,
+		[RTL_GIGA_MAC_VER_43] = rtl_hw_start_8168g_2,
+		[RTL_GIGA_MAC_VER_44] = rtl_hw_start_8411_2,
+		[RTL_GIGA_MAC_VER_45] = rtl_hw_start_8168h_1,
+		[RTL_GIGA_MAC_VER_46] = rtl_hw_start_8168h_1,
+		[RTL_GIGA_MAC_VER_47] = rtl_hw_start_8168h_1,
+		[RTL_GIGA_MAC_VER_48] = rtl_hw_start_8168h_1,
+		[RTL_GIGA_MAC_VER_49] = rtl_hw_start_8168ep_1,
+		[RTL_GIGA_MAC_VER_50] = rtl_hw_start_8168ep_2,
+		[RTL_GIGA_MAC_VER_51] = rtl_hw_start_8168ep_3,
+		[RTL_GIGA_MAC_VER_60] = rtl_hw_start_8125_1,
+		[RTL_GIGA_MAC_VER_61] = rtl_hw_start_8125_2,
+	};
+
+	if (hw_configs[tp->mac_version])
+		hw_configs[tp->mac_version](tp);
+}
+
+static void rtl_hw_start_8125(struct rtl8169_private *tp)
+{
+	int i;
+
+	/* disable interrupt coalescing */
+	for (i = 0xa00; i < 0xb00; i += 4)
+		RTL_W32(tp, i, 0);
+
+	rtl_hw_config(tp);
+}
+
+static void rtl_hw_start_8168(struct rtl8169_private *tp)
+{
+	if (tp->mac_version == RTL_GIGA_MAC_VER_13 ||
+	    tp->mac_version == RTL_GIGA_MAC_VER_16)
+		pcie_capability_set_word(tp->pci_dev, PCI_EXP_DEVCTL,
+					 PCI_EXP_DEVCTL_NOSNOOP_EN);
+
+	if (rtl_is_8168evl_up(tp))
+		RTL_W8(tp, MaxTxPacketSize, EarlySize);
+	else
+		RTL_W8(tp, MaxTxPacketSize, TxPacketMax);
+
+	rtl_hw_config(tp);
+
+	/* disable interrupt coalescing */
+	RTL_W16(tp, IntrMitigate, 0x0000);
+}
+
+static void rtl_hw_start_8169(struct rtl8169_private *tp)
+{
+	if (tp->mac_version == RTL_GIGA_MAC_VER_05)
+		pci_write_config_byte(tp->pci_dev, PCI_CACHE_LINE_SIZE, 0x08);
+
+	RTL_W8(tp, EarlyTxThres, NoEarlyTx);
+
+	tp->cp_cmd |= PCIMulRW;
+
+	if (tp->mac_version == RTL_GIGA_MAC_VER_02 ||
+	    tp->mac_version == RTL_GIGA_MAC_VER_03) {
+		netif_dbg(tp, drv, tp->dev,
+			  "Set MAC Reg C+CR Offset 0xe0. Bit 3 and Bit 14 MUST be 1\n");
+		tp->cp_cmd |= (1 << 14);
+	}
+
+	RTL_W16(tp, CPlusCmd, tp->cp_cmd);
+
+	rtl8169_set_magic_reg(tp, tp->mac_version);
+
+	RTL_W32(tp, RxMissed, 0);
+
+	/* disable interrupt coalescing */
+	RTL_W16(tp, IntrMitigate, 0x0000);
+}
+
+static void rtl_hw_start(struct  rtl8169_private *tp)
+{
+	rtl_unlock_config_regs(tp);
+
+	tp->cp_cmd &= CPCMD_MASK;
+	RTL_W16(tp, CPlusCmd, tp->cp_cmd);
+
+	if (tp->mac_version <= RTL_GIGA_MAC_VER_06)
+		rtl_hw_start_8169(tp);
+	else if (rtl_is_8125(tp))
+		rtl_hw_start_8125(tp);
+	else
+		rtl_hw_start_8168(tp);
+
+	rtl_set_rx_max_size(tp);
+	rtl_set_rx_tx_desc_registers(tp);
+	rtl_lock_config_regs(tp);
+
+	rtl_jumbo_config(tp, tp->dev->mtu);
+
+	/* Initially a 10 us delay. Turned it into a PCI commit. - FR */
+	RTL_R16(tp, CPlusCmd);
+	RTL_W8(tp, ChipCmd, CmdTxEnb | CmdRxEnb);
+	rtl_init_rxcfg(tp);
+	rtl_set_tx_config_registers(tp);
+	rtl_set_rx_mode(tp->dev);
+	rtl_irq_enable(tp);
+}
+
+static int rtl8169_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl_jumbo_config(tp, new_mtu);
+
+	dev->mtu = new_mtu;
+	netdev_update_features(dev);
+
+	return 0;
+}
+
+static inline void rtl8169_make_unusable_by_asic(struct RxDesc *desc)
+{
+	desc->addr = cpu_to_le64(0x0badbadbadbadbadull);
+	desc->opts1 &= ~cpu_to_le32(DescOwn | RsvdMask);
+}
+
+static inline void rtl8169_mark_to_asic(struct RxDesc *desc)
+{
+	u32 eor = le32_to_cpu(desc->opts1) & RingEnd;
+
+	/* Force memory writes to complete before releasing descriptor */
+	dma_wmb();
+
+	desc->opts1 = cpu_to_le32(DescOwn | eor | R8169_RX_BUF_SIZE);
+}
+
+static struct page *rtl8169_alloc_rx_data(struct rtl8169_private *tp,
+					  struct RxDesc *desc)
+{
+	struct device *d = tp_to_dev(tp);
+	int node = dev_to_node(d);
+	dma_addr_t mapping;
+	struct page *data;
+
+	data = alloc_pages_node(node, GFP_KERNEL, get_order(R8169_RX_BUF_SIZE));
+	if (!data)
+		return NULL;
+
+	mapping = dma_map_page(d, data, 0, R8169_RX_BUF_SIZE, DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(d, mapping))) {
+		if (net_ratelimit())
+			netif_err(tp, drv, tp->dev, "Failed to map RX DMA!\n");
+		__free_pages(data, get_order(R8169_RX_BUF_SIZE));
+		return NULL;
+	}
+
+	desc->addr = cpu_to_le64(mapping);
+	rtl8169_mark_to_asic(desc);
+
+	return data;
+}
+
+static void rtl8169_rx_clear(struct rtl8169_private *tp)
+{
+	unsigned int i;
+
+	for (i = 0; i < NUM_RX_DESC && tp->Rx_databuff[i]; i++) {
+		dma_unmap_page(tp_to_dev(tp),
+			       le64_to_cpu(tp->RxDescArray[i].addr),
+			       R8169_RX_BUF_SIZE, DMA_FROM_DEVICE);
+		__free_pages(tp->Rx_databuff[i], get_order(R8169_RX_BUF_SIZE));
+		tp->Rx_databuff[i] = NULL;
+		rtl8169_make_unusable_by_asic(tp->RxDescArray + i);
+	}
+}
+
+static inline void rtl8169_mark_as_last_descriptor(struct RxDesc *desc)
+{
+	desc->opts1 |= cpu_to_le32(RingEnd);
+}
+
+static int rtl8169_rx_fill(struct rtl8169_private *tp)
+{
+	unsigned int i;
+
+	for (i = 0; i < NUM_RX_DESC; i++) {
+		struct page *data;
+
+		data = rtl8169_alloc_rx_data(tp, tp->RxDescArray + i);
+		if (!data) {
+			rtl8169_make_unusable_by_asic(tp->RxDescArray + i);
+			goto err_out;
+		}
+		tp->Rx_databuff[i] = data;
+	}
+
+	rtl8169_mark_as_last_descriptor(tp->RxDescArray + NUM_RX_DESC - 1);
+	return 0;
+
+err_out:
+	rtl8169_rx_clear(tp);
+	return -ENOMEM;
+}
+
+static int rtl8169_init_ring(struct rtl8169_private *tp)
+{
+	rtl8169_init_ring_indexes(tp);
+
+	memset(tp->tx_skb, 0, sizeof(tp->tx_skb));
+	memset(tp->Rx_databuff, 0, sizeof(tp->Rx_databuff));
+
+	return rtl8169_rx_fill(tp);
+}
+
+static void rtl8169_unmap_tx_skb(struct device *d, struct ring_info *tx_skb,
+				 struct TxDesc *desc)
+{
+	unsigned int len = tx_skb->len;
+
+	dma_unmap_single(d, le64_to_cpu(desc->addr), len, DMA_TO_DEVICE);
+
+	desc->opts1 = 0x00;
+	desc->opts2 = 0x00;
+	desc->addr = 0x00;
+	tx_skb->len = 0;
+}
+
+static void rtl8169_tx_clear_range(struct rtl8169_private *tp, u32 start,
+				   unsigned int n)
+{
+	unsigned int i;
+
+	for (i = 0; i < n; i++) {
+		unsigned int entry = (start + i) % NUM_TX_DESC;
+		struct ring_info *tx_skb = tp->tx_skb + entry;
+		unsigned int len = tx_skb->len;
+
+		if (len) {
+			struct sk_buff *skb = tx_skb->skb;
+
+			rtl8169_unmap_tx_skb(tp_to_dev(tp), tx_skb,
+					     tp->TxDescArray + entry);
+			if (skb) {
+				dev_consume_skb_any(skb);
+				tx_skb->skb = NULL;
+			}
+		}
+	}
+}
+
+static void rtl8169_tx_clear(struct rtl8169_private *tp)
+{
+	rtl8169_tx_clear_range(tp, tp->dirty_tx, NUM_TX_DESC);
+	tp->cur_tx = tp->dirty_tx = 0;
+	netdev_reset_queue(tp->dev);
+}
+
+static void rtl_reset_work(struct rtl8169_private *tp)
+{
+	struct net_device *dev = tp->dev;
+	int i;
+
+	napi_disable(&tp->napi);
+	netif_stop_queue(dev);
+	synchronize_rcu();
+
+	rtl8169_hw_reset(tp);
+
+	for (i = 0; i < NUM_RX_DESC; i++)
+		rtl8169_mark_to_asic(tp->RxDescArray + i);
+
+	rtl8169_tx_clear(tp);
+	rtl8169_init_ring_indexes(tp);
+
+	napi_enable(&tp->napi);
+	rtl_hw_start(tp);
+	netif_wake_queue(dev);
+}
+
+static void rtl8169_tx_timeout(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl_schedule_task(tp, RTL_FLAG_TASK_RESET_PENDING);
+}
+
+static __le32 rtl8169_get_txd_opts1(u32 opts0, u32 len, unsigned int entry)
+{
+	u32 status = opts0 | len;
+
+	if (entry == NUM_TX_DESC - 1)
+		status |= RingEnd;
+
+	return cpu_to_le32(status);
+}
+
+static int rtl8169_xmit_frags(struct rtl8169_private *tp, struct sk_buff *skb,
+			      u32 *opts)
+{
+	struct skb_shared_info *info = skb_shinfo(skb);
+	unsigned int cur_frag, entry;
+	struct TxDesc *uninitialized_var(txd);
+	struct device *d = tp_to_dev(tp);
+
+	entry = tp->cur_tx;
+	for (cur_frag = 0; cur_frag < info->nr_frags; cur_frag++) {
+		const skb_frag_t *frag = info->frags + cur_frag;
+		dma_addr_t mapping;
+		u32 len;
+		void *addr;
+
+		entry = (entry + 1) % NUM_TX_DESC;
+
+		txd = tp->TxDescArray + entry;
+		len = skb_frag_size(frag);
+		addr = skb_frag_address(frag);
+		mapping = dma_map_single(d, addr, len, DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(d, mapping))) {
+			if (net_ratelimit())
+				netif_err(tp, drv, tp->dev,
+					  "Failed to map TX fragments DMA!\n");
+			goto err_out;
+		}
+
+		txd->opts1 = rtl8169_get_txd_opts1(opts[0], len, entry);
+		txd->opts2 = cpu_to_le32(opts[1]);
+		txd->addr = cpu_to_le64(mapping);
+
+		tp->tx_skb[entry].len = len;
+	}
+
+	if (cur_frag) {
+		tp->tx_skb[entry].skb = skb;
+		txd->opts1 |= cpu_to_le32(LastFrag);
+	}
+
+	return cur_frag;
+
+err_out:
+	rtl8169_tx_clear_range(tp, tp->cur_tx + 1, cur_frag);
+	return -EIO;
+}
+
+static bool rtl_test_hw_pad_bug(struct rtl8169_private *tp, struct sk_buff *skb)
+{
+	return skb->len < ETH_ZLEN && tp->mac_version == RTL_GIGA_MAC_VER_34;
+}
+
+/* msdn_giant_send_check()
+ * According to the document of microsoft, the TCP Pseudo Header excludes the
+ * packet length for IPv6 TCP large packets.
+ */
+static int msdn_giant_send_check(struct sk_buff *skb)
+{
+	const struct ipv6hdr *ipv6h;
+	struct tcphdr *th;
+	int ret;
+
+	ret = skb_cow_head(skb, 0);
+	if (ret)
+		return ret;
+
+	ipv6h = ipv6_hdr(skb);
+	th = tcp_hdr(skb);
+
+	th->check = 0;
+	th->check = ~tcp_v6_check(0, &ipv6h->saddr, &ipv6h->daddr, 0);
+
+	return ret;
+}
+
+static void rtl8169_tso_csum_v1(struct sk_buff *skb, u32 *opts)
+{
+	u32 mss = skb_shinfo(skb)->gso_size;
+
+	if (mss) {
+		opts[0] |= TD_LSO;
+		opts[0] |= min(mss, TD_MSS_MAX) << TD0_MSS_SHIFT;
+	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		const struct iphdr *ip = ip_hdr(skb);
+
+		if (ip->protocol == IPPROTO_TCP)
+			opts[0] |= TD0_IP_CS | TD0_TCP_CS;
+		else if (ip->protocol == IPPROTO_UDP)
+			opts[0] |= TD0_IP_CS | TD0_UDP_CS;
+		else
+			WARN_ON_ONCE(1);
+	}
+}
+
+static bool rtl8169_tso_csum_v2(struct rtl8169_private *tp,
+				struct sk_buff *skb, u32 *opts)
+{
+	u32 transport_offset = (u32)skb_transport_offset(skb);
+	u32 mss = skb_shinfo(skb)->gso_size;
+
+	if (mss) {
+		switch (vlan_get_protocol(skb)) {
+		case htons(ETH_P_IP):
+			opts[0] |= TD1_GTSENV4;
+			break;
+
+		case htons(ETH_P_IPV6):
+			if (msdn_giant_send_check(skb))
+				return false;
+
+			opts[0] |= TD1_GTSENV6;
+			break;
+
+		default:
+			WARN_ON_ONCE(1);
+			break;
+		}
+
+		opts[0] |= transport_offset << GTTCPHO_SHIFT;
+		opts[1] |= min(mss, TD_MSS_MAX) << TD1_MSS_SHIFT;
+	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		u8 ip_protocol;
+
+		switch (vlan_get_protocol(skb)) {
+		case htons(ETH_P_IP):
+			opts[1] |= TD1_IPv4_CS;
+			ip_protocol = ip_hdr(skb)->protocol;
+			break;
+
+		case htons(ETH_P_IPV6):
+			opts[1] |= TD1_IPv6_CS;
+			ip_protocol = ipv6_hdr(skb)->nexthdr;
+			break;
+
+		default:
+			ip_protocol = IPPROTO_RAW;
+			break;
+		}
+
+		if (ip_protocol == IPPROTO_TCP)
+			opts[1] |= TD1_TCP_CS;
+		else if (ip_protocol == IPPROTO_UDP)
+			opts[1] |= TD1_UDP_CS;
+		else
+			WARN_ON_ONCE(1);
+
+		opts[1] |= transport_offset << TCPHO_SHIFT;
+	} else {
+		if (unlikely(rtl_test_hw_pad_bug(tp, skb)))
+			return !eth_skb_pad(skb);
+	}
+
+	return true;
+}
+
+static bool rtl_tx_slots_avail(struct rtl8169_private *tp,
+			       unsigned int nr_frags)
+{
+	unsigned int slots_avail = tp->dirty_tx + NUM_TX_DESC - tp->cur_tx;
+
+	/* A skbuff with nr_frags needs nr_frags+1 entries in the tx queue */
+	return slots_avail > nr_frags;
+}
+
+/* Versions RTL8102e and from RTL8168c onwards support csum_v2 */
+static bool rtl_chip_supports_csum_v2(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_02 ... RTL_GIGA_MAC_VER_06:
+	case RTL_GIGA_MAC_VER_10 ... RTL_GIGA_MAC_VER_17:
+		return false;
+	default:
+		return true;
+	}
+}
+
+static void rtl8169_doorbell(struct rtl8169_private *tp)
+{
+	if (rtl_is_8125(tp))
+		RTL_W16(tp, TxPoll_8125, BIT(0));
+	else
+		RTL_W8(tp, TxPoll, NPQ);
+}
+
+static netdev_tx_t rtl8169_start_xmit(struct sk_buff *skb,
+				      struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	unsigned int entry = tp->cur_tx % NUM_TX_DESC;
+	struct TxDesc *txd = tp->TxDescArray + entry;
+	struct device *d = tp_to_dev(tp);
+	dma_addr_t mapping;
+	u32 opts[2], len;
+	bool stop_queue;
+	bool door_bell;
+	int frags;
+
+	if (unlikely(!rtl_tx_slots_avail(tp, skb_shinfo(skb)->nr_frags))) {
+		netif_err(tp, drv, dev, "BUG! Tx Ring full when queue awake!\n");
+		goto err_stop_0;
+	}
+
+	if (unlikely(le32_to_cpu(txd->opts1) & DescOwn))
+		goto err_stop_0;
+
+	opts[1] = rtl8169_tx_vlan_tag(skb);
+	opts[0] = DescOwn;
+
+	if (rtl_chip_supports_csum_v2(tp)) {
+		if (!rtl8169_tso_csum_v2(tp, skb, opts))
+			goto err_dma_0;
+	} else {
+		rtl8169_tso_csum_v1(skb, opts);
+	}
+
+	len = skb_headlen(skb);
+	mapping = dma_map_single(d, skb->data, len, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(d, mapping))) {
+		if (net_ratelimit())
+			netif_err(tp, drv, dev, "Failed to map TX DMA!\n");
+		goto err_dma_0;
+	}
+
+	tp->tx_skb[entry].len = len;
+	txd->addr = cpu_to_le64(mapping);
+
+	frags = rtl8169_xmit_frags(tp, skb, opts);
+	if (frags < 0)
+		goto err_dma_1;
+	else if (frags)
+		opts[0] |= FirstFrag;
+	else {
+		opts[0] |= FirstFrag | LastFrag;
+		tp->tx_skb[entry].skb = skb;
+	}
+
+	txd->opts2 = cpu_to_le32(opts[1]);
+
+	skb_tx_timestamp(skb);
+
+	/* Force memory writes to complete before releasing descriptor */
+	dma_wmb();
+
+	door_bell = __netdev_sent_queue(dev, skb->len, netdev_xmit_more());
+
+	txd->opts1 = rtl8169_get_txd_opts1(opts[0], len, entry);
+
+	/* Force all memory writes to complete before notifying device */
+	wmb();
+
+	tp->cur_tx += frags + 1;
+
+	stop_queue = !rtl_tx_slots_avail(tp, MAX_SKB_FRAGS);
+	if (unlikely(stop_queue)) {
+		/* Avoid wrongly optimistic queue wake-up: rtl_tx thread must
+		 * not miss a ring update when it notices a stopped queue.
+		 */
+		smp_wmb();
+		netif_stop_queue(dev);
+		door_bell = true;
+	}
+
+	if (door_bell)
+		rtl8169_doorbell(tp);
+
+	if (unlikely(stop_queue)) {
+		/* Sync with rtl_tx:
+		 * - publish queue status and cur_tx ring index (write barrier)
+		 * - refresh dirty_tx ring index (read barrier).
+		 * May the current thread have a pessimistic view of the ring
+		 * status and forget to wake up queue, a racing rtl_tx thread
+		 * can't.
+		 */
+		smp_mb();
+		if (rtl_tx_slots_avail(tp, MAX_SKB_FRAGS))
+			netif_start_queue(dev);
+	}
+
+	return NETDEV_TX_OK;
+
+err_dma_1:
+	rtl8169_unmap_tx_skb(d, tp->tx_skb + entry, txd);
+err_dma_0:
+	dev_kfree_skb_any(skb);
+	dev->stats.tx_dropped++;
+	return NETDEV_TX_OK;
+
+err_stop_0:
+	netif_stop_queue(dev);
+	dev->stats.tx_dropped++;
+	return NETDEV_TX_BUSY;
+}
+
+static netdev_features_t rtl8169_features_check(struct sk_buff *skb,
+						struct net_device *dev,
+						netdev_features_t features)
+{
+	int transport_offset = skb_transport_offset(skb);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (skb_is_gso(skb)) {
+		if (transport_offset > GTTCPHO_MAX &&
+		    rtl_chip_supports_csum_v2(tp))
+			features &= ~NETIF_F_ALL_TSO;
+	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		if (skb->len < ETH_ZLEN) {
+			switch (tp->mac_version) {
+			case RTL_GIGA_MAC_VER_11:
+			case RTL_GIGA_MAC_VER_12:
+			case RTL_GIGA_MAC_VER_17:
+			case RTL_GIGA_MAC_VER_34:
+				features &= ~NETIF_F_CSUM_MASK;
+				break;
+			default:
+				break;
+			}
+		}
+
+		if (transport_offset > TCPHO_MAX &&
+		    rtl_chip_supports_csum_v2(tp))
+			features &= ~NETIF_F_CSUM_MASK;
+	}
+
+	return vlan_features_check(skb, features);
+}
+
+static void rtl8169_pcierr_interrupt(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct pci_dev *pdev = tp->pci_dev;
+	u16 pci_status, pci_cmd;
+
+	pci_read_config_word(pdev, PCI_COMMAND, &pci_cmd);
+	pci_read_config_word(pdev, PCI_STATUS, &pci_status);
+
+	netif_err(tp, intr, dev, "PCI error (cmd = 0x%04x, status = 0x%04x)\n",
+		  pci_cmd, pci_status);
+
+	/*
+	 * The recovery sequence below admits a very elaborated explanation:
+	 * - it seems to work;
+	 * - I did not see what else could be done;
+	 * - it makes iop3xx happy.
+	 *
+	 * Feel free to adjust to your needs.
+	 */
+	if (pdev->broken_parity_status)
+		pci_cmd &= ~PCI_COMMAND_PARITY;
+	else
+		pci_cmd |= PCI_COMMAND_SERR | PCI_COMMAND_PARITY;
+
+	pci_write_config_word(pdev, PCI_COMMAND, pci_cmd);
+
+	pci_write_config_word(pdev, PCI_STATUS,
+		pci_status & (PCI_STATUS_DETECTED_PARITY |
+		PCI_STATUS_SIG_SYSTEM_ERROR | PCI_STATUS_REC_MASTER_ABORT |
+		PCI_STATUS_REC_TARGET_ABORT | PCI_STATUS_SIG_TARGET_ABORT));
+
+	rtl_schedule_task(tp, RTL_FLAG_TASK_RESET_PENDING);
+}
+
+static void rtl_tx(struct net_device *dev, struct rtl8169_private *tp,
+		   int budget)
+{
+	unsigned int dirty_tx, tx_left, bytes_compl = 0, pkts_compl = 0;
+
+	dirty_tx = tp->dirty_tx;
+	smp_rmb();
+	tx_left = tp->cur_tx - dirty_tx;
+
+	while (tx_left > 0) {
+		unsigned int entry = dirty_tx % NUM_TX_DESC;
+		struct ring_info *tx_skb = tp->tx_skb + entry;
+		u32 status;
+
+		status = le32_to_cpu(tp->TxDescArray[entry].opts1);
+		if (status & DescOwn)
+			break;
+
+		/* This barrier is needed to keep us from reading
+		 * any other fields out of the Tx descriptor until
+		 * we know the status of DescOwn
+		 */
+		dma_rmb();
+
+		rtl8169_unmap_tx_skb(tp_to_dev(tp), tx_skb,
+				     tp->TxDescArray + entry);
+		if (tx_skb->skb) {
+			pkts_compl++;
+			bytes_compl += tx_skb->skb->len;
+			napi_consume_skb(tx_skb->skb, budget);
+			tx_skb->skb = NULL;
+		}
+		dirty_tx++;
+		tx_left--;
+	}
+
+	if (tp->dirty_tx != dirty_tx) {
+		netdev_completed_queue(dev, pkts_compl, bytes_compl);
+
+		u64_stats_update_begin(&tp->tx_stats.syncp);
+		tp->tx_stats.packets += pkts_compl;
+		tp->tx_stats.bytes += bytes_compl;
+		u64_stats_update_end(&tp->tx_stats.syncp);
+
+		tp->dirty_tx = dirty_tx;
+		/* Sync with rtl8169_start_xmit:
+		 * - publish dirty_tx ring index (write barrier)
+		 * - refresh cur_tx ring index and queue status (read barrier)
+		 * May the current thread miss the stopped queue condition,
+		 * a racing xmit thread can only have a right view of the
+		 * ring status.
+		 */
+		smp_mb();
+		if (netif_queue_stopped(dev) &&
+		    rtl_tx_slots_avail(tp, MAX_SKB_FRAGS)) {
+			netif_wake_queue(dev);
+		}
+		/*
+		 * 8168 hack: TxPoll requests are lost when the Tx packets are
+		 * too close. Let's kick an extra TxPoll request when a burst
+		 * of start_xmit activity is detected (if it is not detected,
+		 * it is slow enough). -- FR
+		 */
+		if (tp->cur_tx != dirty_tx)
+			rtl8169_doorbell(tp);
+	}
+}
+
+static inline int rtl8169_fragmented_frame(u32 status)
+{
+	return (status & (FirstFrag | LastFrag)) != (FirstFrag | LastFrag);
+}
+
+static inline void rtl8169_rx_csum(struct sk_buff *skb, u32 opts1)
+{
+	u32 status = opts1 & RxProtoMask;
+
+	if (((status == RxProtoTCP) && !(opts1 & TCPFail)) ||
+	    ((status == RxProtoUDP) && !(opts1 & UDPFail)))
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	else
+		skb_checksum_none_assert(skb);
+}
+
+static int rtl_rx(struct net_device *dev, struct rtl8169_private *tp, u32 budget)
+{
+	unsigned int cur_rx, rx_left;
+	unsigned int count;
+
+	cur_rx = tp->cur_rx;
+
+	for (rx_left = min(budget, NUM_RX_DESC); rx_left > 0; rx_left--, cur_rx++) {
+		unsigned int entry = cur_rx % NUM_RX_DESC;
+		const void *rx_buf = page_address(tp->Rx_databuff[entry]);
+		struct RxDesc *desc = tp->RxDescArray + entry;
+		u32 status;
+
+		status = le32_to_cpu(desc->opts1);
+		if (status & DescOwn)
+			break;
+
+		/* This barrier is needed to keep us from reading
+		 * any other fields out of the Rx descriptor until
+		 * we know the status of DescOwn
+		 */
+		dma_rmb();
+
+		if (unlikely(status & RxRES)) {
+			netif_info(tp, rx_err, dev, "Rx ERROR. status = %08x\n",
+				   status);
+			dev->stats.rx_errors++;
+			if (status & (RxRWT | RxRUNT))
+				dev->stats.rx_length_errors++;
+			if (status & RxCRC)
+				dev->stats.rx_crc_errors++;
+			if (status & (RxRUNT | RxCRC) && !(status & RxRWT) &&
+			    dev->features & NETIF_F_RXALL) {
+				goto process_pkt;
+			}
+		} else {
+			unsigned int pkt_size;
+			struct sk_buff *skb;
+
+process_pkt:
+			pkt_size = status & GENMASK(13, 0);
+			if (likely(!(dev->features & NETIF_F_RXFCS)))
+				pkt_size -= ETH_FCS_LEN;
+			/*
+			 * The driver does not support incoming fragmented
+			 * frames. They are seen as a symptom of over-mtu
+			 * sized frames.
+			 */
+			if (unlikely(rtl8169_fragmented_frame(status))) {
+				dev->stats.rx_dropped++;
+				dev->stats.rx_length_errors++;
+				goto release_descriptor;
+			}
+
+			skb = napi_alloc_skb(&tp->napi, pkt_size);
+			if (unlikely(!skb)) {
+				dev->stats.rx_dropped++;
+				goto release_descriptor;
+			}
+
+			dma_sync_single_for_cpu(tp_to_dev(tp),
+						le64_to_cpu(desc->addr),
+						pkt_size, DMA_FROM_DEVICE);
+			prefetch(rx_buf);
+			skb_copy_to_linear_data(skb, rx_buf, pkt_size);
+			skb->tail += pkt_size;
+			skb->len = pkt_size;
+
+			dma_sync_single_for_device(tp_to_dev(tp),
+						   le64_to_cpu(desc->addr),
+						   pkt_size, DMA_FROM_DEVICE);
+
+			rtl8169_rx_csum(skb, status);
+			skb->protocol = eth_type_trans(skb, dev);
+
+			rtl8169_rx_vlan_tag(desc, skb);
+
+			if (skb->pkt_type == PACKET_MULTICAST)
+				dev->stats.multicast++;
+
+			napi_gro_receive(&tp->napi, skb);
+
+			u64_stats_update_begin(&tp->rx_stats.syncp);
+			tp->rx_stats.packets++;
+			tp->rx_stats.bytes += pkt_size;
+			u64_stats_update_end(&tp->rx_stats.syncp);
+		}
+release_descriptor:
+		desc->opts2 = 0;
+		rtl8169_mark_to_asic(desc);
+	}
+
+	count = cur_rx - tp->cur_rx;
+	tp->cur_rx = cur_rx;
+
+	return count;
+}
+
+static irqreturn_t rtl8169_interrupt(int irq, void *dev_instance)
+{
+	struct rtl8169_private *tp = dev_instance;
+	u32 status = rtl_get_events(tp);
+
+	if (!tp->irq_enabled || (status & 0xffff) == 0xffff ||
+	    !(status & tp->irq_mask))
+		return IRQ_NONE;
+
+	if (unlikely(status & SYSErr)) {
+		rtl8169_pcierr_interrupt(tp->dev);
+		goto out;
+	}
+
+	if (status & LinkChg)
+		phy_mac_interrupt(tp->phydev);
+
+	if (unlikely(status & RxFIFOOver &&
+	    tp->mac_version == RTL_GIGA_MAC_VER_11)) {
+		netif_stop_queue(tp->dev);
+		/* XXX - Hack alert. See rtl_task(). */
+		set_bit(RTL_FLAG_TASK_RESET_PENDING, tp->wk.flags);
+	}
+
+	rtl_irq_disable(tp);
+	napi_schedule_irqoff(&tp->napi);
+out:
+	rtl_ack_events(tp, status);
+
+	return IRQ_HANDLED;
+}
+
+static void rtl_task(struct work_struct *work)
+{
+	static const struct {
+		int bitnr;
+		void (*action)(struct rtl8169_private *);
+	} rtl_work[] = {
+		{ RTL_FLAG_TASK_RESET_PENDING,	rtl_reset_work },
+	};
+	struct rtl8169_private *tp =
+		container_of(work, struct rtl8169_private, wk.work);
+	struct net_device *dev = tp->dev;
+	int i;
+
+	rtl_lock_work(tp);
+
+	if (!netif_running(dev) ||
+	    !test_bit(RTL_FLAG_TASK_ENABLED, tp->wk.flags))
+		goto out_unlock;
+
+	for (i = 0; i < ARRAY_SIZE(rtl_work); i++) {
+		bool pending;
+
+		pending = test_and_clear_bit(rtl_work[i].bitnr, tp->wk.flags);
+		if (pending)
+			rtl_work[i].action(tp);
+	}
+
+out_unlock:
+	rtl_unlock_work(tp);
+}
+
+static int rtl8169_poll(struct napi_struct *napi, int budget)
+{
+	struct rtl8169_private *tp = container_of(napi, struct rtl8169_private, napi);
+	struct net_device *dev = tp->dev;
+	int work_done;
+
+	work_done = rtl_rx(dev, tp, (u32) budget);
+
+	rtl_tx(dev, tp, budget);
+
+	if (work_done < budget) {
+		napi_complete_done(napi, work_done);
+		rtl_irq_enable(tp);
+	}
+
+	return work_done;
+}
+
+static void rtl8169_rx_missed(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (tp->mac_version > RTL_GIGA_MAC_VER_06)
+		return;
+
+	dev->stats.rx_missed_errors += RTL_R32(tp, RxMissed) & 0xffffff;
+	RTL_W32(tp, RxMissed, 0);
+}
+
+static void r8169_phylink_handler(struct net_device *ndev)
+{
+	struct rtl8169_private *tp = netdev_priv(ndev);
+
+	if (netif_carrier_ok(ndev)) {
+		rtl_link_chg_patch(tp);
+		pm_request_resume(&tp->pci_dev->dev);
+	} else {
+		pm_runtime_idle(&tp->pci_dev->dev);
+	}
+
+	if (net_ratelimit())
+		phy_print_status(tp->phydev);
+}
+
+static int r8169_phy_connect(struct rtl8169_private *tp)
+{
+	struct phy_device *phydev = tp->phydev;
+	phy_interface_t phy_mode;
+	int ret;
+
+	phy_mode = tp->supports_gmii ? PHY_INTERFACE_MODE_GMII :
+		   PHY_INTERFACE_MODE_MII;
+
+	ret = phy_connect_direct(tp->dev, phydev, r8169_phylink_handler,
+				 phy_mode);
+	if (ret)
+		return ret;
+
+	if (!tp->supports_gmii)
+		phy_set_max_speed(phydev, SPEED_100);
+
+	phy_support_asym_pause(phydev);
+
+	phy_attached_info(phydev);
+
+	return 0;
+}
+
+static void rtl8169_down(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	phy_stop(tp->phydev);
+
+	napi_disable(&tp->napi);
+	netif_stop_queue(dev);
+
+	rtl8169_hw_reset(tp);
+	/*
+	 * At this point device interrupts can not be enabled in any function,
+	 * as netif_running is not true (rtl8169_interrupt, rtl8169_reset_task)
+	 * and napi is disabled (rtl8169_poll).
+	 */
+	rtl8169_rx_missed(dev);
+
+	/* Give a racing hard_start_xmit a few cycles to complete. */
+	synchronize_rcu();
+
+	rtl8169_tx_clear(tp);
+
+	rtl8169_rx_clear(tp);
+
+	rtl_pll_power_down(tp);
+}
+
+static int rtl8169_close(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct pci_dev *pdev = tp->pci_dev;
+
+	pm_runtime_get_sync(&pdev->dev);
+
+	/* Update counters before going down */
+	rtl8169_update_counters(tp);
+
+	rtl_lock_work(tp);
+	/* Clear all task flags */
+	bitmap_zero(tp->wk.flags, RTL_FLAG_MAX);
+
+	rtl8169_down(dev);
+	rtl_unlock_work(tp);
+
+	cancel_work_sync(&tp->wk.work);
+
+	phy_disconnect(tp->phydev);
+
+	pci_free_irq(pdev, 0, tp);
+
+	dma_free_coherent(&pdev->dev, R8169_RX_RING_BYTES, tp->RxDescArray,
+			  tp->RxPhyAddr);
+	dma_free_coherent(&pdev->dev, R8169_TX_RING_BYTES, tp->TxDescArray,
+			  tp->TxPhyAddr);
+	tp->TxDescArray = NULL;
+	tp->RxDescArray = NULL;
+
+	pm_runtime_put_sync(&pdev->dev);
+
+	return 0;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void rtl8169_netpoll(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl8169_interrupt(pci_irq_vector(tp->pci_dev, 0), tp);
+}
+#endif
+
+static int rtl_open(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct pci_dev *pdev = tp->pci_dev;
+	int retval = -ENOMEM;
+
+	pm_runtime_get_sync(&pdev->dev);
+
+	/*
+	 * Rx and Tx descriptors needs 256 bytes alignment.
+	 * dma_alloc_coherent provides more.
+	 */
+	tp->TxDescArray = dma_alloc_coherent(&pdev->dev, R8169_TX_RING_BYTES,
+					     &tp->TxPhyAddr, GFP_KERNEL);
+	if (!tp->TxDescArray)
+		goto err_pm_runtime_put;
+
+	tp->RxDescArray = dma_alloc_coherent(&pdev->dev, R8169_RX_RING_BYTES,
+					     &tp->RxPhyAddr, GFP_KERNEL);
+	if (!tp->RxDescArray)
+		goto err_free_tx_0;
+
+	retval = rtl8169_init_ring(tp);
+	if (retval < 0)
+		goto err_free_rx_1;
+
+	rtl_request_firmware(tp);
+
+	retval = pci_request_irq(pdev, 0, rtl8169_interrupt, NULL, tp,
+				 dev->name);
+	if (retval < 0)
+		goto err_release_fw_2;
+
+	retval = r8169_phy_connect(tp);
+	if (retval)
+		goto err_free_irq;
+
+	rtl_lock_work(tp);
+
+	set_bit(RTL_FLAG_TASK_ENABLED, tp->wk.flags);
+
+	napi_enable(&tp->napi);
+
+	rtl8169_init_phy(dev, tp);
+
+	rtl_pll_power_up(tp);
+
+	rtl_hw_start(tp);
+
+	if (!rtl8169_init_counter_offsets(tp))
+		netif_warn(tp, hw, dev, "counter reset/update failed\n");
+
+	phy_start(tp->phydev);
+	netif_start_queue(dev);
+
+	rtl_unlock_work(tp);
+
+	pm_runtime_put_sync(&pdev->dev);
+out:
+	return retval;
+
+err_free_irq:
+	pci_free_irq(pdev, 0, tp);
+err_release_fw_2:
+	rtl_release_firmware(tp);
+	rtl8169_rx_clear(tp);
+err_free_rx_1:
+	dma_free_coherent(&pdev->dev, R8169_RX_RING_BYTES, tp->RxDescArray,
+			  tp->RxPhyAddr);
+	tp->RxDescArray = NULL;
+err_free_tx_0:
+	dma_free_coherent(&pdev->dev, R8169_TX_RING_BYTES, tp->TxDescArray,
+			  tp->TxPhyAddr);
+	tp->TxDescArray = NULL;
+err_pm_runtime_put:
+	pm_runtime_put_noidle(&pdev->dev);
+	goto out;
+}
+
+static void
+rtl8169_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+	struct pci_dev *pdev = tp->pci_dev;
+	struct rtl8169_counters *counters = tp->counters;
+	unsigned int start;
+
+	pm_runtime_get_noresume(&pdev->dev);
+
+	if (netif_running(dev) && pm_runtime_active(&pdev->dev))
+		rtl8169_rx_missed(dev);
+
+	do {
+		start = u64_stats_fetch_begin_irq(&tp->rx_stats.syncp);
+		stats->rx_packets = tp->rx_stats.packets;
+		stats->rx_bytes	= tp->rx_stats.bytes;
+	} while (u64_stats_fetch_retry_irq(&tp->rx_stats.syncp, start));
+
+	do {
+		start = u64_stats_fetch_begin_irq(&tp->tx_stats.syncp);
+		stats->tx_packets = tp->tx_stats.packets;
+		stats->tx_bytes	= tp->tx_stats.bytes;
+	} while (u64_stats_fetch_retry_irq(&tp->tx_stats.syncp, start));
+
+	stats->rx_dropped	= dev->stats.rx_dropped;
+	stats->tx_dropped	= dev->stats.tx_dropped;
+	stats->rx_length_errors = dev->stats.rx_length_errors;
+	stats->rx_errors	= dev->stats.rx_errors;
+	stats->rx_crc_errors	= dev->stats.rx_crc_errors;
+	stats->rx_fifo_errors	= dev->stats.rx_fifo_errors;
+	stats->rx_missed_errors = dev->stats.rx_missed_errors;
+	stats->multicast	= dev->stats.multicast;
+
+	/*
+	 * Fetch additional counter values missing in stats collected by driver
+	 * from tally counters.
+	 */
+	if (pm_runtime_active(&pdev->dev))
+		rtl8169_update_counters(tp);
+
+	/*
+	 * Subtract values fetched during initalization.
+	 * See rtl8169_init_counter_offsets for a description why we do that.
+	 */
+	stats->tx_errors = le64_to_cpu(counters->tx_errors) -
+		le64_to_cpu(tp->tc_offset.tx_errors);
+	stats->collisions = le32_to_cpu(counters->tx_multi_collision) -
+		le32_to_cpu(tp->tc_offset.tx_multi_collision);
+	stats->tx_aborted_errors = le16_to_cpu(counters->tx_aborted) -
+		le16_to_cpu(tp->tc_offset.tx_aborted);
+
+	pm_runtime_put_noidle(&pdev->dev);
+}
+
+static void rtl8169_net_suspend(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (!netif_running(dev))
+		return;
+
+	phy_stop(tp->phydev);
+	netif_device_detach(dev);
+
+	rtl_lock_work(tp);
+	napi_disable(&tp->napi);
+	/* Clear all task flags */
+	bitmap_zero(tp->wk.flags, RTL_FLAG_MAX);
+
+	rtl_unlock_work(tp);
+
+	rtl_pll_power_down(tp);
+}
+
+#ifdef CONFIG_PM
+
+static int rtl8169_suspend(struct device *device)
+{
+	struct net_device *dev = dev_get_drvdata(device);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl8169_net_suspend(dev);
+	clk_disable_unprepare(tp->clk);
+
+	return 0;
+}
+
+static void __rtl8169_resume(struct net_device *dev)
+{
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	netif_device_attach(dev);
+
+	rtl_pll_power_up(tp);
+	rtl8169_init_phy(dev, tp);
+
+	phy_start(tp->phydev);
+
+	rtl_lock_work(tp);
+	napi_enable(&tp->napi);
+	set_bit(RTL_FLAG_TASK_ENABLED, tp->wk.flags);
+	rtl_reset_work(tp);
+	rtl_unlock_work(tp);
+}
+
+static int rtl8169_resume(struct device *device)
+{
+	struct net_device *dev = dev_get_drvdata(device);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl_rar_set(tp, dev->dev_addr);
+
+	clk_prepare_enable(tp->clk);
+
+	if (netif_running(dev))
+		__rtl8169_resume(dev);
+
+	return 0;
+}
+
+static int rtl8169_runtime_suspend(struct device *device)
+{
+	struct net_device *dev = dev_get_drvdata(device);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (!tp->TxDescArray)
+		return 0;
+
+	rtl_lock_work(tp);
+	__rtl8169_set_wol(tp, WAKE_ANY);
+	rtl_unlock_work(tp);
+
+	rtl8169_net_suspend(dev);
+
+	/* Update counters before going runtime suspend */
+	rtl8169_rx_missed(dev);
+	rtl8169_update_counters(tp);
+
+	return 0;
+}
+
+static int rtl8169_runtime_resume(struct device *device)
+{
+	struct net_device *dev = dev_get_drvdata(device);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl_rar_set(tp, dev->dev_addr);
+
+	if (!tp->TxDescArray)
+		return 0;
+
+	rtl_lock_work(tp);
+	__rtl8169_set_wol(tp, tp->saved_wolopts);
+	rtl_unlock_work(tp);
+
+	__rtl8169_resume(dev);
+
+	return 0;
+}
+
+static int rtl8169_runtime_idle(struct device *device)
+{
+	struct net_device *dev = dev_get_drvdata(device);
+
+	if (!netif_running(dev) || !netif_carrier_ok(dev))
+		pm_schedule_suspend(device, 10000);
+
+	return -EBUSY;
+}
+
+static const struct dev_pm_ops rtl8169_pm_ops = {
+	.suspend		= rtl8169_suspend,
+	.resume			= rtl8169_resume,
+	.freeze			= rtl8169_suspend,
+	.thaw			= rtl8169_resume,
+	.poweroff		= rtl8169_suspend,
+	.restore		= rtl8169_resume,
+	.runtime_suspend	= rtl8169_runtime_suspend,
+	.runtime_resume		= rtl8169_runtime_resume,
+	.runtime_idle		= rtl8169_runtime_idle,
+};
+
+#define RTL8169_PM_OPS	(&rtl8169_pm_ops)
+
+#else /* !CONFIG_PM */
+
+#define RTL8169_PM_OPS	NULL
+
+#endif /* !CONFIG_PM */
+
+static void rtl_wol_shutdown_quirk(struct rtl8169_private *tp)
+{
+	/* WoL fails with 8168b when the receiver is disabled. */
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_11:
+	case RTL_GIGA_MAC_VER_12:
+	case RTL_GIGA_MAC_VER_17:
+		pci_clear_master(tp->pci_dev);
+
+		RTL_W8(tp, ChipCmd, CmdRxEnb);
+		/* PCI commit */
+		RTL_R8(tp, ChipCmd);
+		break;
+	default:
+		break;
+	}
+}
+
+static void rtl_shutdown(struct pci_dev *pdev)
+{
+	struct net_device *dev = pci_get_drvdata(pdev);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	rtl8169_net_suspend(dev);
+
+	/* Restore original MAC address */
+	rtl_rar_set(tp, dev->perm_addr);
+
+	rtl8169_hw_reset(tp);
+
+	if (system_state == SYSTEM_POWER_OFF) {
+		if (tp->saved_wolopts) {
+			rtl_wol_suspend_quirk(tp);
+			rtl_wol_shutdown_quirk(tp);
+		}
+
+		pci_wake_from_d3(pdev, true);
+		pci_set_power_state(pdev, PCI_D3hot);
+	}
+}
+
+static void rtl_remove_one(struct pci_dev *pdev)
+{
+	struct net_device *dev = pci_get_drvdata(pdev);
+	struct rtl8169_private *tp = netdev_priv(dev);
+
+	if (r8168_check_dash(tp))
+		rtl8168_driver_stop(tp);
+
+	netif_napi_del(&tp->napi);
+
+	unregister_netdev(dev);
+	mdiobus_unregister(tp->phydev->mdio.bus);
+
+	rtl_release_firmware(tp);
+
+	if (pci_dev_run_wake(pdev))
+		pm_runtime_get_noresume(&pdev->dev);
+
+	/* restore original MAC address */
+	rtl_rar_set(tp, dev->perm_addr);
+}
+
+static const struct net_device_ops rtl_netdev_ops = {
+	.ndo_open		= rtl_open,
+	.ndo_stop		= rtl8169_close,
+	.ndo_get_stats64	= rtl8169_get_stats64,
+	.ndo_start_xmit		= rtl8169_start_xmit,
+	.ndo_features_check	= rtl8169_features_check,
+	.ndo_tx_timeout		= rtl8169_tx_timeout,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_change_mtu		= rtl8169_change_mtu,
+	.ndo_fix_features	= rtl8169_fix_features,
+	.ndo_set_features	= rtl8169_set_features,
+	.ndo_set_mac_address	= rtl_set_mac_address,
+	.ndo_do_ioctl		= rtl8169_ioctl,
+	.ndo_set_rx_mode	= rtl_set_rx_mode,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= rtl8169_netpoll,
+#endif
+
+};
+
+static void rtl_set_irq_mask(struct rtl8169_private *tp)
+{
+	tp->irq_mask = RTL_EVENT_NAPI | LinkChg;
+
+	if (tp->mac_version <= RTL_GIGA_MAC_VER_06)
+		tp->irq_mask |= SYSErr | RxOverflow | RxFIFOOver;
+	else if (tp->mac_version == RTL_GIGA_MAC_VER_11)
+		/* special workaround needed */
+		tp->irq_mask |= RxFIFOOver;
+	else
+		tp->irq_mask |= RxOverflow;
+}
+
+static int rtl_alloc_irq(struct rtl8169_private *tp)
+{
+	unsigned int flags;
+
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_02 ... RTL_GIGA_MAC_VER_06:
+		rtl_unlock_config_regs(tp);
+		RTL_W8(tp, Config2, RTL_R8(tp, Config2) & ~MSIEnable);
+		rtl_lock_config_regs(tp);
+		/* fall through */
+	case RTL_GIGA_MAC_VER_07 ... RTL_GIGA_MAC_VER_24:
+		flags = PCI_IRQ_LEGACY;
+		break;
+	default:
+		flags = PCI_IRQ_ALL_TYPES;
+		break;
+	}
+
+	return pci_alloc_irq_vectors(tp->pci_dev, 1, 1, flags);
+}
+
+static void rtl_read_mac_address(struct rtl8169_private *tp,
+				 u8 mac_addr[ETH_ALEN])
+{
+	/* Get MAC address */
+	if (rtl_is_8168evl_up(tp) && tp->mac_version != RTL_GIGA_MAC_VER_34) {
+		u32 value = rtl_eri_read(tp, 0xe0);
+
+		mac_addr[0] = (value >>  0) & 0xff;
+		mac_addr[1] = (value >>  8) & 0xff;
+		mac_addr[2] = (value >> 16) & 0xff;
+		mac_addr[3] = (value >> 24) & 0xff;
+
+		value = rtl_eri_read(tp, 0xe4);
+		mac_addr[4] = (value >>  0) & 0xff;
+		mac_addr[5] = (value >>  8) & 0xff;
+	} else if (rtl_is_8125(tp)) {
+		rtl_read_mac_from_reg(tp, mac_addr, MAC0_BKP);
+	}
+}
+
+DECLARE_RTL_COND(rtl_link_list_ready_cond)
+{
+	return RTL_R8(tp, MCU) & LINK_LIST_RDY;
+}
+
+DECLARE_RTL_COND(rtl_rxtx_empty_cond)
+{
+	return (RTL_R8(tp, MCU) & RXTX_EMPTY) == RXTX_EMPTY;
+}
+
+static int r8169_mdio_read_reg(struct mii_bus *mii_bus, int phyaddr, int phyreg)
+{
+	struct rtl8169_private *tp = mii_bus->priv;
+
+	if (phyaddr > 0)
+		return -ENODEV;
+
+	return rtl_readphy(tp, phyreg);
+}
+
+static int r8169_mdio_write_reg(struct mii_bus *mii_bus, int phyaddr,
+				int phyreg, u16 val)
+{
+	struct rtl8169_private *tp = mii_bus->priv;
+
+	if (phyaddr > 0)
+		return -ENODEV;
+
+	rtl_writephy(tp, phyreg, val);
+
+	return 0;
+}
+
+static int r8169_mdio_register(struct rtl8169_private *tp)
+{
+	struct pci_dev *pdev = tp->pci_dev;
+	struct mii_bus *new_bus;
+	int ret;
+
+	new_bus = devm_mdiobus_alloc(&pdev->dev);
+	if (!new_bus)
+		return -ENOMEM;
+
+	new_bus->name = "r8169";
+	new_bus->priv = tp;
+	new_bus->parent = &pdev->dev;
+	new_bus->irq[0] = PHY_IGNORE_INTERRUPT;
+	snprintf(new_bus->id, MII_BUS_ID_SIZE, "r8169-%x", pci_dev_id(pdev));
+
+	new_bus->read = r8169_mdio_read_reg;
+	new_bus->write = r8169_mdio_write_reg;
+
+	ret = mdiobus_register(new_bus);
+	if (ret)
+		return ret;
+
+	tp->phydev = mdiobus_get_phy(new_bus, 0);
+	if (!tp->phydev) {
+		mdiobus_unregister(new_bus);
+		return -ENODEV;
+	}
+
+	/* PHY will be woken up in rtl_open() */
+	phy_suspend(tp->phydev);
+
+	return 0;
+}
+
+static void rtl_hw_init_8168g(struct rtl8169_private *tp)
+{
+	tp->ocp_base = OCP_STD_PHY_BASE;
+
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) | RXDV_GATED_EN);
+
+	if (!rtl_udelay_loop_wait_high(tp, &rtl_txcfg_empty_cond, 100, 42))
+		return;
+
+	if (!rtl_udelay_loop_wait_high(tp, &rtl_rxtx_empty_cond, 100, 42))
+		return;
+
+	RTL_W8(tp, ChipCmd, RTL_R8(tp, ChipCmd) & ~(CmdTxEnb | CmdRxEnb));
+	msleep(1);
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) & ~NOW_IS_OOB);
+
+	r8168_mac_ocp_modify(tp, 0xe8de, BIT(14), 0);
+
+	if (!rtl_udelay_loop_wait_high(tp, &rtl_link_list_ready_cond, 100, 42))
+		return;
+
+	r8168_mac_ocp_modify(tp, 0xe8de, 0, BIT(15));
+
+	rtl_udelay_loop_wait_high(tp, &rtl_link_list_ready_cond, 100, 42);
+}
+
+static void rtl_hw_init_8125(struct rtl8169_private *tp)
+{
+	tp->ocp_base = OCP_STD_PHY_BASE;
+
+	RTL_W32(tp, MISC, RTL_R32(tp, MISC) | RXDV_GATED_EN);
+
+	if (!rtl_udelay_loop_wait_high(tp, &rtl_rxtx_empty_cond, 100, 42))
+		return;
+
+	RTL_W8(tp, ChipCmd, RTL_R8(tp, ChipCmd) & ~(CmdTxEnb | CmdRxEnb));
+	msleep(1);
+	RTL_W8(tp, MCU, RTL_R8(tp, MCU) & ~NOW_IS_OOB);
+
+	r8168_mac_ocp_modify(tp, 0xe8de, BIT(14), 0);
+
+	if (!rtl_udelay_loop_wait_high(tp, &rtl_link_list_ready_cond, 100, 42))
+		return;
+
+	r8168_mac_ocp_write(tp, 0xc0aa, 0x07d0);
+	r8168_mac_ocp_write(tp, 0xc0a6, 0x0150);
+	r8168_mac_ocp_write(tp, 0xc01e, 0x5555);
+
+	rtl_udelay_loop_wait_high(tp, &rtl_link_list_ready_cond, 100, 42);
+}
+
+static void rtl_hw_initialize(struct rtl8169_private *tp)
+{
+	switch (tp->mac_version) {
+	case RTL_GIGA_MAC_VER_49 ... RTL_GIGA_MAC_VER_51:
+		rtl8168ep_stop_cmac(tp);
+		/* fall through */
+	case RTL_GIGA_MAC_VER_40 ... RTL_GIGA_MAC_VER_48:
+		rtl_hw_init_8168g(tp);
+		break;
+	case RTL_GIGA_MAC_VER_60 ... RTL_GIGA_MAC_VER_61:
+		rtl_hw_init_8125(tp);
+		break;
+	default:
+		break;
+	}
+}
+
+static int rtl_jumbo_max(struct rtl8169_private *tp)
+{
+	/* Non-GBit versions don't support jumbo frames */
+	if (!tp->supports_gmii)
+		return JUMBO_1K;
+
+	switch (tp->mac_version) {
+	/* RTL8169 */
+	case RTL_GIGA_MAC_VER_02 ... RTL_GIGA_MAC_VER_06:
+		return JUMBO_7K;
+	/* RTL8168b */
+	case RTL_GIGA_MAC_VER_11:
+	case RTL_GIGA_MAC_VER_12:
+	case RTL_GIGA_MAC_VER_17:
+		return JUMBO_4K;
+	/* RTL8168c */
+	case RTL_GIGA_MAC_VER_18 ... RTL_GIGA_MAC_VER_24:
+		return JUMBO_6K;
+	default:
+		return JUMBO_9K;
+	}
+}
+
+static void rtl_disable_clk(void *data)
+{
+	clk_disable_unprepare(data);
+}
+
+static int rtl_get_ether_clk(struct rtl8169_private *tp)
+{
+	struct device *d = tp_to_dev(tp);
+	struct clk *clk;
+	int rc;
+
+	clk = devm_clk_get(d, "ether_clk");
+	if (IS_ERR(clk)) {
+		rc = PTR_ERR(clk);
+		if (rc == -ENOENT)
+			/* clk-core allows NULL (for suspend / resume) */
+			rc = 0;
+		else if (rc != -EPROBE_DEFER)
+			dev_err(d, "failed to get clk: %d\n", rc);
+	} else {
+		tp->clk = clk;
+		rc = clk_prepare_enable(clk);
+		if (rc)
+			dev_err(d, "failed to enable clk: %d\n", rc);
+		else
+			rc = devm_add_action_or_reset(d, rtl_disable_clk, clk);
+	}
+
+	return rc;
+}
+
+static void rtl_init_mac_address(struct rtl8169_private *tp)
+{
+	struct net_device *dev = tp->dev;
+	u8 *mac_addr = dev->dev_addr;
+	int rc;
+
+	rc = eth_platform_get_mac_address(tp_to_dev(tp), mac_addr);
+	if (!rc)
+		goto done;
+
+	rtl_read_mac_address(tp, mac_addr);
+	if (is_valid_ether_addr(mac_addr))
+		goto done;
+
+	rtl_read_mac_from_reg(tp, mac_addr, MAC0);
+	if (is_valid_ether_addr(mac_addr))
+		goto done;
+
+	eth_hw_addr_random(dev);
+	dev_warn(tp_to_dev(tp), "can't read MAC address, setting random one\n");
+done:
+	rtl_rar_set(tp, mac_addr);
+}
+
+static int rtl_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct rtl8169_private *tp;
+	struct net_device *dev;
+	int chipset, region;
+	int jumbo_max, rc;
+
+	dev = devm_alloc_etherdev(&pdev->dev, sizeof (*tp));
+	if (!dev)
+		return -ENOMEM;
+
+	SET_NETDEV_DEV(dev, &pdev->dev);
+	dev->netdev_ops = &rtl_netdev_ops;
+	tp = netdev_priv(dev);
+	tp->dev = dev;
+	tp->pci_dev = pdev;
+	tp->msg_enable = netif_msg_init(debug.msg_enable, R8169_MSG_DEFAULT);
+	tp->supports_gmii = ent->driver_data == RTL_CFG_NO_GBIT ? 0 : 1;
+
+	/* Get the *optional* external "ether_clk" used on some boards */
+	rc = rtl_get_ether_clk(tp);
+	if (rc)
+		return rc;
+
+	/* Disable ASPM completely as that cause random device stop working
+	 * problems as well as full system hangs for some PCIe devices users.
+	 */
+	rc = pci_disable_link_state(pdev, PCIE_LINK_STATE_L0S |
+					  PCIE_LINK_STATE_L1);
+	tp->aspm_manageable = !rc;
+
+	/* enable device (incl. PCI PM wakeup and hotplug setup) */
+	rc = pcim_enable_device(pdev);
+	if (rc < 0) {
+		dev_err(&pdev->dev, "enable failure\n");
+		return rc;
+	}
+
+	if (pcim_set_mwi(pdev) < 0)
+		dev_info(&pdev->dev, "Mem-Wr-Inval unavailable\n");
+
+	/* use first MMIO region */
+	region = ffs(pci_select_bars(pdev, IORESOURCE_MEM)) - 1;
+	if (region < 0) {
+		dev_err(&pdev->dev, "no MMIO resource found\n");
+		return -ENODEV;
+	}
+
+	/* check for weird/broken PCI region reporting */
+	if (pci_resource_len(pdev, region) < R8169_REGS_SIZE) {
+		dev_err(&pdev->dev, "Invalid PCI region size(s), aborting\n");
+		return -ENODEV;
+	}
+
+	rc = pcim_iomap_regions(pdev, BIT(region), MODULENAME);
+	if (rc < 0) {
+		dev_err(&pdev->dev, "cannot remap MMIO, aborting\n");
+		return rc;
+	}
+
+	tp->mmio_addr = pcim_iomap_table(pdev)[region];
+
+	/* Identify chip attached to board */
+	rtl8169_get_mac_version(tp);
+	if (tp->mac_version == RTL_GIGA_MAC_NONE)
+		return -ENODEV;
+
+	tp->cp_cmd = RTL_R16(tp, CPlusCmd);
+
+	if (sizeof(dma_addr_t) > 4 && tp->mac_version >= RTL_GIGA_MAC_VER_18 &&
+	    !dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64)))
+		dev->features |= NETIF_F_HIGHDMA;
+
+	rtl_init_rxcfg(tp);
+
+	rtl8169_irq_mask_and_ack(tp);
+
+	rtl_hw_initialize(tp);
+
+	rtl_hw_reset(tp);
+
+	pci_set_master(pdev);
+
+	chipset = tp->mac_version;
+
+	rc = rtl_alloc_irq(tp);
+	if (rc < 0) {
+		dev_err(&pdev->dev, "Can't allocate interrupt\n");
+		return rc;
+	}
+
+	mutex_init(&tp->wk.mutex);
+	INIT_WORK(&tp->wk.work, rtl_task);
+	u64_stats_init(&tp->rx_stats.syncp);
+	u64_stats_init(&tp->tx_stats.syncp);
+
+	rtl_init_mac_address(tp);
+
+	dev->ethtool_ops = &rtl8169_ethtool_ops;
+
+	netif_napi_add(dev, &tp->napi, rtl8169_poll, NAPI_POLL_WEIGHT);
+
+	dev->features |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |
+		NETIF_F_RXCSUM | NETIF_F_HW_VLAN_CTAG_TX |
+		NETIF_F_HW_VLAN_CTAG_RX;
+	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |
+		NETIF_F_RXCSUM | NETIF_F_HW_VLAN_CTAG_TX |
+		NETIF_F_HW_VLAN_CTAG_RX;
+	dev->vlan_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |
+		NETIF_F_HIGHDMA;
+	dev->priv_flags |= IFF_LIVE_ADDR_CHANGE;
+
+	tp->cp_cmd |= RxChkSum;
+	/* RTL8125 uses register RxConfig for VLAN offloading config */
+	if (!rtl_is_8125(tp))
+		tp->cp_cmd |= RxVlan;
+	/*
+	 * Pretend we are using VLANs; This bypasses a nasty bug where
+	 * Interrupts stop flowing on high load on 8110SCd controllers.
+	 */
+	if (tp->mac_version == RTL_GIGA_MAC_VER_05)
+		/* Disallow toggling */
+		dev->hw_features &= ~NETIF_F_HW_VLAN_CTAG_RX;
+
+	if (rtl_chip_supports_csum_v2(tp)) {
+		dev->hw_features |= NETIF_F_IPV6_CSUM | NETIF_F_TSO6;
+		dev->features |= NETIF_F_IPV6_CSUM | NETIF_F_TSO6;
+		dev->gso_max_size = RTL_GSO_MAX_SIZE_V2;
+		dev->gso_max_segs = RTL_GSO_MAX_SEGS_V2;
+	} else {
+		dev->gso_max_size = RTL_GSO_MAX_SIZE_V1;
+		dev->gso_max_segs = RTL_GSO_MAX_SEGS_V1;
+	}
+
+	/* RTL8168e-vl and one RTL8168c variant are known to have a
+	 * HW issue with TSO.
+	 */
+	if (tp->mac_version == RTL_GIGA_MAC_VER_34 ||
+	    tp->mac_version == RTL_GIGA_MAC_VER_22) {
+		dev->vlan_features &= ~(NETIF_F_ALL_TSO | NETIF_F_SG);
+		dev->hw_features &= ~(NETIF_F_ALL_TSO | NETIF_F_SG);
+		dev->features &= ~(NETIF_F_ALL_TSO | NETIF_F_SG);
+	}
+
+	dev->hw_features |= NETIF_F_RXALL;
+	dev->hw_features |= NETIF_F_RXFCS;
+
+	/* MTU range: 60 - hw-specific max */
+	dev->min_mtu = ETH_ZLEN;
+	jumbo_max = rtl_jumbo_max(tp);
+	dev->max_mtu = jumbo_max;
+
+	rtl_set_irq_mask(tp);
+
+	tp->fw_name = rtl_chip_infos[chipset].fw_name;
+
+	tp->counters = dmam_alloc_coherent (&pdev->dev, sizeof(*tp->counters),
+					    &tp->counters_phys_addr,
+					    GFP_KERNEL);
+	if (!tp->counters)
+		return -ENOMEM;
+
+	pci_set_drvdata(pdev, dev);
+
+	rc = r8169_mdio_register(tp);
+	if (rc)
+		return rc;
+
+	/* chip gets powered up in rtl_open() */
+	rtl_pll_power_down(tp);
+
+	rc = register_netdev(dev);
+	if (rc)
+		goto err_mdio_unregister;
+
+	netif_info(tp, probe, dev, "%s, %pM, XID %03x, IRQ %d\n",
+		   rtl_chip_infos[chipset].name, dev->dev_addr,
+		   (RTL_R32(tp, TxConfig) >> 20) & 0xfcf,
+		   pci_irq_vector(pdev, 0));
+
+	if (jumbo_max > JUMBO_1K)
+		netif_info(tp, probe, dev,
+			   "jumbo features [frames: %d bytes, tx checksumming: %s]\n",
+			   jumbo_max, tp->mac_version <= RTL_GIGA_MAC_VER_06 ?
+			   "ok" : "ko");
+
+	if (r8168_check_dash(tp))
+		rtl8168_driver_start(tp);
+
+	if (pci_dev_run_wake(pdev))
+		pm_runtime_put_sync(&pdev->dev);
+
+	return 0;
+
+err_mdio_unregister:
+	mdiobus_unregister(tp->phydev->mdio.bus);
+	return rc;
+}
+
+static struct pci_driver rtl8169_pci_driver = {
+	.name		= MODULENAME,
+	.id_table	= rtl8169_pci_tbl,
+	.probe		= rtl_init_one,
+	.remove		= rtl_remove_one,
+	.shutdown	= rtl_shutdown,
+	.driver.pm	= RTL8169_PM_OPS,
+};
+
+module_pci_driver(rtl8169_pci_driver);
diff -Naur a/net/rtnet/drivers/rpi-4/bcmgenet.c b/net/rtnet/drivers/rpi-4/bcmgenet.c
--- a/net/rtnet/drivers/rpi-4/bcmgenet.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rpi-4/bcmgenet.c	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,4464 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Broadcom GENET (Gigabit Ethernet) controller driver
+ *
+ * Copyright (c) 2014-2020 Broadcom
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ */
+
+#define pr_fmt(fmt)				"bcmgenet: " fmt
+
+#include <linux/acpi.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/fcntl.h>
+#include <linux/interrupt.h>
+#include <linux/string.h>
+#include <linux/if_ether.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/pm.h>
+#include <linux/clk.h>
+#include <net/arp.h>
+
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/phy.h>
+#include <linux/platform_data/bcmgenet.h>
+#include <linux/timekeeping.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/ip.h>
+#include <uapi/linux/ip.h>
+
+#include <asm/unaligned.h>
+
+#include "bcmgenet.h"
+
+#define RX_RING_SIZE	TOTAL_DESC
+static unsigned int bcmgenet_rtskb_pool_size = RX_RING_SIZE * 2;
+module_param(bcmgenet_rtskb_pool_size, uint, 0444);
+MODULE_PARM_DESC(bcmgenet_rtskb_pool_size, "Number of realtime socket buffers in proxy pool");
+
+/* Maximum number of hardware queues, downsized if needed */
+#define GENET_MAX_MQ_CNT	0
+
+/* Default highest priority queue for multi queue support */
+#define GENET_Q0_PRIORITY	0
+
+#define GENET_Q16_RX_BD_CNT	\
+	(TOTAL_DESC - priv->hw_params->rx_queues * priv->hw_params->rx_bds_per_q)
+#define GENET_Q16_TX_BD_CNT	\
+	(TOTAL_DESC - priv->hw_params->tx_queues * priv->hw_params->tx_bds_per_q)
+
+#define RX_BUF_LENGTH		2048
+#define SKB_ALIGNMENT		32
+
+/* Tx/Rx DMA register offset, skip 256 descriptors */
+#define WORDS_PER_BD(p)		(p->hw_params->words_per_bd)
+#define DMA_DESC_SIZE		(WORDS_PER_BD(priv) * sizeof(u32))
+
+#define GENET_TDMA_REG_OFF	(priv->hw_params->tdma_offset + \
+				TOTAL_DESC * DMA_DESC_SIZE)
+
+#define GENET_RDMA_REG_OFF	(priv->hw_params->rdma_offset + \
+				TOTAL_DESC * DMA_DESC_SIZE)
+
+/* Forward declarations */
+static void bcmgenet_set_rx_mode(struct rtnet_device *dev);
+static int bcmgenet_tx_poll(void *vpriv);
+static int bcmgenet_rx_poll(void *vpriv);
+
+static int stop_tx_task = 0, stop_rx_task = 0;
+static rtdm_event_t tx_event, rx_event;
+struct task_struct *tx_task, *rx_task;
+
+static inline void bcmgenet_writel(u32 value, void __iomem *offset)
+{
+	/* MIPS chips strapped for BE will automagically configure the
+	 * peripheral registers for CPU-native byte order.
+	 */
+	if (IS_ENABLED(CONFIG_MIPS) && IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+		__raw_writel(value, offset);
+	else
+		writel_relaxed(value, offset);
+}
+
+static inline u32 bcmgenet_readl(void __iomem *offset)
+{
+	if (IS_ENABLED(CONFIG_MIPS) && IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+		return __raw_readl(offset);
+	else
+		return readl_relaxed(offset);
+}
+
+static inline void dmadesc_set_length_status(struct bcmgenet_priv *priv,
+					     void __iomem *d, u32 value)
+{
+	bcmgenet_writel(value, d + DMA_DESC_LENGTH_STATUS);
+}
+
+static inline void dmadesc_set_addr(struct bcmgenet_priv *priv,
+				    void __iomem *d,
+				    dma_addr_t addr)
+{
+	bcmgenet_writel(lower_32_bits(addr), d + DMA_DESC_ADDRESS_LO);
+
+	/* Register writes to GISB bus can take couple hundred nanoseconds
+	 * and are done for each packet, save these expensive writes unless
+	 * the platform is explicitly configured for 64-bits/LPAE.
+	 */
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	if (priv->hw_params->flags & GENET_HAS_40BITS)
+		bcmgenet_writel(upper_32_bits(addr), d + DMA_DESC_ADDRESS_HI);
+#endif
+}
+
+/* Combined address + length/status setter */
+static inline void dmadesc_set(struct bcmgenet_priv *priv,
+			       void __iomem *d, dma_addr_t addr, u32 val)
+{
+	dmadesc_set_addr(priv, d, addr);
+	dmadesc_set_length_status(priv, d, val);
+}
+
+static inline dma_addr_t dmadesc_get_addr(struct bcmgenet_priv *priv,
+					  void __iomem *d)
+{
+	dma_addr_t addr;
+
+	addr = bcmgenet_readl(d + DMA_DESC_ADDRESS_LO);
+
+	/* Register writes to GISB bus can take couple hundred nanoseconds
+	 * and are done for each packet, save these expensive writes unless
+	 * the platform is explicitly configured for 64-bits/LPAE.
+	 */
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	if (priv->hw_params->flags & GENET_HAS_40BITS)
+		addr |= (u64)bcmgenet_readl(d + DMA_DESC_ADDRESS_HI) << 32;
+#endif
+	return addr;
+}
+
+#define GENET_VER_FMT	"%1d.%1d EPHY: 0x%04x"
+
+#define GENET_MSG_DEFAULT	(NETIF_MSG_DRV | NETIF_MSG_PROBE | \
+				NETIF_MSG_LINK)
+
+static inline u32 bcmgenet_rbuf_ctrl_get(struct bcmgenet_priv *priv)
+{
+	if (GENET_IS_V1(priv))
+		return bcmgenet_rbuf_readl(priv, RBUF_FLUSH_CTRL_V1);
+	else
+		return bcmgenet_sys_readl(priv, SYS_RBUF_FLUSH_CTRL);
+}
+
+static inline void bcmgenet_rbuf_ctrl_set(struct bcmgenet_priv *priv, u32 val)
+{
+	if (GENET_IS_V1(priv))
+		bcmgenet_rbuf_writel(priv, val, RBUF_FLUSH_CTRL_V1);
+	else
+		bcmgenet_sys_writel(priv, val, SYS_RBUF_FLUSH_CTRL);
+}
+
+/* These macros are defined to deal with register map change
+ * between GENET1.1 and GENET2. Only those currently being used
+ * by driver are defined.
+ */
+static inline u32 bcmgenet_tbuf_ctrl_get(struct bcmgenet_priv *priv)
+{
+	if (GENET_IS_V1(priv))
+		return bcmgenet_rbuf_readl(priv, TBUF_CTRL_V1);
+	else
+		return bcmgenet_readl(priv->base +
+				      priv->hw_params->tbuf_offset + TBUF_CTRL);
+}
+
+static inline void bcmgenet_tbuf_ctrl_set(struct bcmgenet_priv *priv, u32 val)
+{
+	if (GENET_IS_V1(priv))
+		bcmgenet_rbuf_writel(priv, val, TBUF_CTRL_V1);
+	else
+		bcmgenet_writel(val, priv->base +
+				priv->hw_params->tbuf_offset + TBUF_CTRL);
+}
+
+static inline u32 bcmgenet_bp_mc_get(struct bcmgenet_priv *priv)
+{
+	if (GENET_IS_V1(priv))
+		return bcmgenet_rbuf_readl(priv, TBUF_BP_MC_V1);
+	else
+		return bcmgenet_readl(priv->base +
+				      priv->hw_params->tbuf_offset + TBUF_BP_MC);
+}
+
+static inline void bcmgenet_bp_mc_set(struct bcmgenet_priv *priv, u32 val)
+{
+	if (GENET_IS_V1(priv))
+		bcmgenet_rbuf_writel(priv, val, TBUF_BP_MC_V1);
+	else
+		bcmgenet_writel(val, priv->base +
+				priv->hw_params->tbuf_offset + TBUF_BP_MC);
+}
+
+/* RX/TX DMA register accessors */
+enum dma_reg {
+	DMA_RING_CFG = 0,
+	DMA_CTRL,
+	DMA_STATUS,
+	DMA_SCB_BURST_SIZE,
+	DMA_ARB_CTRL,
+	DMA_PRIORITY_0,
+	DMA_PRIORITY_1,
+	DMA_PRIORITY_2,
+	DMA_INDEX2RING_0,
+	DMA_INDEX2RING_1,
+	DMA_INDEX2RING_2,
+	DMA_INDEX2RING_3,
+	DMA_INDEX2RING_4,
+	DMA_INDEX2RING_5,
+	DMA_INDEX2RING_6,
+	DMA_INDEX2RING_7,
+	DMA_RING0_TIMEOUT,
+	DMA_RING1_TIMEOUT,
+	DMA_RING2_TIMEOUT,
+	DMA_RING3_TIMEOUT,
+	DMA_RING4_TIMEOUT,
+	DMA_RING5_TIMEOUT,
+	DMA_RING6_TIMEOUT,
+	DMA_RING7_TIMEOUT,
+	DMA_RING8_TIMEOUT,
+	DMA_RING9_TIMEOUT,
+	DMA_RING10_TIMEOUT,
+	DMA_RING11_TIMEOUT,
+	DMA_RING12_TIMEOUT,
+	DMA_RING13_TIMEOUT,
+	DMA_RING14_TIMEOUT,
+	DMA_RING15_TIMEOUT,
+	DMA_RING16_TIMEOUT,
+};
+
+static const u8 bcmgenet_dma_regs_v3plus[] = {
+	[DMA_RING_CFG]		= 0x00,
+	[DMA_CTRL]		= 0x04,
+	[DMA_STATUS]		= 0x08,
+	[DMA_SCB_BURST_SIZE]	= 0x0C,
+	[DMA_ARB_CTRL]		= 0x2C,
+	[DMA_PRIORITY_0]	= 0x30,
+	[DMA_PRIORITY_1]	= 0x34,
+	[DMA_PRIORITY_2]	= 0x38,
+	[DMA_RING0_TIMEOUT]	= 0x2C,
+	[DMA_RING1_TIMEOUT]	= 0x30,
+	[DMA_RING2_TIMEOUT]	= 0x34,
+	[DMA_RING3_TIMEOUT]	= 0x38,
+	[DMA_RING4_TIMEOUT]	= 0x3c,
+	[DMA_RING5_TIMEOUT]	= 0x40,
+	[DMA_RING6_TIMEOUT]	= 0x44,
+	[DMA_RING7_TIMEOUT]	= 0x48,
+	[DMA_RING8_TIMEOUT]	= 0x4c,
+	[DMA_RING9_TIMEOUT]	= 0x50,
+	[DMA_RING10_TIMEOUT]	= 0x54,
+	[DMA_RING11_TIMEOUT]	= 0x58,
+	[DMA_RING12_TIMEOUT]	= 0x5c,
+	[DMA_RING13_TIMEOUT]	= 0x60,
+	[DMA_RING14_TIMEOUT]	= 0x64,
+	[DMA_RING15_TIMEOUT]	= 0x68,
+	[DMA_RING16_TIMEOUT]	= 0x6C,
+	[DMA_INDEX2RING_0]	= 0x70,
+	[DMA_INDEX2RING_1]	= 0x74,
+	[DMA_INDEX2RING_2]	= 0x78,
+	[DMA_INDEX2RING_3]	= 0x7C,
+	[DMA_INDEX2RING_4]	= 0x80,
+	[DMA_INDEX2RING_5]	= 0x84,
+	[DMA_INDEX2RING_6]	= 0x88,
+	[DMA_INDEX2RING_7]	= 0x8C,
+};
+
+static const u8 bcmgenet_dma_regs_v2[] = {
+	[DMA_RING_CFG]		= 0x00,
+	[DMA_CTRL]		= 0x04,
+	[DMA_STATUS]		= 0x08,
+	[DMA_SCB_BURST_SIZE]	= 0x0C,
+	[DMA_ARB_CTRL]		= 0x30,
+	[DMA_PRIORITY_0]	= 0x34,
+	[DMA_PRIORITY_1]	= 0x38,
+	[DMA_PRIORITY_2]	= 0x3C,
+	[DMA_RING0_TIMEOUT]	= 0x2C,
+	[DMA_RING1_TIMEOUT]	= 0x30,
+	[DMA_RING2_TIMEOUT]	= 0x34,
+	[DMA_RING3_TIMEOUT]	= 0x38,
+	[DMA_RING4_TIMEOUT]	= 0x3c,
+	[DMA_RING5_TIMEOUT]	= 0x40,
+	[DMA_RING6_TIMEOUT]	= 0x44,
+	[DMA_RING7_TIMEOUT]	= 0x48,
+	[DMA_RING8_TIMEOUT]	= 0x4c,
+	[DMA_RING9_TIMEOUT]	= 0x50,
+	[DMA_RING10_TIMEOUT]	= 0x54,
+	[DMA_RING11_TIMEOUT]	= 0x58,
+	[DMA_RING12_TIMEOUT]	= 0x5c,
+	[DMA_RING13_TIMEOUT]	= 0x60,
+	[DMA_RING14_TIMEOUT]	= 0x64,
+	[DMA_RING15_TIMEOUT]	= 0x68,
+	[DMA_RING16_TIMEOUT]	= 0x6C,
+};
+
+static const u8 bcmgenet_dma_regs_v1[] = {
+	[DMA_CTRL]		= 0x00,
+	[DMA_STATUS]		= 0x04,
+	[DMA_SCB_BURST_SIZE]	= 0x0C,
+	[DMA_ARB_CTRL]		= 0x30,
+	[DMA_PRIORITY_0]	= 0x34,
+	[DMA_PRIORITY_1]	= 0x38,
+	[DMA_PRIORITY_2]	= 0x3C,
+	[DMA_RING0_TIMEOUT]	= 0x2C,
+	[DMA_RING1_TIMEOUT]	= 0x30,
+	[DMA_RING2_TIMEOUT]	= 0x34,
+	[DMA_RING3_TIMEOUT]	= 0x38,
+	[DMA_RING4_TIMEOUT]	= 0x3c,
+	[DMA_RING5_TIMEOUT]	= 0x40,
+	[DMA_RING6_TIMEOUT]	= 0x44,
+	[DMA_RING7_TIMEOUT]	= 0x48,
+	[DMA_RING8_TIMEOUT]	= 0x4c,
+	[DMA_RING9_TIMEOUT]	= 0x50,
+	[DMA_RING10_TIMEOUT]	= 0x54,
+	[DMA_RING11_TIMEOUT]	= 0x58,
+	[DMA_RING12_TIMEOUT]	= 0x5c,
+	[DMA_RING13_TIMEOUT]	= 0x60,
+	[DMA_RING14_TIMEOUT]	= 0x64,
+	[DMA_RING15_TIMEOUT]	= 0x68,
+	[DMA_RING16_TIMEOUT]	= 0x6C,
+};
+
+/* Set at runtime once bcmgenet version is known */
+static const u8 *bcmgenet_dma_regs;
+
+static inline struct bcmgenet_priv *dev_to_priv(struct device *dev)
+{
+	return rtnetdev_priv((struct rtnet_device*)dev_get_drvdata(dev));
+}
+
+static inline u32 bcmgenet_tdma_readl(struct bcmgenet_priv *priv,
+				      enum dma_reg r)
+{
+	return bcmgenet_readl(priv->base + GENET_TDMA_REG_OFF +
+			      DMA_RINGS_SIZE + bcmgenet_dma_regs[r]);
+}
+
+static inline void bcmgenet_tdma_writel(struct bcmgenet_priv *priv,
+					u32 val, enum dma_reg r)
+{
+	bcmgenet_writel(val, priv->base + GENET_TDMA_REG_OFF +
+			DMA_RINGS_SIZE + bcmgenet_dma_regs[r]);
+}
+
+static inline u32 bcmgenet_rdma_readl(struct bcmgenet_priv *priv,
+				      enum dma_reg r)
+{
+	return bcmgenet_readl(priv->base + GENET_RDMA_REG_OFF +
+			      DMA_RINGS_SIZE + bcmgenet_dma_regs[r]);
+}
+
+static inline void bcmgenet_rdma_writel(struct bcmgenet_priv *priv,
+					u32 val, enum dma_reg r)
+{
+	bcmgenet_writel(val, priv->base + GENET_RDMA_REG_OFF +
+			DMA_RINGS_SIZE + bcmgenet_dma_regs[r]);
+}
+
+/* RDMA/TDMA ring registers and accessors
+ * we merge the common fields and just prefix with T/D the registers
+ * having different meaning depending on the direction
+ */
+enum dma_ring_reg {
+	TDMA_READ_PTR = 0,
+	RDMA_WRITE_PTR = TDMA_READ_PTR,
+	TDMA_READ_PTR_HI,
+	RDMA_WRITE_PTR_HI = TDMA_READ_PTR_HI,
+	TDMA_CONS_INDEX,
+	RDMA_PROD_INDEX = TDMA_CONS_INDEX,
+	TDMA_PROD_INDEX,
+	RDMA_CONS_INDEX = TDMA_PROD_INDEX,
+	DMA_RING_BUF_SIZE,
+	DMA_START_ADDR,
+	DMA_START_ADDR_HI,
+	DMA_END_ADDR,
+	DMA_END_ADDR_HI,
+	DMA_MBUF_DONE_THRESH,
+	TDMA_FLOW_PERIOD,
+	RDMA_XON_XOFF_THRESH = TDMA_FLOW_PERIOD,
+	TDMA_WRITE_PTR,
+	RDMA_READ_PTR = TDMA_WRITE_PTR,
+	TDMA_WRITE_PTR_HI,
+	RDMA_READ_PTR_HI = TDMA_WRITE_PTR_HI
+};
+
+/* GENET v4 supports 40-bits pointer addressing
+ * for obvious reasons the LO and HI word parts
+ * are contiguous, but this offsets the other
+ * registers.
+ */
+static const u8 genet_dma_ring_regs_v4[] = {
+	[TDMA_READ_PTR]			= 0x00,
+	[TDMA_READ_PTR_HI]		= 0x04,
+	[TDMA_CONS_INDEX]		= 0x08,
+	[TDMA_PROD_INDEX]		= 0x0C,
+	[DMA_RING_BUF_SIZE]		= 0x10,
+	[DMA_START_ADDR]		= 0x14,
+	[DMA_START_ADDR_HI]		= 0x18,
+	[DMA_END_ADDR]			= 0x1C,
+	[DMA_END_ADDR_HI]		= 0x20,
+	[DMA_MBUF_DONE_THRESH]		= 0x24,
+	[TDMA_FLOW_PERIOD]		= 0x28,
+	[TDMA_WRITE_PTR]		= 0x2C,
+	[TDMA_WRITE_PTR_HI]		= 0x30,
+};
+
+static const u8 genet_dma_ring_regs_v123[] = {
+	[TDMA_READ_PTR]			= 0x00,
+	[TDMA_CONS_INDEX]		= 0x04,
+	[TDMA_PROD_INDEX]		= 0x08,
+	[DMA_RING_BUF_SIZE]		= 0x0C,
+	[DMA_START_ADDR]		= 0x10,
+	[DMA_END_ADDR]			= 0x14,
+	[DMA_MBUF_DONE_THRESH]		= 0x18,
+	[TDMA_FLOW_PERIOD]		= 0x1C,
+	[TDMA_WRITE_PTR]		= 0x20,
+};
+
+/* Set at runtime once GENET version is known */
+static const u8 *genet_dma_ring_regs;
+
+static inline u32 bcmgenet_tdma_ring_readl(struct bcmgenet_priv *priv,
+					   unsigned int ring,
+					   enum dma_ring_reg r)
+{
+	return bcmgenet_readl(priv->base + GENET_TDMA_REG_OFF +
+			      (DMA_RING_SIZE * ring) +
+			      genet_dma_ring_regs[r]);
+}
+
+static inline void bcmgenet_tdma_ring_writel(struct bcmgenet_priv *priv,
+					     unsigned int ring, u32 val,
+					     enum dma_ring_reg r)
+{
+	bcmgenet_writel(val, priv->base + GENET_TDMA_REG_OFF +
+			(DMA_RING_SIZE * ring) +
+			genet_dma_ring_regs[r]);
+}
+
+static inline u32 bcmgenet_rdma_ring_readl(struct bcmgenet_priv *priv,
+					   unsigned int ring,
+					   enum dma_ring_reg r)
+{
+	return bcmgenet_readl(priv->base + GENET_RDMA_REG_OFF +
+			      (DMA_RING_SIZE * ring) +
+			      genet_dma_ring_regs[r]);
+}
+
+static inline void bcmgenet_rdma_ring_writel(struct bcmgenet_priv *priv,
+					     unsigned int ring, u32 val,
+					     enum dma_ring_reg r)
+{
+	bcmgenet_writel(val, priv->base + GENET_RDMA_REG_OFF +
+			(DMA_RING_SIZE * ring) +
+			genet_dma_ring_regs[r]);
+}
+
+#if 0
+static void bcmgenet_hfb_enable_filter(struct bcmgenet_priv *priv, u32 f_index)
+{
+	u32 offset;
+	u32 reg;
+
+	offset = HFB_FLT_ENABLE_V3PLUS + (f_index < 32) * sizeof(u32);
+	reg = bcmgenet_hfb_reg_readl(priv, offset);
+	reg |= (1 << (f_index % 32));
+	bcmgenet_hfb_reg_writel(priv, reg, offset);
+	reg = bcmgenet_hfb_reg_readl(priv, HFB_CTRL);
+	reg |= RBUF_HFB_EN;
+	bcmgenet_hfb_reg_writel(priv, reg, HFB_CTRL);
+}
+
+static void bcmgenet_hfb_disable_filter(struct bcmgenet_priv *priv, u32 f_index)
+{
+	u32 offset, reg, reg1;
+
+	offset = HFB_FLT_ENABLE_V3PLUS;
+	reg = bcmgenet_hfb_reg_readl(priv, offset);
+	reg1 = bcmgenet_hfb_reg_readl(priv, offset + sizeof(u32));
+	if  (f_index < 32) {
+		reg1 &= ~(1 << (f_index % 32));
+		bcmgenet_hfb_reg_writel(priv, reg1, offset + sizeof(u32));
+	} else {
+		reg &= ~(1 << (f_index % 32));
+		bcmgenet_hfb_reg_writel(priv, reg, offset);
+	}
+	if (!reg && !reg1) {
+		reg = bcmgenet_hfb_reg_readl(priv, HFB_CTRL);
+		reg &= ~RBUF_HFB_EN;
+		bcmgenet_hfb_reg_writel(priv, reg, HFB_CTRL);
+	}
+}
+
+static void bcmgenet_hfb_set_filter_rx_queue_mapping(struct bcmgenet_priv *priv,
+						     u32 f_index, u32 rx_queue)
+{
+	u32 offset;
+	u32 reg;
+
+	offset = f_index / 8;
+	reg = bcmgenet_rdma_readl(priv, DMA_INDEX2RING_0 + offset);
+	reg &= ~(0xF << (4 * (f_index % 8)));
+	reg |= ((rx_queue & 0xF) << (4 * (f_index % 8)));
+	bcmgenet_rdma_writel(priv, reg, DMA_INDEX2RING_0 + offset);
+}
+
+static void bcmgenet_hfb_set_filter_length(struct bcmgenet_priv *priv,
+					   u32 f_index, u32 f_length)
+{
+	u32 offset;
+	u32 reg;
+
+	offset = HFB_FLT_LEN_V3PLUS +
+		 ((priv->hw_params->hfb_filter_cnt - 1 - f_index) / 4) *
+		 sizeof(u32);
+	reg = bcmgenet_hfb_reg_readl(priv, offset);
+	reg &= ~(0xFF << (8 * (f_index % 4)));
+	reg |= ((f_length & 0xFF) << (8 * (f_index % 4)));
+	bcmgenet_hfb_reg_writel(priv, reg, offset);
+}
+
+static int bcmgenet_hfb_validate_mask(void *mask, size_t size)
+{
+	while (size) {
+		switch (*(unsigned char *)mask++) {
+		case 0x00:
+		case 0x0f:
+		case 0xf0:
+		case 0xff:
+			size--;
+			continue;
+		default:
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+#define VALIDATE_MASK(x) \
+	bcmgenet_hfb_validate_mask(&(x), sizeof(x))
+
+static int bcmgenet_hfb_insert_data(struct bcmgenet_priv *priv, u32 f_index,
+				    u32 offset, void *val, void *mask,
+				    size_t size)
+{
+	u32 index, tmp;
+
+	index = f_index * priv->hw_params->hfb_filter_size + offset / 2;
+	tmp = bcmgenet_hfb_readl(priv, index * sizeof(u32));
+
+	while (size--) {
+		if (offset++ & 1) {
+			tmp &= ~0x300FF;
+			tmp |= (*(unsigned char *)val++);
+			switch ((*(unsigned char *)mask++)) {
+			case 0xFF:
+				tmp |= 0x30000;
+				break;
+			case 0xF0:
+				tmp |= 0x20000;
+				break;
+			case 0x0F:
+				tmp |= 0x10000;
+				break;
+			}
+			bcmgenet_hfb_writel(priv, tmp, index++ * sizeof(u32));
+			if (size)
+				tmp = bcmgenet_hfb_readl(priv,
+							 index * sizeof(u32));
+		} else {
+			tmp &= ~0xCFF00;
+			tmp |= (*(unsigned char *)val++) << 8;
+			switch ((*(unsigned char *)mask++)) {
+			case 0xFF:
+				tmp |= 0xC0000;
+				break;
+			case 0xF0:
+				tmp |= 0x80000;
+				break;
+			case 0x0F:
+				tmp |= 0x40000;
+				break;
+			}
+			if (!size)
+				bcmgenet_hfb_writel(priv, tmp, index * sizeof(u32));
+		}
+	}
+
+	return 0;
+}
+
+static void bcmgenet_hfb_create_rxnfc_filter(struct bcmgenet_priv *priv,
+					     struct bcmgenet_rxnfc_rule *rule)
+{
+	struct ethtool_rx_flow_spec *fs = &rule->fs;
+	u32 offset = 0, f_length = 0, f;
+	u8 val_8, mask_8;
+	__be16 val_16;
+	u16 mask_16;
+	size_t size;
+
+	f = fs->location;
+	if (fs->flow_type & FLOW_MAC_EXT) {
+		bcmgenet_hfb_insert_data(priv, f, 0,
+					 &fs->h_ext.h_dest, &fs->m_ext.h_dest,
+					 sizeof(fs->h_ext.h_dest));
+	}
+
+	if (fs->flow_type & FLOW_EXT) {
+		if (fs->m_ext.vlan_etype ||
+		    fs->m_ext.vlan_tci) {
+			bcmgenet_hfb_insert_data(priv, f, 12,
+						 &fs->h_ext.vlan_etype,
+						 &fs->m_ext.vlan_etype,
+						 sizeof(fs->h_ext.vlan_etype));
+			bcmgenet_hfb_insert_data(priv, f, 14,
+						 &fs->h_ext.vlan_tci,
+						 &fs->m_ext.vlan_tci,
+						 sizeof(fs->h_ext.vlan_tci));
+			offset += VLAN_HLEN;
+			f_length += DIV_ROUND_UP(VLAN_HLEN, 2);
+		}
+	}
+
+	switch (fs->flow_type & ~(FLOW_EXT | FLOW_MAC_EXT)) {
+	case ETHER_FLOW:
+		f_length += DIV_ROUND_UP(ETH_HLEN, 2);
+		bcmgenet_hfb_insert_data(priv, f, 0,
+					 &fs->h_u.ether_spec.h_dest,
+					 &fs->m_u.ether_spec.h_dest,
+					 sizeof(fs->h_u.ether_spec.h_dest));
+		bcmgenet_hfb_insert_data(priv, f, ETH_ALEN,
+					 &fs->h_u.ether_spec.h_source,
+					 &fs->m_u.ether_spec.h_source,
+					 sizeof(fs->h_u.ether_spec.h_source));
+		bcmgenet_hfb_insert_data(priv, f, (2 * ETH_ALEN) + offset,
+					 &fs->h_u.ether_spec.h_proto,
+					 &fs->m_u.ether_spec.h_proto,
+					 sizeof(fs->h_u.ether_spec.h_proto));
+		break;
+	case IP_USER_FLOW:
+		f_length += DIV_ROUND_UP(ETH_HLEN + 20, 2);
+		/* Specify IP Ether Type */
+		val_16 = htons(ETH_P_IP);
+		mask_16 = 0xFFFF;
+		bcmgenet_hfb_insert_data(priv, f, (2 * ETH_ALEN) + offset,
+					 &val_16, &mask_16, sizeof(val_16));
+		bcmgenet_hfb_insert_data(priv, f, 15 + offset,
+					 &fs->h_u.usr_ip4_spec.tos,
+					 &fs->m_u.usr_ip4_spec.tos,
+					 sizeof(fs->h_u.usr_ip4_spec.tos));
+		bcmgenet_hfb_insert_data(priv, f, 23 + offset,
+					 &fs->h_u.usr_ip4_spec.proto,
+					 &fs->m_u.usr_ip4_spec.proto,
+					 sizeof(fs->h_u.usr_ip4_spec.proto));
+		bcmgenet_hfb_insert_data(priv, f, 26 + offset,
+					 &fs->h_u.usr_ip4_spec.ip4src,
+					 &fs->m_u.usr_ip4_spec.ip4src,
+					 sizeof(fs->h_u.usr_ip4_spec.ip4src));
+		bcmgenet_hfb_insert_data(priv, f, 30 + offset,
+					 &fs->h_u.usr_ip4_spec.ip4dst,
+					 &fs->m_u.usr_ip4_spec.ip4dst,
+					 sizeof(fs->h_u.usr_ip4_spec.ip4dst));
+		if (!fs->m_u.usr_ip4_spec.l4_4_bytes)
+			break;
+
+		/* Only supports 20 byte IPv4 header */
+		val_8 = 0x45;
+		mask_8 = 0xFF;
+		bcmgenet_hfb_insert_data(priv, f, ETH_HLEN + offset,
+					 &val_8, &mask_8,
+					 sizeof(val_8));
+		size = sizeof(fs->h_u.usr_ip4_spec.l4_4_bytes);
+		bcmgenet_hfb_insert_data(priv, f,
+					 ETH_HLEN + 20 + offset,
+					 &fs->h_u.usr_ip4_spec.l4_4_bytes,
+					 &fs->m_u.usr_ip4_spec.l4_4_bytes,
+					 size);
+		f_length += DIV_ROUND_UP(size, 2);
+		break;
+	}
+
+	bcmgenet_hfb_set_filter_length(priv, f, 2 * f_length);
+	if (!fs->ring_cookie || fs->ring_cookie == RX_CLS_FLOW_WAKE) {
+		/* Ring 0 flows can be handled by the default Descriptor Ring
+		 * We'll map them to ring 0, but don't enable the filter
+		 */
+		bcmgenet_hfb_set_filter_rx_queue_mapping(priv, f, 0);
+		rule->state = BCMGENET_RXNFC_STATE_DISABLED;
+	} else {
+		/* Other Rx rings are direct mapped here */
+		bcmgenet_hfb_set_filter_rx_queue_mapping(priv, f,
+							 fs->ring_cookie);
+		bcmgenet_hfb_enable_filter(priv, f);
+		rule->state = BCMGENET_RXNFC_STATE_ENABLED;
+	}
+}
+#endif
+
+/* bcmgenet_hfb_clear
+ *
+ * Clear Hardware Filter Block and disable all filtering.
+ */
+static void bcmgenet_hfb_clear_filter(struct bcmgenet_priv *priv, u32 f_index)
+{
+	u32 base, i;
+
+	base = f_index * priv->hw_params->hfb_filter_size;
+	for (i = 0; i < priv->hw_params->hfb_filter_size; i++)
+		bcmgenet_hfb_writel(priv, 0x0, (base + i) * sizeof(u32));
+}
+
+static void bcmgenet_hfb_clear(struct bcmgenet_priv *priv)
+{
+	u32 i;
+
+	if (GENET_IS_V1(priv) || GENET_IS_V2(priv))
+		return;
+
+	bcmgenet_hfb_reg_writel(priv, 0x0, HFB_CTRL);
+	bcmgenet_hfb_reg_writel(priv, 0x0, HFB_FLT_ENABLE_V3PLUS);
+	bcmgenet_hfb_reg_writel(priv, 0x0, HFB_FLT_ENABLE_V3PLUS + 4);
+
+	for (i = DMA_INDEX2RING_0; i <= DMA_INDEX2RING_7; i++)
+		bcmgenet_rdma_writel(priv, 0x0, i);
+
+	for (i = 0; i < (priv->hw_params->hfb_filter_cnt / 4); i++)
+		bcmgenet_hfb_reg_writel(priv, 0x0,
+					HFB_FLT_LEN_V3PLUS + i * sizeof(u32));
+
+	for (i = 0; i < priv->hw_params->hfb_filter_cnt; i++)
+		bcmgenet_hfb_clear_filter(priv, i);
+}
+
+static void bcmgenet_hfb_init(struct bcmgenet_priv *priv)
+{
+	int i;
+
+	INIT_LIST_HEAD(&priv->rxnfc_list);
+	if (GENET_IS_V1(priv) || GENET_IS_V2(priv))
+		return;
+
+	for (i = 0; i < MAX_NUM_OF_FS_RULES; i++) {
+		INIT_LIST_HEAD(&priv->rxnfc_rules[i].list);
+		priv->rxnfc_rules[i].state = BCMGENET_RXNFC_STATE_UNUSED;
+	}
+
+	bcmgenet_hfb_clear(priv);
+}
+
+#if 0
+static int bcmgenet_begin(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	/* Turn on the clock */
+	return clk_prepare_enable(priv->clk);
+}
+
+static void bcmgenet_complete(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	/* Turn off the clock */
+	clk_disable_unprepare(priv->clk);
+}
+
+static int bcmgenet_get_link_ksettings(struct net_device *dummy,
+				       struct ethtool_link_ksettings *cmd)
+{
+	struct dummy_rtnetdev_priv *priv = netdev_priv(dummy);
+	if (!rtnetif_running(priv->rtdev))
+		return -EINVAL;
+
+	if (!dummy->phydev)
+		return -ENODEV;
+
+	phy_ethtool_ksettings_get(dummy->phydev, cmd);
+
+	return 0;
+}
+
+static int bcmgenet_set_link_ksettings(struct net_device *dummy,
+				       const struct ethtool_link_ksettings *cmd)
+{
+	struct dummy_rtnetdev_priv *priv = netdev_priv(dummy);
+	if (!rtnetif_running(priv->rtdev))
+		return -EINVAL;
+
+	if (!priv->dummy->phydev)
+		return -ENODEV;
+
+	return phy_ethtool_ksettings_set(dummy->phydev, cmd);
+}
+#endif
+
+static int bcmgenet_set_features(struct rtnet_device *dev,
+				 netdev_features_t features)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	u32 reg;
+	int ret;
+
+	ret = clk_prepare_enable(priv->clk);
+	if (ret)
+		return ret;
+
+	/* Make sure we reflect the value of CRC_CMD_FWD */
+	reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+	priv->crc_fwd_en = !!(reg & CMD_CRC_FWD);
+
+	clk_disable_unprepare(priv->clk);
+
+	return ret;
+}
+
+#if 0
+static u32 bcmgenet_get_msglevel(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	return priv->msg_enable;
+}
+
+static void bcmgenet_set_msglevel(struct rtnet_device *dev, u32 level)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	priv->msg_enable = level;
+}
+
+
+static int bcmgenet_get_coalesce(struct rtnet_device *dev,
+				 struct ethtool_coalesce *ec)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct bcmgenet_rx_ring *ring;
+	unsigned int i;
+
+	ec->tx_max_coalesced_frames =
+		bcmgenet_tdma_ring_readl(priv, DESC_INDEX,
+					 DMA_MBUF_DONE_THRESH);
+	ec->rx_max_coalesced_frames =
+		bcmgenet_rdma_ring_readl(priv, DESC_INDEX,
+					 DMA_MBUF_DONE_THRESH);
+	ec->rx_coalesce_usecs =
+		bcmgenet_rdma_readl(priv, DMA_RING16_TIMEOUT) * 8192 / 1000;
+
+	for (i = 0; i < priv->hw_params->rx_queues; i++) {
+		ring = &priv->rx_rings[i];
+		ec->use_adaptive_rx_coalesce |= ring->dim.use_dim;
+	}
+	ring = &priv->rx_rings[DESC_INDEX];
+	ec->use_adaptive_rx_coalesce |= ring->dim.use_dim;
+
+	return 0;
+}
+#endif
+
+static void bcmgenet_set_rx_coalesce(struct bcmgenet_rx_ring *ring,
+				     u32 usecs, u32 pkts)
+{
+	struct bcmgenet_priv *priv = ring->priv;
+	unsigned int i = ring->index;
+	u32 reg;
+
+	bcmgenet_rdma_ring_writel(priv, i, pkts, DMA_MBUF_DONE_THRESH);
+
+	reg = bcmgenet_rdma_readl(priv, DMA_RING0_TIMEOUT + i);
+	reg &= ~DMA_TIMEOUT_MASK;
+	reg |= DIV_ROUND_UP(usecs * 1000, 8192);
+	bcmgenet_rdma_writel(priv, reg, DMA_RING0_TIMEOUT + i);
+}
+
+#if 0
+static void bcmgenet_set_ring_rx_coalesce(struct bcmgenet_rx_ring *ring,
+					  struct ethtool_coalesce *ec)
+{
+	struct dim_cq_moder moder;
+	u32 usecs, pkts;
+
+	ring->rx_coalesce_usecs = ec->rx_coalesce_usecs;
+	ring->rx_max_coalesced_frames = ec->rx_max_coalesced_frames;
+	usecs = ring->rx_coalesce_usecs;
+	pkts = ring->rx_max_coalesced_frames;
+
+	if (ec->use_adaptive_rx_coalesce && !ring->dim.use_dim) {
+		moder = net_dim_get_def_rx_moderation(ring->dim.dim.mode);
+		usecs = moder.usec;
+		pkts = moder.pkts;
+	}
+
+	ring->dim.use_dim = ec->use_adaptive_rx_coalesce;
+	bcmgenet_set_rx_coalesce(ring, usecs, pkts);
+}
+
+static int bcmgenet_set_coalesce(struct rtnet_device *dev,
+				 struct ethtool_coalesce *ec)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	unsigned int i;
+
+	/* Base system clock is 125Mhz, DMA timeout is this reference clock
+	 * divided by 1024, which yields roughly 8.192us, our maximum value
+	 * has to fit in the DMA_TIMEOUT_MASK (16 bits)
+	 */
+	if (ec->tx_max_coalesced_frames > DMA_INTR_THRESHOLD_MASK ||
+	    ec->tx_max_coalesced_frames == 0 ||
+	    ec->rx_max_coalesced_frames > DMA_INTR_THRESHOLD_MASK ||
+	    ec->rx_coalesce_usecs > (DMA_TIMEOUT_MASK * 8) + 1)
+		return -EINVAL;
+
+	if (ec->rx_coalesce_usecs == 0 && ec->rx_max_coalesced_frames == 0)
+		return -EINVAL;
+
+	/* GENET TDMA hardware does not support a configurable timeout, but will
+	 * always generate an interrupt either after MBDONE packets have been
+	 * transmitted, or when the ring is empty.
+	 */
+
+	/* Program all TX queues with the same values, as there is no
+	 * ethtool knob to do coalescing on a per-queue basis
+	 */
+	for (i = 0; i < priv->hw_params->tx_queues; i++)
+		bcmgenet_tdma_ring_writel(priv, i,
+					  ec->tx_max_coalesced_frames,
+					  DMA_MBUF_DONE_THRESH);
+	bcmgenet_tdma_ring_writel(priv, DESC_INDEX,
+				  ec->tx_max_coalesced_frames,
+				  DMA_MBUF_DONE_THRESH);
+
+	for (i = 0; i < priv->hw_params->rx_queues; i++)
+		bcmgenet_set_ring_rx_coalesce(&priv->rx_rings[i], ec);
+	bcmgenet_set_ring_rx_coalesce(&priv->rx_rings[DESC_INDEX], ec);
+
+	return 0;
+}
+#endif
+
+/* standard ethtool support functions. */
+enum bcmgenet_stat_type {
+	BCMGENET_STAT_NETDEV = -1,
+	BCMGENET_STAT_MIB_RX,
+	BCMGENET_STAT_MIB_TX,
+	BCMGENET_STAT_RUNT,
+	BCMGENET_STAT_MISC,
+	BCMGENET_STAT_SOFT,
+};
+
+struct bcmgenet_stats {
+	char stat_string[ETH_GSTRING_LEN];
+	int stat_sizeof;
+	int stat_offset;
+	enum bcmgenet_stat_type type;
+	/* reg offset from UMAC base for misc counters */
+	u16 reg_offset;
+};
+
+#define STAT_NETDEV(m) { \
+	.stat_string = __stringify(m), \
+	.stat_sizeof = sizeof(((struct net_device_stats *)0)->m), \
+	.stat_offset = offsetof(struct net_device_stats, m), \
+	.type = BCMGENET_STAT_NETDEV, \
+}
+
+#define STAT_GENET_MIB(str, m, _type) { \
+	.stat_string = str, \
+	.stat_sizeof = sizeof(((struct bcmgenet_priv *)0)->m), \
+	.stat_offset = offsetof(struct bcmgenet_priv, m), \
+	.type = _type, \
+}
+
+#define STAT_GENET_MIB_RX(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_MIB_RX)
+#define STAT_GENET_MIB_TX(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_MIB_TX)
+#define STAT_GENET_RUNT(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_RUNT)
+#define STAT_GENET_SOFT_MIB(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_SOFT)
+
+#define STAT_GENET_MISC(str, m, offset) { \
+	.stat_string = str, \
+	.stat_sizeof = sizeof(((struct bcmgenet_priv *)0)->m), \
+	.stat_offset = offsetof(struct bcmgenet_priv, m), \
+	.type = BCMGENET_STAT_MISC, \
+	.reg_offset = offset, \
+}
+
+#define STAT_GENET_Q(num) \
+	STAT_GENET_SOFT_MIB("txq" __stringify(num) "_packets", \
+			tx_rings[num].packets), \
+	STAT_GENET_SOFT_MIB("txq" __stringify(num) "_bytes", \
+			tx_rings[num].bytes), \
+	STAT_GENET_SOFT_MIB("rxq" __stringify(num) "_bytes", \
+			rx_rings[num].bytes),	 \
+	STAT_GENET_SOFT_MIB("rxq" __stringify(num) "_packets", \
+			rx_rings[num].packets), \
+	STAT_GENET_SOFT_MIB("rxq" __stringify(num) "_errors", \
+			rx_rings[num].errors), \
+	STAT_GENET_SOFT_MIB("rxq" __stringify(num) "_dropped", \
+			rx_rings[num].dropped)
+
+/* There is a 0xC gap between the end of RX and beginning of TX stats and then
+ * between the end of TX stats and the beginning of the RX RUNT
+ */
+#define BCMGENET_STAT_OFFSET	0xc
+
+/* Hardware counters must be kept in sync because the order/offset
+ * is important here (order in structure declaration = order in hardware)
+ */
+static const struct bcmgenet_stats bcmgenet_gstrings_stats[] = {
+	/* general stats */
+	STAT_NETDEV(rx_packets),
+	STAT_NETDEV(tx_packets),
+	STAT_NETDEV(rx_bytes),
+	STAT_NETDEV(tx_bytes),
+	STAT_NETDEV(rx_errors),
+	STAT_NETDEV(tx_errors),
+	STAT_NETDEV(rx_dropped),
+	STAT_NETDEV(tx_dropped),
+	STAT_NETDEV(multicast),
+	/* UniMAC RSV counters */
+	STAT_GENET_MIB_RX("rx_64_octets", mib.rx.pkt_cnt.cnt_64),
+	STAT_GENET_MIB_RX("rx_65_127_oct", mib.rx.pkt_cnt.cnt_127),
+	STAT_GENET_MIB_RX("rx_128_255_oct", mib.rx.pkt_cnt.cnt_255),
+	STAT_GENET_MIB_RX("rx_256_511_oct", mib.rx.pkt_cnt.cnt_511),
+	STAT_GENET_MIB_RX("rx_512_1023_oct", mib.rx.pkt_cnt.cnt_1023),
+	STAT_GENET_MIB_RX("rx_1024_1518_oct", mib.rx.pkt_cnt.cnt_1518),
+	STAT_GENET_MIB_RX("rx_vlan_1519_1522_oct", mib.rx.pkt_cnt.cnt_mgv),
+	STAT_GENET_MIB_RX("rx_1522_2047_oct", mib.rx.pkt_cnt.cnt_2047),
+	STAT_GENET_MIB_RX("rx_2048_4095_oct", mib.rx.pkt_cnt.cnt_4095),
+	STAT_GENET_MIB_RX("rx_4096_9216_oct", mib.rx.pkt_cnt.cnt_9216),
+	STAT_GENET_MIB_RX("rx_pkts", mib.rx.pkt),
+	STAT_GENET_MIB_RX("rx_bytes", mib.rx.bytes),
+	STAT_GENET_MIB_RX("rx_multicast", mib.rx.mca),
+	STAT_GENET_MIB_RX("rx_broadcast", mib.rx.bca),
+	STAT_GENET_MIB_RX("rx_fcs", mib.rx.fcs),
+	STAT_GENET_MIB_RX("rx_control", mib.rx.cf),
+	STAT_GENET_MIB_RX("rx_pause", mib.rx.pf),
+	STAT_GENET_MIB_RX("rx_unknown", mib.rx.uo),
+	STAT_GENET_MIB_RX("rx_align", mib.rx.aln),
+	STAT_GENET_MIB_RX("rx_outrange", mib.rx.flr),
+	STAT_GENET_MIB_RX("rx_code", mib.rx.cde),
+	STAT_GENET_MIB_RX("rx_carrier", mib.rx.fcr),
+	STAT_GENET_MIB_RX("rx_oversize", mib.rx.ovr),
+	STAT_GENET_MIB_RX("rx_jabber", mib.rx.jbr),
+	STAT_GENET_MIB_RX("rx_mtu_err", mib.rx.mtue),
+	STAT_GENET_MIB_RX("rx_good_pkts", mib.rx.pok),
+	STAT_GENET_MIB_RX("rx_unicast", mib.rx.uc),
+	STAT_GENET_MIB_RX("rx_ppp", mib.rx.ppp),
+	STAT_GENET_MIB_RX("rx_crc", mib.rx.rcrc),
+	/* UniMAC TSV counters */
+	STAT_GENET_MIB_TX("tx_64_octets", mib.tx.pkt_cnt.cnt_64),
+	STAT_GENET_MIB_TX("tx_65_127_oct", mib.tx.pkt_cnt.cnt_127),
+	STAT_GENET_MIB_TX("tx_128_255_oct", mib.tx.pkt_cnt.cnt_255),
+	STAT_GENET_MIB_TX("tx_256_511_oct", mib.tx.pkt_cnt.cnt_511),
+	STAT_GENET_MIB_TX("tx_512_1023_oct", mib.tx.pkt_cnt.cnt_1023),
+	STAT_GENET_MIB_TX("tx_1024_1518_oct", mib.tx.pkt_cnt.cnt_1518),
+	STAT_GENET_MIB_TX("tx_vlan_1519_1522_oct", mib.tx.pkt_cnt.cnt_mgv),
+	STAT_GENET_MIB_TX("tx_1522_2047_oct", mib.tx.pkt_cnt.cnt_2047),
+	STAT_GENET_MIB_TX("tx_2048_4095_oct", mib.tx.pkt_cnt.cnt_4095),
+	STAT_GENET_MIB_TX("tx_4096_9216_oct", mib.tx.pkt_cnt.cnt_9216),
+	STAT_GENET_MIB_TX("tx_pkts", mib.tx.pkts),
+	STAT_GENET_MIB_TX("tx_multicast", mib.tx.mca),
+	STAT_GENET_MIB_TX("tx_broadcast", mib.tx.bca),
+	STAT_GENET_MIB_TX("tx_pause", mib.tx.pf),
+	STAT_GENET_MIB_TX("tx_control", mib.tx.cf),
+	STAT_GENET_MIB_TX("tx_fcs_err", mib.tx.fcs),
+	STAT_GENET_MIB_TX("tx_oversize", mib.tx.ovr),
+	STAT_GENET_MIB_TX("tx_defer", mib.tx.drf),
+	STAT_GENET_MIB_TX("tx_excess_defer", mib.tx.edf),
+	STAT_GENET_MIB_TX("tx_single_col", mib.tx.scl),
+	STAT_GENET_MIB_TX("tx_multi_col", mib.tx.mcl),
+	STAT_GENET_MIB_TX("tx_late_col", mib.tx.lcl),
+	STAT_GENET_MIB_TX("tx_excess_col", mib.tx.ecl),
+	STAT_GENET_MIB_TX("tx_frags", mib.tx.frg),
+	STAT_GENET_MIB_TX("tx_total_col", mib.tx.ncl),
+	STAT_GENET_MIB_TX("tx_jabber", mib.tx.jbr),
+	STAT_GENET_MIB_TX("tx_bytes", mib.tx.bytes),
+	STAT_GENET_MIB_TX("tx_good_pkts", mib.tx.pok),
+	STAT_GENET_MIB_TX("tx_unicast", mib.tx.uc),
+	/* UniMAC RUNT counters */
+	STAT_GENET_RUNT("rx_runt_pkts", mib.rx_runt_cnt),
+	STAT_GENET_RUNT("rx_runt_valid_fcs", mib.rx_runt_fcs),
+	STAT_GENET_RUNT("rx_runt_inval_fcs_align", mib.rx_runt_fcs_align),
+	STAT_GENET_RUNT("rx_runt_bytes", mib.rx_runt_bytes),
+	/* Misc UniMAC counters */
+	STAT_GENET_MISC("rbuf_ovflow_cnt", mib.rbuf_ovflow_cnt,
+			UMAC_RBUF_OVFL_CNT_V1),
+	STAT_GENET_MISC("rbuf_err_cnt", mib.rbuf_err_cnt,
+			UMAC_RBUF_ERR_CNT_V1),
+	STAT_GENET_MISC("mdf_err_cnt", mib.mdf_err_cnt, UMAC_MDF_ERR_CNT),
+	STAT_GENET_SOFT_MIB("alloc_rx_buff_failed", mib.alloc_rx_buff_failed),
+	STAT_GENET_SOFT_MIB("rx_dma_failed", mib.rx_dma_failed),
+	STAT_GENET_SOFT_MIB("tx_dma_failed", mib.tx_dma_failed),
+	STAT_GENET_SOFT_MIB("tx_realloc_tsb", mib.tx_realloc_tsb),
+	STAT_GENET_SOFT_MIB("tx_realloc_tsb_failed",
+			    mib.tx_realloc_tsb_failed),
+	/* Per TX queues */
+	STAT_GENET_Q(0),
+	STAT_GENET_Q(1),
+	STAT_GENET_Q(2),
+	STAT_GENET_Q(3),
+	STAT_GENET_Q(16),
+};
+
+#define BCMGENET_STATS_LEN	ARRAY_SIZE(bcmgenet_gstrings_stats)
+
+#if 0
+static void bcmgenet_get_drvinfo(struct rtnet_device *dev,
+				 struct ethtool_drvinfo *info)
+{
+	strlcpy(info->driver, "bcmgenet", sizeof(info->driver));
+}
+
+static int bcmgenet_get_sset_count(struct rtnet_device *dev, int string_set)
+{
+	switch (string_set) {
+	case ETH_SS_STATS:
+		return BCMGENET_STATS_LEN;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void bcmgenet_get_strings(struct rtnet_device *dev, u32 stringset,
+				 u8 *data)
+{
+	int i;
+
+	switch (stringset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < BCMGENET_STATS_LEN; i++) {
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       bcmgenet_gstrings_stats[i].stat_string,
+			       ETH_GSTRING_LEN);
+		}
+		break;
+	}
+}
+
+static u32 bcmgenet_update_stat_misc(struct bcmgenet_priv *priv, u16 offset)
+{
+	u16 new_offset;
+	u32 val;
+
+	switch (offset) {
+	case UMAC_RBUF_OVFL_CNT_V1:
+		if (GENET_IS_V2(priv))
+			new_offset = RBUF_OVFL_CNT_V2;
+		else
+			new_offset = RBUF_OVFL_CNT_V3PLUS;
+
+		val = bcmgenet_rbuf_readl(priv,	new_offset);
+		/* clear if overflowed */
+		if (val == ~0)
+			bcmgenet_rbuf_writel(priv, 0, new_offset);
+		break;
+	case UMAC_RBUF_ERR_CNT_V1:
+		if (GENET_IS_V2(priv))
+			new_offset = RBUF_ERR_CNT_V2;
+		else
+			new_offset = RBUF_ERR_CNT_V3PLUS;
+
+		val = bcmgenet_rbuf_readl(priv,	new_offset);
+		/* clear if overflowed */
+		if (val == ~0)
+			bcmgenet_rbuf_writel(priv, 0, new_offset);
+		break;
+	default:
+		val = bcmgenet_umac_readl(priv, offset);
+		/* clear if overflowed */
+		if (val == ~0)
+			bcmgenet_umac_writel(priv, 0, offset);
+		break;
+	}
+
+	return val;
+}
+
+static void bcmgenet_update_mib_counters(struct bcmgenet_priv *priv)
+{
+	int i, j = 0;
+
+	for (i = 0; i < BCMGENET_STATS_LEN; i++) {
+		const struct bcmgenet_stats *s;
+		u8 offset = 0;
+		u32 val = 0;
+		char *p;
+
+		s = &bcmgenet_gstrings_stats[i];
+		switch (s->type) {
+		case BCMGENET_STAT_NETDEV:
+		case BCMGENET_STAT_SOFT:
+			continue;
+		case BCMGENET_STAT_RUNT:
+			offset += BCMGENET_STAT_OFFSET;
+			fallthrough;
+		case BCMGENET_STAT_MIB_TX:
+			offset += BCMGENET_STAT_OFFSET;
+			fallthrough;
+		case BCMGENET_STAT_MIB_RX:
+			val = bcmgenet_umac_readl(priv,
+						  UMAC_MIB_START + j + offset);
+			offset = 0;	/* Reset Offset */
+			break;
+		case BCMGENET_STAT_MISC:
+			if (GENET_IS_V1(priv)) {
+				val = bcmgenet_umac_readl(priv, s->reg_offset);
+				/* clear if overflowed */
+				if (val == ~0)
+					bcmgenet_umac_writel(priv, 0,
+							     s->reg_offset);
+			} else {
+				val = bcmgenet_update_stat_misc(priv,
+								s->reg_offset);
+			}
+			break;
+		}
+
+		j += s->stat_sizeof;
+		p = (char *)priv + s->stat_offset;
+		*(u32 *)p = val;
+	}
+}
+
+static void bcmgenet_get_ethtool_stats(struct rtnet_device *dev,
+				       struct ethtool_stats *stats,
+				       u64 *data)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	int i;
+
+	if (rtnetif_running(priv->dev))
+		bcmgenet_update_mib_counters(priv);
+
+	dev->netdev_ops->ndo_get_stats(dev);
+
+	for (i = 0; i < BCMGENET_STATS_LEN; i++) {
+		const struct bcmgenet_stats *s;
+		char *p;
+
+		s = &bcmgenet_gstrings_stats[i];
+		if (s->type == BCMGENET_STAT_NETDEV)
+			p = (char *)&priv->stats;
+		else
+			p = (char *)priv;
+		p += s->stat_offset;
+		if (sizeof(unsigned long) != sizeof(u32) &&
+		    s->stat_sizeof == sizeof(unsigned long))
+			data[i] = *(unsigned long *)p;
+		else
+			data[i] = *(u32 *)p;
+	}
+}
+
+static void bcmgenet_eee_enable_set(struct rtnet_device *dev, bool enable)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	u32 off = priv->hw_params->tbuf_offset + TBUF_ENERGY_CTRL;
+	u32 reg;
+
+	if (enable && !priv->clk_eee_enabled) {
+		clk_prepare_enable(priv->clk_eee);
+		priv->clk_eee_enabled = true;
+	}
+
+	reg = bcmgenet_umac_readl(priv, UMAC_EEE_CTRL);
+	if (enable)
+		reg |= EEE_EN;
+	else
+		reg &= ~EEE_EN;
+	bcmgenet_umac_writel(priv, reg, UMAC_EEE_CTRL);
+
+	/* Enable EEE and switch to a 27Mhz clock automatically */
+	reg = bcmgenet_readl(priv->base + off);
+	if (enable)
+		reg |= TBUF_EEE_EN | TBUF_PM_EN;
+	else
+		reg &= ~(TBUF_EEE_EN | TBUF_PM_EN);
+	bcmgenet_writel(reg, priv->base + off);
+
+	/* Do the same for thing for RBUF */
+	reg = bcmgenet_rbuf_readl(priv, RBUF_ENERGY_CTRL);
+	if (enable)
+		reg |= RBUF_EEE_EN | RBUF_PM_EN;
+	else
+		reg &= ~(RBUF_EEE_EN | RBUF_PM_EN);
+	bcmgenet_rbuf_writel(priv, reg, RBUF_ENERGY_CTRL);
+
+	if (!enable && priv->clk_eee_enabled) {
+		clk_disable_unprepare(priv->clk_eee);
+		priv->clk_eee_enabled = false;
+	}
+
+	priv->eee.eee_enabled = enable;
+	priv->eee.eee_active = enable;
+}
+
+static int bcmgenet_get_eee(struct rtnet_device *dev, struct ethtool_eee *e)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct ethtool_eee *p = &priv->eee;
+
+	if (GENET_IS_V1(priv))
+		return -EOPNOTSUPP;
+
+	if (!priv->dummy->phydev)
+		return -ENODEV;
+
+	e->eee_enabled = p->eee_enabled;
+	e->eee_active = p->eee_active;
+	e->tx_lpi_timer = bcmgenet_umac_readl(priv, UMAC_EEE_LPI_TIMER);
+
+	return phy_ethtool_get_eee(priv->dummy->phydev, e);
+}
+
+static int bcmgenet_set_eee(struct rtnet_device *dev, struct ethtool_eee *e)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct ethtool_eee *p = &priv->eee;
+	int ret = 0;
+
+	if (GENET_IS_V1(priv))
+		return -EOPNOTSUPP;
+
+	if (!priv->dummy->phydev)
+		return -ENODEV;
+
+	p->eee_enabled = e->eee_enabled;
+
+	if (!p->eee_enabled) {
+		bcmgenet_eee_enable_set(dev, false);
+	} else {
+		ret = phy_init_eee(priv->dummy->phydev, 0);
+		if (ret) {
+			netif_err(priv, hw, priv->dummy, "EEE initialization failed\n");
+			return ret;
+		}
+
+		bcmgenet_umac_writel(priv, e->tx_lpi_timer, UMAC_EEE_LPI_TIMER);
+		bcmgenet_eee_enable_set(dev, true);
+	}
+
+	return phy_ethtool_set_eee(priv->dummy->phydev, e);
+}
+
+static int bcmgenet_validate_flow(struct rtnet_device *dev,
+				  struct ethtool_rxnfc *cmd)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct ethtool_usrip4_spec *l4_mask;
+	struct ethhdr *eth_mask;
+
+	if (cmd->fs.location >= MAX_NUM_OF_FS_RULES) {
+		netdev_err(priv->dummy, "rxnfc: Invalid location (%d)\n",
+			   cmd->fs.location);
+		return -EINVAL;
+	}
+
+	switch (cmd->fs.flow_type & ~(FLOW_EXT | FLOW_MAC_EXT)) {
+	case IP_USER_FLOW:
+		l4_mask = &cmd->fs.m_u.usr_ip4_spec;
+		/* don't allow mask which isn't valid */
+		if (VALIDATE_MASK(l4_mask->ip4src) ||
+		    VALIDATE_MASK(l4_mask->ip4dst) ||
+		    VALIDATE_MASK(l4_mask->l4_4_bytes) ||
+		    VALIDATE_MASK(l4_mask->proto) ||
+		    VALIDATE_MASK(l4_mask->ip_ver) ||
+		    VALIDATE_MASK(l4_mask->tos)) {
+			netdev_err(priv->dummy, "rxnfc: Unsupported mask\n");
+			return -EINVAL;
+		}
+		break;
+	case ETHER_FLOW:
+		eth_mask = &cmd->fs.m_u.ether_spec;
+		/* don't allow mask which isn't valid */
+		if (VALIDATE_MASK(eth_mask->h_dest) ||
+		    VALIDATE_MASK(eth_mask->h_source) ||
+		    VALIDATE_MASK(eth_mask->h_proto)) {
+			netdev_err(priv->dummy, "rxnfc: Unsupported mask\n");
+			return -EINVAL;
+		}
+		break;
+	default:
+		netdev_err(priv->dummy, "rxnfc: Unsupported flow type (0x%x)\n",
+			   cmd->fs.flow_type);
+		return -EINVAL;
+	}
+
+	if ((cmd->fs.flow_type & FLOW_EXT)) {
+		/* don't allow mask which isn't valid */
+		if (VALIDATE_MASK(cmd->fs.m_ext.vlan_etype) ||
+		    VALIDATE_MASK(cmd->fs.m_ext.vlan_tci)) {
+			netdev_err(priv->dummy, "rxnfc: Unsupported mask\n");
+			return -EINVAL;
+		}
+		if (cmd->fs.m_ext.data[0] || cmd->fs.m_ext.data[1]) {
+			netdev_err(priv->dummy, "rxnfc: user-def not supported\n");
+			return -EINVAL;
+		}
+	}
+
+	if ((cmd->fs.flow_type & FLOW_MAC_EXT)) {
+		/* don't allow mask which isn't valid */
+		if (VALIDATE_MASK(cmd->fs.m_ext.h_dest)) {
+			netdev_err(priv->dummy, "rxnfc: Unsupported mask\n");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int bcmgenet_insert_flow(struct rtnet_device *dev,
+				struct ethtool_rxnfc *cmd)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct bcmgenet_rxnfc_rule *loc_rule;
+	int err;
+
+	if (priv->hw_params->hfb_filter_size < 128) {
+		netdev_err(priv->dummy, "rxnfc: Not supported by this device\n");
+		return -EINVAL;
+	}
+
+	if (cmd->fs.ring_cookie > priv->hw_params->rx_queues &&
+	    cmd->fs.ring_cookie != RX_CLS_FLOW_WAKE) {
+		netdev_err(priv->dummy, "rxnfc: Unsupported action (%llu)\n",
+			   cmd->fs.ring_cookie);
+		return -EINVAL;
+	}
+
+	err = bcmgenet_validate_flow(dev, cmd);
+	if (err)
+		return err;
+
+	loc_rule = &priv->rxnfc_rules[cmd->fs.location];
+	if (loc_rule->state == BCMGENET_RXNFC_STATE_ENABLED)
+		bcmgenet_hfb_disable_filter(priv, cmd->fs.location);
+	if (loc_rule->state != BCMGENET_RXNFC_STATE_UNUSED) {
+		list_del(&loc_rule->list);
+		bcmgenet_hfb_clear_filter(priv, cmd->fs.location);
+	}
+	loc_rule->state = BCMGENET_RXNFC_STATE_UNUSED;
+	memcpy(&loc_rule->fs, &cmd->fs,
+	       sizeof(struct ethtool_rx_flow_spec));
+
+	bcmgenet_hfb_create_rxnfc_filter(priv, loc_rule);
+
+	list_add_tail(&loc_rule->list, &priv->rxnfc_list);
+
+	return 0;
+}
+
+static int bcmgenet_delete_flow(struct rtnet_device *dev,
+				struct ethtool_rxnfc *cmd)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct bcmgenet_rxnfc_rule *rule;
+	int err = 0;
+
+	if (cmd->fs.location >= MAX_NUM_OF_FS_RULES)
+		return -EINVAL;
+
+	rule = &priv->rxnfc_rules[cmd->fs.location];
+	if (rule->state == BCMGENET_RXNFC_STATE_UNUSED) {
+		err =  -ENOENT;
+		goto out;
+	}
+
+	if (rule->state == BCMGENET_RXNFC_STATE_ENABLED)
+		bcmgenet_hfb_disable_filter(priv, cmd->fs.location);
+	if (rule->state != BCMGENET_RXNFC_STATE_UNUSED) {
+		list_del(&rule->list);
+		bcmgenet_hfb_clear_filter(priv, cmd->fs.location);
+	}
+	rule->state = BCMGENET_RXNFC_STATE_UNUSED;
+	memset(&rule->fs, 0, sizeof(struct ethtool_rx_flow_spec));
+
+out:
+	return err;
+}
+
+static int bcmgenet_set_rxnfc(struct rtnet_device *dev, struct ethtool_rxnfc *cmd)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	int err = 0;
+
+	switch (cmd->cmd) {
+	case ETHTOOL_SRXCLSRLINS:
+		err = bcmgenet_insert_flow(dev, cmd);
+		break;
+	case ETHTOOL_SRXCLSRLDEL:
+		err = bcmgenet_delete_flow(dev, cmd);
+		break;
+	default:
+		netdev_warn(priv->dummy, "Unsupported ethtool command. (%d)\n",
+			    cmd->cmd);
+		return -EINVAL;
+	}
+
+	return err;
+}
+
+static int bcmgenet_get_flow(struct rtnet_device *dev, struct ethtool_rxnfc *cmd,
+			     int loc)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct bcmgenet_rxnfc_rule *rule;
+	int err = 0;
+
+	if (loc < 0 || loc >= MAX_NUM_OF_FS_RULES)
+		return -EINVAL;
+
+	rule = &priv->rxnfc_rules[loc];
+	if (rule->state == BCMGENET_RXNFC_STATE_UNUSED)
+		err = -ENOENT;
+	else
+		memcpy(&cmd->fs, &rule->fs,
+		       sizeof(struct ethtool_rx_flow_spec));
+
+	return err;
+}
+
+static int bcmgenet_get_num_flows(struct bcmgenet_priv *priv)
+{
+	struct list_head *pos;
+	int res = 0;
+
+	list_for_each(pos, &priv->rxnfc_list)
+		res++;
+
+	return res;
+}
+
+static int bcmgenet_get_rxnfc(struct rtnet_device *dev, struct ethtool_rxnfc *cmd,
+			      u32 *rule_locs)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct bcmgenet_rxnfc_rule *rule;
+	int err = 0;
+	int i = 0;
+
+	switch (cmd->cmd) {
+	case ETHTOOL_GRXRINGS:
+		cmd->data = priv->hw_params->rx_queues ?: 1;
+		break;
+	case ETHTOOL_GRXCLSRLCNT:
+		cmd->rule_cnt = bcmgenet_get_num_flows(priv);
+		cmd->data = MAX_NUM_OF_FS_RULES;
+		break;
+	case ETHTOOL_GRXCLSRULE:
+		err = bcmgenet_get_flow(dev, cmd, cmd->fs.location);
+		break;
+	case ETHTOOL_GRXCLSRLALL:
+		list_for_each_entry(rule, &priv->rxnfc_list, list)
+			if (i < cmd->rule_cnt)
+				rule_locs[i++] = rule->fs.location;
+		cmd->rule_cnt = i;
+		cmd->data = MAX_NUM_OF_FS_RULES;
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+#endif
+
+#if 0
+/* standard ethtool support functions. */
+static const struct ethtool_ops bcmgenet_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_RX_USECS |
+				     ETHTOOL_COALESCE_MAX_FRAMES |
+				     ETHTOOL_COALESCE_USE_ADAPTIVE_RX,
+	.begin			= bcmgenet_begin,
+	.complete		= bcmgenet_complete,
+	.get_strings		= bcmgenet_get_strings,
+	.get_sset_count		= bcmgenet_get_sset_count,
+	.get_ethtool_stats	= bcmgenet_get_ethtool_stats,
+	.get_drvinfo		= bcmgenet_get_drvinfo,
+	.get_link		= ethtool_op_get_link,
+	.get_msglevel		= bcmgenet_get_msglevel,
+	.set_msglevel		= bcmgenet_set_msglevel,
+	.get_wol		= bcmgenet_get_wol,
+	.set_wol		= bcmgenet_set_wol,
+	.get_eee		= bcmgenet_get_eee,
+	.set_eee		= bcmgenet_set_eee,
+	.nway_reset		= phy_ethtool_nway_reset,
+	.get_coalesce		= bcmgenet_get_coalesce,
+	.set_coalesce		= bcmgenet_set_coalesce,
+	.get_link_ksettings	= bcmgenet_get_link_ksettings,
+	.set_link_ksettings	= bcmgenet_set_link_ksettings,
+	.get_ts_info		= ethtool_op_get_ts_info,
+	.get_rxnfc		= bcmgenet_get_rxnfc,
+	.set_rxnfc		= bcmgenet_set_rxnfc,
+};
+#endif
+
+/* Power down the unimac, based on mode. */
+static int bcmgenet_power_down(struct bcmgenet_priv *priv,
+				enum bcmgenet_power_mode mode)
+{
+	int ret = 0;
+	u32 reg;
+
+	switch (mode) {
+	case GENET_POWER_CABLE_SENSE:
+		phy_detach(priv->dummy->phydev);
+		break;
+
+	case GENET_POWER_WOL_MAGIC:
+		ret = bcmgenet_wol_power_down_cfg(priv, mode);
+		break;
+
+	case GENET_POWER_PASSIVE:
+		/* Power down LED */
+		if (priv->hw_params->flags & GENET_HAS_EXT) {
+			reg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);
+			if (GENET_IS_V5(priv))
+				reg |= EXT_PWR_DOWN_PHY_EN |
+				       EXT_PWR_DOWN_PHY_RD |
+				       EXT_PWR_DOWN_PHY_SD |
+				       EXT_PWR_DOWN_PHY_RX |
+				       EXT_PWR_DOWN_PHY_TX |
+				       EXT_IDDQ_GLBL_PWR;
+			else
+				reg |= EXT_PWR_DOWN_PHY;
+
+			reg |= (EXT_PWR_DOWN_DLL | EXT_PWR_DOWN_BIAS);
+			bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+
+			bcmgenet_phy_power_set(priv->dummy, false);
+		}
+		break;
+	default:
+		break;
+	}
+
+	return ret;
+}
+
+static void bcmgenet_power_up(struct bcmgenet_priv *priv,
+			      enum bcmgenet_power_mode mode)
+{
+	u32 reg;
+
+	if (!(priv->hw_params->flags & GENET_HAS_EXT))
+		return;
+
+	reg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);
+
+	switch (mode) {
+	case GENET_POWER_PASSIVE:
+		reg &= ~(EXT_PWR_DOWN_DLL | EXT_PWR_DOWN_BIAS);
+		if (GENET_IS_V5(priv)) {
+			reg &= ~(EXT_PWR_DOWN_PHY_EN |
+				 EXT_PWR_DOWN_PHY_RD |
+				 EXT_PWR_DOWN_PHY_SD |
+				 EXT_PWR_DOWN_PHY_RX |
+				 EXT_PWR_DOWN_PHY_TX |
+				 EXT_IDDQ_GLBL_PWR);
+			reg |=   EXT_PHY_RESET;
+			bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+			mdelay(1);
+
+			reg &=  ~EXT_PHY_RESET;
+		} else {
+			reg &= ~EXT_PWR_DOWN_PHY;
+			reg |= EXT_PWR_DN_EN_LD;
+		}
+		bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+		bcmgenet_phy_power_set(priv->dummy, true);
+		break;
+
+	case GENET_POWER_CABLE_SENSE:
+		/* enable APD */
+		if (!GENET_IS_V5(priv)) {
+			reg |= EXT_PWR_DN_EN_LD;
+			bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+		}
+		break;
+	case GENET_POWER_WOL_MAGIC:
+		bcmgenet_wol_power_up_cfg(priv, mode);
+		return;
+	default:
+		break;
+	}
+}
+
+static struct enet_cb *bcmgenet_get_txcb(struct bcmgenet_priv *priv,
+					 struct bcmgenet_tx_ring *ring)
+{
+	struct enet_cb *tx_cb_ptr;
+
+	tx_cb_ptr = ring->cbs;
+	tx_cb_ptr += ring->write_ptr - ring->cb_ptr;
+
+	/* Advancing local write pointer */
+	if (ring->write_ptr == ring->end_ptr)
+		ring->write_ptr = ring->cb_ptr;
+	else
+		ring->write_ptr++;
+
+	return tx_cb_ptr;
+}
+
+static struct enet_cb *bcmgenet_put_txcb(struct bcmgenet_priv *priv,
+					 struct bcmgenet_tx_ring *ring)
+{
+	struct enet_cb *tx_cb_ptr;
+
+	tx_cb_ptr = ring->cbs;
+	tx_cb_ptr += ring->write_ptr - ring->cb_ptr;
+
+	/* Rewinding local write pointer */
+	if (ring->write_ptr == ring->cb_ptr)
+		ring->write_ptr = ring->end_ptr;
+	else
+		ring->write_ptr--;
+
+	return tx_cb_ptr;
+}
+
+static inline void bcmgenet_rx_ring16_int_disable(struct bcmgenet_rx_ring *ring)
+{
+	bcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_RXDMA_DONE,
+				 INTRL2_CPU_MASK_SET);
+}
+
+static inline void bcmgenet_rx_ring16_int_enable(struct bcmgenet_rx_ring *ring)
+{
+	bcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_RXDMA_DONE,
+				 INTRL2_CPU_MASK_CLEAR);
+}
+
+static inline void bcmgenet_rx_ring_int_disable(struct bcmgenet_rx_ring *ring)
+{
+	bcmgenet_intrl2_1_writel(ring->priv,
+				 1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index),
+				 INTRL2_CPU_MASK_SET);
+}
+
+static inline void bcmgenet_rx_ring_int_enable(struct bcmgenet_rx_ring *ring)
+{
+	bcmgenet_intrl2_1_writel(ring->priv,
+				 1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index),
+				 INTRL2_CPU_MASK_CLEAR);
+}
+
+static inline void bcmgenet_tx_ring16_int_disable(struct bcmgenet_tx_ring *ring)
+{
+	bcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_TXDMA_DONE,
+				 INTRL2_CPU_MASK_SET);
+}
+
+static inline void bcmgenet_tx_ring16_int_enable(struct bcmgenet_tx_ring *ring)
+{
+	bcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_TXDMA_DONE,
+				 INTRL2_CPU_MASK_CLEAR);
+}
+
+static inline void bcmgenet_tx_ring_int_enable(struct bcmgenet_tx_ring *ring)
+{
+	bcmgenet_intrl2_1_writel(ring->priv, 1 << ring->index,
+				 INTRL2_CPU_MASK_CLEAR);
+}
+
+static inline void bcmgenet_tx_ring_int_disable(struct bcmgenet_tx_ring *ring)
+{
+	bcmgenet_intrl2_1_writel(ring->priv, 1 << ring->index,
+				 INTRL2_CPU_MASK_SET);
+}
+
+/* Simple helper to free a transmit control block's resources
+ * Returns an skb when the last transmit control block associated with the
+ * skb is freed.  The skb should be freed by the caller if necessary.
+ */
+static struct rtskb *bcmgenet_free_tx_cb(struct device *dev,
+					   struct enet_cb *cb)
+{
+	struct rtskb *skb;
+
+	skb = cb->skb;
+
+	if (skb) {
+		cb->skb = NULL;
+		if (cb == GENET_CB(skb)->first_cb)
+			dma_unmap_single(dev, dma_unmap_addr(cb, dma_addr),
+					 dma_unmap_len(cb, dma_len),
+					 DMA_TO_DEVICE);
+		else
+			dma_unmap_page(dev, dma_unmap_addr(cb, dma_addr),
+				       dma_unmap_len(cb, dma_len),
+				       DMA_TO_DEVICE);
+		dma_unmap_addr_set(cb, dma_addr, 0);
+
+		if (cb == GENET_CB(skb)->last_cb)
+			return skb;
+
+	} else if (dma_unmap_addr(cb, dma_addr)) {
+		dma_unmap_page(dev,
+			       dma_unmap_addr(cb, dma_addr),
+			       dma_unmap_len(cb, dma_len),
+			       DMA_TO_DEVICE);
+		dma_unmap_addr_set(cb, dma_addr, 0);
+	}
+
+	return NULL;
+}
+
+/* Simple helper to free a receive control block's resources */
+static struct rtskb *bcmgenet_free_rx_cb(struct device *dev,
+					   struct enet_cb *cb)
+{
+	struct rtskb *skb;
+
+	skb = cb->skb;
+	cb->skb = NULL;
+
+	if (dma_unmap_addr(cb, dma_addr)) {
+		dma_unmap_single(dev, dma_unmap_addr(cb, dma_addr),
+				 dma_unmap_len(cb, dma_len), DMA_FROM_DEVICE);
+		dma_unmap_addr_set(cb, dma_addr, 0);
+	}
+
+	return skb;
+}
+
+/* Unlocked version of the reclaim routine */
+static unsigned int __bcmgenet_tx_reclaim(struct rtnet_device *dev,
+					  struct bcmgenet_tx_ring *ring)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	unsigned int txbds_processed = 0;
+	unsigned int bytes_compl = 0;
+	unsigned int pkts_compl = 0;
+	unsigned int txbds_ready;
+	unsigned int c_index;
+	struct rtskb *skb;
+
+	/* Clear status before servicing to reduce spurious interrupts */
+	if (ring->index == DESC_INDEX)
+		bcmgenet_intrl2_0_writel(priv, UMAC_IRQ_TXDMA_DONE,
+					 INTRL2_CPU_CLEAR);
+	else
+		bcmgenet_intrl2_1_writel(priv, (1 << ring->index),
+					 INTRL2_CPU_CLEAR);
+
+	/* Compute how many buffers are transmitted since last xmit call */
+	c_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_CONS_INDEX)
+		& DMA_C_INDEX_MASK;
+	txbds_ready = (c_index - ring->c_index) & DMA_C_INDEX_MASK;
+
+	netif_dbg(priv, tx_done, priv->dummy,
+		  "%s ring=%d old_c_index=%u c_index=%u txbds_ready=%u\n",
+		  __func__, ring->index, ring->c_index, c_index, txbds_ready);
+
+	/* Reclaim transmitted buffers */
+	while (txbds_processed < txbds_ready) {
+		skb = bcmgenet_free_tx_cb(&priv->pdev->dev,
+					  &priv->tx_cbs[ring->clean_ptr]);
+		if (skb) {
+			pkts_compl++;
+			bytes_compl += GENET_CB(skb)->bytes_sent;
+			dev_kfree_rtskb(skb);
+		}
+
+		txbds_processed++;
+		if (likely(ring->clean_ptr < ring->end_ptr))
+			ring->clean_ptr++;
+		else
+			ring->clean_ptr = ring->cb_ptr;
+	}
+
+	ring->free_bds += txbds_processed;
+	ring->c_index = c_index;
+
+	ring->packets += pkts_compl;
+	ring->bytes += bytes_compl;
+
+	rtnetdev_tx_completed_queue(dev, rtnetdev_get_tx_queue(dev, ring->queue),
+				  pkts_compl, bytes_compl);
+
+	return txbds_processed;
+}
+
+static unsigned int bcmgenet_tx_reclaim(struct rtnet_device *dev,
+				struct bcmgenet_tx_ring *ring)
+{
+	unsigned int released;
+
+	/* initially was raw_spin_lock_bh */
+	raw_spin_lock(&ring->lock); 
+	released = __bcmgenet_tx_reclaim(dev, ring);
+	raw_spin_unlock(&ring->lock);
+
+	return released;
+}
+
+static int bcmgenet_tx_poll(void *vpriv)
+{
+	struct bcmgenet_priv *priv=vpriv;
+	struct bcmgenet_tx_ring *ring;
+	unsigned int work_done = 0;
+	struct netdev_queue *txq;
+
+	ring = &priv->tx_rings[DESC_INDEX];
+	
+    while (!stop_tx_task) {
+        if (rtdm_event_wait_one(&tx_event) < 0) {
+			printk(KERN_ERR "%s rtdm_event_wait_one error\n", __func__);
+            break;
+		}
+		
+		if(stop_tx_task)
+			break;
+		
+		raw_spin_lock(&ring->lock);
+		do {
+			work_done = __bcmgenet_tx_reclaim(ring->priv->dev, ring);
+			if (ring->free_bds > (MAX_SKB_FRAGS + 1)) {
+				txq = rtnetdev_get_tx_queue(ring->priv->dev, ring->queue);
+				rtnetif_tx_wake_queue(priv->dev, txq);
+			}
+		} while (work_done);
+		raw_spin_unlock(&ring->lock);
+	
+		ring->int_enable(ring);
+	}
+	
+	do_exit(0);
+    return 0;
+}
+
+static void bcmgenet_tx_reclaim_all(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	int i;
+
+	if (rtnetif_is_multiqueue(priv->dev)) {
+		for (i = 0; i < priv->hw_params->tx_queues; i++)
+			bcmgenet_tx_reclaim(dev, &priv->tx_rings[i]);
+	}
+
+	bcmgenet_tx_reclaim(dev, &priv->tx_rings[DESC_INDEX]);
+}
+
+/* Reallocate the SKB to put enough headroom in front of it and insert
+ * the transmit checksum offsets in the descriptors
+ */
+static struct rtskb *bcmgenet_add_tsb(struct rtnet_device *dev,
+					struct rtskb *skb)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct status_64 *status = NULL;
+	struct rtskb *new_skb;
+#if 0
+	u16 offset;
+	u8 ip_proto;
+	__be16 ip_ver;
+	u32 tx_csum_info;
+#endif
+	
+	/* RTnet is likely */
+	if (likely(rtskb_headroom(skb) < sizeof(*status))) {
+		/* If 64 byte status block enabled, must make sure skb has
+		 * enough headroom for us to insert 64B status block.
+		 */	
+		new_skb = rtskb_realloc_headroom(dev, skb, sizeof(*status));
+		if (!new_skb) {
+			dev_kfree_rtskb(skb);
+			priv->mib.tx_realloc_tsb_failed++;
+			priv->stats.tx_dropped++;
+			trace_printk("rtskb_realloc_headroom returns NULL\n");
+			return NULL;
+		}
+		dev_kfree_rtskb(skb);
+		skb = new_skb;
+		priv->mib.tx_realloc_tsb++;
+	}
+	
+	rtskb_push(skb, sizeof(*status));
+	status = (struct status_64 *)skb->data;
+
+	if (skb->ip_summed  == CHECKSUM_PARTIAL) {
+		/* RTnet: do not know how to set up rtskb->(csum_start and csum_offset) */
+		printk(KERN_ERR "skb->ip_summed  == CHECKSUM_PARTIAL\n");
+		return NULL;
+#if 0
+		ip_ver = skb->protocol;
+		switch (ip_ver) {
+		case htons(ETH_P_IP):
+			ip_proto = ip_hdr(skb)->protocol;
+			break;
+		case htons(ETH_P_IPV6):
+			ip_proto = ipv6_hdr(skb)->nexthdr;
+			break;
+		default:
+			/* don't use UDP flag */
+			ip_proto = 0;
+			break;
+		}
+
+		offset = skb_checksum_start_offset(skb) - sizeof(*status);
+		tx_csum_info = (offset << STATUS_TX_CSUM_START_SHIFT) |
+				(offset + skb->csum_offset) |
+				STATUS_TX_CSUM_LV;
+
+		/* Set the special UDP flag for UDP */
+		if (ip_proto == IPPROTO_UDP)
+			tx_csum_info |= STATUS_TX_CSUM_PROTO_UDP;
+
+		status->tx_csum_info = tx_csum_info;
+#endif
+	}
+	
+	return skb;
+}
+
+static netdev_tx_t bcmgenet_xmit(struct rtskb *skb, struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct device *kdev = &priv->pdev->dev;
+	struct bcmgenet_tx_ring *ring = NULL;
+	struct enet_cb *tx_cb_ptr;
+	struct netdev_queue *txq;
+	int nr_frags, index;
+	dma_addr_t mapping;
+	unsigned int size;
+	u32 len_stat;
+	int ret;
+	int i;
+#if 0	
+	unsigned int len = skb->len;
+	skb_frag_t *frag;
+#endif
+	
+	if(netif_carrier_ok(priv->dummy))
+		rtnetif_carrier_on(dev);
+	else
+		rtnetif_carrier_off(dev);
+	
+	index = 0;
+	/* Mapping strategy:
+	 * queue_mapping = 0, unclassified, packet xmited through ring16
+	 * queue_mapping = 1, goes to ring 0. (highest priority queue
+	 * queue_mapping = 2, goes to ring 1.
+	 * queue_mapping = 3, goes to ring 2.
+	 * queue_mapping = 4, goes to ring 3.
+	 */
+	if (index == 0)
+		index = DESC_INDEX;
+	else
+		index -= 1;
+
+	ring = &priv->tx_rings[index];
+	txq = rtnetdev_get_tx_queue(dev, ring->queue);
+
+#if 0
+	nr_frags = skb_shinfo(skb)->nr_frags;
+#endif
+	/* RTnet does not know about skb_shinfo */
+	nr_frags = 0;
+	
+	raw_spin_lock(&ring->lock);
+	if (ring->free_bds <= (nr_frags + 1)) {
+		trace_printk("%s ring->free_bds=%d\n", __func__, ring->free_bds);
+		if (!rtnetif_tx_queue_stopped(priv->dev, txq)) {
+			rtnetif_tx_stop_queue(priv->dev, txq);
+			netdev_err(priv->dummy,
+				   "%s: tx ring %d full when queue %d awake\n",
+				   __func__, index, ring->queue);
+		}
+		ret = NETDEV_TX_BUSY;
+		goto out;
+	}
+
+	/* Retain how many bytes will be sent on the wire, without TSB inserted
+	 * by transmit checksum offload
+	 */
+	GENET_CB(skb)->bytes_sent = skb->len;
+
+	/* add the Transmit Status Block */
+	skb = bcmgenet_add_tsb(dev, skb);
+	if (!skb) {
+		trace_printk("%s bcmgenet_add_tsb returns NULL\n", __func__);
+		ret = NETDEV_TX_OK;
+		goto out;
+	}
+	
+	for (i = 0; i <= nr_frags; i++) {
+		tx_cb_ptr = bcmgenet_get_txcb(priv, ring);
+
+		BUG_ON(!tx_cb_ptr);
+
+		if (!i) {
+			/* Transmit single SKB or head of fragment list */
+			GENET_CB(skb)->first_cb = tx_cb_ptr;
+			size = rtskb_headlen(skb);
+			mapping = dma_map_single(kdev, skb->data, size,
+						 DMA_TO_DEVICE);
+		}
+#if 0
+		/* RTnet does not know about skb_shinfo */
+		else {
+			/* xmit fragment */
+			frag = &skb_shinfo(skb)->frags[i - 1];
+			size = skb_frag_size(frag);
+			mapping = skb_frag_dma_map(kdev, frag, 0, size,
+						   DMA_TO_DEVICE);
+		}
+#endif
+		ret = dma_mapping_error(kdev, mapping);
+		if (ret) {
+			priv->mib.tx_dma_failed++;
+			printk(KERN_ERR "Tx DMA map failed\n");
+			ret = NETDEV_TX_OK;
+			goto out_unmap_frags;
+		}
+		dma_unmap_addr_set(tx_cb_ptr, dma_addr, mapping);
+		dma_unmap_len_set(tx_cb_ptr, dma_len, size);
+
+		tx_cb_ptr->skb = skb;
+
+		len_stat = (size << DMA_BUFLENGTH_SHIFT) |
+			   (priv->hw_params->qtag_mask << DMA_TX_QTAG_SHIFT);
+
+		/* Note: if we ever change from DMA_TX_APPEND_CRC below we
+		 * will need to restore software padding of "runt" packets
+		 */
+		if (!i) {
+			len_stat |= DMA_TX_APPEND_CRC | DMA_SOP;
+			if (skb->ip_summed == CHECKSUM_PARTIAL) {
+				trace_printk("%s skb->ip_summed == CHECKSUM_PARTIAL\n", __func__);
+				len_stat |= DMA_TX_DO_CSUM;
+			}
+		}
+		if (i == nr_frags)
+			len_stat |= DMA_EOP;
+
+		dmadesc_set(priv, tx_cb_ptr->bd_addr, mapping, len_stat);
+	}
+
+	GENET_CB(skb)->last_cb = tx_cb_ptr;
+	rtskb_tx_timestamp(skb);
+
+	/* Decrement total BD count and advance our write pointer */
+	ring->free_bds -= nr_frags + 1;
+	ring->prod_index += nr_frags + 1;
+	ring->prod_index &= DMA_P_INDEX_MASK;
+
+	rtnetdev_tx_sent_queue(txq, GENET_CB(skb)->bytes_sent);
+
+	if (ring->free_bds <= (MAX_SKB_FRAGS + 1))
+		rtnetif_tx_stop_queue(priv->dev, txq);
+
+#if 0
+	if (!netdev_xmit_more() || rtnetif_xmit_stopped(priv->dev, txq))
+#endif
+		/* Packets are ready, update producer index */
+		bcmgenet_tdma_ring_writel(priv, ring->index,
+					  ring->prod_index, TDMA_PROD_INDEX);
+out:
+	raw_spin_unlock(&ring->lock);
+
+	return ret;
+
+out_unmap_frags:
+	/* Back up for failed control block mapping */
+	bcmgenet_put_txcb(priv, ring);
+
+	/* Unmap successfully mapped control blocks */
+	while (i-- > 0) {
+		tx_cb_ptr = bcmgenet_put_txcb(priv, ring);
+		bcmgenet_free_tx_cb(kdev, tx_cb_ptr);
+	}
+
+	dev_kfree_rtskb(skb);
+	goto out;
+}
+
+static struct rtskb *bcmgenet_rx_refill(struct bcmgenet_priv *priv,
+					  struct enet_cb *cb)
+{
+	struct device *kdev = &priv->pdev->dev;
+	struct rtskb *skb;
+	struct rtskb *rx_skb;
+	dma_addr_t mapping;
+
+	/* Allocate a new Rx skb */
+	skb = rtnetdev_alloc_rtskb(priv->dev, priv->rx_buf_len + SKB_ALIGNMENT);
+	if (!skb) {
+		priv->mib.alloc_rx_buff_failed++;
+		printk(KERN_ERR "%s: Rx skb allocation failed\n", __func__);
+		return NULL;
+	}
+
+	/* DMA-map the new Rx skb */
+	mapping = dma_map_single(kdev, skb->data, priv->rx_buf_len,
+				 DMA_FROM_DEVICE);
+	if (dma_mapping_error(kdev, mapping)) {
+		priv->mib.rx_dma_failed++;
+		dev_kfree_rtskb(skb);
+		printk(KERN_ERR 
+			  "%s: Rx skb DMA mapping failed\n", __func__);
+		return NULL;
+	}
+
+	/* Grab the current Rx skb from the ring and DMA-unmap it */
+	rx_skb = bcmgenet_free_rx_cb(kdev, cb);
+	
+	/* Put the new Rx skb on the ring */
+	cb->skb = skb;
+	dma_unmap_addr_set(cb, dma_addr, mapping);
+	dma_unmap_len_set(cb, dma_len, priv->rx_buf_len);
+	dmadesc_set_addr(priv, cb->bd_addr, mapping);
+
+	/* Return the current Rx skb to caller */
+	return rx_skb;
+}
+
+/* bcmgenet_desc_rx - descriptor based rx process.
+ (old: from bottom half, or from NAPI polling method) 
+ */
+static unsigned int bcmgenet_desc_rx(struct bcmgenet_rx_ring *ring,
+				     unsigned int budget)
+{
+	struct bcmgenet_priv *priv = ring->priv;
+	struct rtnet_device *dev = priv->dev;
+	struct enet_cb *cb;
+	struct rtskb *skb;
+	u32 dma_length_status;
+	unsigned long dma_flag;
+	int len;
+	unsigned int rxpktprocessed = 0, rxpkttoprocess;
+	unsigned int bytes_processed = 0;
+	unsigned int p_index, mask;
+	unsigned int discards;
+
+	/* Clear status before servicing to reduce spurious interrupts */
+	if (ring->index == DESC_INDEX) {
+		bcmgenet_intrl2_0_writel(priv, UMAC_IRQ_RXDMA_DONE,
+					 INTRL2_CPU_CLEAR);
+	} else {
+		mask = 1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index);
+		bcmgenet_intrl2_1_writel(priv,
+					 mask,
+					 INTRL2_CPU_CLEAR);
+	}
+
+	p_index = bcmgenet_rdma_ring_readl(priv, ring->index, RDMA_PROD_INDEX);
+
+	discards = (p_index >> DMA_P_INDEX_DISCARD_CNT_SHIFT) &
+		   DMA_P_INDEX_DISCARD_CNT_MASK;
+	if (discards > ring->old_discards) {
+		discards = discards - ring->old_discards;
+		ring->errors += discards;
+		ring->old_discards += discards;
+
+		/* Clear HW register when we reach 75% of maximum 0xFFFF */
+		if (ring->old_discards >= 0xC000) {
+			ring->old_discards = 0;
+			bcmgenet_rdma_ring_writel(priv, ring->index, 0,
+						  RDMA_PROD_INDEX);
+		}
+	}
+
+	p_index &= DMA_P_INDEX_MASK;
+	rxpkttoprocess = (p_index - ring->c_index) & DMA_C_INDEX_MASK;
+
+	netif_dbg(priv, rx_status, priv->dummy,
+		  "RDMA: rxpkttoprocess=%d\n", rxpkttoprocess);
+
+	while ((rxpktprocessed < rxpkttoprocess) &&
+	       (rxpktprocessed < budget)) {
+		struct status_64 *status;
+		__be16 rx_csum;
+
+		cb = &priv->rx_cbs[ring->read_ptr];
+		skb = bcmgenet_rx_refill(priv, cb);
+
+		if (unlikely(!skb)) {
+			ring->dropped++;
+			goto next;
+		}
+
+		status = (struct status_64 *)skb->data;
+		dma_length_status = status->length_status;
+		if (dev->features & NETIF_F_RXCSUM) {
+			rx_csum = (__force __be16)(status->rx_csum & 0xffff);
+			skb->csum = (__force __wsum)ntohs(rx_csum);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+		}
+
+		/* DMA flags and length are still valid no matter how
+		 * we got the Receive Status Vector (64B RSB or register)
+		 */
+		dma_flag = dma_length_status & 0xffff;
+		len = dma_length_status >> DMA_BUFLENGTH_SHIFT;
+
+		netif_dbg(priv, rx_status, priv->dummy,
+			  "%s:p_ind=%d c_ind=%d read_ptr=%d len_stat=0x%08x\n",
+			  __func__, p_index, ring->c_index,
+			  ring->read_ptr, dma_length_status);
+
+		if (unlikely(!(dma_flag & DMA_EOP) || !(dma_flag & DMA_SOP))) {
+			netif_err(priv, rx_status, priv->dummy,
+				  "dropping fragmented packet!\n");
+			ring->errors++;
+			dev_kfree_rtskb(skb);
+			goto next;
+		}
+
+		/* report errors */
+		if (unlikely(dma_flag & (DMA_RX_CRC_ERROR |
+						DMA_RX_OV |
+						DMA_RX_NO |
+						DMA_RX_LG |
+						DMA_RX_RXER))) {
+			netif_err(priv, rx_status, priv->dummy, "dma_flag=0x%x\n",
+				  (unsigned int)dma_flag);
+			if (dma_flag & DMA_RX_CRC_ERROR)
+				priv->stats.rx_crc_errors++;
+			if (dma_flag & DMA_RX_OV)
+				priv->stats.rx_over_errors++;
+			if (dma_flag & DMA_RX_NO)
+				priv->stats.rx_frame_errors++;
+			if (dma_flag & DMA_RX_LG)
+				priv->stats.rx_length_errors++;
+			priv->stats.rx_errors++;
+			dev_kfree_rtskb(skb);
+			goto next;
+		} /* error packet */
+
+		rtskb_put(skb, len);
+
+		/* remove RSB and hardware 2bytes added for IP alignment */
+		rtskb_pull(skb, 66);
+		len -= 66;
+
+		if (priv->crc_fwd_en) {
+			rtskb_trim(skb, len - ETH_FCS_LEN);
+			len -= ETH_FCS_LEN;
+		}
+
+		bytes_processed += len;
+
+		/*Finish setting up the received SKB and send it to the kernel*/
+		skb->protocol = rt_eth_type_trans(skb, priv->dev);
+		ring->packets++;
+		ring->bytes += len;
+		if (dma_flag & DMA_RX_MULT)
+			priv->stats.multicast++;
+
+		/* Notify kernel */
+		rtnetif_rx(skb);
+		netif_dbg(priv, rx_status, priv->dummy, "pushed up to kernel\n");
+
+next:
+		rxpktprocessed++;
+		if (likely(ring->read_ptr < ring->end_ptr))
+			ring->read_ptr++;
+		else
+			ring->read_ptr = ring->cb_ptr;
+
+		ring->c_index = (ring->c_index + 1) & DMA_C_INDEX_MASK;
+		bcmgenet_rdma_ring_writel(priv, ring->index, ring->c_index, RDMA_CONS_INDEX);
+	}
+
+	ring->dim.bytes = bytes_processed;
+	ring->dim.packets = rxpktprocessed;
+
+	return rxpktprocessed;
+}
+
+static int bcmgenet_rx_poll(void *vpriv)
+{
+	struct bcmgenet_priv *priv=vpriv;
+	struct bcmgenet_rx_ring *ring;
+	struct dim_sample dim_sample = {};
+	unsigned int work_done;
+	int poll_weight = NAPI_POLL_WEIGHT;
+
+	ring = &priv->rx_rings[DESC_INDEX];
+
+    while (!stop_rx_task) {
+        if (rtdm_event_wait_one(&rx_event) < 0) {
+			printk(KERN_ERR "%s rtdm_event_wait_one error\n", __func__);
+            break;
+		}
+		
+		if(stop_rx_task)
+			break;
+	
+		do {
+			work_done = bcmgenet_desc_rx(ring, poll_weight);
+			if(work_done)
+				rt_mark_stack_mgr(priv->dev);
+		} while(work_done >= poll_weight);
+	
+		ring->int_enable(ring);
+	
+		if (ring->dim.use_dim) {
+			dim_update_sample(ring->dim.event_ctr, ring->dim.packets,
+				  ring->dim.bytes, &dim_sample);
+			net_dim(&ring->dim.dim, dim_sample);
+		}
+	}
+
+	do_exit(0);
+    return 0;
+}
+
+static void bcmgenet_dim_work(struct work_struct *work)
+{
+	struct dim *dim = container_of(work, struct dim, work);
+	struct bcmgenet_net_dim *ndim =
+			container_of(dim, struct bcmgenet_net_dim, dim);
+	struct bcmgenet_rx_ring *ring =
+			container_of(ndim, struct bcmgenet_rx_ring, dim);
+	struct dim_cq_moder cur_profile =
+			net_dim_get_rx_moderation(dim->mode, dim->profile_ix);
+
+	bcmgenet_set_rx_coalesce(ring, cur_profile.usec, cur_profile.pkts);
+	dim->state = DIM_START_MEASURE;
+}
+
+/* Assign skb to RX DMA descriptor. */
+static int bcmgenet_alloc_rx_buffers(struct bcmgenet_priv *priv,
+				     struct bcmgenet_rx_ring *ring)
+{
+	struct enet_cb *cb;
+	struct rtskb *skb;
+	int i;
+
+	netif_dbg(priv, hw, priv->dummy, "%s\n", __func__);
+
+	/* loop here for each buffer needing assign */
+	for (i = 0; i < ring->size; i++) {
+		cb = ring->cbs + i;
+		skb = bcmgenet_rx_refill(priv, cb);
+		if (skb)
+			dev_kfree_rtskb(skb);
+		if (!cb->skb)
+			return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void bcmgenet_free_rx_buffers(struct bcmgenet_priv *priv)
+{
+	struct rtskb *skb;
+	struct enet_cb *cb;
+	int i;
+
+	for (i = 0; i < priv->num_rx_bds; i++) {
+		cb = &priv->rx_cbs[i];
+
+		skb = bcmgenet_free_rx_cb(&priv->pdev->dev, cb);
+		if (skb)
+			dev_kfree_rtskb(skb);
+	}
+}
+
+static void umac_enable_set(struct bcmgenet_priv *priv, u32 mask, bool enable)
+{
+	u32 reg;
+
+	reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+	if (reg & CMD_SW_RESET)
+		return;
+	if (enable)
+		reg |= mask;
+	else
+		reg &= ~mask;
+	bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+
+	/* UniMAC stops on a packet boundary, wait for a full-size packet
+	 * to be processed
+	 */
+	if (enable == 0)
+		usleep_range(1000, 2000);
+}
+
+static void reset_umac(struct bcmgenet_priv *priv)
+{
+	/* 7358a0/7552a0: bad default in RBUF_FLUSH_CTRL.umac_sw_rst */
+	bcmgenet_rbuf_ctrl_set(priv, 0);
+	udelay(10);
+
+	/* issue soft reset and disable MAC while updating its registers */
+	bcmgenet_umac_writel(priv, CMD_SW_RESET, UMAC_CMD);
+	udelay(2);
+}
+
+static void bcmgenet_intr_disable(struct bcmgenet_priv *priv)
+{
+	/* Mask all interrupts.*/
+	bcmgenet_intrl2_0_writel(priv, 0xFFFFFFFF, INTRL2_CPU_MASK_SET);
+	bcmgenet_intrl2_0_writel(priv, 0xFFFFFFFF, INTRL2_CPU_CLEAR);
+	bcmgenet_intrl2_1_writel(priv, 0xFFFFFFFF, INTRL2_CPU_MASK_SET);
+	bcmgenet_intrl2_1_writel(priv, 0xFFFFFFFF, INTRL2_CPU_CLEAR);
+}
+
+static void bcmgenet_link_intr_enable(struct bcmgenet_priv *priv)
+{
+	u32 int0_enable = 0;
+
+	/* Monitor cable plug/unplugged event for internal PHY, external PHY
+	 * and MoCA PHY
+	 */
+	if (priv->internal_phy) {
+		int0_enable |= UMAC_IRQ_LINK_EVENT;
+		if (GENET_IS_V1(priv) || GENET_IS_V2(priv) || GENET_IS_V3(priv))
+			int0_enable |= UMAC_IRQ_PHY_DET_R;
+	} else if (priv->ext_phy) {
+		int0_enable |= UMAC_IRQ_LINK_EVENT;
+	} else if (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {
+		if (priv->hw_params->flags & GENET_HAS_MOCA_LINK_DET)
+			int0_enable |= UMAC_IRQ_LINK_EVENT;
+	}
+	bcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);
+}
+
+static void init_umac(struct bcmgenet_priv *priv)
+{
+	struct device *kdev = &priv->pdev->dev;
+	u32 reg;
+	u32 int0_enable = 0;
+
+	dev_dbg(&priv->pdev->dev, "bcmgenet: init_umac\n");
+
+	reset_umac(priv);
+
+	/* clear tx/rx counter */
+	bcmgenet_umac_writel(priv,
+			     MIB_RESET_RX | MIB_RESET_TX | MIB_RESET_RUNT,
+			     UMAC_MIB_CTRL);
+	bcmgenet_umac_writel(priv, 0, UMAC_MIB_CTRL);
+
+	bcmgenet_umac_writel(priv, ENET_MAX_MTU_SIZE, UMAC_MAX_FRAME_LEN);
+
+	/* init tx registers, enable TSB */
+	reg = bcmgenet_tbuf_ctrl_get(priv);
+	reg |= TBUF_64B_EN;
+	bcmgenet_tbuf_ctrl_set(priv, reg);
+
+	/* init rx registers, enable ip header optimization and RSB */
+	reg = bcmgenet_rbuf_readl(priv, RBUF_CTRL);
+	reg |= RBUF_ALIGN_2B | RBUF_64B_EN;
+	bcmgenet_rbuf_writel(priv, reg, RBUF_CTRL);
+
+	/* enable rx checksumming */
+	reg = bcmgenet_rbuf_readl(priv, RBUF_CHK_CTRL);
+	reg |= RBUF_RXCHK_EN | RBUF_L3_PARSE_DIS;
+	/* If UniMAC forwards CRC, we need to skip over it to get
+	 * a valid CHK bit to be set in the per-packet status word
+	 */
+	if (priv->crc_fwd_en)
+		reg |= RBUF_SKIP_FCS;
+	else
+		reg &= ~RBUF_SKIP_FCS;
+	bcmgenet_rbuf_writel(priv, reg, RBUF_CHK_CTRL);
+
+	if (!GENET_IS_V1(priv) && !GENET_IS_V2(priv))
+		bcmgenet_rbuf_writel(priv, 1, RBUF_TBUF_SIZE_CTRL);
+
+	bcmgenet_intr_disable(priv);
+
+	/* Configure backpressure vectors for MoCA */
+	if (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {
+		reg = bcmgenet_bp_mc_get(priv);
+		reg |= BIT(priv->hw_params->bp_in_en_shift);
+
+		/* bp_mask: back pressure mask */
+		if (rtnetif_is_multiqueue(priv->dev))
+			reg |= priv->hw_params->bp_in_mask;
+		else
+			reg &= ~priv->hw_params->bp_in_mask;
+		bcmgenet_bp_mc_set(priv, reg);
+	}
+
+	/* Enable MDIO interrupts on GENET v3+ */
+	if (priv->hw_params->flags & GENET_HAS_MDIO_INTR)
+		int0_enable |= (UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR);
+
+	bcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);
+
+	dev_dbg(kdev, "done init umac\n");
+}
+
+static void bcmgenet_init_dim(struct bcmgenet_rx_ring *ring,
+			      void (*cb)(struct work_struct *work))
+{
+	struct bcmgenet_net_dim *dim = &ring->dim;
+
+	INIT_WORK(&dim->dim.work, cb);
+	dim->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;
+	dim->event_ctr = 0;
+	dim->packets = 0;
+	dim->bytes = 0;
+}
+
+static void bcmgenet_init_rx_coalesce(struct bcmgenet_rx_ring *ring)
+{
+	struct bcmgenet_net_dim *dim = &ring->dim;
+	struct dim_cq_moder moder;
+	u32 usecs, pkts;
+
+	usecs = ring->rx_coalesce_usecs;
+	pkts = ring->rx_max_coalesced_frames;
+
+	/* If DIM was enabled, re-apply default parameters */
+	if (dim->use_dim) {
+		moder = net_dim_get_def_rx_moderation(dim->dim.mode);
+		usecs = moder.usec;
+		pkts = moder.pkts;
+	}
+
+	bcmgenet_set_rx_coalesce(ring, usecs, pkts);
+}
+
+/* Initialize a Tx ring along with corresponding hardware registers */
+static void bcmgenet_init_tx_ring(struct bcmgenet_priv *priv,
+				  unsigned int index, unsigned int size,
+				  unsigned int start_ptr, unsigned int end_ptr)
+{
+	struct bcmgenet_tx_ring *ring = &priv->tx_rings[index];
+	u32 words_per_bd = WORDS_PER_BD(priv);
+	u32 flow_period_val = 0;
+
+	raw_spin_lock_init(&ring->lock);
+	ring->priv = priv;
+	ring->index = index;
+	if (index == DESC_INDEX) {
+		ring->queue = 0;
+		ring->int_enable = bcmgenet_tx_ring16_int_enable;
+		ring->int_disable = bcmgenet_tx_ring16_int_disable;
+	} else {
+		ring->queue = index + 1;
+		ring->int_enable = bcmgenet_tx_ring_int_enable;
+		ring->int_disable = bcmgenet_tx_ring_int_disable;
+	}
+	ring->cbs = priv->tx_cbs + start_ptr;
+	ring->size = size;
+	ring->clean_ptr = start_ptr;
+	ring->c_index = 0;
+	ring->free_bds = size;
+	ring->write_ptr = start_ptr;
+	ring->cb_ptr = start_ptr;
+	ring->end_ptr = end_ptr - 1;
+	ring->prod_index = 0;
+
+	/* Set flow period for ring != 16 */
+	if (index != DESC_INDEX)
+		flow_period_val = ENET_MAX_MTU_SIZE << 16;
+
+	bcmgenet_tdma_ring_writel(priv, index, 0, TDMA_PROD_INDEX);
+	bcmgenet_tdma_ring_writel(priv, index, 0, TDMA_CONS_INDEX);
+	bcmgenet_tdma_ring_writel(priv, index, 1, DMA_MBUF_DONE_THRESH);
+	/* Disable rate control for now */
+	bcmgenet_tdma_ring_writel(priv, index, flow_period_val,
+				  TDMA_FLOW_PERIOD);
+	bcmgenet_tdma_ring_writel(priv, index,
+				  ((size << DMA_RING_SIZE_SHIFT) |
+				   RX_BUF_LENGTH), DMA_RING_BUF_SIZE);
+
+	/* Set start and end address, read and write pointers */
+	bcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,
+				  DMA_START_ADDR);
+	bcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,
+				  TDMA_READ_PTR);
+	bcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,
+				  TDMA_WRITE_PTR);
+	bcmgenet_tdma_ring_writel(priv, index, end_ptr * words_per_bd - 1,
+				  DMA_END_ADDR);
+}
+
+/* Initialize a RDMA ring */
+static int bcmgenet_init_rx_ring(struct bcmgenet_priv *priv,
+				 unsigned int index, unsigned int size,
+				 unsigned int start_ptr, unsigned int end_ptr)
+{
+	struct bcmgenet_rx_ring *ring = &priv->rx_rings[index];
+	u32 words_per_bd = WORDS_PER_BD(priv);
+	int ret;
+
+	ring->priv = priv;
+	ring->index = index;
+	if (index == DESC_INDEX) {
+		ring->int_enable = bcmgenet_rx_ring16_int_enable;
+		ring->int_disable = bcmgenet_rx_ring16_int_disable;
+	} else {
+		ring->int_enable = bcmgenet_rx_ring_int_enable;
+		ring->int_disable = bcmgenet_rx_ring_int_disable;
+	}
+	ring->cbs = priv->rx_cbs + start_ptr;
+	ring->size = size;
+	ring->c_index = 0;
+	ring->read_ptr = start_ptr;
+	ring->cb_ptr = start_ptr;
+	ring->end_ptr = end_ptr - 1;
+
+	ret = bcmgenet_alloc_rx_buffers(priv, ring);
+	if (ret) {
+		printk(KERN_ERR "bcmgenet_alloc_rx_buffers ret=%d\n", ret);
+		return ret;
+	}
+
+	bcmgenet_init_dim(ring, bcmgenet_dim_work);
+	bcmgenet_init_rx_coalesce(ring);
+
+	bcmgenet_rdma_ring_writel(priv, index, 0, RDMA_PROD_INDEX);
+	bcmgenet_rdma_ring_writel(priv, index, 0, RDMA_CONS_INDEX);
+	bcmgenet_rdma_ring_writel(priv, index,
+				  ((size << DMA_RING_SIZE_SHIFT) |
+				   RX_BUF_LENGTH), DMA_RING_BUF_SIZE);
+	bcmgenet_rdma_ring_writel(priv, index,
+				  (DMA_FC_THRESH_LO <<
+				   DMA_XOFF_THRESHOLD_SHIFT) |
+				   DMA_FC_THRESH_HI, RDMA_XON_XOFF_THRESH);
+
+	/* Set start and end address, read and write pointers */
+	bcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,
+				  DMA_START_ADDR);
+	bcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,
+				  RDMA_READ_PTR);
+	bcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,
+				  RDMA_WRITE_PTR);
+	bcmgenet_rdma_ring_writel(priv, index, end_ptr * words_per_bd - 1,
+				  DMA_END_ADDR);
+
+	return ret;
+}
+
+static void bcmgenet_enable_tx_napi(struct bcmgenet_priv *priv)
+{
+	unsigned int i;
+	struct bcmgenet_tx_ring *ring;
+
+	for (i = 0; i < priv->hw_params->tx_queues; ++i) {
+		ring = &priv->tx_rings[i];
+		ring->int_enable(ring);
+	}
+
+	ring = &priv->tx_rings[DESC_INDEX];
+	ring->int_enable(ring);
+}
+
+/* Initialize Tx queues
+ *
+ * Queues 0-3 are priority-based, each one has 32 descriptors,
+ * with queue 0 being the highest priority queue.
+ *
+ * Queue 16 is the default Tx queue with
+ * GENET_Q16_TX_BD_CNT = 256 - 4 * 32 = 128 descriptors.
+ *
+ * The transmit control block pool is then partitioned as follows:
+ * - Tx queue 0 uses tx_cbs[0..31]
+ * - Tx queue 1 uses tx_cbs[32..63]
+ * - Tx queue 2 uses tx_cbs[64..95]
+ * - Tx queue 3 uses tx_cbs[96..127]
+ * - Tx queue 16 uses tx_cbs[128..255]
+ */
+static void bcmgenet_init_tx_queues(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	u32 i, dma_enable;
+	u32 dma_ctrl, ring_cfg;
+	u32 dma_priority[3] = {0, 0, 0};
+
+	dma_ctrl = bcmgenet_tdma_readl(priv, DMA_CTRL);
+	dma_enable = dma_ctrl & DMA_EN;
+	dma_ctrl &= ~DMA_EN;
+	bcmgenet_tdma_writel(priv, dma_ctrl, DMA_CTRL);
+
+	dma_ctrl = 0;
+	ring_cfg = 0;
+
+	/* Enable strict priority arbiter mode */
+	bcmgenet_tdma_writel(priv, DMA_ARBITER_SP, DMA_ARB_CTRL);
+
+	/* Initialize Tx priority queues */
+	for (i = 0; i < priv->hw_params->tx_queues; i++) {
+		bcmgenet_init_tx_ring(priv, i, priv->hw_params->tx_bds_per_q,
+				      i * priv->hw_params->tx_bds_per_q,
+				      (i + 1) * priv->hw_params->tx_bds_per_q);
+		ring_cfg |= (1 << i);
+		dma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));
+		dma_priority[DMA_PRIO_REG_INDEX(i)] |=
+			((GENET_Q0_PRIORITY + i) << DMA_PRIO_REG_SHIFT(i));
+	}
+
+	/* Initialize Tx default queue 16 */
+	bcmgenet_init_tx_ring(priv, DESC_INDEX, GENET_Q16_TX_BD_CNT,
+			      priv->hw_params->tx_queues *
+			      priv->hw_params->tx_bds_per_q,
+			      TOTAL_DESC);
+	ring_cfg |= (1 << DESC_INDEX);
+	dma_ctrl |= (1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT));
+	dma_priority[DMA_PRIO_REG_INDEX(DESC_INDEX)] |=
+		((GENET_Q0_PRIORITY + priv->hw_params->tx_queues) <<
+		 DMA_PRIO_REG_SHIFT(DESC_INDEX));
+
+	/* Set Tx queue priorities */
+	bcmgenet_tdma_writel(priv, dma_priority[0], DMA_PRIORITY_0);
+	bcmgenet_tdma_writel(priv, dma_priority[1], DMA_PRIORITY_1);
+	bcmgenet_tdma_writel(priv, dma_priority[2], DMA_PRIORITY_2);
+
+	/* Enable Tx queues */
+	bcmgenet_tdma_writel(priv, ring_cfg, DMA_RING_CFG);
+
+	/* Enable Tx DMA */
+	if (dma_enable)
+		dma_ctrl |= DMA_EN;
+	bcmgenet_tdma_writel(priv, dma_ctrl, DMA_CTRL);
+}
+
+static void bcmgenet_enable_rx_napi(struct bcmgenet_priv *priv)
+{
+	unsigned int i;
+	struct bcmgenet_rx_ring *ring;
+
+	for (i = 0; i < priv->hw_params->rx_queues; ++i) {
+		ring = &priv->rx_rings[i];
+		ring->int_enable(ring);
+	}
+
+	ring = &priv->rx_rings[DESC_INDEX];
+	ring->int_enable(ring);
+}
+
+static void bcmgenet_disable_rx_napi(struct bcmgenet_priv *priv)
+{
+	unsigned int i;
+	struct bcmgenet_rx_ring *ring;
+
+	for (i = 0; i < priv->hw_params->rx_queues; ++i) {
+		ring = &priv->rx_rings[i];
+		cancel_work_sync(&ring->dim.dim.work);
+	}
+
+	ring = &priv->rx_rings[DESC_INDEX];
+	cancel_work_sync(&ring->dim.dim.work);
+}
+
+/* Initialize Rx queues
+ *
+ * Queues 0-15 are priority queues. Hardware Filtering Block (HFB) can be
+ * used to direct traffic to these queues.
+ *
+ * Queue 16 is the default Rx queue with GENET_Q16_RX_BD_CNT descriptors.
+ */
+static int bcmgenet_init_rx_queues(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	u32 i;
+	u32 dma_enable;
+	u32 dma_ctrl;
+	u32 ring_cfg;
+	int ret;
+
+	dma_ctrl = bcmgenet_rdma_readl(priv, DMA_CTRL);
+	dma_enable = dma_ctrl & DMA_EN;
+	dma_ctrl &= ~DMA_EN;
+	bcmgenet_rdma_writel(priv, dma_ctrl, DMA_CTRL);
+
+	dma_ctrl = 0;
+	ring_cfg = 0;
+
+	/* Initialize Rx priority queues */
+	for (i = 0; i < priv->hw_params->rx_queues; i++) {
+		ret = bcmgenet_init_rx_ring(priv, i,
+					    priv->hw_params->rx_bds_per_q,
+					    i * priv->hw_params->rx_bds_per_q,
+					    (i + 1) *
+					    priv->hw_params->rx_bds_per_q);
+		if (ret) {
+			printk(KERN_ERR "bcmgenet_init_rx_ring (i) ret=%d\n", ret);
+			return ret;
+		}
+
+		ring_cfg |= (1 << i);
+		dma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));
+	}
+
+	/* Initialize Rx default queue 16 */
+	ret = bcmgenet_init_rx_ring(priv, DESC_INDEX, GENET_Q16_RX_BD_CNT,
+				    priv->hw_params->rx_queues *
+				    priv->hw_params->rx_bds_per_q,
+				    TOTAL_DESC);
+	if (ret) {
+		printk(KERN_ERR "bcmgenet_init_rx_ring (DESC_INDEX) ret=%d\n", ret);
+		return ret;
+	}
+
+	ring_cfg |= (1 << DESC_INDEX);
+	dma_ctrl |= (1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT));
+
+	/* Enable rings */
+	bcmgenet_rdma_writel(priv, ring_cfg, DMA_RING_CFG);
+
+	/* Configure ring as descriptor ring and re-enable DMA if enabled */
+	if (dma_enable)
+		dma_ctrl |= DMA_EN;
+	bcmgenet_rdma_writel(priv, dma_ctrl, DMA_CTRL);
+
+	return 0;
+}
+
+static int bcmgenet_dma_teardown(struct bcmgenet_priv *priv)
+{
+	int ret = 0;
+	int timeout = 0;
+	u32 reg;
+	u32 dma_ctrl;
+	int i;
+
+	/* Disable TDMA to stop add more frames in TX DMA */
+	reg = bcmgenet_tdma_readl(priv, DMA_CTRL);
+	reg &= ~DMA_EN;
+	bcmgenet_tdma_writel(priv, reg, DMA_CTRL);
+
+	/* Check TDMA status register to confirm TDMA is disabled */
+	while (timeout++ < DMA_TIMEOUT_VAL) {
+		reg = bcmgenet_tdma_readl(priv, DMA_STATUS);
+		if (reg & DMA_DISABLED)
+			break;
+
+		udelay(1);
+	}
+
+	if (timeout == DMA_TIMEOUT_VAL) {
+		netdev_warn(priv->dummy, "Timed out while disabling TX DMA\n");
+		ret = -ETIMEDOUT;
+	}
+
+	/* Wait 10ms for packet drain in both tx and rx dma */
+	usleep_range(10000, 20000);
+
+	/* Disable RDMA */
+	reg = bcmgenet_rdma_readl(priv, DMA_CTRL);
+	reg &= ~DMA_EN;
+	bcmgenet_rdma_writel(priv, reg, DMA_CTRL);
+
+	timeout = 0;
+	/* Check RDMA status register to confirm RDMA is disabled */
+	while (timeout++ < DMA_TIMEOUT_VAL) {
+		reg = bcmgenet_rdma_readl(priv, DMA_STATUS);
+		if (reg & DMA_DISABLED)
+			break;
+
+		udelay(1);
+	}
+
+	if (timeout == DMA_TIMEOUT_VAL) {
+		netdev_warn(priv->dummy, "Timed out while disabling RX DMA\n");
+		ret = -ETIMEDOUT;
+	}
+
+	dma_ctrl = 0;
+	for (i = 0; i < priv->hw_params->rx_queues; i++)
+		dma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));
+	reg = bcmgenet_rdma_readl(priv, DMA_CTRL);
+	reg &= ~dma_ctrl;
+	bcmgenet_rdma_writel(priv, reg, DMA_CTRL);
+
+	dma_ctrl = 0;
+	for (i = 0; i < priv->hw_params->tx_queues; i++)
+		dma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));
+	reg = bcmgenet_tdma_readl(priv, DMA_CTRL);
+	reg &= ~dma_ctrl;
+	bcmgenet_tdma_writel(priv, reg, DMA_CTRL);
+
+	return ret;
+}
+
+static void bcmgenet_fini_dma(struct bcmgenet_priv *priv)
+{
+	struct netdev_queue *txq;
+	int i;
+	struct rtskb *skb;
+	
+	for (i = 0; i < priv->num_tx_bds; i++) {
+		skb = bcmgenet_free_tx_cb(&priv->pdev->dev,
+						  priv->tx_cbs + i);
+		if(skb)
+			dev_kfree_rtskb(skb);
+	}
+
+	for (i = 0; i < priv->hw_params->tx_queues; i++) {
+		txq = rtnetdev_get_tx_queue(priv->dev, priv->tx_rings[i].queue);
+		rtnetdev_tx_reset_queue(txq);
+	}
+
+	txq = rtnetdev_get_tx_queue(priv->dev, priv->tx_rings[DESC_INDEX].queue);
+	rtnetdev_tx_reset_queue(txq);
+
+	bcmgenet_free_rx_buffers(priv);
+	kfree(priv->rx_cbs);
+	kfree(priv->tx_cbs);
+}
+
+/* init_edma: Initialize DMA control register */
+static int bcmgenet_init_dma(struct bcmgenet_priv *priv)
+{
+	int ret;
+	unsigned int i;
+	struct enet_cb *cb;
+
+	netif_dbg(priv, hw, priv->dummy, "%s\n", __func__);
+
+	/* Initialize common Rx ring structures */
+	priv->rx_bds = priv->base + priv->hw_params->rdma_offset;
+	priv->num_rx_bds = TOTAL_DESC;
+	priv->rx_cbs = kcalloc(priv->num_rx_bds, sizeof(struct enet_cb),
+			       GFP_KERNEL);
+	if (!priv->rx_cbs)
+		return -ENOMEM;
+
+	for (i = 0; i < priv->num_rx_bds; i++) {
+		cb = priv->rx_cbs + i;
+		cb->bd_addr = priv->rx_bds + i * DMA_DESC_SIZE;
+	}
+
+	/* Initialize common TX ring structures */
+	priv->tx_bds = priv->base + priv->hw_params->tdma_offset;
+	priv->num_tx_bds = TOTAL_DESC;
+	priv->tx_cbs = kcalloc(priv->num_tx_bds, sizeof(struct enet_cb),
+			       GFP_KERNEL);
+	if (!priv->tx_cbs) {
+		kfree(priv->rx_cbs);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < priv->num_tx_bds; i++) {
+		cb = priv->tx_cbs + i;
+		cb->bd_addr = priv->tx_bds + i * DMA_DESC_SIZE;
+	}
+
+	/* Init rDma */
+	bcmgenet_rdma_writel(priv, priv->dma_max_burst_length,
+			     DMA_SCB_BURST_SIZE);
+
+	/* Initialize Rx queues */
+	ret = bcmgenet_init_rx_queues(priv->dev);
+	if (ret) {
+		netdev_err(priv->dummy, "failed to initialize Rx queues ret=%d\n", ret);
+		bcmgenet_free_rx_buffers(priv);
+		kfree(priv->rx_cbs);
+		kfree(priv->tx_cbs);
+		return ret;
+	}
+
+	/* Init tDma */
+	bcmgenet_tdma_writel(priv, priv->dma_max_burst_length,
+			     DMA_SCB_BURST_SIZE);
+
+	/* Initialize Tx queues */
+	bcmgenet_init_tx_queues(priv->dev);
+
+	return 0;
+}
+
+/* Interrupt bottom half */
+static void bcmgenet_irq_task(struct work_struct *work)
+{
+	unsigned long context;
+	unsigned int status;
+	struct bcmgenet_priv *priv = container_of(
+			work, struct bcmgenet_priv, bcmgenet_irq_work);
+
+	netif_dbg(priv, intr, priv->dummy, "%s\n", __func__);
+
+	raw_spin_lock_irqsave(&priv->lock, context);
+	status = priv->irq0_stat;
+	priv->irq0_stat = 0;
+	raw_spin_unlock_irqrestore(&priv->lock, context);
+
+	if (status & UMAC_IRQ_PHY_DET_R &&
+	    priv->dummy->phydev->autoneg != AUTONEG_ENABLE) {
+		phy_init_hw(priv->dummy->phydev);
+		genphy_config_aneg(priv->dummy->phydev);
+	}
+
+	/* Link UP/DOWN event */
+	if (status & UMAC_IRQ_LINK_EVENT) {
+		phy_mac_interrupt(priv->dummy->phydev);
+		if(!(priv->dummy->phydev->state & PHY_DOWN) ||
+		   !test_bit(__LINK_STATE_NOCARRIER, &priv->dummy->state))
+			rtnetif_carrier_on(priv->dev);
+		else
+			rtnetif_carrier_off(priv->dev);
+	}
+}
+
+/* bcmgenet_isr1: handle Rx and Tx priority queues */
+static irqreturn_t bcmgenet_isr1(int irq, void *dev_id)
+{
+	struct bcmgenet_priv *priv = dev_id;
+	struct bcmgenet_rx_ring *rx_ring;
+	struct bcmgenet_tx_ring *tx_ring;
+	unsigned int index, status;
+
+	/* Read irq status */
+	status = bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_STAT) &
+		~bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);
+
+	/* clear interrupts */
+	bcmgenet_intrl2_1_writel(priv, status, INTRL2_CPU_CLEAR);
+
+	netif_dbg(priv, intr, priv->dummy,
+		  "%s: IRQ=0x%x\n", __func__, status);
+
+	/* Check Rx priority queue interrupts */
+	for (index = 0; index < priv->hw_params->rx_queues; index++) {
+		if (!(status & BIT(UMAC_IRQ1_RX_INTR_SHIFT + index)))
+			continue;
+
+		rx_ring = &priv->rx_rings[index];
+		rx_ring->dim.event_ctr++;
+		rx_ring->int_disable(rx_ring);
+		rtdm_event_signal_one(&rx_event);
+	}
+
+	/* Check Tx priority queue interrupts */
+	/* RTnet, rpi-4, bcmgenet_hw_params[GENET_V5].tx_queues is 0 */
+	for (index = 0; index < priv->hw_params->tx_queues; index++) {
+		if (!(status & BIT(index)))
+			continue;
+
+		tx_ring = &priv->tx_rings[index];
+		tx_ring->int_disable(tx_ring);
+		rtdm_event_signal_one(&tx_event);
+	}
+	
+	return IRQ_HANDLED;
+}
+
+/* bcmgenet_isr0: handle Rx and Tx default queues + other stuff */
+static irqreturn_t bcmgenet_isr0(int irq, void *dev_id)
+{
+	struct bcmgenet_priv *priv = dev_id;
+	struct bcmgenet_rx_ring *rx_ring;
+	struct bcmgenet_tx_ring *tx_ring;
+	unsigned int status;
+	unsigned long flags;
+
+	/* Read irq status */
+	status = bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT) &
+		~bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);
+
+	/* clear interrupts */
+	bcmgenet_intrl2_0_writel(priv, status, INTRL2_CPU_CLEAR);
+
+	netif_dbg(priv, intr, priv->dummy,
+		  "IRQ=0x%x\n", status);
+
+	if (status & UMAC_IRQ_RXDMA_DONE) {
+		rx_ring = &priv->rx_rings[DESC_INDEX];
+		rx_ring->dim.event_ctr++;
+		rx_ring->int_disable(rx_ring);
+		rtdm_event_signal_one(&rx_event);
+	}
+
+	if (status & UMAC_IRQ_TXDMA_DONE) {
+		tx_ring = &priv->tx_rings[DESC_INDEX];
+		tx_ring->int_disable(tx_ring);
+		rtdm_event_signal_one(&tx_event);
+	}
+
+	if ((priv->hw_params->flags & GENET_HAS_MDIO_INTR) &&
+		status & (UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR))
+		swake_up_one(&priv->swq);
+
+	/* all other interested interrupts handled in bottom half */
+	status &= (UMAC_IRQ_LINK_EVENT | UMAC_IRQ_PHY_DET_R);
+	if (status) {
+		/* Save irq status for bottom-half processing. */
+		raw_spin_lock_irqsave(&priv->lock, flags);
+		priv->irq0_stat |= status;
+		raw_spin_unlock_irqrestore(&priv->lock, flags);
+
+		schedule_work(&priv->bcmgenet_irq_work);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t bcmgenet_wol_isr(int irq, void *dev_id)
+{
+	/* Acknowledge the interrupt */
+	return IRQ_HANDLED;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void bcmgenet_poll_controller(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	/* Invoke the main RX/TX interrupt handler */
+	disable_irq(priv->irq0);
+	bcmgenet_isr0(priv->irq0, priv);
+	enable_irq(priv->irq0);
+
+	/* And the interrupt handler for RX/TX priority queues */
+	disable_irq(priv->irq1);
+	bcmgenet_isr1(priv->irq1, priv);
+	enable_irq(priv->irq1);
+}
+#endif
+
+static void bcmgenet_umac_reset(struct bcmgenet_priv *priv)
+{
+	u32 reg;
+
+	reg = bcmgenet_rbuf_ctrl_get(priv);
+	reg |= BIT(1);
+	bcmgenet_rbuf_ctrl_set(priv, reg);
+	udelay(10);
+
+	reg &= ~BIT(1);
+	bcmgenet_rbuf_ctrl_set(priv, reg);
+	udelay(10);
+}
+
+static void bcmgenet_set_hw_addr(struct bcmgenet_priv *priv,
+				 unsigned char *addr)
+{
+	bcmgenet_umac_writel(priv, get_unaligned_be32(&addr[0]), UMAC_MAC0);
+	bcmgenet_umac_writel(priv, get_unaligned_be16(&addr[4]), UMAC_MAC1);
+}
+
+static void bcmgenet_get_hw_addr(struct bcmgenet_priv *priv,
+				 unsigned char *addr)
+{
+	u32 addr_tmp;
+
+	addr_tmp = bcmgenet_umac_readl(priv, UMAC_MAC0);
+	put_unaligned_be32(addr_tmp, &addr[0]);
+	addr_tmp = bcmgenet_umac_readl(priv, UMAC_MAC1);
+	put_unaligned_be16(addr_tmp, &addr[4]);
+}
+
+/* Returns a reusable dma control register value */
+static u32 bcmgenet_dma_disable(struct bcmgenet_priv *priv)
+{
+	u32 reg;
+	u32 dma_ctrl;
+
+	/* disable DMA */
+	dma_ctrl = 1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT) | DMA_EN;
+	reg = bcmgenet_tdma_readl(priv, DMA_CTRL);
+	reg &= ~dma_ctrl;
+	bcmgenet_tdma_writel(priv, reg, DMA_CTRL);
+
+	reg = bcmgenet_rdma_readl(priv, DMA_CTRL);
+	reg &= ~dma_ctrl;
+	bcmgenet_rdma_writel(priv, reg, DMA_CTRL);
+
+	bcmgenet_umac_writel(priv, 1, UMAC_TX_FLUSH);
+	udelay(10);
+	bcmgenet_umac_writel(priv, 0, UMAC_TX_FLUSH);
+
+	return dma_ctrl;
+}
+
+static void bcmgenet_enable_dma(struct bcmgenet_priv *priv, u32 dma_ctrl)
+{
+	u32 reg;
+
+	reg = bcmgenet_rdma_readl(priv, DMA_CTRL);
+	reg |= dma_ctrl;
+	bcmgenet_rdma_writel(priv, reg, DMA_CTRL);
+
+	reg = bcmgenet_tdma_readl(priv, DMA_CTRL);
+	reg |= dma_ctrl;
+	bcmgenet_tdma_writel(priv, reg, DMA_CTRL);
+}
+
+static void bcmgenet_netif_start(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	/* Start the network engine */
+	bcmgenet_set_rx_mode(dev);
+	bcmgenet_enable_rx_napi(priv);
+
+	umac_enable_set(priv, CMD_TX_EN | CMD_RX_EN, true);
+
+	bcmgenet_enable_tx_napi(priv);
+	
+	/* Monitor link interrupts now */
+	bcmgenet_link_intr_enable(priv);
+
+	phy_start(priv->dummy->phydev);
+}
+
+static int bcmgenet_open(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	unsigned long dma_ctrl;
+	u32 reg;
+	int ret;
+	/* soft interrupts have priority MAX_RT_PRIO / 2 in preempt_rt */
+	struct sched_param tx_schedule_param = 
+		{ .sched_priority = RTNET_TX_THREAD_RT_PRIO };
+	struct sched_param rx_schedule_param = 
+		{ .sched_priority = RTNET_RX_THREAD_RT_PRIO };
+
+	netif_dbg(priv, ifup, priv->dummy, "bcmgenet_open\n");
+
+	/* Turn on the clock */
+	clk_prepare_enable(priv->clk);
+
+	/* If this is an internal GPHY, power it back on now, before UniMAC is
+	 * brought out of reset as absolutely no UniMAC activity is allowed
+	 */
+	if (priv->internal_phy)
+		bcmgenet_power_up(priv, GENET_POWER_PASSIVE);
+
+	/* take MAC out of reset */
+	bcmgenet_umac_reset(priv);
+
+	init_umac(priv);
+
+	/* Apply features again in case we changed them while interface was
+	 * down
+	 */
+	bcmgenet_set_features(dev, dev->features);
+
+	bcmgenet_set_hw_addr(priv, dev->dev_addr);
+
+	if (priv->internal_phy) {
+		reg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);
+		reg |= EXT_ENERGY_DET_MASK;
+		bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+	}
+
+	/* Disable RX/TX DMA and flush TX queues */
+	dma_ctrl = bcmgenet_dma_disable(priv);
+
+	/* Reinitialize TDMA and RDMA and SW housekeeping */
+	ret = bcmgenet_init_dma(priv);
+	if (ret) {
+		netdev_err(priv->dummy, "failed to initialize DMA ret=%d\n", ret);
+		goto err_clk_disable;
+	}
+
+	/* Always enable ring 16 - descriptor ring */
+	bcmgenet_enable_dma(priv, dma_ctrl);
+
+	/* HFB init */
+	bcmgenet_hfb_init(priv);
+
+	ret = request_irq(priv->irq0, bcmgenet_isr0, IRQF_NO_THREAD,
+			  dev->name, priv);
+	if (ret < 0) {
+		netdev_err(priv->dummy, "can't request IRQ %d\n", priv->irq0);
+		goto err_fini_dma;
+	}
+
+	ret = request_irq(priv->irq1, bcmgenet_isr1, IRQF_NO_THREAD,
+			  dev->name, priv);
+	if (ret < 0) {
+		netdev_err(priv->dummy, "can't request IRQ %d\n", priv->irq1);
+		goto err_irq0;
+	}
+
+	rt_stack_connect(dev, &STACK_manager);
+
+#if 0
+	/* we must be running, in order to receive link change notifications */
+	set_bit(__LINK_STATE_START, &priv->dummy->state);
+	ret = bcmgenet_mii_probe(priv->dummy);
+#endif
+	ret = bcmgenet_mii_probe(priv->dummy);
+	if (ret) {
+		netdev_err(priv->dummy, "failed to connect to PHY\n");
+		goto err_irq1;
+	}
+	
+	/* setup the rx task */
+	rtdm_event_init(&rx_event, 0);
+    stop_rx_task = 0;
+    rx_task = kthread_create(bcmgenet_rx_poll, priv, "bcmgenet_rx_poll");
+    if (!rx_task) {
+		ret = -ENOMEM;
+		goto err_irq1;
+	}
+    sched_setscheduler(rx_task, SCHED_FIFO, &rx_schedule_param);
+    wake_up_process(rx_task);
+
+	/* setup the tx task */
+	rtdm_event_init(&tx_event, 0);
+    stop_tx_task = 0;
+    tx_task = kthread_create(bcmgenet_tx_poll, priv, "bcmgenet_tx_poll");
+    if (!tx_task) {
+		ret = -ENOMEM;
+		goto err_irq1;
+	}
+    sched_setscheduler(tx_task, SCHED_FIFO, &tx_schedule_param);
+    wake_up_process(tx_task);
+	
+	bcmgenet_netif_start(dev);
+
+	rtnetif_tx_start_all_queues(priv->dev);
+
+	return 0;
+
+err_irq1:
+	free_irq(priv->irq1, priv);
+err_irq0:
+	free_irq(priv->irq0, priv);
+err_fini_dma:
+	bcmgenet_dma_teardown(priv);
+	bcmgenet_fini_dma(priv);
+err_clk_disable:
+	if (priv->internal_phy)
+		bcmgenet_power_down(priv, GENET_POWER_PASSIVE);
+	clk_disable_unprepare(priv->clk);
+	return ret;
+}
+
+static void bcmgenet_netif_stop(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	rtnetif_tx_disable(priv->dev);
+
+	/* Disable MAC receive */
+	umac_enable_set(priv, CMD_RX_EN, false);
+
+	bcmgenet_dma_teardown(priv);
+
+	/* Disable MAC transmit. TX DMA disabled must be done before this */
+	umac_enable_set(priv, CMD_TX_EN, false);
+
+	phy_stop(priv->dummy->phydev);
+	bcmgenet_disable_rx_napi(priv);
+	bcmgenet_intr_disable(priv);
+
+	/* Wait for pending work items to complete. Since interrupts are
+	 * disabled no new work will be scheduled.
+	 */
+	cancel_work_sync(&priv->bcmgenet_irq_work);
+
+	priv->old_link = -1;
+	priv->old_speed = -1;
+	priv->old_duplex = -1;
+	priv->old_pause = -1;
+
+	/* tx reclaim */
+	bcmgenet_tx_reclaim_all(dev);
+	bcmgenet_fini_dma(priv);
+}
+
+static int bcmgenet_close(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	int ret = 0;
+
+	netif_dbg(priv, ifdown, priv->dummy, "bcmgenet_close\n");
+
+	bcmgenet_netif_stop(dev);
+
+	/* Really kill the PHY state machine and disconnect from it */
+
+	if(priv->dummy->phydev) {
+		phy_disconnect(priv->dummy->phydev);
+	} else
+		trace_printk("priv->dummy->phydev is null\n");
+	
+	free_irq(priv->irq0, priv);
+	free_irq(priv->irq1, priv);
+	
+	if (priv->internal_phy)
+		ret = bcmgenet_power_down(priv, GENET_POWER_PASSIVE);
+
+	clk_disable_unprepare(priv->clk);
+
+	stop_rx_task = 1;
+	stop_tx_task = 1;
+	smp_mb();
+	rtdm_event_signal_one(&rx_event);
+    kthread_stop(rx_task);
+#if 0
+    put_task_struct(rx_task);
+#endif
+	rtdm_event_signal_one(&tx_event);
+    kthread_stop(tx_task);
+#if 0
+    put_task_struct(tx_task);
+#endif
+	
+	return ret;
+}
+
+#if 0
+static void bcmgenet_dump_tx_queue(struct bcmgenet_tx_ring *ring)
+{
+	struct bcmgenet_priv *priv = ring->priv;
+	u32 p_index, c_index, intsts, intmsk;
+	struct netdev_queue *txq;
+	unsigned int free_bds;
+	bool txq_stopped;
+
+	if (!netif_msg_tx_err(priv))
+		return;
+
+	txq = rtnetdev_get_tx_queue(priv->dev, ring->queue);
+
+	raw_spin_lock(&ring->lock);
+	if (ring->index == DESC_INDEX) {
+		intsts = ~bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);
+		intmsk = UMAC_IRQ_TXDMA_DONE | UMAC_IRQ_TXDMA_MBDONE;
+	} else {
+		intsts = ~bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);
+		intmsk = 1 << ring->index;
+	}
+	c_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_CONS_INDEX);
+	p_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_PROD_INDEX);
+	txq_stopped = rtnetif_tx_queue_stopped(priv->dev, txq);
+	free_bds = ring->free_bds;
+	raw_spin_unlock(&ring->lock);
+
+	netif_err(priv, tx_err, priv->dummy, "Ring %d queue %d status summary\n"
+		  "TX queue status: %s, interrupts: %s\n"
+		  "(sw)free_bds: %d (sw)size: %d\n"
+		  "(sw)p_index: %d (hw)p_index: %d\n"
+		  "(sw)c_index: %d (hw)c_index: %d\n"
+		  "(sw)clean_p: %d (sw)write_p: %d\n"
+		  "(sw)cb_ptr: %d (sw)end_ptr: %d\n",
+		  ring->index, ring->queue,
+		  txq_stopped ? "stopped" : "active",
+		  intsts & intmsk ? "enabled" : "disabled",
+		  free_bds, ring->size,
+		  ring->prod_index, p_index & DMA_P_INDEX_MASK,
+		  ring->c_index, c_index & DMA_C_INDEX_MASK,
+		  ring->clean_ptr, ring->write_ptr,
+		  ring->cb_ptr, ring->end_ptr);
+}
+
+static void bcmgenet_timeout(struct rtnet_device *dev, unsigned int txqueue)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	u32 int0_enable = 0;
+	u32 int1_enable = 0;
+	unsigned int q;
+
+	netif_dbg(priv, tx_err, priv->dummy, "bcmgenet_timeout\n");
+
+	for (q = 0; q < priv->hw_params->tx_queues; q++)
+		bcmgenet_dump_tx_queue(&priv->tx_rings[q]);
+	bcmgenet_dump_tx_queue(&priv->tx_rings[DESC_INDEX]);
+
+	bcmgenet_tx_reclaim_all(dev);
+
+	for (q = 0; q < priv->hw_params->tx_queues; q++)
+		int1_enable |= (1 << q);
+
+	int0_enable = UMAC_IRQ_TXDMA_DONE;
+
+	/* Re-enable TX interrupts if disabled */
+	bcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);
+	bcmgenet_intrl2_1_writel(priv, int1_enable, INTRL2_CPU_MASK_CLEAR);
+
+	rtnetif_trans_update(priv->dev);
+
+	priv->stats.tx_errors++;
+
+	rtnetif_tx_wake_all_queues(priv->dev);
+}
+#endif
+
+#define MAX_MDF_FILTER	17
+
+static inline void bcmgenet_set_mdf_addr(struct bcmgenet_priv *priv,
+					 unsigned char *addr,
+					 int *i)
+{
+	bcmgenet_umac_writel(priv, addr[0] << 8 | addr[1],
+			     UMAC_MDF_ADDR + (*i * 4));
+	bcmgenet_umac_writel(priv, addr[2] << 24 | addr[3] << 16 |
+			     addr[4] << 8 | addr[5],
+			     UMAC_MDF_ADDR + ((*i + 1) * 4));
+	*i += 2;
+}
+
+static void bcmgenet_set_rx_mode(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct netdev_hw_addr *ha;
+	int i, nfilter;
+	u32 reg;
+
+	netif_dbg(priv, hw, priv->dummy, "%s: %08X\n", __func__, dev->flags);
+
+	/* Number of filters needed */
+	nfilter = netdev_uc_count(priv->dummy) + netdev_mc_count(priv->dummy) + 2;
+
+	/*
+	 * Turn on promicuous mode for three scenarios
+	 * 1. IFF_PROMISC flag is set
+	 * 2. IFF_ALLMULTI flag is set
+	 * 3. The number of filters needed exceeds the number filters
+	 *    supported by the hardware.
+	*/
+	reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+	if ((dev->flags & (IFF_PROMISC | IFF_ALLMULTI)) ||
+	    (nfilter > MAX_MDF_FILTER)) {
+		reg |= CMD_PROMISC;
+		bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+		bcmgenet_umac_writel(priv, 0, UMAC_MDF_CTRL);
+		return;
+	} else {
+		reg &= ~CMD_PROMISC;
+		bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+	}
+
+	/* update MDF filter */
+	i = 0;
+	/* Broadcast */
+	bcmgenet_set_mdf_addr(priv, dev->broadcast, &i);
+	/* my own address.*/
+	bcmgenet_set_mdf_addr(priv, dev->dev_addr, &i);
+
+	/* Unicast */
+	netdev_for_each_uc_addr(ha, priv->dummy)
+		bcmgenet_set_mdf_addr(priv, ha->addr, &i);
+
+	/* Multicast */
+	netdev_for_each_mc_addr(ha, priv->dummy)
+		bcmgenet_set_mdf_addr(priv, ha->addr, &i);
+
+	/* Enable filters */
+	reg = GENMASK(MAX_MDF_FILTER - 1, MAX_MDF_FILTER - nfilter);
+	bcmgenet_umac_writel(priv, reg, UMAC_MDF_CTRL);
+}
+
+#if 0
+/* Set the hardware MAC address. */
+static int bcmgenet_set_mac_addr(struct rtnet_device *dev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	/* Setting the MAC address at the hardware level is not possible
+	 * without disabling the UniMAC RX/TX enable bits.
+	 */
+	if (rtnetif_running(dev))
+		return -EBUSY;
+
+	ether_addr_copy(dev->dev_addr, addr->sa_data);
+
+	return 0;
+}
+#endif
+
+static struct net_device_stats *bcmgenet_get_stats(struct rtnet_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	unsigned long tx_bytes = 0, tx_packets = 0;
+	unsigned long rx_bytes = 0, rx_packets = 0;
+	unsigned long rx_errors = 0, rx_dropped = 0;
+	struct bcmgenet_tx_ring *tx_ring;
+	struct bcmgenet_rx_ring *rx_ring;
+	unsigned int q;
+
+	for (q = 0; q < priv->hw_params->tx_queues; q++) {
+		tx_ring = &priv->tx_rings[q];
+		tx_bytes += tx_ring->bytes;
+		tx_packets += tx_ring->packets;
+	}
+	tx_ring = &priv->tx_rings[DESC_INDEX];
+	tx_bytes += tx_ring->bytes;
+	tx_packets += tx_ring->packets;
+
+	for (q = 0; q < priv->hw_params->rx_queues; q++) {
+		rx_ring = &priv->rx_rings[q];
+
+		rx_bytes += rx_ring->bytes;
+		rx_packets += rx_ring->packets;
+		rx_errors += rx_ring->errors;
+		rx_dropped += rx_ring->dropped;
+	}
+	rx_ring = &priv->rx_rings[DESC_INDEX];
+	rx_bytes += rx_ring->bytes;
+	rx_packets += rx_ring->packets;
+	rx_errors += rx_ring->errors;
+	rx_dropped += rx_ring->dropped;
+
+	priv->stats.tx_bytes = tx_bytes;
+	priv->stats.tx_packets = tx_packets;
+	priv->stats.rx_bytes = rx_bytes;
+	priv->stats.rx_packets = rx_packets;
+	priv->stats.rx_errors = rx_errors;
+	priv->stats.rx_missed_errors = rx_errors;
+	priv->stats.rx_dropped = rx_dropped;
+	return &priv->stats;
+}
+
+#if 0
+/* ndo_change_carrier() is not ment for physical devices.
+ * phy_link_change() from drivers/net/phy/phy_device.c
+ * calls netif_carrier_on/off(netdev);
+ */
+static int bcmgenet_change_carrier(struct net_device *dummy, bool new_carrier)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dummy))->rtdev);
+
+	if (!priv->dummy->phydev || !phy_is_pseudo_fixed_link(priv->dummy->phydev) ||
+	    priv->phy_interface != PHY_INTERFACE_MODE_MOCA)
+		return -EOPNOTSUPP;
+
+	if (new_carrier)
+		rtnetif_carrier_on(priv->dev);
+	else
+		rtnetif_carrier_off(priv->dev);
+
+	return 0;
+}
+#endif
+
+static const struct net_device_ops bcmgenet_dummy_netdev_ops = {
+#if 0
+	.ndo_open		= bcmgenet_dummy_open,
+	.ndo_stop		= bcmgenet_dummy_close,
+	.ndo_start_xmit		= bcmgenet_dummy_xmit,
+	.ndo_tx_timeout		= bcmgenet_timeout,
+	.ndo_set_rx_mode	= bcmgenet_set_rx_mode,
+	.ndo_set_mac_address	= bcmgenet_set_mac_addr,
+	.ndo_do_ioctl		= phy_do_ioctl_running,
+	.ndo_set_features	= bcmgenet_set_features,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= bcmgenet_poll_controller,
+#endif
+	.ndo_get_stats		= bcmgenet_get_stats,
+	.ndo_change_carrier	= bcmgenet_change_carrier,
+#endif
+};
+
+/* Array of GENET hardware parameters/characteristics */
+static struct bcmgenet_hw_params bcmgenet_hw_params[] = {
+	[GENET_V1] = {
+		.tx_queues = 0,
+		.tx_bds_per_q = 0,
+		.rx_queues = 0,
+		.rx_bds_per_q = 0,
+		.bp_in_en_shift = 16,
+		.bp_in_mask = 0xffff,
+		.hfb_filter_cnt = 16,
+		.qtag_mask = 0x1F,
+		.hfb_offset = 0x1000,
+		.rdma_offset = 0x2000,
+		.tdma_offset = 0x3000,
+		.words_per_bd = 2,
+	},
+	[GENET_V2] = {
+		.tx_queues = 4,
+		.tx_bds_per_q = 32,
+		.rx_queues = 0,
+		.rx_bds_per_q = 0,
+		.bp_in_en_shift = 16,
+		.bp_in_mask = 0xffff,
+		.hfb_filter_cnt = 16,
+		.qtag_mask = 0x1F,
+		.tbuf_offset = 0x0600,
+		.hfb_offset = 0x1000,
+		.hfb_reg_offset = 0x2000,
+		.rdma_offset = 0x3000,
+		.tdma_offset = 0x4000,
+		.words_per_bd = 2,
+		.flags = GENET_HAS_EXT,
+	},
+	[GENET_V3] = {
+		.tx_queues = 4,
+		.tx_bds_per_q = 32,
+		.rx_queues = 0,
+		.rx_bds_per_q = 0,
+		.bp_in_en_shift = 17,
+		.bp_in_mask = 0x1ffff,
+		.hfb_filter_cnt = 48,
+		.hfb_filter_size = 128,
+		.qtag_mask = 0x3F,
+		.tbuf_offset = 0x0600,
+		.hfb_offset = 0x8000,
+		.hfb_reg_offset = 0xfc00,
+		.rdma_offset = 0x10000,
+		.tdma_offset = 0x11000,
+		.words_per_bd = 2,
+		.flags = GENET_HAS_EXT | GENET_HAS_MDIO_INTR |
+			 GENET_HAS_MOCA_LINK_DET,
+	},
+	[GENET_V4] = {
+		.tx_queues = 4,
+		.tx_bds_per_q = 32,
+		.rx_queues = 0,
+		.rx_bds_per_q = 0,
+		.bp_in_en_shift = 17,
+		.bp_in_mask = 0x1ffff,
+		.hfb_filter_cnt = 48,
+		.hfb_filter_size = 128,
+		.qtag_mask = 0x3F,
+		.tbuf_offset = 0x0600,
+		.hfb_offset = 0x8000,
+		.hfb_reg_offset = 0xfc00,
+		.rdma_offset = 0x2000,
+		.tdma_offset = 0x4000,
+		.words_per_bd = 3,
+		.flags = GENET_HAS_40BITS | GENET_HAS_EXT |
+			 GENET_HAS_MDIO_INTR | GENET_HAS_MOCA_LINK_DET,
+	},
+	[GENET_V5] = {
+		.tx_queues = 0,
+		.tx_bds_per_q = 0,
+		.rx_queues = 0,
+		.rx_bds_per_q = 0,
+		.bp_in_en_shift = 17,
+		.bp_in_mask = 0x1ffff,
+		.hfb_filter_cnt = 48,
+		.hfb_filter_size = 128,
+		.qtag_mask = 0x3F,
+		.tbuf_offset = 0x0600,
+		.hfb_offset = 0x8000,
+		.hfb_reg_offset = 0xfc00,
+		.rdma_offset = 0x2000,
+		.tdma_offset = 0x4000,
+		.words_per_bd = 3,
+		.flags = GENET_HAS_40BITS | GENET_HAS_EXT |
+			 GENET_HAS_MDIO_INTR | GENET_HAS_MOCA_LINK_DET,
+	},
+};
+
+/* Infer hardware parameters from the detected GENET version */
+static void bcmgenet_set_hw_params(struct bcmgenet_priv *priv)
+{
+	struct bcmgenet_hw_params *params;
+	u32 reg;
+	u8 major;
+	u16 gphy_rev;
+
+	if (GENET_IS_V5(priv) || GENET_IS_V4(priv)) {
+		bcmgenet_dma_regs = bcmgenet_dma_regs_v3plus;
+		genet_dma_ring_regs = genet_dma_ring_regs_v4;
+	} else if (GENET_IS_V3(priv)) {
+		bcmgenet_dma_regs = bcmgenet_dma_regs_v3plus;
+		genet_dma_ring_regs = genet_dma_ring_regs_v123;
+	} else if (GENET_IS_V2(priv)) {
+		bcmgenet_dma_regs = bcmgenet_dma_regs_v2;
+		genet_dma_ring_regs = genet_dma_ring_regs_v123;
+	} else if (GENET_IS_V1(priv)) {
+		bcmgenet_dma_regs = bcmgenet_dma_regs_v1;
+		genet_dma_ring_regs = genet_dma_ring_regs_v123;
+	}
+
+	/* enum genet_version starts at 1 */
+	priv->hw_params = &bcmgenet_hw_params[priv->version];
+	params = priv->hw_params;
+
+	/* Read GENET HW version */
+	reg = bcmgenet_sys_readl(priv, SYS_REV_CTRL);
+	major = (reg >> 24 & 0x0f);
+	if (major == 6)
+		major = 5;
+	else if (major == 5)
+		major = 4;
+	else if (major == 0)
+		major = 1;
+	if (major != priv->version) {
+		dev_err(&priv->pdev->dev,
+			"GENET version mismatch, got: %d, configured for: %d\n",
+			major, priv->version);
+	}
+
+	/* Print the GENET core version */
+	dev_info(&priv->pdev->dev, "GENET " GENET_VER_FMT,
+		 major, (reg >> 16) & 0x0f, reg & 0xffff);
+
+	/* Store the integrated PHY revision for the MDIO probing function
+	 * to pass this information to the PHY driver. The PHY driver expects
+	 * to find the PHY major revision in bits 15:8 while the GENET register
+	 * stores that information in bits 7:0, account for that.
+	 *
+	 * On newer chips, starting with PHY revision G0, a new scheme is
+	 * deployed similar to the Starfighter 2 switch with GPHY major
+	 * revision in bits 15:8 and patch level in bits 7:0. Major revision 0
+	 * is reserved as well as special value 0x01ff, we have a small
+	 * heuristic to check for the new GPHY revision and re-arrange things
+	 * so the GPHY driver is happy.
+	 */
+	gphy_rev = reg & 0xffff;
+
+	if (GENET_IS_V5(priv)) {
+		/* The EPHY revision should come from the MDIO registers of
+		 * the PHY not from GENET.
+		 */
+		if (gphy_rev != 0) {
+			pr_warn("GENET is reporting EPHY revision: 0x%04x\n",
+				gphy_rev);
+		}
+	/* This is reserved so should require special treatment */
+	} else if (gphy_rev == 0 || gphy_rev == 0x01ff) {
+		pr_warn("Invalid GPHY revision detected: 0x%04x\n", gphy_rev);
+		return;
+	/* This is the good old scheme, just GPHY major, no minor nor patch */
+	} else if ((gphy_rev & 0xf0) != 0) {
+		priv->gphy_rev = gphy_rev << 8;
+	/* This is the new scheme, GPHY major rolls over with 0x10 = rev G0 */
+	} else if ((gphy_rev & 0xff00) != 0) {
+		priv->gphy_rev = gphy_rev;
+	}
+
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	if (!(params->flags & GENET_HAS_40BITS))
+		pr_warn("GENET does not support 40-bits PA\n");
+#endif
+
+	pr_debug("Configuration for version: %d\n"
+		"TXq: %1d, TXqBDs: %1d, RXq: %1d, RXqBDs: %1d\n"
+		"BP << en: %2d, BP msk: 0x%05x\n"
+		"HFB count: %2d, QTAQ msk: 0x%05x\n"
+		"TBUF: 0x%04x, HFB: 0x%04x, HFBreg: 0x%04x\n"
+		"RDMA: 0x%05x, TDMA: 0x%05x\n"
+		"Words/BD: %d\n",
+		priv->version,
+		params->tx_queues, params->tx_bds_per_q,
+		params->rx_queues, params->rx_bds_per_q,
+		params->bp_in_en_shift, params->bp_in_mask,
+		params->hfb_filter_cnt, params->qtag_mask,
+		params->tbuf_offset, params->hfb_offset,
+		params->hfb_reg_offset,
+		params->rdma_offset, params->tdma_offset,
+		params->words_per_bd);
+}
+
+struct bcmgenet_plat_data {
+	enum bcmgenet_version version;
+	u32 dma_max_burst_length;
+};
+
+static const struct bcmgenet_plat_data v1_plat_data = {
+	.version = GENET_V1,
+	.dma_max_burst_length = DMA_MAX_BURST_LENGTH,
+};
+
+static const struct bcmgenet_plat_data v2_plat_data = {
+	.version = GENET_V2,
+	.dma_max_burst_length = DMA_MAX_BURST_LENGTH,
+};
+
+static const struct bcmgenet_plat_data v3_plat_data = {
+	.version = GENET_V3,
+	.dma_max_burst_length = DMA_MAX_BURST_LENGTH,
+};
+
+static const struct bcmgenet_plat_data v4_plat_data = {
+	.version = GENET_V4,
+	.dma_max_burst_length = DMA_MAX_BURST_LENGTH,
+};
+
+static const struct bcmgenet_plat_data v5_plat_data = {
+	.version = GENET_V5,
+	.dma_max_burst_length = DMA_MAX_BURST_LENGTH,
+};
+
+static const struct bcmgenet_plat_data bcm2711_plat_data = {
+	.version = GENET_V5,
+	.dma_max_burst_length = 0x08,
+};
+
+static const struct of_device_id bcmgenet_match[] = {
+	{ .compatible = "brcm,genet-v1", .data = &v1_plat_data },
+	{ .compatible = "brcm,genet-v2", .data = &v2_plat_data },
+	{ .compatible = "brcm,genet-v3", .data = &v3_plat_data },
+	{ .compatible = "brcm,genet-v4", .data = &v4_plat_data },
+	{ .compatible = "brcm,genet-v5", .data = &v5_plat_data },
+	{ .compatible = "brcm,bcm2711-genet-v5", .data = &bcm2711_plat_data },
+	{ },
+};
+MODULE_DEVICE_TABLE(of, bcmgenet_match);
+
+static int bcmgenet_probe(struct platform_device *pdev)
+{
+	struct bcmgenet_platform_data *pd = pdev->dev.platform_data;
+	const struct bcmgenet_plat_data *pdata;
+	struct bcmgenet_priv *priv;
+	struct rtnet_device *dev;
+	struct net_device *dummy;
+	struct dummy_rtnetdev_priv *dummy_priv;
+
+	unsigned int i;
+	int err = -EIO;
+
+	/* Up to GENET_MAX_MQ_CNT + 1 TX queues and RX queues */
+	dev = rt_alloc_etherdev_mqs(sizeof(*priv), bcmgenet_rtskb_pool_size, 
+							   GENET_MAX_MQ_CNT + 1, GENET_MAX_MQ_CNT + 1);
+	if (!dev) {
+		dev_err(&pdev->dev, "can't allocate rtnet device\n");
+		return -ENOMEM;
+	}
+    rtdev_alloc_name(dev, "rteth%d");
+	dev->vers = RTDEV_VERS_2_0;
+    rt_rtdev_connect(dev, &RTDEV_manager);
+	
+	dummy = alloc_etherdev(sizeof(struct dummy_rtnetdev_priv));
+	if (!dummy) {
+		dev_err(&pdev->dev, "can't allocate dummy net device\n");
+		err = -ENOMEM;
+		goto err_0;
+	}
+	dev_alloc_name(dummy, "dummy_rteth%d");
+	
+	priv = rtnetdev_priv(dev);
+	priv->dummy = dummy;
+	dummy_priv = netdev_priv(dummy);
+	dummy_priv->rtdev = dev;
+	priv->irq0 = platform_get_irq(pdev, 0);
+	if (priv->irq0 < 0) {
+		err = priv->irq0;
+		dev_err(&pdev->dev, "platform_get_irq0 err=%d \n", err);
+		goto err;
+	}
+	priv->irq1 = platform_get_irq(pdev, 1);
+	if (priv->irq1 < 0) {
+		err = priv->irq1;
+		dev_err(&pdev->dev, "platform_get_irq1 err=%d \n", err);
+		goto err;
+	}
+	priv->wol_irq = platform_get_irq_optional(pdev, 2);
+
+	priv->base = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(priv->base)) {
+		err = PTR_ERR(priv->base);
+		dev_err(&pdev->dev, "devm_platform_ioremap_resource err=%d \n", err);
+		goto err;
+	}
+
+	raw_spin_lock_init(&priv->lock);
+
+	SET_NETDEV_DEV(dummy, &pdev->dev);
+	dev_set_drvdata(&pdev->dev, dev);
+#if 0
+	dev->watchdog_timeo = 2 * HZ;
+	dev->ethtool_ops = &bcmgenet_ethtool_ops;
+	dev->netdev_ops = &bcmgenet_netdev_ops;
+#endif
+	priv->msg_enable = netif_msg_init(-1, GENET_MSG_DEFAULT);
+
+	/* The RTnet specific entries in the device structure. */
+	dev->open = bcmgenet_open;
+	dev->stop = bcmgenet_close;
+	dev->hard_header = &rt_eth_header;
+	dev->hard_start_xmit = bcmgenet_xmit;
+	dev->start_xmit = dev->hard_start_xmit;
+	dev->get_stats = bcmgenet_get_stats;	
+	dummy->netdev_ops	= &bcmgenet_dummy_netdev_ops;
+	/* Set default features */
+	dev->features |= NETIF_F_SG | NETIF_F_HIGHDMA | NETIF_F_HW_CSUM |
+			 NETIF_F_RXCSUM;
+#if 0
+	dev->hw_features |= dev->features;
+	dev->vlan_features |= dev->features;
+#endif
+	
+	/* Request the WOL interrupt and advertise suspend if available */
+	priv->wol_irq_disabled = true;
+	err = devm_request_irq(&pdev->dev, priv->wol_irq, bcmgenet_wol_isr, 0,
+			       dev->name, priv);
+	if (!err)
+		device_set_wakeup_capable(&pdev->dev, 1);
+
+#if 0
+	/* Set the needed headroom to account for any possible
+	 * features enabling/disabling at runtime
+	 */
+	dev->needed_headroom += 64;
+
+	/* On rpi-4 netdev_boot_setup_check returns 0 for eth%d */
+	netdev_boot_setup_check(dev);
+#endif
+	
+	priv->dev = dev;
+	priv->pdev = pdev;
+
+	pdata = device_get_match_data(&pdev->dev);
+	if (pdata) {
+		priv->version = pdata->version;
+		priv->dma_max_burst_length = pdata->dma_max_burst_length;
+	} else {
+		priv->version = pd->genet_version;
+		priv->dma_max_burst_length = DMA_MAX_BURST_LENGTH;
+	}
+
+	priv->clk = devm_clk_get_optional(&priv->pdev->dev, "enet");
+	if (IS_ERR(priv->clk)) {
+		dev_err(&priv->pdev->dev, "failed to get enet clock\n");
+		err = PTR_ERR(priv->clk);
+		goto err;
+	}
+
+	err = clk_prepare_enable(priv->clk);
+	if (err)
+		goto err;
+
+	bcmgenet_set_hw_params(priv);
+
+	err = -EIO;
+	if (priv->hw_params->flags & GENET_HAS_40BITS)
+		err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(40));
+	if (err)
+		err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	if (err) {
+		dev_err(&priv->pdev->dev, "dma_set_mask_and_coherent err=%d\n", err);
+		goto err_clk_disable;
+	}
+	
+	/* Mii wait queue */
+	init_swait_queue_head(&priv->swq);
+	
+	/* Always use RX_BUF_LENGTH (2KB) buffer for all chips */
+	priv->rx_buf_len = RX_BUF_LENGTH;
+	INIT_WORK(&priv->bcmgenet_irq_work, bcmgenet_irq_task);
+
+	priv->clk_wol = devm_clk_get_optional(&priv->pdev->dev, "enet-wol");
+	if (IS_ERR(priv->clk_wol)) {
+		dev_dbg(&priv->pdev->dev, "failed to get enet-wol clock\n");
+		err = PTR_ERR(priv->clk_wol);
+		goto err_clk_disable;
+	}
+
+	priv->clk_eee = devm_clk_get_optional(&priv->pdev->dev, "enet-eee");
+	if (IS_ERR(priv->clk_eee)) {
+		dev_dbg(&priv->pdev->dev, "failed to get enet-eee clock\n");
+		err = PTR_ERR(priv->clk_eee);
+		goto err_clk_disable;
+	}
+
+	/* If this is an internal GPHY, power it on now, before UniMAC is
+	 * brought out of reset as absolutely no UniMAC activity is allowed
+	 */
+	if (device_get_phy_mode(&pdev->dev) == PHY_INTERFACE_MODE_INTERNAL)
+		bcmgenet_power_up(priv, GENET_POWER_PASSIVE);
+
+	if (pd && !IS_ERR_OR_NULL(pd->mac_address))
+		ether_addr_copy(dev->dev_addr, pd->mac_address);
+	else
+		if (!device_get_mac_address(&pdev->dev, dev->dev_addr, ETH_ALEN))
+			if (has_acpi_companion(&pdev->dev))
+				bcmgenet_get_hw_addr(priv, dev->dev_addr);
+
+	if (!is_valid_ether_addr(dev->dev_addr)) {
+		dev_warn(&pdev->dev, "using random Ethernet MAC\n");
+		eth_random_addr(dev->dev_addr);
+	}
+	/* dummy mac addr must be different than proxy-dev addr which is dev addr */
+	eth_random_addr(dummy->dev_addr);
+
+	reset_umac(priv);
+
+	err = bcmgenet_mii_init(dummy);
+	if (err) {
+		dev_err(&priv->pdev->dev, "bcmgenet_mii_init err=%d\n", err);
+		goto err_clk_disable;
+	}
+
+	/* setup number of real queues  + 1 (GENET_V1 has 0 hardware queues
+	 * just the ring 16 descriptor based TX
+	 */
+	rtnetif_set_real_num_tx_queues(priv->dev, priv->hw_params->tx_queues + 1);
+	rtnetif_set_real_num_rx_queues(priv->dev, priv->hw_params->rx_queues + 1);
+
+	/* Set default coalescing parameters */
+	for (i = 0; i < priv->hw_params->rx_queues; i++)
+		priv->rx_rings[i].rx_max_coalesced_frames = 1;
+	priv->rx_rings[DESC_INDEX].rx_max_coalesced_frames = 1;
+
+	/* libphy will determine the link state */
+	rtnetif_carrier_off(dev);
+
+	/* Turn off the main clock, WOL clock is handled separately */
+	clk_disable_unprepare(priv->clk);
+
+	err = rt_register_rtnetdev(dev);
+	if (err) {
+		dev_err(&priv->pdev->dev, "rt_register_rtnetdev err=%d\n", err);
+		goto err;
+	}
+	err = register_netdev(dummy);
+	if (err) {
+		dev_err(&priv->pdev->dev, "register_netdev err=%d\n", err);
+		printk(KERN_ERR "register_netdev err=%d\n", err);
+		goto err;
+	} else {
+		printk(KERN_INFO "dummy-rteth0: register_netdev err=%d"
+			   " dummy->state=%lx\n", 
+			   err, dummy->state);
+	}
+	
+	return err;
+
+err_clk_disable:
+	clk_disable_unprepare(priv->clk);
+err:
+	free_netdev(dummy);
+err_0:
+	rt_unregister_rtnetdev(priv->dev);
+	rt_rtdev_disconnect(priv->dev);
+	rtdev_free(priv->dev);
+	return err;
+}
+
+static int bcmgenet_remove(struct platform_device *pdev)
+{
+	struct bcmgenet_priv *priv = dev_to_priv(&pdev->dev);
+	
+	dev_set_drvdata(&pdev->dev, NULL);
+	
+	rt_unregister_rtnetdev(priv->dev);
+#if 0
+	rt_stack_disconnect(priv->dev);
+#endif
+	rt_rtdev_disconnect(priv->dev);
+	rtdev_free(priv->dev);
+	
+	unregister_netdev(priv->dummy);
+	bcmgenet_mii_exit(priv->dummy);
+	free_netdev(priv->dummy);
+
+	return 0;
+}
+
+static void bcmgenet_shutdown(struct platform_device *pdev)
+{
+	bcmgenet_remove(pdev);
+}
+
+#if 0
+#ifdef CONFIG_PM_SLEEP
+static int bcmgenet_resume_noirq(struct device *d)
+{
+	struct rtnet_device *dev = dev_get_drvdata(d);
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	int ret;
+	u32 reg;
+
+	if (!rtnetif_running(priv->dev))
+		return 0;
+
+	/* Turn on the clock */
+	ret = clk_prepare_enable(priv->clk);
+	if (ret)
+		return ret;
+
+	if (device_may_wakeup(d) && priv->wolopts) {
+		/* Account for Wake-on-LAN events and clear those events
+		 * (Some devices need more time between enabling the clocks
+		 *  and the interrupt register reflecting the wake event so
+		 *  read the register twice)
+		 */
+		reg = bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT);
+		reg = bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT);
+		if (reg & UMAC_IRQ_WAKE_EVENT)
+			pm_wakeup_event(&priv->pdev->dev, 0);
+	}
+
+	bcmgenet_intrl2_0_writel(priv, UMAC_IRQ_WAKE_EVENT, INTRL2_CPU_CLEAR);
+
+	return 0;
+}
+
+static int bcmgenet_resume(struct device *d)
+{
+	struct rtnet_device *dev = dev_get_drvdata(d);
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	struct bcmgenet_rxnfc_rule *rule;
+	unsigned long dma_ctrl;
+	u32 reg;
+	int ret;
+
+	if (!rtnetif_running(priv->dev))
+		return 0;
+
+	/* From WOL-enabled suspend, switch to regular clock */
+	if (device_may_wakeup(d) && priv->wolopts)
+		bcmgenet_power_up(priv, GENET_POWER_WOL_MAGIC);
+
+	/* If this is an internal GPHY, power it back on now, before UniMAC is
+	 * brought out of reset as absolutely no UniMAC activity is allowed
+	 */
+	if (priv->internal_phy)
+		bcmgenet_power_up(priv, GENET_POWER_PASSIVE);
+
+	bcmgenet_umac_reset(priv);
+
+	init_umac(priv);
+
+	phy_init_hw(priv->dummy->phydev);
+
+	/* Speed settings must be restored */
+	genphy_config_aneg(priv->dummy->phydev);
+	bcmgenet_mii_config(priv->dev, false);
+
+	/* Restore enabled features */
+	bcmgenet_set_features(dev, dev->features);
+
+	bcmgenet_set_hw_addr(priv, dev->dev_addr);
+
+	/* Restore hardware filters */
+	bcmgenet_hfb_clear(priv);
+	list_for_each_entry(rule, &priv->rxnfc_list, list)
+		if (rule->state != BCMGENET_RXNFC_STATE_UNUSED)
+			bcmgenet_hfb_create_rxnfc_filter(priv, rule);
+
+	if (priv->internal_phy) {
+		reg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);
+		reg |= EXT_ENERGY_DET_MASK;
+		bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+	}
+
+	/* Disable RX/TX DMA and flush TX queues */
+	dma_ctrl = bcmgenet_dma_disable(priv);
+
+	/* Reinitialize TDMA and RDMA and SW housekeeping */
+	ret = bcmgenet_init_dma(priv);
+	if (ret) {
+		netdev_err(priv->dummy, "failed to initialize DMA\n");
+		goto out_clk_disable;
+	}
+
+	/* Always enable ring 16 - descriptor ring */
+	bcmgenet_enable_dma(priv, dma_ctrl);
+
+	if (!device_may_wakeup(d))
+		phy_resume(priv->dummy->phydev);
+
+	if (priv->eee.eee_enabled)
+		bcmgenet_eee_enable_set(dev, true);
+
+	bcmgenet_netif_start(dev);
+
+	rtnetif_device_attach(priv->dev);
+
+	return 0;
+
+out_clk_disable:
+	if (priv->internal_phy)
+		bcmgenet_power_down(priv, GENET_POWER_PASSIVE);
+	clk_disable_unprepare(priv->clk);
+	return ret;
+}
+
+static int bcmgenet_suspend(struct device *d)
+{
+	struct rtnet_device *dev = dev_get_drvdata(d);
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+
+	if (!rtnetif_running(priv->dev))
+		return 0;
+
+	rtnetif_device_detach(priv->dev);
+
+	bcmgenet_netif_stop(dev);
+
+	if (!device_may_wakeup(d))
+		phy_suspend(priv->dummy->phydev);
+
+	/* Disable filtering */
+	bcmgenet_hfb_reg_writel(priv, 0, HFB_CTRL);
+
+	return 0;
+}
+
+static int bcmgenet_suspend_noirq(struct device *d)
+{
+	struct rtnet_device *dev = dev_get_drvdata(d);
+	struct bcmgenet_priv *priv = rtnetdev_priv(dev);
+	int ret = 0;
+
+	if (!rtnetif_running(priv->dev))
+		return 0;
+
+	/* Prepare the device for Wake-on-LAN and switch to the slow clock */
+	if (device_may_wakeup(d) && priv->wolopts)
+		ret = bcmgenet_power_down(priv, GENET_POWER_WOL_MAGIC);
+	else if (priv->internal_phy)
+		ret = bcmgenet_power_down(priv, GENET_POWER_PASSIVE);
+
+	/* Let the framework handle resumption and leave the clocks on */
+	if (ret)
+		return ret;
+
+	/* Turn off the clocks */
+	clk_disable_unprepare(priv->clk);
+
+	return 0;
+}
+#else
+#define bcmgenet_suspend	NULL
+#define bcmgenet_suspend_noirq	NULL
+#define bcmgenet_resume		NULL
+#define bcmgenet_resume_noirq	NULL
+#endif /* CONFIG_PM_SLEEP */
+#endif /* if 0 */
+#define bcmgenet_suspend	NULL
+#define bcmgenet_suspend_noirq	NULL
+#define bcmgenet_resume		NULL
+#define bcmgenet_resume_noirq	NULL
+
+static const struct dev_pm_ops bcmgenet_pm_ops = {
+	.suspend	= bcmgenet_suspend,
+	.suspend_noirq	= bcmgenet_suspend_noirq,
+	.resume		= bcmgenet_resume,
+	.resume_noirq	= bcmgenet_resume_noirq,
+};
+
+static const struct acpi_device_id genet_acpi_match[] = {
+	{ "BCM6E4E", (kernel_ulong_t)&bcm2711_plat_data },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, genet_acpi_match);
+
+static struct platform_driver bcmgenet_driver = {
+	.probe	= bcmgenet_probe,
+	.remove	= bcmgenet_remove,
+	.shutdown = bcmgenet_shutdown,
+	.driver	= {
+		.name	= "bcmgenet",
+		.of_match_table = bcmgenet_match,
+		.pm	= &bcmgenet_pm_ops,
+		.acpi_match_table = genet_acpi_match,
+	},
+};
+module_platform_driver(bcmgenet_driver);
+
+MODULE_AUTHOR("Broadcom Corporation");
+MODULE_DESCRIPTION("Broadcom GENET Ethernet controller driver");
+MODULE_ALIAS("platform:bcmgenet");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/drivers/rpi-4/bcmgenet.h b/net/rtnet/drivers/rpi-4/bcmgenet.h
--- a/net/rtnet/drivers/rpi-4/bcmgenet.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rpi-4/bcmgenet.h	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,767 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (c) 2014-2020 Broadcom
+ */
+
+#ifndef __BCMGENET_H__
+#define __BCMGENET_H__
+
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/spinlock.h>
+#include <linux/clk.h>
+#include <linux/mii.h>
+#include <linux/if_vlan.h>
+#include <linux/phy.h>
+#include <linux/dim.h>
+#include <linux/ethtool.h>
+#include <linux/swait.h>
+
+/* *** RTnet *** */
+#include <rtnet_port.h>
+#include <rtnet_multiple_queues.h>
+
+/* total number of Buffer Descriptors, same for Rx/Tx */
+#define TOTAL_DESC				256
+
+/* which ring is descriptor based */
+#define DESC_INDEX				16
+
+/* Body(1500) + EH_SIZE(14) + VLANTAG(4) + BRCMTAG(6) + FCS(4) = 1528.
+ * 1536 is multiple of 256 bytes
+ */
+#define ENET_BRCM_TAG_LEN	6
+#define ENET_PAD		8
+#define ENET_MAX_MTU_SIZE	(ETH_DATA_LEN + ETH_HLEN + VLAN_HLEN + \
+				 ENET_BRCM_TAG_LEN + ETH_FCS_LEN + ENET_PAD)
+#define DMA_MAX_BURST_LENGTH    0x10
+
+/* misc. configuration */
+#define MAX_NUM_OF_FS_RULES		16
+#define CLEAR_ALL_HFB			0xFF
+#define DMA_FC_THRESH_HI		(TOTAL_DESC >> 4)
+#define DMA_FC_THRESH_LO		5
+
+/* 64B receive/transmit status block */
+struct status_64 {
+	u32	length_status;		/* length and peripheral status */
+	u32	ext_status;		/* Extended status*/
+	u32	rx_csum;		/* partial rx checksum */
+	u32	unused1[9];		/* unused */
+	u32	tx_csum_info;		/* Tx checksum info. */
+	u32	unused2[3];		/* unused */
+};
+
+/* Rx status bits */
+#define STATUS_RX_EXT_MASK		0x1FFFFF
+#define STATUS_RX_CSUM_MASK		0xFFFF
+#define STATUS_RX_CSUM_OK		0x10000
+#define STATUS_RX_CSUM_FR		0x20000
+#define STATUS_RX_PROTO_TCP		0
+#define STATUS_RX_PROTO_UDP		1
+#define STATUS_RX_PROTO_ICMP		2
+#define STATUS_RX_PROTO_OTHER		3
+#define STATUS_RX_PROTO_MASK		3
+#define STATUS_RX_PROTO_SHIFT		18
+#define STATUS_FILTER_INDEX_MASK	0xFFFF
+/* Tx status bits */
+#define STATUS_TX_CSUM_START_MASK	0X7FFF
+#define STATUS_TX_CSUM_START_SHIFT	16
+#define STATUS_TX_CSUM_PROTO_UDP	0x8000
+#define STATUS_TX_CSUM_OFFSET_MASK	0x7FFF
+#define STATUS_TX_CSUM_LV		0x80000000
+
+/* DMA Descriptor */
+#define DMA_DESC_LENGTH_STATUS	0x00	/* in bytes of data in buffer */
+#define DMA_DESC_ADDRESS_LO	0x04	/* lower bits of PA */
+#define DMA_DESC_ADDRESS_HI	0x08	/* upper 32 bits of PA, GENETv4+ */
+
+/* Rx/Tx common counter group */
+struct bcmgenet_pkt_counters {
+	u32	cnt_64;		/* RO Received/Transmited 64 bytes packet */
+	u32	cnt_127;	/* RO Rx/Tx 127 bytes packet */
+	u32	cnt_255;	/* RO Rx/Tx 65-255 bytes packet */
+	u32	cnt_511;	/* RO Rx/Tx 256-511 bytes packet */
+	u32	cnt_1023;	/* RO Rx/Tx 512-1023 bytes packet */
+	u32	cnt_1518;	/* RO Rx/Tx 1024-1518 bytes packet */
+	u32	cnt_mgv;	/* RO Rx/Tx 1519-1522 good VLAN packet */
+	u32	cnt_2047;	/* RO Rx/Tx 1522-2047 bytes packet*/
+	u32	cnt_4095;	/* RO Rx/Tx 2048-4095 bytes packet*/
+	u32	cnt_9216;	/* RO Rx/Tx 4096-9216 bytes packet*/
+};
+
+/* RSV, Receive Status Vector */
+struct bcmgenet_rx_counters {
+	struct  bcmgenet_pkt_counters pkt_cnt;
+	u32	pkt;		/* RO (0x428) Received pkt count*/
+	u32	bytes;		/* RO Received byte count */
+	u32	mca;		/* RO # of Received multicast pkt */
+	u32	bca;		/* RO # of Receive broadcast pkt */
+	u32	fcs;		/* RO # of Received FCS error  */
+	u32	cf;		/* RO # of Received control frame pkt*/
+	u32	pf;		/* RO # of Received pause frame pkt */
+	u32	uo;		/* RO # of unknown op code pkt */
+	u32	aln;		/* RO # of alignment error count */
+	u32	flr;		/* RO # of frame length out of range count */
+	u32	cde;		/* RO # of code error pkt */
+	u32	fcr;		/* RO # of carrier sense error pkt */
+	u32	ovr;		/* RO # of oversize pkt*/
+	u32	jbr;		/* RO # of jabber count */
+	u32	mtue;		/* RO # of MTU error pkt*/
+	u32	pok;		/* RO # of Received good pkt */
+	u32	uc;		/* RO # of unicast pkt */
+	u32	ppp;		/* RO # of PPP pkt */
+	u32	rcrc;		/* RO (0x470),# of CRC match pkt */
+};
+
+/* TSV, Transmit Status Vector */
+struct bcmgenet_tx_counters {
+	struct bcmgenet_pkt_counters pkt_cnt;
+	u32	pkts;		/* RO (0x4a8) Transmited pkt */
+	u32	mca;		/* RO # of xmited multicast pkt */
+	u32	bca;		/* RO # of xmited broadcast pkt */
+	u32	pf;		/* RO # of xmited pause frame count */
+	u32	cf;		/* RO # of xmited control frame count */
+	u32	fcs;		/* RO # of xmited FCS error count */
+	u32	ovr;		/* RO # of xmited oversize pkt */
+	u32	drf;		/* RO # of xmited deferral pkt */
+	u32	edf;		/* RO # of xmited Excessive deferral pkt*/
+	u32	scl;		/* RO # of xmited single collision pkt */
+	u32	mcl;		/* RO # of xmited multiple collision pkt*/
+	u32	lcl;		/* RO # of xmited late collision pkt */
+	u32	ecl;		/* RO # of xmited excessive collision pkt*/
+	u32	frg;		/* RO # of xmited fragments pkt*/
+	u32	ncl;		/* RO # of xmited total collision count */
+	u32	jbr;		/* RO # of xmited jabber count*/
+	u32	bytes;		/* RO # of xmited byte count */
+	u32	pok;		/* RO # of xmited good pkt */
+	u32	uc;		/* RO (0x0x4f0)# of xmited unitcast pkt */
+};
+
+struct bcmgenet_mib_counters {
+	struct bcmgenet_rx_counters rx;
+	struct bcmgenet_tx_counters tx;
+	u32	rx_runt_cnt;
+	u32	rx_runt_fcs;
+	u32	rx_runt_fcs_align;
+	u32	rx_runt_bytes;
+	u32	rbuf_ovflow_cnt;
+	u32	rbuf_err_cnt;
+	u32	mdf_err_cnt;
+	u32	alloc_rx_buff_failed;
+	u32	rx_dma_failed;
+	u32	tx_dma_failed;
+	u32	tx_realloc_tsb;
+	u32	tx_realloc_tsb_failed;
+};
+
+#define UMAC_HD_BKP_CTRL		0x004
+#define	 HD_FC_EN			(1 << 0)
+#define  HD_FC_BKOFF_OK			(1 << 1)
+#define  IPG_CONFIG_RX_SHIFT		2
+#define  IPG_CONFIG_RX_MASK		0x1F
+
+#define UMAC_CMD			0x008
+#define  CMD_TX_EN			(1 << 0)
+#define  CMD_RX_EN			(1 << 1)
+#define  UMAC_SPEED_10			0
+#define  UMAC_SPEED_100			1
+#define  UMAC_SPEED_1000		2
+#define  UMAC_SPEED_2500		3
+#define  CMD_SPEED_SHIFT		2
+#define  CMD_SPEED_MASK			3
+#define  CMD_PROMISC			(1 << 4)
+#define  CMD_PAD_EN			(1 << 5)
+#define  CMD_CRC_FWD			(1 << 6)
+#define  CMD_PAUSE_FWD			(1 << 7)
+#define  CMD_RX_PAUSE_IGNORE		(1 << 8)
+#define  CMD_TX_ADDR_INS		(1 << 9)
+#define  CMD_HD_EN			(1 << 10)
+#define  CMD_SW_RESET			(1 << 13)
+#define  CMD_LCL_LOOP_EN		(1 << 15)
+#define  CMD_AUTO_CONFIG		(1 << 22)
+#define  CMD_CNTL_FRM_EN		(1 << 23)
+#define  CMD_NO_LEN_CHK			(1 << 24)
+#define  CMD_RMT_LOOP_EN		(1 << 25)
+#define  CMD_PRBL_EN			(1 << 27)
+#define  CMD_TX_PAUSE_IGNORE		(1 << 28)
+#define  CMD_TX_RX_EN			(1 << 29)
+#define  CMD_RUNT_FILTER_DIS		(1 << 30)
+
+#define UMAC_MAC0			0x00C
+#define UMAC_MAC1			0x010
+#define UMAC_MAX_FRAME_LEN		0x014
+
+#define UMAC_MODE			0x44
+#define  MODE_LINK_STATUS		(1 << 5)
+
+#define UMAC_EEE_CTRL			0x064
+#define  EN_LPI_RX_PAUSE		(1 << 0)
+#define  EN_LPI_TX_PFC			(1 << 1)
+#define  EN_LPI_TX_PAUSE		(1 << 2)
+#define  EEE_EN				(1 << 3)
+#define  RX_FIFO_CHECK			(1 << 4)
+#define  EEE_TX_CLK_DIS			(1 << 5)
+#define  DIS_EEE_10M			(1 << 6)
+#define  LP_IDLE_PREDICTION_MODE	(1 << 7)
+
+#define UMAC_EEE_LPI_TIMER		0x068
+#define UMAC_EEE_WAKE_TIMER		0x06C
+#define UMAC_EEE_REF_COUNT		0x070
+#define  EEE_REFERENCE_COUNT_MASK	0xffff
+
+#define UMAC_TX_FLUSH			0x334
+
+#define UMAC_MIB_START			0x400
+
+#define UMAC_MDIO_CMD			0x614
+#define  MDIO_START_BUSY		(1 << 29)
+#define  MDIO_READ_FAIL			(1 << 28)
+#define  MDIO_RD			(2 << 26)
+#define  MDIO_WR			(1 << 26)
+#define  MDIO_PMD_SHIFT			21
+#define  MDIO_PMD_MASK			0x1F
+#define  MDIO_REG_SHIFT			16
+#define  MDIO_REG_MASK			0x1F
+
+#define UMAC_RBUF_OVFL_CNT_V1		0x61C
+#define RBUF_OVFL_CNT_V2		0x80
+#define RBUF_OVFL_CNT_V3PLUS		0x94
+
+#define UMAC_MPD_CTRL			0x620
+#define  MPD_EN				(1 << 0)
+#define  MPD_PW_EN			(1 << 27)
+#define  MPD_MSEQ_LEN_SHIFT		16
+#define  MPD_MSEQ_LEN_MASK		0xFF
+
+#define UMAC_MPD_PW_MS			0x624
+#define UMAC_MPD_PW_LS			0x628
+#define UMAC_RBUF_ERR_CNT_V1		0x634
+#define RBUF_ERR_CNT_V2			0x84
+#define RBUF_ERR_CNT_V3PLUS		0x98
+#define UMAC_MDF_ERR_CNT		0x638
+#define UMAC_MDF_CTRL			0x650
+#define UMAC_MDF_ADDR			0x654
+#define UMAC_MIB_CTRL			0x580
+#define  MIB_RESET_RX			(1 << 0)
+#define  MIB_RESET_RUNT			(1 << 1)
+#define  MIB_RESET_TX			(1 << 2)
+
+#define RBUF_CTRL			0x00
+#define  RBUF_64B_EN			(1 << 0)
+#define  RBUF_ALIGN_2B			(1 << 1)
+#define  RBUF_BAD_DIS			(1 << 2)
+
+#define RBUF_STATUS			0x0C
+#define  RBUF_STATUS_WOL		(1 << 0)
+#define  RBUF_STATUS_MPD_INTR_ACTIVE	(1 << 1)
+#define  RBUF_STATUS_ACPI_INTR_ACTIVE	(1 << 2)
+
+#define RBUF_CHK_CTRL			0x14
+#define  RBUF_RXCHK_EN			(1 << 0)
+#define  RBUF_SKIP_FCS			(1 << 4)
+#define  RBUF_L3_PARSE_DIS		(1 << 5)
+
+#define RBUF_ENERGY_CTRL		0x9c
+#define  RBUF_EEE_EN			(1 << 0)
+#define  RBUF_PM_EN			(1 << 1)
+
+#define RBUF_TBUF_SIZE_CTRL		0xb4
+
+#define RBUF_HFB_CTRL_V1		0x38
+#define  RBUF_HFB_FILTER_EN_SHIFT	16
+#define  RBUF_HFB_FILTER_EN_MASK	0xffff0000
+#define  RBUF_HFB_EN			(1 << 0)
+#define  RBUF_HFB_256B			(1 << 1)
+#define  RBUF_ACPI_EN			(1 << 2)
+
+#define RBUF_HFB_LEN_V1			0x3C
+#define  RBUF_FLTR_LEN_MASK		0xFF
+#define  RBUF_FLTR_LEN_SHIFT		8
+
+#define TBUF_CTRL			0x00
+#define  TBUF_64B_EN			(1 << 0)
+#define TBUF_BP_MC			0x0C
+#define TBUF_ENERGY_CTRL		0x14
+#define  TBUF_EEE_EN			(1 << 0)
+#define  TBUF_PM_EN			(1 << 1)
+
+#define TBUF_CTRL_V1			0x80
+#define TBUF_BP_MC_V1			0xA0
+
+#define HFB_CTRL			0x00
+#define HFB_FLT_ENABLE_V3PLUS		0x04
+#define HFB_FLT_LEN_V2			0x04
+#define HFB_FLT_LEN_V3PLUS		0x1C
+
+/* uniMac intrl2 registers */
+#define INTRL2_CPU_STAT			0x00
+#define INTRL2_CPU_SET			0x04
+#define INTRL2_CPU_CLEAR		0x08
+#define INTRL2_CPU_MASK_STATUS		0x0C
+#define INTRL2_CPU_MASK_SET		0x10
+#define INTRL2_CPU_MASK_CLEAR		0x14
+
+/* INTRL2 instance 0 definitions */
+#define UMAC_IRQ_SCB			(1 << 0)
+#define UMAC_IRQ_EPHY			(1 << 1)
+#define UMAC_IRQ_PHY_DET_R		(1 << 2)
+#define UMAC_IRQ_PHY_DET_F		(1 << 3)
+#define UMAC_IRQ_LINK_UP		(1 << 4)
+#define UMAC_IRQ_LINK_DOWN		(1 << 5)
+#define UMAC_IRQ_LINK_EVENT		(UMAC_IRQ_LINK_UP | UMAC_IRQ_LINK_DOWN)
+#define UMAC_IRQ_UMAC			(1 << 6)
+#define UMAC_IRQ_UMAC_TSV		(1 << 7)
+#define UMAC_IRQ_TBUF_UNDERRUN		(1 << 8)
+#define UMAC_IRQ_RBUF_OVERFLOW		(1 << 9)
+#define UMAC_IRQ_HFB_SM			(1 << 10)
+#define UMAC_IRQ_HFB_MM			(1 << 11)
+#define UMAC_IRQ_MPD_R			(1 << 12)
+#define UMAC_IRQ_WAKE_EVENT		(UMAC_IRQ_HFB_SM | UMAC_IRQ_HFB_MM | \
+					 UMAC_IRQ_MPD_R)
+#define UMAC_IRQ_RXDMA_MBDONE		(1 << 13)
+#define UMAC_IRQ_RXDMA_PDONE		(1 << 14)
+#define UMAC_IRQ_RXDMA_BDONE		(1 << 15)
+#define UMAC_IRQ_RXDMA_DONE		UMAC_IRQ_RXDMA_MBDONE
+#define UMAC_IRQ_TXDMA_MBDONE		(1 << 16)
+#define UMAC_IRQ_TXDMA_PDONE		(1 << 17)
+#define UMAC_IRQ_TXDMA_BDONE		(1 << 18)
+#define UMAC_IRQ_TXDMA_DONE		UMAC_IRQ_TXDMA_MBDONE
+
+/* Only valid for GENETv3+ */
+#define UMAC_IRQ_MDIO_DONE		(1 << 23)
+#define UMAC_IRQ_MDIO_ERROR		(1 << 24)
+
+/* INTRL2 instance 1 definitions */
+#define UMAC_IRQ1_TX_INTR_MASK		0xFFFF
+#define UMAC_IRQ1_RX_INTR_MASK		0xFFFF
+#define UMAC_IRQ1_RX_INTR_SHIFT		16
+
+/* Register block offsets */
+#define GENET_SYS_OFF			0x0000
+#define GENET_GR_BRIDGE_OFF		0x0040
+#define GENET_EXT_OFF			0x0080
+#define GENET_INTRL2_0_OFF		0x0200
+#define GENET_INTRL2_1_OFF		0x0240
+#define GENET_RBUF_OFF			0x0300
+#define GENET_UMAC_OFF			0x0800
+
+/* SYS block offsets and register definitions */
+#define SYS_REV_CTRL			0x00
+#define SYS_PORT_CTRL			0x04
+#define  PORT_MODE_INT_EPHY		0
+#define  PORT_MODE_INT_GPHY		1
+#define  PORT_MODE_EXT_EPHY		2
+#define  PORT_MODE_EXT_GPHY		3
+#define  PORT_MODE_EXT_RVMII_25		(4 | BIT(4))
+#define  PORT_MODE_EXT_RVMII_50		4
+#define  LED_ACT_SOURCE_MAC		(1 << 9)
+
+#define SYS_RBUF_FLUSH_CTRL		0x08
+#define SYS_TBUF_FLUSH_CTRL		0x0C
+#define RBUF_FLUSH_CTRL_V1		0x04
+
+/* Ext block register offsets and definitions */
+#define EXT_EXT_PWR_MGMT		0x00
+#define  EXT_PWR_DOWN_BIAS		(1 << 0)
+#define  EXT_PWR_DOWN_DLL		(1 << 1)
+#define  EXT_PWR_DOWN_PHY		(1 << 2)
+#define  EXT_PWR_DN_EN_LD		(1 << 3)
+#define  EXT_ENERGY_DET			(1 << 4)
+#define  EXT_IDDQ_FROM_PHY		(1 << 5)
+#define  EXT_IDDQ_GLBL_PWR		(1 << 7)
+#define  EXT_PHY_RESET			(1 << 8)
+#define  EXT_ENERGY_DET_MASK		(1 << 12)
+#define  EXT_PWR_DOWN_PHY_TX		(1 << 16)
+#define  EXT_PWR_DOWN_PHY_RX		(1 << 17)
+#define  EXT_PWR_DOWN_PHY_SD		(1 << 18)
+#define  EXT_PWR_DOWN_PHY_RD		(1 << 19)
+#define  EXT_PWR_DOWN_PHY_EN		(1 << 20)
+
+#define EXT_RGMII_OOB_CTRL		0x0C
+#define  RGMII_MODE_EN_V123		(1 << 0)
+#define  RGMII_LINK			(1 << 4)
+#define  OOB_DISABLE			(1 << 5)
+#define  RGMII_MODE_EN			(1 << 6)
+#define  ID_MODE_DIS			(1 << 16)
+
+#define EXT_GPHY_CTRL			0x1C
+#define  EXT_CFG_IDDQ_BIAS		(1 << 0)
+#define  EXT_CFG_PWR_DOWN		(1 << 1)
+#define  EXT_CK25_DIS			(1 << 4)
+#define  EXT_GPHY_RESET			(1 << 5)
+
+/* DMA rings size */
+#define DMA_RING_SIZE			(0x40)
+#define DMA_RINGS_SIZE			(DMA_RING_SIZE * (DESC_INDEX + 1))
+
+/* DMA registers common definitions */
+#define DMA_RW_POINTER_MASK		0x1FF
+#define DMA_P_INDEX_DISCARD_CNT_MASK	0xFFFF
+#define DMA_P_INDEX_DISCARD_CNT_SHIFT	16
+#define DMA_BUFFER_DONE_CNT_MASK	0xFFFF
+#define DMA_BUFFER_DONE_CNT_SHIFT	16
+#define DMA_P_INDEX_MASK		0xFFFF
+#define DMA_C_INDEX_MASK		0xFFFF
+
+/* DMA ring size register */
+#define DMA_RING_SIZE_MASK		0xFFFF
+#define DMA_RING_SIZE_SHIFT		16
+#define DMA_RING_BUFFER_SIZE_MASK	0xFFFF
+
+/* DMA interrupt threshold register */
+#define DMA_INTR_THRESHOLD_MASK		0x01FF
+
+/* DMA XON/XOFF register */
+#define DMA_XON_THREHOLD_MASK		0xFFFF
+#define DMA_XOFF_THRESHOLD_MASK		0xFFFF
+#define DMA_XOFF_THRESHOLD_SHIFT	16
+
+/* DMA flow period register */
+#define DMA_FLOW_PERIOD_MASK		0xFFFF
+#define DMA_MAX_PKT_SIZE_MASK		0xFFFF
+#define DMA_MAX_PKT_SIZE_SHIFT		16
+
+
+/* DMA control register */
+#define DMA_EN				(1 << 0)
+#define DMA_RING_BUF_EN_SHIFT		0x01
+#define DMA_RING_BUF_EN_MASK		0xFFFF
+#define DMA_TSB_SWAP_EN			(1 << 20)
+
+/* DMA status register */
+#define DMA_DISABLED			(1 << 0)
+#define DMA_DESC_RAM_INIT_BUSY		(1 << 1)
+
+/* DMA SCB burst size register */
+#define DMA_SCB_BURST_SIZE_MASK		0x1F
+
+/* DMA activity vector register */
+#define DMA_ACTIVITY_VECTOR_MASK	0x1FFFF
+
+/* DMA backpressure mask register */
+#define DMA_BACKPRESSURE_MASK		0x1FFFF
+#define DMA_PFC_ENABLE			(1 << 31)
+
+/* DMA backpressure status register */
+#define DMA_BACKPRESSURE_STATUS_MASK	0x1FFFF
+
+/* DMA override register */
+#define DMA_LITTLE_ENDIAN_MODE		(1 << 0)
+#define DMA_REGISTER_MODE		(1 << 1)
+
+/* DMA timeout register */
+#define DMA_TIMEOUT_MASK		0xFFFF
+#define DMA_TIMEOUT_VAL			5000	/* micro seconds */
+
+/* TDMA rate limiting control register */
+#define DMA_RATE_LIMIT_EN_MASK		0xFFFF
+
+/* TDMA arbitration control register */
+#define DMA_ARBITER_MODE_MASK		0x03
+#define DMA_RING_BUF_PRIORITY_MASK	0x1F
+#define DMA_RING_BUF_PRIORITY_SHIFT	5
+#define DMA_PRIO_REG_INDEX(q)		((q) / 6)
+#define DMA_PRIO_REG_SHIFT(q)		(((q) % 6) * DMA_RING_BUF_PRIORITY_SHIFT)
+#define DMA_RATE_ADJ_MASK		0xFF
+
+/* Tx/Rx Dma Descriptor common bits*/
+#define DMA_BUFLENGTH_MASK		0x0fff
+#define DMA_BUFLENGTH_SHIFT		16
+#define DMA_OWN				0x8000
+#define DMA_EOP				0x4000
+#define DMA_SOP				0x2000
+#define DMA_WRAP			0x1000
+/* Tx specific Dma descriptor bits */
+#define DMA_TX_UNDERRUN			0x0200
+#define DMA_TX_APPEND_CRC		0x0040
+#define DMA_TX_OW_CRC			0x0020
+#define DMA_TX_DO_CSUM			0x0010
+#define DMA_TX_QTAG_SHIFT		7
+
+/* Rx Specific Dma descriptor bits */
+#define DMA_RX_CHK_V3PLUS		0x8000
+#define DMA_RX_CHK_V12			0x1000
+#define DMA_RX_BRDCAST			0x0040
+#define DMA_RX_MULT			0x0020
+#define DMA_RX_LG			0x0010
+#define DMA_RX_NO			0x0008
+#define DMA_RX_RXER			0x0004
+#define DMA_RX_CRC_ERROR		0x0002
+#define DMA_RX_OV			0x0001
+#define DMA_RX_FI_MASK			0x001F
+#define DMA_RX_FI_SHIFT			0x0007
+#define DMA_DESC_ALLOC_MASK		0x00FF
+
+#define DMA_ARBITER_RR			0x00
+#define DMA_ARBITER_WRR			0x01
+#define DMA_ARBITER_SP			0x02
+
+struct enet_cb {
+	struct rtskb      *skb;
+	void __iomem *bd_addr;
+	DEFINE_DMA_UNMAP_ADDR(dma_addr);
+	DEFINE_DMA_UNMAP_LEN(dma_len);
+};
+
+/* power management mode */
+enum bcmgenet_power_mode {
+	GENET_POWER_CABLE_SENSE = 0,
+	GENET_POWER_PASSIVE,
+	GENET_POWER_WOL_MAGIC,
+};
+
+struct bcmgenet_priv;
+
+/* We support both runtime GENET detection and compile-time
+ * to optimize code-paths for a given hardware
+ */
+enum bcmgenet_version {
+	GENET_V1 = 1,
+	GENET_V2,
+	GENET_V3,
+	GENET_V4,
+	GENET_V5
+};
+
+#define GENET_IS_V1(p)	((p)->version == GENET_V1)
+#define GENET_IS_V2(p)	((p)->version == GENET_V2)
+#define GENET_IS_V3(p)	((p)->version == GENET_V3)
+#define GENET_IS_V4(p)	((p)->version == GENET_V4)
+#define GENET_IS_V5(p)	((p)->version == GENET_V5)
+
+/* Hardware flags */
+#define GENET_HAS_40BITS	(1 << 0)
+#define GENET_HAS_EXT		(1 << 1)
+#define GENET_HAS_MDIO_INTR	(1 << 2)
+#define GENET_HAS_MOCA_LINK_DET	(1 << 3)
+
+/* BCMGENET hardware parameters, keep this structure nicely aligned
+ * since it is going to be used in hot paths
+ */
+struct bcmgenet_hw_params {
+	u8		tx_queues;
+	u8		tx_bds_per_q;
+	u8		rx_queues;
+	u8		rx_bds_per_q;
+	u8		bp_in_en_shift;
+	u32		bp_in_mask;
+	u8		hfb_filter_cnt;
+	u8		hfb_filter_size;
+	u8		qtag_mask;
+	u16		tbuf_offset;
+	u32		hfb_offset;
+	u32		hfb_reg_offset;
+	u32		rdma_offset;
+	u32		tdma_offset;
+	u32		words_per_bd;
+	u32		flags;
+};
+
+struct bcmgenet_skb_cb {
+	struct enet_cb *first_cb;	/* First control block of SKB */
+	struct enet_cb *last_cb;	/* Last control block of SKB */
+	unsigned int bytes_sent;	/* bytes on the wire (no TSB) */
+};
+
+#define GENET_CB(skb)	((struct bcmgenet_skb_cb *)((skb)->cb))
+
+struct bcmgenet_tx_ring {
+	raw_spinlock_t	lock;		/* ring lock */
+	struct napi_struct napi;	/* NAPI per tx queue */
+	unsigned long	packets;
+	unsigned long	bytes;
+	unsigned int	index;		/* ring index */
+	unsigned int	queue;		/* queue index */
+	struct enet_cb	*cbs;		/* tx ring buffer control block*/
+	unsigned int	size;		/* size of each tx ring */
+	unsigned int    clean_ptr;      /* Tx ring clean pointer */
+	unsigned int	c_index;	/* last consumer index of each ring*/
+	unsigned int	free_bds;	/* # of free bds for each ring */
+	unsigned int	write_ptr;	/* Tx ring write pointer SW copy */
+	unsigned int	prod_index;	/* Tx ring producer index SW copy */
+	unsigned int	cb_ptr;		/* Tx ring initial CB ptr */
+	unsigned int	end_ptr;	/* Tx ring end CB ptr */
+	void (*int_enable)(struct bcmgenet_tx_ring *);
+	void (*int_disable)(struct bcmgenet_tx_ring *);
+	struct bcmgenet_priv *priv;
+};
+
+struct bcmgenet_net_dim {
+	u16		use_dim;
+	u16		event_ctr;
+	unsigned long	packets;
+	unsigned long	bytes;
+	struct dim	dim;
+};
+
+struct bcmgenet_rx_ring {
+	struct napi_struct napi;	/* Rx NAPI struct */
+	unsigned long	bytes;
+	unsigned long	packets;
+	unsigned long	errors;
+	unsigned long	dropped;
+	unsigned int	index;		/* Rx ring index */
+	struct enet_cb	*cbs;		/* Rx ring buffer control block */
+	unsigned int	size;		/* Rx ring size */
+	unsigned int	c_index;	/* Rx last consumer index */
+	unsigned int	read_ptr;	/* Rx ring read pointer */
+	unsigned int	cb_ptr;		/* Rx ring initial CB ptr */
+	unsigned int	end_ptr;	/* Rx ring end CB ptr */
+	unsigned int	old_discards;
+	struct bcmgenet_net_dim dim;
+	u32		rx_max_coalesced_frames;
+	u32		rx_coalesce_usecs;
+	void (*int_enable)(struct bcmgenet_rx_ring *);
+	void (*int_disable)(struct bcmgenet_rx_ring *);
+	struct bcmgenet_priv *priv;
+};
+
+enum bcmgenet_rxnfc_state {
+	BCMGENET_RXNFC_STATE_UNUSED = 0,
+	BCMGENET_RXNFC_STATE_DISABLED,
+	BCMGENET_RXNFC_STATE_ENABLED
+};
+
+struct bcmgenet_rxnfc_rule {
+	struct	list_head list;
+	struct ethtool_rx_flow_spec	fs;
+	enum bcmgenet_rxnfc_state state;
+};
+
+/* device context */
+struct bcmgenet_priv {
+	void __iomem *base;
+	enum bcmgenet_version version;
+	struct net_device *dummy;
+	struct rtnet_device *dev;
+
+	/* transmit variables */
+	void __iomem *tx_bds;
+	struct enet_cb *tx_cbs;
+	unsigned int num_tx_bds;
+
+	struct bcmgenet_tx_ring tx_rings[DESC_INDEX + 1];
+
+	/* receive variables */
+	void __iomem *rx_bds;
+	struct enet_cb *rx_cbs;
+	unsigned int num_rx_bds;
+	unsigned int rx_buf_len;
+	struct bcmgenet_rxnfc_rule rxnfc_rules[MAX_NUM_OF_FS_RULES];
+	struct list_head rxnfc_list;
+
+	struct bcmgenet_rx_ring rx_rings[DESC_INDEX + 1];
+
+	/* other misc variables */
+	struct bcmgenet_hw_params *hw_params;
+
+	/* MDIO bus variables */
+	struct swait_queue_head swq;
+	bool internal_phy;
+	struct device_node *phy_dn;
+	struct device_node *mdio_dn;
+	struct mii_bus *mii_bus;
+	u16 gphy_rev;
+	struct clk *clk_eee;
+	bool clk_eee_enabled;
+
+	/* PHY device variables */
+	int old_link;
+	int old_speed;
+	int old_duplex;
+	int old_pause;
+	phy_interface_t phy_interface;
+	int phy_addr;
+	int ext_phy;
+
+	/* Interrupt variables */
+	struct work_struct bcmgenet_irq_work;
+	int irq0;
+	int irq1;
+	int wol_irq;
+	bool wol_irq_disabled;
+
+	/* shared status */
+	raw_spinlock_t lock;
+	unsigned int irq0_stat;
+
+	/* HW descriptors/checksum variables */
+	bool crc_fwd_en;
+
+	u32 dma_max_burst_length;
+
+	u32 msg_enable;
+
+	struct clk *clk;
+	struct platform_device *pdev;
+	struct platform_device *mii_pdev;
+
+	/* WOL */
+	struct clk *clk_wol;
+	u32 wolopts;
+	u8 sopass[SOPASS_MAX];
+	bool wol_active;
+
+	struct bcmgenet_mib_counters mib;
+
+	struct ethtool_eee eee;
+	
+	struct net_device_stats stats;
+};
+
+#define GENET_IO_MACRO(name, offset)					\
+static inline u32 bcmgenet_##name##_readl(struct bcmgenet_priv *priv,	\
+					u32 off)			\
+{									\
+	/* MIPS chips strapped for BE will automagically configure the	\
+	 * peripheral registers for CPU-native byte order.		\
+	 */								\
+	if (IS_ENABLED(CONFIG_MIPS) && IS_ENABLED(CONFIG_CPU_BIG_ENDIAN)) \
+		return __raw_readl(priv->base + offset + off);		\
+	else								\
+		return readl_relaxed(priv->base + offset + off);	\
+}									\
+static inline void bcmgenet_##name##_writel(struct bcmgenet_priv *priv,	\
+					u32 val, u32 off)		\
+{									\
+	if (IS_ENABLED(CONFIG_MIPS) && IS_ENABLED(CONFIG_CPU_BIG_ENDIAN)) \
+		__raw_writel(val, priv->base + offset + off);		\
+	else								\
+		writel_relaxed(val, priv->base + offset + off);		\
+}
+
+GENET_IO_MACRO(ext, GENET_EXT_OFF);
+GENET_IO_MACRO(umac, GENET_UMAC_OFF);
+GENET_IO_MACRO(sys, GENET_SYS_OFF);
+
+/* interrupt l2 registers accessors */
+GENET_IO_MACRO(intrl2_0, GENET_INTRL2_0_OFF);
+GENET_IO_MACRO(intrl2_1, GENET_INTRL2_1_OFF);
+
+/* HFB register accessors  */
+GENET_IO_MACRO(hfb, priv->hw_params->hfb_offset);
+
+/* GENET v2+ HFB control and filter len helpers */
+GENET_IO_MACRO(hfb_reg, priv->hw_params->hfb_reg_offset);
+
+/* RBUF register accessors */
+GENET_IO_MACRO(rbuf, GENET_RBUF_OFF);
+
+/* MDIO routines */
+int bcmgenet_mii_init(struct net_device *dev);
+int bcmgenet_mii_config(struct net_device *dev, bool init);
+int bcmgenet_mii_probe(struct net_device *dev);
+void bcmgenet_mii_exit(struct net_device *dev);
+void bcmgenet_phy_power_set(struct net_device *dev, bool enable);
+void bcmgenet_mii_setup(struct net_device *dev);
+
+/* Wake-on-LAN routines */
+void bcmgenet_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol);
+int bcmgenet_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol);
+int bcmgenet_wol_power_down_cfg(struct bcmgenet_priv *priv,
+				enum bcmgenet_power_mode mode);
+void bcmgenet_wol_power_up_cfg(struct bcmgenet_priv *priv,
+			       enum bcmgenet_power_mode mode);
+
+#endif /* __BCMGENET_H__ */
diff -Naur a/net/rtnet/drivers/rpi-4/bcmgenet_wol.c b/net/rtnet/drivers/rpi-4/bcmgenet_wol.c
--- a/net/rtnet/drivers/rpi-4/bcmgenet_wol.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rpi-4/bcmgenet_wol.c	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,244 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Broadcom GENET (Gigabit Ethernet) Wake-on-LAN support
+ *
+ * Copyright (c) 2014-2020 Broadcom
+ */
+
+#define pr_fmt(fmt)				"bcmgenet_wol: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/interrupt.h>
+#include <linux/string.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/pm.h>
+#include <linux/clk.h>
+#include <linux/version.h>
+#include <linux/platform_device.h>
+#include <net/arp.h>
+
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/phy.h>
+
+#include "bcmgenet.h"
+
+/* ethtool function - get WOL (Wake on LAN) settings, Only Magic Packet
+ * Detection is supported through ethtool
+ */
+void bcmgenet_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	struct bcmgenet_priv *priv = netdev_priv(dev);
+
+	wol->supported = WAKE_MAGIC | WAKE_MAGICSECURE | WAKE_FILTER;
+	wol->wolopts = priv->wolopts;
+	memset(wol->sopass, 0, sizeof(wol->sopass));
+
+	if (wol->wolopts & WAKE_MAGICSECURE)
+		memcpy(wol->sopass, priv->sopass, sizeof(priv->sopass));
+}
+
+/* ethtool function - set WOL (Wake on LAN) settings.
+ * Only for magic packet detection mode.
+ */
+int bcmgenet_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	struct bcmgenet_priv *priv = netdev_priv(dev);
+	struct device *kdev = &priv->pdev->dev;
+
+	if (!device_can_wakeup(kdev))
+		return -ENOTSUPP;
+
+	if (wol->wolopts & ~(WAKE_MAGIC | WAKE_MAGICSECURE | WAKE_FILTER))
+		return -EINVAL;
+
+	if (wol->wolopts & WAKE_MAGICSECURE)
+		memcpy(priv->sopass, wol->sopass, sizeof(priv->sopass));
+
+	/* Flag the device and relevant IRQ as wakeup capable */
+	if (wol->wolopts) {
+		device_set_wakeup_enable(kdev, 1);
+		/* Avoid unbalanced enable_irq_wake calls */
+		if (priv->wol_irq_disabled)
+			enable_irq_wake(priv->wol_irq);
+		priv->wol_irq_disabled = false;
+	} else {
+		device_set_wakeup_enable(kdev, 0);
+		/* Avoid unbalanced disable_irq_wake calls */
+		if (!priv->wol_irq_disabled)
+			disable_irq_wake(priv->wol_irq);
+		priv->wol_irq_disabled = true;
+	}
+
+	priv->wolopts = wol->wolopts;
+
+	return 0;
+}
+
+static int bcmgenet_poll_wol_status(struct bcmgenet_priv *priv)
+{
+	struct net_device *dev = priv->dummy;
+	int retries = 0;
+
+	while (!(bcmgenet_rbuf_readl(priv, RBUF_STATUS)
+		& RBUF_STATUS_WOL)) {
+		retries++;
+		if (retries > 5) {
+			netdev_crit(dev, "polling wol mode timeout\n");
+			return -ETIMEDOUT;
+		}
+		mdelay(1);
+	}
+
+	return retries;
+}
+
+static void bcmgenet_set_mpd_password(struct bcmgenet_priv *priv)
+{
+	bcmgenet_umac_writel(priv, get_unaligned_be16(&priv->sopass[0]),
+			     UMAC_MPD_PW_MS);
+	bcmgenet_umac_writel(priv, get_unaligned_be32(&priv->sopass[2]),
+			     UMAC_MPD_PW_LS);
+}
+
+int bcmgenet_wol_power_down_cfg(struct bcmgenet_priv *priv,
+				enum bcmgenet_power_mode mode)
+{
+	struct net_device *dev = priv->dummy;
+	struct bcmgenet_rxnfc_rule *rule;
+	u32 reg, hfb_ctrl_reg, hfb_enable = 0;
+	int retries = 0;
+
+	if (mode != GENET_POWER_WOL_MAGIC) {
+		netif_err(priv, wol, dev, "unsupported mode: %d\n", mode);
+		return -EINVAL;
+	}
+
+	/* Can't suspend with WoL if MAC is still in reset */
+	reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+	if (reg & CMD_SW_RESET)
+		reg &= ~CMD_SW_RESET;
+
+	/* disable RX */
+	reg &= ~CMD_RX_EN;
+	bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+	mdelay(10);
+
+	if (priv->wolopts & (WAKE_MAGIC | WAKE_MAGICSECURE)) {
+		reg = bcmgenet_umac_readl(priv, UMAC_MPD_CTRL);
+		reg |= MPD_EN;
+		if (priv->wolopts & WAKE_MAGICSECURE) {
+			bcmgenet_set_mpd_password(priv);
+			reg |= MPD_PW_EN;
+		}
+		bcmgenet_umac_writel(priv, reg, UMAC_MPD_CTRL);
+	}
+
+	hfb_ctrl_reg = bcmgenet_hfb_reg_readl(priv, HFB_CTRL);
+	if (priv->wolopts & WAKE_FILTER) {
+		list_for_each_entry(rule, &priv->rxnfc_list, list)
+			if (rule->fs.ring_cookie == RX_CLS_FLOW_WAKE)
+				hfb_enable |= (1 << rule->fs.location);
+		reg = (hfb_ctrl_reg & ~RBUF_HFB_EN) | RBUF_ACPI_EN;
+		bcmgenet_hfb_reg_writel(priv, reg, HFB_CTRL);
+	}
+
+	/* Do not leave UniMAC in MPD mode only */
+	retries = bcmgenet_poll_wol_status(priv);
+	if (retries < 0) {
+		reg = bcmgenet_umac_readl(priv, UMAC_MPD_CTRL);
+		reg &= ~(MPD_EN | MPD_PW_EN);
+		bcmgenet_umac_writel(priv, reg, UMAC_MPD_CTRL);
+		bcmgenet_hfb_reg_writel(priv, hfb_ctrl_reg, HFB_CTRL);
+		return retries;
+	}
+
+	netif_dbg(priv, wol, dev, "MPD WOL-ready status set after %d msec\n",
+		  retries);
+
+	clk_prepare_enable(priv->clk_wol);
+	priv->wol_active = 1;
+
+	if (hfb_enable) {
+		bcmgenet_hfb_reg_writel(priv, hfb_enable,
+					HFB_FLT_ENABLE_V3PLUS + 4);
+		hfb_ctrl_reg = RBUF_HFB_EN | RBUF_ACPI_EN;
+		bcmgenet_hfb_reg_writel(priv, hfb_ctrl_reg, HFB_CTRL);
+	}
+
+	/* Enable CRC forward */
+	reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+	priv->crc_fwd_en = 1;
+	reg |= CMD_CRC_FWD;
+
+	/* Receiver must be enabled for WOL MP detection */
+	reg |= CMD_RX_EN;
+	bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+
+	if (priv->hw_params->flags & GENET_HAS_EXT) {
+		reg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);
+		reg &= ~EXT_ENERGY_DET_MASK;
+		bcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);
+	}
+
+	reg = UMAC_IRQ_MPD_R;
+	if (hfb_enable)
+		reg |=  UMAC_IRQ_HFB_SM | UMAC_IRQ_HFB_MM;
+
+	bcmgenet_intrl2_0_writel(priv, reg, INTRL2_CPU_MASK_CLEAR);
+
+	return 0;
+}
+
+void bcmgenet_wol_power_up_cfg(struct bcmgenet_priv *priv,
+			       enum bcmgenet_power_mode mode)
+{
+	u32 reg;
+
+	if (mode != GENET_POWER_WOL_MAGIC) {
+		netif_err(priv, wol, priv->dummy, "invalid mode: %d\n", mode);
+		return;
+	}
+
+	if (!priv->wol_active)
+		return;	/* failed to suspend so skip the rest */
+
+	priv->wol_active = 0;
+	clk_disable_unprepare(priv->clk_wol);
+	priv->crc_fwd_en = 0;
+
+	/* Disable Magic Packet Detection */
+	if (priv->wolopts & (WAKE_MAGIC | WAKE_MAGICSECURE)) {
+		reg = bcmgenet_umac_readl(priv, UMAC_MPD_CTRL);
+		if (!(reg & MPD_EN))
+			return;	/* already reset so skip the rest */
+		reg &= ~(MPD_EN | MPD_PW_EN);
+		bcmgenet_umac_writel(priv, reg, UMAC_MPD_CTRL);
+	}
+
+	/* Disable WAKE_FILTER Detection */
+	if (priv->wolopts & WAKE_FILTER) {
+		reg = bcmgenet_hfb_reg_readl(priv, HFB_CTRL);
+		if (!(reg & RBUF_ACPI_EN))
+			return;	/* already reset so skip the rest */
+		reg &= ~(RBUF_HFB_EN | RBUF_ACPI_EN);
+		bcmgenet_hfb_reg_writel(priv, reg, HFB_CTRL);
+	}
+
+	/* Disable CRC Forward */
+	reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+	reg &= ~CMD_CRC_FWD;
+	bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+}
diff -Naur a/net/rtnet/drivers/rpi-4/bcmmii.c b/net/rtnet/drivers/rpi-4/bcmmii.c
--- a/net/rtnet/drivers/rpi-4/bcmmii.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rpi-4/bcmmii.c	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,644 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Broadcom GENET MDIO routines
+ *
+ * Copyright (c) 2014-2017 Broadcom
+ */
+
+#include <linux/acpi.h>
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/bitops.h>
+#include <linux/netdevice.h>
+#include <linux/platform_device.h>
+#include <linux/phy.h>
+#include <linux/phy_fixed.h>
+#include <linux/brcmphy.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/platform_data/bcmgenet.h>
+#include <linux/platform_data/mdio-bcm-unimac.h>
+
+#include "bcmgenet.h"
+
+/* setup netdev link state when PHY link status change and
+ * update UMAC and RGMII block when link up
+ */
+void bcmgenet_mii_setup(struct net_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+	struct phy_device *phydev = dev->phydev;
+	u32 reg, cmd_bits = 0;
+	bool status_changed = false;
+
+	if (priv->old_link != phydev->link) {
+		status_changed = true;
+		priv->old_link = phydev->link;
+	}
+
+	if (phydev->link) {
+		/* check speed/duplex/pause changes */
+		if (priv->old_speed != phydev->speed) {
+			status_changed = true;
+			priv->old_speed = phydev->speed;
+		}
+
+		if (priv->old_duplex != phydev->duplex) {
+			status_changed = true;
+			priv->old_duplex = phydev->duplex;
+		}
+
+		if (priv->old_pause != phydev->pause) {
+			status_changed = true;
+			priv->old_pause = phydev->pause;
+		}
+
+		/* done if nothing has changed */
+		if (!status_changed)
+			return;
+
+		/* speed */
+		if (phydev->speed == SPEED_1000)
+			cmd_bits = UMAC_SPEED_1000;
+		else if (phydev->speed == SPEED_100)
+			cmd_bits = UMAC_SPEED_100;
+		else
+			cmd_bits = UMAC_SPEED_10;
+		cmd_bits <<= CMD_SPEED_SHIFT;
+
+		/* duplex */
+		if (phydev->duplex != DUPLEX_FULL)
+			cmd_bits |= CMD_HD_EN;
+
+		/* pause capability */
+		if (!phydev->pause)
+			cmd_bits |= CMD_RX_PAUSE_IGNORE | CMD_TX_PAUSE_IGNORE;
+
+		/*
+		 * Program UMAC and RGMII block based on established
+		 * link speed, duplex, and pause. The speed set in
+		 * umac->cmd tell RGMII block which clock to use for
+		 * transmit -- 25MHz(100Mbps) or 125MHz(1Gbps).
+		 * Receive clock is provided by the PHY.
+		 */
+		reg = bcmgenet_ext_readl(priv, EXT_RGMII_OOB_CTRL);
+		reg &= ~OOB_DISABLE;
+		reg |= RGMII_LINK;
+		bcmgenet_ext_writel(priv, reg, EXT_RGMII_OOB_CTRL);
+
+		reg = bcmgenet_umac_readl(priv, UMAC_CMD);
+		reg &= ~((CMD_SPEED_MASK << CMD_SPEED_SHIFT) |
+			       CMD_HD_EN |
+			       CMD_RX_PAUSE_IGNORE | CMD_TX_PAUSE_IGNORE);
+		reg |= cmd_bits;
+		if (reg & CMD_SW_RESET) {
+			reg &= ~CMD_SW_RESET;
+			bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+			udelay(2);
+			reg |= CMD_TX_EN | CMD_RX_EN;
+		}
+		bcmgenet_umac_writel(priv, reg, UMAC_CMD);
+	} else {
+		/* done if nothing has changed */
+		if (!status_changed)
+			return;
+
+		/* needed for MoCA fixed PHY to reflect correct link status */
+		netif_carrier_off(dev);
+	}
+
+	phy_print_status(phydev);
+}
+
+
+static int bcmgenet_fixed_phy_link_update(struct net_device *dev,
+					  struct fixed_phy_status *status)
+{
+	struct bcmgenet_priv *priv;
+	u32 reg;
+
+	if (dev && dev->phydev && status) {
+		priv = rtnetdev_priv(
+			((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+		reg = bcmgenet_umac_readl(priv, UMAC_MODE);
+		status->link = !!(reg & MODE_LINK_STATUS);
+	}
+
+	return 0;
+}
+
+void bcmgenet_phy_power_set(struct net_device *dev, bool enable)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+	u32 reg = 0;
+
+	/* EXT_GPHY_CTRL is only valid for GENETv4 and onward */
+	if (GENET_IS_V4(priv)) {
+		reg = bcmgenet_ext_readl(priv, EXT_GPHY_CTRL);
+		if (enable) {
+			reg &= ~EXT_CK25_DIS;
+			bcmgenet_ext_writel(priv, reg, EXT_GPHY_CTRL);
+			mdelay(1);
+
+			reg &= ~(EXT_CFG_IDDQ_BIAS | EXT_CFG_PWR_DOWN);
+			reg |= EXT_GPHY_RESET;
+			bcmgenet_ext_writel(priv, reg, EXT_GPHY_CTRL);
+			mdelay(1);
+
+			reg &= ~EXT_GPHY_RESET;
+		} else {
+			reg |= EXT_CFG_IDDQ_BIAS | EXT_CFG_PWR_DOWN |
+			       EXT_GPHY_RESET;
+			bcmgenet_ext_writel(priv, reg, EXT_GPHY_CTRL);
+			mdelay(1);
+			reg |= EXT_CK25_DIS;
+		}
+		bcmgenet_ext_writel(priv, reg, EXT_GPHY_CTRL);
+		udelay(60);
+	} else {
+		mdelay(1);
+	}
+}
+
+static void bcmgenet_moca_phy_setup(struct bcmgenet_priv *priv)
+{
+	u32 reg;
+
+	if (!GENET_IS_V5(priv)) {
+		/* Speed settings are set in bcmgenet_mii_setup() */
+		reg = bcmgenet_sys_readl(priv, SYS_PORT_CTRL);
+		reg |= LED_ACT_SOURCE_MAC;
+		bcmgenet_sys_writel(priv, reg, SYS_PORT_CTRL);
+	}
+
+	if (priv->hw_params->flags & GENET_HAS_MOCA_LINK_DET)
+		fixed_phy_set_link_update(priv->dummy->phydev,
+					  bcmgenet_fixed_phy_link_update);
+}
+
+int bcmgenet_mii_config(struct net_device *dev, bool init)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+	struct phy_device *phydev = dev->phydev;
+	struct device *kdev = &priv->pdev->dev;
+	const char *phy_name = NULL;
+	u32 id_mode_dis = 0;
+	u32 port_ctrl;
+	u32 reg;
+
+	switch (priv->phy_interface) {
+	case PHY_INTERFACE_MODE_INTERNAL:
+		phy_name = "internal PHY";
+		/* fall through */
+	case PHY_INTERFACE_MODE_MOCA:
+		/* Irrespective of the actually configured PHY speed (100 or
+		 * 1000) GENETv4 only has an internal GPHY so we will just end
+		 * up masking the Gigabit features from what we support, not
+		 * switching to the EPHY
+		 */
+		if (GENET_IS_V4(priv))
+			port_ctrl = PORT_MODE_INT_GPHY;
+		else
+			port_ctrl = PORT_MODE_INT_EPHY;
+
+		if (!phy_name) {
+			phy_name = "MoCA";
+			bcmgenet_moca_phy_setup(priv);
+		}
+		break;
+
+	case PHY_INTERFACE_MODE_MII:
+		phy_name = "external MII";
+		phy_set_max_speed(phydev, SPEED_100);
+		port_ctrl = PORT_MODE_EXT_EPHY;
+		break;
+
+	case PHY_INTERFACE_MODE_REVMII:
+		phy_name = "external RvMII";
+		/* of_mdiobus_register took care of reading the 'max-speed'
+		 * PHY property for us, effectively limiting the PHY supported
+		 * capabilities, use that knowledge to also configure the
+		 * Reverse MII interface correctly.
+		 */
+		if (linkmode_test_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+				      dev->phydev->supported))
+			port_ctrl = PORT_MODE_EXT_RVMII_50;
+		else
+			port_ctrl = PORT_MODE_EXT_RVMII_25;
+		break;
+
+	case PHY_INTERFACE_MODE_RGMII:
+		/* RGMII_NO_ID: TXC transitions at the same time as TXD
+		 *		(requires PCB or receiver-side delay)
+		 *
+		 * ID is implicitly disabled for 100Mbps (RG)MII operation.
+		 */
+		phy_name = "external RGMII (no delay)";
+		id_mode_dis = BIT(16);
+		port_ctrl = PORT_MODE_EXT_GPHY;
+		break;
+
+	case PHY_INTERFACE_MODE_RGMII_TXID:
+		/* RGMII_TXID:	Add 2ns delay on TXC (90 degree shift) */
+		phy_name = "external RGMII (TX delay)";
+		port_ctrl = PORT_MODE_EXT_GPHY;
+		break;
+
+	case PHY_INTERFACE_MODE_RGMII_RXID:
+		phy_name = "external RGMII (RX delay)";
+		port_ctrl = PORT_MODE_EXT_GPHY;
+		break;
+	default:
+		dev_err(kdev, "unknown phy mode: %d\n", priv->phy_interface);
+		return -EINVAL;
+	}
+
+	bcmgenet_sys_writel(priv, port_ctrl, SYS_PORT_CTRL);
+
+	priv->ext_phy = !priv->internal_phy &&
+			(priv->phy_interface != PHY_INTERFACE_MODE_MOCA);
+
+	/* This is an external PHY (xMII), so we need to enable the RGMII
+	 * block for the interface to work
+	 */
+	if (priv->ext_phy) {
+		reg = bcmgenet_ext_readl(priv, EXT_RGMII_OOB_CTRL);
+		reg &= ~ID_MODE_DIS;
+		reg |= id_mode_dis;
+		if (GENET_IS_V1(priv) || GENET_IS_V2(priv) || GENET_IS_V3(priv))
+			reg |= RGMII_MODE_EN_V123;
+		else
+			reg |= RGMII_MODE_EN;
+		bcmgenet_ext_writel(priv, reg, EXT_RGMII_OOB_CTRL);
+	}
+
+	if (init)
+		dev_info(kdev, "configuring instance for %s\n", phy_name);
+
+	return 0;
+}
+
+int bcmgenet_mii_probe(struct net_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+	struct device *kdev = &priv->pdev->dev;
+	struct device_node *dn = kdev->of_node;
+	struct phy_device *phydev;
+	u32 phy_flags = 0;
+	int ret;
+
+	/* Communicate the integrated PHY revision */
+	if (priv->internal_phy)
+		phy_flags = priv->gphy_rev;
+
+	/* Initialize link state variables that bcmgenet_mii_setup() uses */
+	priv->old_link = -1;
+	priv->old_speed = -1;
+	priv->old_duplex = -1;
+	priv->old_pause = -1;
+
+	if (dn) {
+		phydev = of_phy_connect(dev, priv->phy_dn, bcmgenet_mii_setup,
+					phy_flags, priv->phy_interface);
+		if (!phydev) {
+			pr_err("could not attach to PHY\n");
+			return -ENODEV;
+		}
+	} else {
+		if (has_acpi_companion(kdev)) {
+			char mdio_bus_id[MII_BUS_ID_SIZE];
+			struct mii_bus *unimacbus;
+
+			snprintf(mdio_bus_id, MII_BUS_ID_SIZE, "%s-%d",
+				 UNIMAC_MDIO_DRV_NAME, priv->pdev->id);
+
+			unimacbus = mdio_find_bus(mdio_bus_id);
+			if (!unimacbus) {
+				pr_err("Unable to find mii\n");
+				return -ENODEV;
+			}
+			phydev = phy_find_first(unimacbus);
+			put_device(&unimacbus->dev);
+			if (!phydev) {
+				pr_err("Unable to find PHY\n");
+				return -ENODEV;
+			}
+		} else {
+			phydev = dev->phydev;
+		}
+		phydev->dev_flags = phy_flags;
+
+		ret = phy_connect_direct(dev, phydev, bcmgenet_mii_setup,
+					 priv->phy_interface);
+		if (ret) {
+			pr_err("could not attach to PHY\n");
+			return -ENODEV;
+		}
+	}
+
+	/* Configure port multiplexer based on what the probed PHY device since
+	 * reading the 'max-speed' property determines the maximum supported
+	 * PHY speed which is needed for bcmgenet_mii_config() to configure
+	 * things appropriately.
+	 */
+	ret = bcmgenet_mii_config(dev, true);
+	if (ret) {
+		phy_disconnect(dev->phydev);
+		return ret;
+	}
+
+	linkmode_copy(phydev->advertising, phydev->supported);
+
+	/* The internal PHY has its link interrupts routed to the
+	 * Ethernet MAC ISRs. On GENETv5 there is a hardware issue
+	 * that prevents the signaling of link UP interrupts when
+	 * the link operates at 10Mbps, so fallback to polling for
+	 * those versions of GENET.
+	 */
+	if (priv->internal_phy && !GENET_IS_V5(priv))
+		dev->phydev->irq = PHY_IGNORE_INTERRUPT;
+
+	return 0;
+}
+
+static struct device_node *bcmgenet_mii_of_find_mdio(struct bcmgenet_priv *priv)
+{
+	struct device_node *dn = priv->pdev->dev.of_node;
+	struct device *kdev = &priv->pdev->dev;
+	char *compat;
+
+	compat = kasprintf(GFP_KERNEL, "brcm,genet-mdio-v%d", priv->version);
+	if (!compat)
+		return NULL;
+
+	priv->mdio_dn = of_get_compatible_child(dn, compat);
+	kfree(compat);
+	if (!priv->mdio_dn) {
+		dev_err(kdev, "unable to find MDIO bus node\n");
+		return NULL;
+	}
+
+	return priv->mdio_dn;
+}
+
+static void bcmgenet_mii_pdata_init(struct bcmgenet_priv *priv,
+				    struct unimac_mdio_pdata *ppd)
+{
+	struct device *kdev = &priv->pdev->dev;
+	struct bcmgenet_platform_data *pd = kdev->platform_data;
+
+	if (pd->phy_interface != PHY_INTERFACE_MODE_MOCA && pd->mdio_enabled) {
+		/*
+		 * Internal or external PHY with MDIO access
+		 */
+		if (pd->phy_address >= 0 && pd->phy_address < PHY_MAX_ADDR)
+			ppd->phy_mask = 1 << pd->phy_address;
+		else
+			ppd->phy_mask = 0;
+	}
+}
+
+static int bcmgenet_mii_wait(void *wait_func_data)
+{
+	struct bcmgenet_priv *priv = wait_func_data;
+
+#if 0
+	wait_event_timeout(priv->wq,
+			   !(bcmgenet_umac_readl(priv, UMAC_MDIO_CMD)
+			   & MDIO_START_BUSY),
+			   HZ / 100);
+#endif
+	swait_event_timeout_exclusive(priv->swq,
+				!(bcmgenet_umac_readl(priv, UMAC_MDIO_CMD)
+			   	& MDIO_START_BUSY),
+				HZ / 100);
+	return 0;
+}
+
+static int bcmgenet_mii_register(struct bcmgenet_priv *priv)
+{
+	struct platform_device *pdev = priv->pdev;
+	struct bcmgenet_platform_data *pdata = pdev->dev.platform_data;
+	struct device_node *dn = pdev->dev.of_node;
+	struct unimac_mdio_pdata ppd;
+	struct platform_device *ppdev;
+	struct resource *pres, res;
+	int id, ret;
+
+	pres = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	memset(&res, 0, sizeof(res));
+	memset(&ppd, 0, sizeof(ppd));
+
+	ppd.wait_func = bcmgenet_mii_wait;
+	ppd.wait_func_data = priv;
+	ppd.bus_name = "bcmgenet MII bus";
+
+	/* Unimac MDIO bus controller starts at UniMAC offset + MDIO_CMD
+	 * and is 2 * 32-bits word long, 8 bytes total.
+	 */
+	res.start = pres->start + GENET_UMAC_OFF + UMAC_MDIO_CMD;
+	res.end = res.start + 8;
+	res.flags = IORESOURCE_MEM;
+
+	if (dn)
+		id = of_alias_get_id(dn, "eth");
+	else
+		id = pdev->id;
+
+	ppdev = platform_device_alloc(UNIMAC_MDIO_DRV_NAME, id);
+	if (!ppdev)
+		return -ENOMEM;
+
+	/* Retain this platform_device pointer for later cleanup */
+	priv->mii_pdev = ppdev;
+	ppdev->dev.parent = &pdev->dev;
+	if (dn)
+		ppdev->dev.of_node = bcmgenet_mii_of_find_mdio(priv);
+	else if (pdata)
+		bcmgenet_mii_pdata_init(priv, &ppd);
+	else
+		ppd.phy_mask = ~0;
+
+	ret = platform_device_add_resources(ppdev, &res, 1);
+	if (ret)
+		goto out;
+
+	ret = platform_device_add_data(ppdev, &ppd, sizeof(ppd));
+	if (ret)
+		goto out;
+
+	ret = platform_device_add(ppdev);
+	if (ret)
+		goto out;
+
+	return 0;
+out:
+	platform_device_put(ppdev);
+	return ret;
+}
+
+static int bcmgenet_phy_interface_init(struct bcmgenet_priv *priv)
+{
+	struct device *kdev = &priv->pdev->dev;
+	int phy_mode = device_get_phy_mode(kdev);
+
+	if (phy_mode < 0) {
+		dev_err(kdev, "invalid PHY mode property\n");
+		return phy_mode;
+	}
+
+	priv->phy_interface = phy_mode;
+
+	/* We need to specifically look up whether this PHY interface is
+	 * internal or not *before* we even try to probe the PHY driver
+	 * over MDIO as we may have shut down the internal PHY for power
+	 * saving purposes.
+	 */
+	if (priv->phy_interface == PHY_INTERFACE_MODE_INTERNAL)
+		priv->internal_phy = true;
+
+	return 0;
+}
+
+static int bcmgenet_mii_of_init(struct bcmgenet_priv *priv)
+{
+	struct device_node *dn = priv->pdev->dev.of_node;
+	struct phy_device *phydev;
+	int ret;
+
+	/* Fetch the PHY phandle */
+	priv->phy_dn = of_parse_phandle(dn, "phy-handle", 0);
+
+	/* In the case of a fixed PHY, the DT node associated
+	 * to the PHY is the Ethernet MAC DT node.
+	 */
+	if (!priv->phy_dn && of_phy_is_fixed_link(dn)) {
+		ret = of_phy_register_fixed_link(dn);
+		if (ret)
+			return ret;
+
+		priv->phy_dn = of_node_get(dn);
+	}
+
+	/* Get the link mode */
+	ret = bcmgenet_phy_interface_init(priv);
+	if (ret)
+		return ret;
+
+	/* Make sure we initialize MoCA PHYs with a link down */
+	if (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {
+		phydev = of_phy_find_device(dn);
+		if (phydev) {
+			phydev->link = 0;
+			put_device(&phydev->mdio.dev);
+		}
+	}
+
+	return 0;
+}
+
+static int bcmgenet_mii_pd_init(struct bcmgenet_priv *priv)
+{
+	struct device *kdev = &priv->pdev->dev;
+	struct bcmgenet_platform_data *pd = kdev->platform_data;
+	char phy_name[MII_BUS_ID_SIZE + 3];
+	char mdio_bus_id[MII_BUS_ID_SIZE];
+	struct phy_device *phydev;
+
+	snprintf(mdio_bus_id, MII_BUS_ID_SIZE, "%s-%d",
+		 UNIMAC_MDIO_DRV_NAME, priv->pdev->id);
+
+	if (pd->phy_interface != PHY_INTERFACE_MODE_MOCA && pd->mdio_enabled) {
+		snprintf(phy_name, MII_BUS_ID_SIZE, PHY_ID_FMT,
+			 mdio_bus_id, pd->phy_address);
+
+		/*
+		 * Internal or external PHY with MDIO access
+		 */
+		phydev = phy_attach(priv->dummy, phy_name, pd->phy_interface);
+		if (!phydev) {
+			dev_err(kdev, "failed to register PHY device\n");
+			return -ENODEV;
+		}
+	} else {
+		/*
+		 * MoCA port or no MDIO access.
+		 * Use fixed PHY to represent the link layer.
+		 */
+		struct fixed_phy_status fphy_status = {
+			.link = 1,
+			.speed = pd->phy_speed,
+			.duplex = pd->phy_duplex,
+			.pause = 0,
+			.asym_pause = 0,
+		};
+
+		phydev = fixed_phy_register(PHY_POLL, &fphy_status, NULL);
+		if (!phydev || IS_ERR(phydev)) {
+			dev_err(kdev, "failed to register fixed PHY device\n");
+			return -ENODEV;
+		}
+
+		/* Make sure we initialize MoCA PHYs with a link down */
+		phydev->link = 0;
+
+	}
+
+	priv->phy_interface = pd->phy_interface;
+
+	return 0;
+}
+
+static int bcmgenet_mii_bus_init(struct bcmgenet_priv *priv)
+{
+	struct device *kdev = &priv->pdev->dev;
+	struct device_node *dn = kdev->of_node;
+
+	if (dn)
+		return bcmgenet_mii_of_init(priv);
+	else if (has_acpi_companion(kdev))
+		return bcmgenet_phy_interface_init(priv);
+	else
+		return bcmgenet_mii_pd_init(priv);
+}
+
+int bcmgenet_mii_init(struct net_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+	int ret;
+
+	ret = bcmgenet_mii_register(priv);
+	if (ret)
+		return ret;
+
+	ret = bcmgenet_mii_bus_init(priv);
+	if (ret)
+		goto out;
+
+	return 0;
+
+out:
+	bcmgenet_mii_exit(dev);
+	return ret;
+}
+
+void bcmgenet_mii_exit(struct net_device *dev)
+{
+	struct bcmgenet_priv *priv = rtnetdev_priv(
+		((struct dummy_rtnetdev_priv*)netdev_priv(dev))->rtdev);
+	struct device_node *dn = priv->pdev->dev.of_node;
+
+	if (of_phy_is_fixed_link(dn))
+		of_phy_deregister_fixed_link(dn);
+	of_node_put(priv->phy_dn);
+	platform_device_unregister(priv->mii_pdev);
+}
diff -Naur a/net/rtnet/drivers/rpi-4/Kconfig b/net/rtnet/drivers/rpi-4/Kconfig
--- a/net/rtnet/drivers/rpi-4/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rpi-4/Kconfig	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,20 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Broadcom device configuration
+#
+
+config RTNET_DRV_BCMGENET
+	tristate "Broadcom GENET internal MAC support"
+	depends on HAS_IOMEM
+	default n
+	select MII
+	select PHYLIB
+	select FIXED_PHY
+	select BCM7XXX_PHY
+	select MDIO_BCM_UNIMAC
+	select DIMLIB
+	select BROADCOM_PHY if ARCH_BCM2835
+	help
+	  This driver supports the built-in Ethernet MACs found in the
+	  Broadcom BCM7xxx Set Top Box family chipset.
+
diff -Naur a/net/rtnet/drivers/rpi-4/Makefile b/net/rtnet/drivers/rpi-4/Makefile
--- a/net/rtnet/drivers/rpi-4/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rpi-4/Makefile	2021-07-14 15:39:13.258125333 +0300
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_DRV_BCMGENET) += genet.o
+genet-objs := bcmgenet.o bcmmii.o bcmgenet_wol.o
diff -Naur a/net/rtnet/drivers/rt_davinci_mdio.c b/net/rtnet/drivers/rt_davinci_mdio.c
--- a/net/rtnet/drivers/rt_davinci_mdio.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rt_davinci_mdio.c	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,534 @@
+/*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * 2019/09/20
+ * Small modifications to Geoffrey Bonneville
+ * and Hidde Verstoep RTNet port to beaglebone-black,
+ * modifications made by Laurentiu-Cristian Duca
+ * in order to compile on kernel 4.14.71
+ *
+ * DaVinci MDIO Module driver
+ *
+ * Copyright (C) 2010 Texas Instruments.
+ *
+ * Shamelessly ripped out of davinci_emac.c, original copyrights follow:
+ *
+ * Copyright (C) 2009 Texas Instruments.
+ *
+ * ---------------------------------------------------------------------------
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ * ---------------------------------------------------------------------------
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/phy.h>
+#include <linux/clk.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/pm_runtime.h>
+#include <linux/davinci_emac.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+
+#include <rtnet_port.h>
+
+/*
+ * This timeout definition is a worst-case ultra defensive measure against
+ * unexpected controller lock ups.  Ideally, we should never ever hit this
+ * scenario in practice.
+ */
+#define MDIO_TIMEOUT		100 /* msecs */
+
+#define PHY_REG_MASK		0x1f
+#define PHY_ID_MASK		0x1f
+
+#define DEF_OUT_FREQ		2200000		/* 2.2 MHz */
+
+struct davinci_mdio_regs {
+	u32	version;
+	u32	control;
+#define CONTROL_IDLE		BIT(31)
+#define CONTROL_ENABLE		BIT(30)
+#define CONTROL_MAX_DIV		(0xffff)
+
+	u32	alive;
+	u32	link;
+	u32	linkintraw;
+	u32	linkintmasked;
+	u32	__reserved_0[2];
+	u32	userintraw;
+	u32	userintmasked;
+	u32	userintmaskset;
+	u32	userintmaskclr;
+	u32	__reserved_1[20];
+
+	struct {
+		u32	access;
+#define USERACCESS_GO		BIT(31)
+#define USERACCESS_WRITE	BIT(30)
+#define USERACCESS_ACK		BIT(29)
+#define USERACCESS_READ		(0)
+#define USERACCESS_DATA		(0xffff)
+
+		u32	physel;
+	}	user[0];
+};
+
+struct mdio_platform_data default_pdata = {
+	.bus_freq = DEF_OUT_FREQ,
+};
+
+struct davinci_mdio_data {
+	struct mdio_platform_data pdata;
+	struct davinci_mdio_regs __iomem *regs;
+	raw_spinlock_t	lock;
+	struct clk	*clk;
+	struct device	*dev;
+	struct mii_bus	*bus;
+	bool		suspended;
+	unsigned long	access_time; /* jiffies */
+};
+
+static void __davinci_mdio_reset(struct davinci_mdio_data *data)
+{
+	u32 mdio_in, div, mdio_out_khz, access_time;
+
+	mdio_in = clk_get_rate(data->clk);
+	div = (mdio_in / data->pdata.bus_freq) - 1;
+	if (div > CONTROL_MAX_DIV)
+		div = CONTROL_MAX_DIV;
+
+	/* set enable and clock divider */
+	__raw_writel(div | CONTROL_ENABLE, &data->regs->control);
+
+	/*
+	 * One mdio transaction consists of:
+	 *	32 bits of preamble
+	 *	32 bits of transferred data
+	 *	24 bits of bus yield (not needed unless shared?)
+	 */
+	mdio_out_khz = mdio_in / (1000 * (div + 1));
+	access_time  = (88 * 1000) / mdio_out_khz;
+
+	/*
+	 * In the worst case, we could be kicking off a user-access immediately
+	 * after the mdio bus scan state-machine triggered its own read.  If
+	 * so, our request could get deferred by one access cycle.  We
+	 * defensively allow for 4 access cycles.
+	 */
+	data->access_time = usecs_to_jiffies(access_time * 4);
+	if (!data->access_time)
+		data->access_time = 1;
+}
+
+static int davinci_mdio_reset(struct mii_bus *bus)
+{
+	struct davinci_mdio_data *data = bus->priv;
+	u32 phy_mask, ver;
+
+	__davinci_mdio_reset(data);
+
+	/* wait for scan logic to settle */
+	msleep(PHY_MAX_ADDR * data->access_time);
+
+	/* dump hardware version info */
+	ver = __raw_readl(&data->regs->version);
+	dev_info(data->dev, "davinci mdio revision %d.%d\n",
+		 (ver >> 8) & 0xff, ver & 0xff);
+
+	/* get phy mask from the alive register */
+	phy_mask = __raw_readl(&data->regs->alive);
+	if (phy_mask) {
+		/* restrict mdio bus to live phys only */
+		dev_info(data->dev, "detected phy mask %x\n", ~phy_mask);
+		phy_mask = ~phy_mask;
+	} else {
+		/* desperately scan all phys */
+		dev_warn(data->dev, "no live phy, scanning all\n");
+		phy_mask = 0;
+	}
+	data->bus->phy_mask = phy_mask;
+
+	return 0;
+}
+
+/* wait until hardware is ready for another user access */
+static inline int wait_for_user_access(struct davinci_mdio_data *data)
+{
+	struct davinci_mdio_regs __iomem *regs = data->regs;
+	unsigned long timeout = jiffies + msecs_to_jiffies(MDIO_TIMEOUT);
+	u32 reg;
+
+	while (time_after(timeout, jiffies)) {
+		reg = __raw_readl(&regs->user[0].access);
+		if ((reg & USERACCESS_GO) == 0)
+			return 0;
+
+		reg = __raw_readl(&regs->control);
+		if ((reg & CONTROL_IDLE) == 0)
+			continue;
+
+		/*
+		 * An emac soft_reset may have clobbered the mdio controller's
+		 * state machine.  We need to reset and retry the current
+		 * operation
+		 */
+		dev_warn(data->dev, "resetting idled controller\n");
+		__davinci_mdio_reset(data);
+		return -EAGAIN;
+	}
+
+	reg = __raw_readl(&regs->user[0].access);
+	if ((reg & USERACCESS_GO) == 0)
+		return 0;
+
+	dev_err(data->dev, "timed out waiting for user access\n");
+	return -ETIMEDOUT;
+}
+
+/* wait until hardware state machine is idle */
+static inline int wait_for_idle(struct davinci_mdio_data *data)
+{
+	struct davinci_mdio_regs __iomem *regs = data->regs;
+	unsigned long timeout = jiffies + msecs_to_jiffies(MDIO_TIMEOUT);
+
+	while (time_after(timeout, jiffies)) {
+		if (__raw_readl(&regs->control) & CONTROL_IDLE)
+			return 0;
+	}
+	dev_err(data->dev, "timed out waiting for idle\n");
+	return -ETIMEDOUT;
+}
+
+static int davinci_mdio_read(struct mii_bus *bus, int phy_id, int phy_reg)
+{
+	struct davinci_mdio_data *data = bus->priv;
+	u32 reg;
+	int ret;
+
+	if (phy_reg & ~PHY_REG_MASK || phy_id & ~PHY_ID_MASK)
+		return -EINVAL;
+
+	raw_spin_lock(&data->lock);
+
+	if (data->suspended) {
+		raw_spin_unlock(&data->lock);
+		return -ENODEV;
+	}
+
+	reg = (USERACCESS_GO | USERACCESS_READ | (phy_reg << 21) |
+	       (phy_id << 16));
+
+	while (1) {
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		__raw_writel(reg, &data->regs->user[0].access);
+
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		reg = __raw_readl(&data->regs->user[0].access);
+		ret = (reg & USERACCESS_ACK) ? (reg & USERACCESS_DATA) : -EIO;
+		break;
+	}
+
+	raw_spin_unlock(&data->lock);
+
+	return ret;
+}
+
+static int davinci_mdio_write(struct mii_bus *bus, int phy_id,
+			      int phy_reg, u16 phy_data)
+{
+	struct davinci_mdio_data *data = bus->priv;
+	u32 reg;
+	int ret;
+
+	if (phy_reg & ~PHY_REG_MASK || phy_id & ~PHY_ID_MASK)
+		return -EINVAL;
+
+	raw_spin_lock(&data->lock);
+
+	if (data->suspended) {
+		raw_spin_unlock(&data->lock);
+		return -ENODEV;
+	}
+
+	reg = (USERACCESS_GO | USERACCESS_WRITE | (phy_reg << 21) |
+		   (phy_id << 16) | (phy_data & USERACCESS_DATA));
+
+	while (1) {
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		__raw_writel(reg, &data->regs->user[0].access);
+
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		break;
+	}
+
+	raw_spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static int davinci_mdio_probe_dt(struct mdio_platform_data *data,
+			 struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	u32 prop;
+
+	if (!node)
+		return -EINVAL;
+
+	if (of_property_read_u32(node, "bus_freq", &prop)) {
+		pr_err("Missing bus_freq property in the DT.\n");
+		return -EINVAL;
+	}
+	data->bus_freq = prop;
+
+	return 0;
+}
+
+
+static int davinci_mdio_probe(struct platform_device *pdev)
+{
+	struct mdio_platform_data *pdata = pdev->dev.platform_data;
+	struct device *dev = &pdev->dev;
+	struct davinci_mdio_data *data;
+	struct resource *res;
+	struct phy_device *phy;
+	int ret, addr;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data) {
+		dev_err(dev, "failed to alloc device data\n");
+		return -ENOMEM;
+	}
+
+	data->bus = mdiobus_alloc();
+	if (!data->bus) {
+		dev_err(dev, "failed to alloc mii bus\n");
+		ret = -ENOMEM;
+		goto bail_out;
+	}
+
+	if (dev->of_node) {
+		if (davinci_mdio_probe_dt(&data->pdata, pdev))
+			data->pdata = default_pdata;
+		snprintf(data->bus->id, MII_BUS_ID_SIZE, "%s", pdev->name);
+	} else {
+		data->pdata = pdata ? (*pdata) : default_pdata;
+		snprintf(data->bus->id, MII_BUS_ID_SIZE, "%s-%x",
+			 pdev->name, pdev->id);
+	}
+
+	data->bus->name		= dev_name(dev);
+	data->bus->read		= davinci_mdio_read,
+	data->bus->write	= davinci_mdio_write,
+	data->bus->reset	= davinci_mdio_reset,
+	data->bus->parent	= dev;
+	data->bus->priv		= data;
+
+	pm_runtime_enable(&pdev->dev);
+	pm_runtime_get_sync(&pdev->dev);
+	data->clk = clk_get(&pdev->dev, "fck");
+	if (IS_ERR(data->clk)) {
+		dev_err(dev, "failed to get device clock\n");
+		ret = PTR_ERR(data->clk);
+		data->clk = NULL;
+		goto bail_out;
+	}
+
+	dev_set_drvdata(dev, data);
+	data->dev = dev;
+	raw_spin_lock_init(&data->lock);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "could not find register map resource\n");
+		ret = -ENOENT;
+		goto bail_out;
+	}
+
+	res = devm_request_mem_region(dev, res->start, resource_size(res),
+					    dev_name(dev));
+	if (!res) {
+		dev_err(dev, "could not allocate register map resource\n");
+		ret = -ENXIO;
+		goto bail_out;
+	}
+
+	data->regs = devm_ioremap_nocache(dev, res->start, resource_size(res));
+	if (!data->regs) {
+		dev_err(dev, "could not map mdio registers\n");
+		ret = -ENOMEM;
+		goto bail_out;
+	}
+
+	/* register the mii bus */
+	ret = mdiobus_register(data->bus);
+	if (ret)
+		goto bail_out;
+
+	/* scan and dump the bus */
+	for (addr = 0; addr < PHY_MAX_ADDR; addr++) {
+		phy = mdiobus_get_phy(data->bus, addr);
+		if (phy) {
+			dev_info(dev, "phy[%d]: device %s, driver %s\n",
+				 phy->mdio.addr, phydev_name(phy),
+				 phy->drv ? phy->drv->name : "unknown");
+		}
+	}
+	
+	return 0;
+
+bail_out:
+	if (data->bus)
+		mdiobus_free(data->bus);
+
+	if (data->clk)
+		clk_put(data->clk);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	kfree(data);
+
+	return ret;
+}
+
+static int davinci_mdio_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+
+	if (data->bus) {
+		mdiobus_unregister(data->bus);
+		mdiobus_free(data->bus);
+	}
+
+	if (data->clk)
+		clk_put(data->clk);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	dev_set_drvdata(dev, NULL);
+
+	kfree(data);
+
+	return 0;
+}
+
+static int davinci_mdio_suspend(struct device *dev)
+{
+	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
+
+	raw_spin_lock(&data->lock);
+
+	/* shutdown the scan state machine */
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl &= ~CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
+	wait_for_idle(data);
+
+	pm_runtime_put_sync(data->dev);
+
+	data->suspended = true;
+	raw_spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static int davinci_mdio_resume(struct device *dev)
+{
+	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
+
+	raw_spin_lock(&data->lock);
+	pm_runtime_get_sync(data->dev);
+
+	/* restart the scan state machine */
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl |= CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
+
+	data->suspended = false;
+	raw_spin_unlock(&data->lock);
+
+	return 0;
+}
+
+int load_davinci_mdio_module(int v)
+{
+	return v;
+}
+/* Every module that calls this function will automatically load this module */
+EXPORT_SYMBOL_GPL(load_davinci_mdio_module);
+
+static const struct dev_pm_ops davinci_mdio_pm_ops = {
+	.suspend	= davinci_mdio_suspend,
+	.resume		= davinci_mdio_resume,
+};
+
+static const struct of_device_id davinci_mdio_of_mtable[] = {
+	{ .compatible = "ti,davinci_mdio", },
+	{ /* sentinel */ },
+};
+
+static struct platform_driver davinci_mdio_driver = {
+	.driver = {
+		.name	 = "davinci_mdio",
+		.owner	 = THIS_MODULE,
+		.pm	 = &davinci_mdio_pm_ops,
+		.of_match_table = of_match_ptr(davinci_mdio_of_mtable),
+	},
+	.probe = davinci_mdio_probe,
+	.remove = davinci_mdio_remove,
+};
+
+static int __init davinci_mdio_init(void)
+{
+	return platform_driver_register(&davinci_mdio_driver);
+}
+device_initcall(davinci_mdio_init);
+
+static void __exit davinci_mdio_exit(void)
+{
+	platform_driver_unregister(&davinci_mdio_driver);
+}
+module_exit(davinci_mdio_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("RT DaVinci MDIO driver");
diff -Naur a/net/rtnet/drivers/rt_smsc.c b/net/rtnet/drivers/rt_smsc.c
--- a/net/rtnet/drivers/rt_smsc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/rt_smsc.c	2021-07-14 15:39:13.250125390 +0300
@@ -0,0 +1,404 @@
+/*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2019 Laurentiu-Cristian Duca
+ * Allmost identical to Hidde Verstoep RTNet port to beaglebone-black,
+ * small modifications made by Laurentiu-Cristian Duca
+ *
+ * Driver for SMSC PHYs
+ *
+ * Author: Herbert Valerio Riedel
+ *
+ * Copyright (c) 2006 Herbert Valerio Riedel <hvr@gnu.org>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * Support added for SMSC LAN8187 and LAN8700 by steve.glendinning@shawell.net
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/of.h>
+#include <linux/phy.h>
+#include <linux/netdevice.h>
+#include <linux/smscphy.h>
+
+struct smsc_hw_stat {
+	const char *string;
+	u8 reg;
+	u8 bits;
+};
+
+static struct smsc_hw_stat smsc_hw_stats[] = {
+	{ "phy_symbol_errors", 26, 16},
+};
+
+struct smsc_phy_priv {
+	bool energy_enable;
+};
+
+static int smsc_phy_config_intr(struct phy_device *phydev)
+{
+	int rc = phy_write (phydev, MII_LAN83C185_IM,
+			((PHY_INTERRUPT_ENABLED == phydev->interrupts)
+			? MII_LAN83C185_ISF_INT_PHYLIB_EVENTS
+			: 0));
+
+	return rc < 0 ? rc : 0;
+}
+
+static int smsc_phy_ack_interrupt(struct phy_device *phydev)
+{
+	int rc = phy_read (phydev, MII_LAN83C185_ISF);
+
+	return rc < 0 ? rc : 0;
+}
+
+static int smsc_phy_config_init(struct phy_device *phydev)
+{
+	struct smsc_phy_priv *priv = phydev->priv;
+
+	int rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+
+	if (rc < 0)
+		return rc;
+
+	if (priv->energy_enable) {
+		/* Enable energy detect mode for this SMSC Transceivers */
+		rc = phy_write(phydev, MII_LAN83C185_CTRL_STATUS,
+			       rc | MII_LAN83C185_EDPWRDOWN);
+		if (rc < 0)
+			return rc;
+	}
+
+	return smsc_phy_ack_interrupt(phydev);
+}
+
+static int smsc_phy_reset(struct phy_device *phydev)
+{
+	int rc = phy_read(phydev, MII_LAN83C185_SPECIAL_MODES);
+	if (rc < 0)
+		return rc;
+
+	/* If the SMSC PHY is in power down mode, then set it
+	 * in all capable mode before using it.
+	 */
+	if ((rc & MII_LAN83C185_MODE_MASK) == MII_LAN83C185_MODE_POWERDOWN) {
+		/* set "all capable" mode */
+		rc |= MII_LAN83C185_MODE_ALL;
+		phy_write(phydev, MII_LAN83C185_SPECIAL_MODES, rc);
+	}
+
+	/* reset the phy */
+	return genphy_soft_reset(phydev);
+}
+
+static int lan911x_config_init(struct phy_device *phydev)
+{
+	return smsc_phy_ack_interrupt(phydev);
+}
+
+/*
+ * The LAN87xx suffers from rare absence of the ENERGYON-bit when Ethernet cable
+ * plugs in while LAN87xx is in Energy Detect Power-Down mode. This leads to
+ * unstable detection of plugging in Ethernet cable.
+ * This workaround disables Energy Detect Power-Down mode and waiting for
+ * response on link pulses to detect presence of plugged Ethernet cable.
+ * The Energy Detect Power-Down mode is enabled again in the end of procedure to
+ * save approximately 220 mW of power if cable is unplugged.
+ */
+static int lan87xx_read_status(struct phy_device *phydev)
+{
+	struct smsc_phy_priv *priv = phydev->priv;
+
+	int err = genphy_read_status(phydev);
+
+	if (!phydev->link && priv->energy_enable) {
+		int i;
+
+		/* Disable EDPD to wake up PHY */
+		int rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+		if (rc < 0)
+			return rc;
+
+		rc = phy_write(phydev, MII_LAN83C185_CTRL_STATUS,
+			       rc & ~MII_LAN83C185_EDPWRDOWN);
+		if (rc < 0)
+			return rc;
+
+		/* Wait max 640 ms to detect energy */
+		for (i = 0; i < 64; i++) {
+			/* Sleep to allow link test pulses to be sent */
+			msleep(10);
+			rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+			if (rc < 0)
+				return rc;
+			if (rc & MII_LAN83C185_ENERGYON)
+				break;
+		}
+
+		/* Re-enable EDPD */
+		rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+		if (rc < 0)
+			return rc;
+
+		rc = phy_write(phydev, MII_LAN83C185_CTRL_STATUS,
+			       rc | MII_LAN83C185_EDPWRDOWN);
+		if (rc < 0)
+			return rc;
+	}
+
+	return err;
+}
+
+static int smsc_get_sset_count(struct phy_device *phydev)
+{
+	return ARRAY_SIZE(smsc_hw_stats);
+}
+
+static void smsc_get_strings(struct phy_device *phydev, u8 *data)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(smsc_hw_stats); i++) {
+		strncpy(data + i * ETH_GSTRING_LEN,
+		       smsc_hw_stats[i].string, ETH_GSTRING_LEN);
+	}
+}
+
+#ifndef UINT64_MAX
+#define UINT64_MAX              (u64)(~((u64)0))
+#endif
+static u64 smsc_get_stat(struct phy_device *phydev, int i)
+{
+	struct smsc_hw_stat stat = smsc_hw_stats[i];
+	int val;
+	u64 ret;
+
+	val = phy_read(phydev, stat.reg);
+	if (val < 0)
+		ret = UINT64_MAX;
+	else
+		ret = val;
+
+	return ret;
+}
+
+static void smsc_get_stats(struct phy_device *phydev,
+			   struct ethtool_stats *stats, u64 *data)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(smsc_hw_stats); i++)
+		data[i] = smsc_get_stat(phydev, i);
+}
+
+static int smsc_phy_probe(struct phy_device *phydev)
+{
+	struct device *dev = &phydev->mdio.dev;
+	struct device_node *of_node = dev->of_node;
+	struct smsc_phy_priv *priv;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->energy_enable = true;
+
+	if (of_property_read_bool(of_node, "smsc,disable-energy-detect")) {
+		pr_info("smsc,disable-energy-detect found");
+		priv->energy_enable = false;
+	} else
+		pr_info("smsc,disable-energy-detect missing");
+
+	phydev->priv = priv;
+
+	return 0;
+}
+
+int load_smsc_phy_module(int v)
+{
+	return v;
+}
+/* Every module that calls this function will automatically load this module */
+EXPORT_SYMBOL_GPL(load_smsc_phy_module);
+
+static struct phy_driver smsc_phy_driver[] = {
+{
+	.phy_id		= 0x0007c0a0, /* OUI=0x00800f, Model#=0x0a */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN83C185",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0b0, /* OUI=0x00800f, Model#=0x0b */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN8187",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0c0, /* OUI=0x00800f, Model#=0x0c */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN8700",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= lan87xx_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0d0, /* OUI=0x00800f, Model#=0x0d */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN911x Internal PHY",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.config_init	= lan911x_config_init,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0f0, /* OUI=0x00800f, Model#=0x0f */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "RT SMSC LAN8710/LAN8720",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= lan87xx_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c110,
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN8740",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= lan87xx_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+} };
+
+module_phy_driver(smsc_phy_driver);
+
+MODULE_DESCRIPTION("SMSC PHY driver");
+MODULE_AUTHOR("Herbert Valerio Riedel");
+MODULE_LICENSE("GPL");
+
+static struct mdio_device_id __maybe_unused smsc_tbl[] = {
+	{ 0x0007c0a0, 0xfffffff0 },
+	{ 0x0007c0b0, 0xfffffff0 },
+	{ 0x0007c0c0, 0xfffffff0 },
+	{ 0x0007c0d0, 0xfffffff0 },
+	{ 0x0007c0f0, 0xfffffff0 },
+	{ 0x0007c110, 0xfffffff0 },
+	{ }
+};
+
+MODULE_DEVICE_TABLE(mdio, smsc_tbl);
diff -Naur a/net/rtnet/drivers/ticpsw/cpsw_ale.c b/net/rtnet/drivers/ticpsw/cpsw_ale.c
--- a/net/rtnet/drivers/ticpsw/cpsw_ale.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/cpsw_ale.c	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,665 @@
+/*
+ * Texas Instruments 3-Port Ethernet Switch Address Lookup Engine
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/stat.h>
+#include <linux/sysfs.h>
+#include <linux/etherdevice.h>
+
+#include "cpsw_ale.h"
+
+#define BITMASK(bits)		(BIT(bits) - 1)
+#define ALE_ENTRY_BITS		68
+#define ALE_ENTRY_WORDS	DIV_ROUND_UP(ALE_ENTRY_BITS, 32)
+
+#define ALE_VERSION_MAJOR(rev)	((rev >> 8) & 0xff)
+#define ALE_VERSION_MINOR(rev)	(rev & 0xff)
+
+/* ALE Registers */
+#define ALE_IDVER		0x00
+#define ALE_CONTROL		0x08
+#define ALE_PRESCALE		0x10
+#define ALE_UNKNOWNVLAN		0x18
+#define ALE_TABLE_CONTROL	0x20
+#define ALE_TABLE		0x34
+#define ALE_PORTCTL		0x40
+
+#define ALE_TABLE_WRITE		BIT(31)
+
+#define ALE_TYPE_FREE			0
+#define ALE_TYPE_ADDR			1
+#define ALE_TYPE_VLAN			2
+#define ALE_TYPE_VLAN_ADDR		3
+
+#define ALE_UCAST_PERSISTANT		0
+#define ALE_UCAST_UNTOUCHED		1
+#define ALE_UCAST_OUI			2
+#define ALE_UCAST_TOUCHED		3
+
+static inline int cpsw_ale_get_field(u32 *ale_entry, u32 start, u32 bits)
+{
+	int idx;
+
+	idx    = start / 32;
+	start -= idx * 32;
+	idx    = 2 - idx; /* flip */
+	return (ale_entry[idx] >> start) & BITMASK(bits);
+}
+
+static inline void cpsw_ale_set_field(u32 *ale_entry, u32 start, u32 bits,
+				      u32 value)
+{
+	int idx;
+
+	value &= BITMASK(bits);
+	idx    = start / 32;
+	start -= idx * 32;
+	idx    = 2 - idx; /* flip */
+	ale_entry[idx] &= ~(BITMASK(bits) << start);
+	ale_entry[idx] |=  (value << start);
+}
+
+#define DEFINE_ALE_FIELD(name, start, bits)				\
+static inline int cpsw_ale_get_##name(u32 *ale_entry)			\
+{									\
+	return cpsw_ale_get_field(ale_entry, start, bits);		\
+}									\
+static inline void cpsw_ale_set_##name(u32 *ale_entry, u32 value)	\
+{									\
+	cpsw_ale_set_field(ale_entry, start, bits, value);		\
+}
+
+DEFINE_ALE_FIELD(entry_type,		60,	2)
+DEFINE_ALE_FIELD(vlan_id,		48,	12)
+DEFINE_ALE_FIELD(mcast_state,		62,	2)
+DEFINE_ALE_FIELD(port_mask,		66,     3)
+DEFINE_ALE_FIELD(super,			65,	1)
+DEFINE_ALE_FIELD(ucast_type,		62,     2)
+DEFINE_ALE_FIELD(port_num,		66,     2)
+DEFINE_ALE_FIELD(blocked,		65,     1)
+DEFINE_ALE_FIELD(secure,		64,     1)
+DEFINE_ALE_FIELD(vlan_untag_force,	24,	3)
+DEFINE_ALE_FIELD(vlan_reg_mcast,	16,	3)
+DEFINE_ALE_FIELD(vlan_unreg_mcast,	8,	3)
+DEFINE_ALE_FIELD(vlan_member_list,	0,	3)
+DEFINE_ALE_FIELD(mcast,			40,	1)
+
+/* The MAC address field in the ALE entry cannot be macroized as above */
+static inline void cpsw_ale_get_addr(u32 *ale_entry, u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < 6; i++)
+		addr[i] = cpsw_ale_get_field(ale_entry, 40 - 8*i, 8);
+}
+
+static inline void cpsw_ale_set_addr(u32 *ale_entry, u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < 6; i++)
+		cpsw_ale_set_field(ale_entry, 40 - 8*i, 8, addr[i]);
+}
+
+static int cpsw_ale_read(struct cpsw_ale *ale, int idx, u32 *ale_entry)
+{
+	int i;
+
+	WARN_ON(idx > ale->params.ale_entries);
+
+	__raw_writel(idx, ale->params.ale_regs + ALE_TABLE_CONTROL);
+
+	for (i = 0; i < ALE_ENTRY_WORDS; i++)
+		ale_entry[i] = __raw_readl(ale->params.ale_regs +
+					   ALE_TABLE + 4 * i);
+
+	return idx;
+}
+
+static int cpsw_ale_write(struct cpsw_ale *ale, int idx, u32 *ale_entry)
+{
+	int i;
+
+	WARN_ON(idx > ale->params.ale_entries);
+
+	for (i = 0; i < ALE_ENTRY_WORDS; i++)
+		__raw_writel(ale_entry[i], ale->params.ale_regs +
+			     ALE_TABLE + 4 * i);
+
+	__raw_writel(idx | ALE_TABLE_WRITE, ale->params.ale_regs +
+		     ALE_TABLE_CONTROL);
+
+	return idx;
+}
+
+static int cpsw_ale_match_addr(struct cpsw_ale *ale, u8 *addr)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		u8 entry_addr[6];
+
+		cpsw_ale_read(ale, idx, ale_entry);
+		type = cpsw_ale_get_entry_type(ale_entry);
+		if (type != ALE_TYPE_ADDR && type != ALE_TYPE_VLAN_ADDR)
+			continue;
+		cpsw_ale_get_addr(ale_entry, entry_addr);
+		if (memcmp(entry_addr, addr, 6) == 0)
+			return idx;
+	}
+	return -ENOENT;
+}
+
+static int cpsw_ale_match_free(struct cpsw_ale *ale)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		type = cpsw_ale_get_entry_type(ale_entry);
+		if (type == ALE_TYPE_FREE)
+			return idx;
+	}
+	return -ENOENT;
+}
+
+static int cpsw_ale_find_ageable(struct cpsw_ale *ale)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		type = cpsw_ale_get_entry_type(ale_entry);
+		if (type != ALE_TYPE_ADDR && type != ALE_TYPE_VLAN_ADDR)
+			continue;
+		if (cpsw_ale_get_mcast(ale_entry))
+			continue;
+		type = cpsw_ale_get_ucast_type(ale_entry);
+		if (type != ALE_UCAST_PERSISTANT &&
+		    type != ALE_UCAST_OUI)
+			return idx;
+	}
+	return -ENOENT;
+}
+
+static void cpsw_ale_flush_mcast(struct cpsw_ale *ale, u32 *ale_entry,
+				 int port_mask)
+{
+	int mask;
+
+	mask = cpsw_ale_get_port_mask(ale_entry);
+	if ((mask & port_mask) == 0)
+		return; /* ports dont intersect, not interested */
+	mask &= ~port_mask;
+
+	/* free if only remaining port is host port */
+	if (mask)
+		cpsw_ale_set_port_mask(ale_entry, mask);
+	else
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+}
+
+int cpsw_ale_flush_multicast(struct cpsw_ale *ale, int port_mask)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int ret, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		ret = cpsw_ale_get_entry_type(ale_entry);
+		if (ret != ALE_TYPE_ADDR && ret != ALE_TYPE_VLAN_ADDR)
+			continue;
+
+		if (cpsw_ale_get_mcast(ale_entry)) {
+			u8 addr[6];
+
+			cpsw_ale_get_addr(ale_entry, addr);
+			if (!is_broadcast_ether_addr(addr))
+				cpsw_ale_flush_mcast(ale, ale_entry, port_mask);
+		}
+
+		cpsw_ale_write(ale, idx, ale_entry);
+	}
+	return 0;
+}
+
+static void cpsw_ale_flush_ucast(struct cpsw_ale *ale, u32 *ale_entry,
+				 int port_mask)
+{
+	int port;
+
+	port = cpsw_ale_get_port_num(ale_entry);
+	if ((BIT(port) & port_mask) == 0)
+		return; /* ports dont intersect, not interested */
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+}
+
+int cpsw_ale_flush(struct cpsw_ale *ale, int port_mask)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int ret, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		ret = cpsw_ale_get_entry_type(ale_entry);
+		if (ret != ALE_TYPE_ADDR && ret != ALE_TYPE_VLAN_ADDR)
+			continue;
+
+		if (cpsw_ale_get_mcast(ale_entry))
+			cpsw_ale_flush_mcast(ale, ale_entry, port_mask);
+		else
+			cpsw_ale_flush_ucast(ale, ale_entry, port_mask);
+
+		cpsw_ale_write(ale, idx, ale_entry);
+	}
+	return 0;
+}
+
+int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port, int flags)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
+	cpsw_ale_set_addr(ale_entry, addr);
+	cpsw_ale_set_ucast_type(ale_entry, ALE_UCAST_PERSISTANT);
+	cpsw_ale_set_secure(ale_entry, (flags & ALE_SECURE) ? 1 : 0);
+	cpsw_ale_set_blocked(ale_entry, (flags & ALE_BLOCKED) ? 1 : 0);
+	cpsw_ale_set_port_num(ale_entry, port);
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx < 0)
+		idx = cpsw_ale_match_free(ale);
+	if (idx < 0)
+		idx = cpsw_ale_find_ageable(ale);
+	if (idx < 0)
+		return -ENOMEM;
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx < 0)
+		return -ENOENT;
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
+			int super, int mcast_state)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx, mask;
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx >= 0)
+		cpsw_ale_read(ale, idx, ale_entry);
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
+	cpsw_ale_set_addr(ale_entry, addr);
+	cpsw_ale_set_super(ale_entry, super);
+	cpsw_ale_set_mcast_state(ale_entry, mcast_state);
+
+	mask = cpsw_ale_get_port_mask(ale_entry);
+	port_mask |= mask;
+	cpsw_ale_set_port_mask(ale_entry, port_mask);
+
+	if (idx < 0)
+		idx = cpsw_ale_match_free(ale);
+	if (idx < 0)
+		idx = cpsw_ale_find_ageable(ale);
+	if (idx < 0)
+		return -ENOMEM;
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx < 0)
+		return -EINVAL;
+
+	cpsw_ale_read(ale, idx, ale_entry);
+
+	if (port_mask)
+		cpsw_ale_set_port_mask(ale_entry, port_mask);
+	else
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+struct ale_control_info {
+	const char	*name;
+	int		offset, port_offset;
+	int		shift, port_shift;
+	int		bits;
+};
+
+static const struct ale_control_info ale_controls[ALE_NUM_CONTROLS] = {
+	[ALE_ENABLE]		= {
+		.name		= "enable",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 31,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_CLEAR]		= {
+		.name		= "clear",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 30,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_AGEOUT]		= {
+		.name		= "ageout",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 29,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_VLAN_NOLEARN]	= {
+		.name		= "vlan_nolearn",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 7,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_NO_PORT_VLAN]	= {
+		.name		= "no_port_vlan",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 6,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_OUI_DENY]		= {
+		.name		= "oui_deny",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 5,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_BYPASS]		= {
+		.name		= "bypass",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 4,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_RATE_LIMIT_TX]	= {
+		.name		= "rate_limit_tx",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 3,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_VLAN_AWARE]	= {
+		.name		= "vlan_aware",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 2,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_AUTH_ENABLE]	= {
+		.name		= "auth_enable",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 1,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_RATE_LIMIT]	= {
+		.name		= "rate_limit",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_STATE]	= {
+		.name		= "port_state",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 2,
+	},
+	[ALE_PORT_DROP_UNTAGGED] = {
+		.name		= "drop_untagged",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 2,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_DROP_UNKNOWN_VLAN] = {
+		.name		= "drop_unknown",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 3,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_NOLEARN]	= {
+		.name		= "nolearn",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 4,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_MCAST_LIMIT]	= {
+		.name		= "mcast_limit",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 16,
+		.port_shift	= 0,
+		.bits		= 8,
+	},
+	[ALE_PORT_BCAST_LIMIT]	= {
+		.name		= "bcast_limit",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 24,
+		.port_shift	= 0,
+		.bits		= 8,
+	},
+	[ALE_PORT_UNKNOWN_VLAN_MEMBER] = {
+		.name		= "unknown_vlan_member",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+	[ALE_PORT_UNKNOWN_MCAST_FLOOD] = {
+		.name		= "unknown_mcast_flood",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 8,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+	[ALE_PORT_UNKNOWN_REG_MCAST_FLOOD] = {
+		.name		= "unknown_reg_flood",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 16,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+	[ALE_PORT_UNTAGGED_EGRESS] = {
+		.name		= "untagged_egress",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 24,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+};
+
+int cpsw_ale_control_set(struct cpsw_ale *ale, int port, int control,
+			 int value)
+{
+	const struct ale_control_info *info;
+	int offset, shift;
+	u32 tmp, mask;
+
+	if (control < 0 || control >= ARRAY_SIZE(ale_controls))
+		return -EINVAL;
+
+	info = &ale_controls[control];
+	if (info->port_offset == 0 && info->port_shift == 0)
+		port = 0; /* global, port is a dont care */
+
+	if (port < 0 || port > ale->params.ale_ports)
+		return -EINVAL;
+
+	mask = BITMASK(info->bits);
+	if (value & ~mask)
+		return -EINVAL;
+
+	offset = info->offset + (port * info->port_offset);
+	shift  = info->shift  + (port * info->port_shift);
+
+	tmp = __raw_readl(ale->params.ale_regs + offset);
+	tmp = (tmp & ~(mask << shift)) | (value << shift);
+	__raw_writel(tmp, ale->params.ale_regs + offset);
+
+	return 0;
+}
+
+int cpsw_ale_control_get(struct cpsw_ale *ale, int port, int control)
+{
+	const struct ale_control_info *info;
+	int offset, shift;
+	u32 tmp;
+
+	if (control < 0 || control >= ARRAY_SIZE(ale_controls))
+		return -EINVAL;
+
+	info = &ale_controls[control];
+	if (info->port_offset == 0 && info->port_shift == 0)
+		port = 0; /* global, port is a dont care */
+
+	if (port < 0 || port > ale->params.ale_ports)
+		return -EINVAL;
+
+	offset = info->offset + (port * info->port_offset);
+	shift  = info->shift  + (port * info->port_shift);
+
+	tmp = __raw_readl(ale->params.ale_regs + offset) >> shift;
+	return tmp & BITMASK(info->bits);
+}
+
+static enum hrtimer_restart cpsw_ale_timer(struct hrtimer *timer)
+{
+	struct cpsw_ale *ale = container_of(timer, struct cpsw_ale, timer);
+
+	cpsw_ale_control_set(ale, 0, ALE_AGEOUT, 1);
+
+	return HRTIMER_NORESTART;
+}
+
+int cpsw_ale_set_ageout(struct cpsw_ale *ale, int ageout)
+{
+	hrtimer_cancel(&ale->timer);
+	ale->ageout = ageout * 1000 * 1000 * 1000;
+	if (ale->ageout) {
+		hrtimer_start(&ale->timer, ale->ageout, HRTIMER_MODE_REL_HARD);
+	}
+	return 0;
+}
+
+void cpsw_ale_start(struct cpsw_ale *ale)
+{
+	u32 rev;
+
+	rev = __raw_readl(ale->params.ale_regs + ALE_IDVER);
+	dev_dbg(ale->params.dev, "initialized cpsw ale revision %d.%d\n",
+		ALE_VERSION_MAJOR(rev), ALE_VERSION_MINOR(rev));
+	cpsw_ale_control_set(ale, 0, ALE_ENABLE, 1);
+	cpsw_ale_control_set(ale, 0, ALE_CLEAR, 1);
+
+	if (ale->ageout) {
+		hrtimer_start(&ale->timer, ale->ageout, HRTIMER_MODE_REL_HARD);
+	}
+}
+
+void cpsw_ale_stop(struct cpsw_ale *ale)
+{
+	hrtimer_cancel(&ale->timer);
+}
+
+struct cpsw_ale *cpsw_ale_create(struct cpsw_ale_params *params)
+{
+	struct cpsw_ale *ale;
+
+	ale = kzalloc(sizeof(*ale), GFP_KERNEL);
+	if (!ale)
+		return NULL;
+	hrtimer_init(&ale->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
+	ale->timer.function = cpsw_ale_timer;
+
+	ale->params = *params;
+	ale->ageout = ale->params.ale_ageout * 1000 * 1000 * 1000;
+
+	return ale;
+}
+
+int cpsw_ale_destroy(struct cpsw_ale *ale)
+{
+	if (!ale)
+		return -EINVAL;
+	cpsw_ale_stop(ale);
+	cpsw_ale_control_set(ale, 0, ALE_ENABLE, 0);
+	hrtimer_cancel(&ale->timer);
+	kfree(ale);
+	return 0;
+}
diff -Naur a/net/rtnet/drivers/ticpsw/cpsw_ale.h b/net/rtnet/drivers/ticpsw/cpsw_ale.h
--- a/net/rtnet/drivers/ticpsw/cpsw_ale.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/cpsw_ale.h	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,100 @@
+/*
+ * Texas Instruments 3-Port Ethernet Switch Address Lookup Engine APIs
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __TI_CPSW_ALE_H__
+#define __TI_CPSW_ALE_H__
+
+#include <rtnet_port.h>
+
+struct cpsw_ale_params {
+	struct device		*dev;
+	void __iomem		*ale_regs;
+	unsigned long		ale_ageout;	/* in secs */
+	unsigned long		ale_entries;
+	unsigned long		ale_ports;
+};
+
+struct cpsw_ale {
+	struct cpsw_ale_params	params;
+	struct hrtimer		timer;
+	ktime_t			ageout;
+};
+
+enum cpsw_ale_control {
+	/* global */
+	ALE_ENABLE,
+	ALE_CLEAR,
+	ALE_AGEOUT,
+	ALE_VLAN_NOLEARN,
+	ALE_NO_PORT_VLAN,
+	ALE_OUI_DENY,
+	ALE_BYPASS,
+	ALE_RATE_LIMIT_TX,
+	ALE_VLAN_AWARE,
+	ALE_AUTH_ENABLE,
+	ALE_RATE_LIMIT,
+	/* port controls */
+	ALE_PORT_STATE,
+	ALE_PORT_DROP_UNTAGGED,
+	ALE_PORT_DROP_UNKNOWN_VLAN,
+	ALE_PORT_NOLEARN,
+	ALE_PORT_UNKNOWN_VLAN_MEMBER,
+	ALE_PORT_UNKNOWN_MCAST_FLOOD,
+	ALE_PORT_UNKNOWN_REG_MCAST_FLOOD,
+	ALE_PORT_UNTAGGED_EGRESS,
+	ALE_PORT_BCAST_LIMIT,
+	ALE_PORT_MCAST_LIMIT,
+	ALE_NUM_CONTROLS,
+};
+
+enum cpsw_ale_port_state {
+	ALE_PORT_STATE_DISABLE	= 0x00,
+	ALE_PORT_STATE_BLOCK	= 0x01,
+	ALE_PORT_STATE_LEARN	= 0x02,
+	ALE_PORT_STATE_FORWARD	= 0x03,
+};
+
+/* ALE unicast entry flags - passed into cpsw_ale_add_ucast() */
+#define ALE_SECURE			1
+#define ALE_BLOCKED			2
+
+#define ALE_MCAST_FWD			0
+#define ALE_MCAST_BLOCK_LEARN_FWD	1
+#define ALE_MCAST_FWD_LEARN		2
+#define ALE_MCAST_FWD_2			3
+
+struct cpsw_ale *cpsw_ale_create(struct cpsw_ale_params *params);
+int cpsw_ale_destroy(struct cpsw_ale *ale);
+
+void cpsw_ale_start(struct cpsw_ale *ale);
+void cpsw_ale_stop(struct cpsw_ale *ale);
+
+int cpsw_ale_set_ageout(struct cpsw_ale *ale, int ageout);
+int cpsw_ale_flush(struct cpsw_ale *ale, int port_mask);
+int cpsw_ale_flush_multicast(struct cpsw_ale *ale, int port_mask);
+int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port, int flags);
+int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port);
+int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
+			int super, int mcast_state);
+int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask);
+
+int cpsw_ale_control_get(struct cpsw_ale *ale, int port, int control);
+int cpsw_ale_control_set(struct cpsw_ale *ale, int port,
+			 int control, int value);
+
+#endif
diff -Naur a/net/rtnet/drivers/ticpsw/cpsw.c b/net/rtnet/drivers/ticpsw/cpsw.c
--- a/net/rtnet/drivers/ticpsw/cpsw.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/cpsw.c	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,1841 @@
+/*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * 2019/09/20
+ * Modifications to Geoffrey Bonneville
+ * and Hidde Verstoep RTNet port to beaglebone-black,
+ * modifications made by Laurentiu-Cristian Duca:
+ * - use a dummy net_device as a glue between rtnet_device, 
+ * phy_connect() and cpsw_adjust_link()
+ *
+ * Texas Instruments Ethernet Switch Driver
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/io.h>
+#include <linux/clk.h>
+#include <linux/timer.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/irqreturn.h>
+#include <linux/interrupt.h>
+#include <linux/if_ether.h>
+#include <linux/etherdevice.h>
+#include <linux/netdevice.h>
+#include <linux/net_tstamp.h>
+#include <linux/phy.h>
+#include <linux/workqueue.h>
+#include <linux/delay.h>
+#include <linux/pm_runtime.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/of_device.h>
+#include "cpsw.h"
+
+#include "cpsw_ale.h"
+#include "cpts.h"
+#include "davinci_cpdma.h"
+
+#include <rtnet_multiple_queues.h>
+
+#define RX_RING_SIZE 8
+#define PM_NEGATIVE_DELAY			-2000
+
+/*static inline void rtskb_tx_timestamp(struct rtskb *skb){
+	if (skb->xmit_stamp)
+		*skb->xmit_stamp = cpu_to_be64(rtdm_clock_read() + *skb->xmit_stamp);
+}*/
+
+#define AM33XX_CTRL_MAC_LO_REG(offset, id) ((offset) + 0x8 * (id))
+#define AM33XX_CTRL_MAC_HI_REG(offset, id) ((offset) + 0x8 * (id) + 0x4)
+
+int cpsw_am33xx_cm_get_macid(struct device *dev, u16 offset, int slave,
+			     u8 *mac_addr)
+{
+	u32 macid_lo;
+	u32 macid_hi;
+	struct regmap *syscon;
+
+	syscon = syscon_regmap_lookup_by_phandle(dev->of_node, "syscon");
+	if (IS_ERR(syscon)) {
+		if (PTR_ERR(syscon) == -ENODEV)
+			return 0;
+		return PTR_ERR(syscon);
+	}
+
+	regmap_read(syscon, AM33XX_CTRL_MAC_LO_REG(offset, slave),
+		    &macid_lo);
+	regmap_read(syscon, AM33XX_CTRL_MAC_HI_REG(offset, slave),
+		    &macid_hi);
+
+	mac_addr[5] = (macid_lo >> 8) & 0xff;
+	mac_addr[4] = macid_lo & 0xff;
+	mac_addr[3] = (macid_hi >> 24) & 0xff;
+	mac_addr[2] = (macid_hi >> 16) & 0xff;
+	mac_addr[1] = (macid_hi >> 8) & 0xff;
+	mac_addr[0] = macid_hi & 0xff;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpsw_am33xx_cm_get_macid);
+
+static inline int rtskb_tailroom(const struct rtskb *skb){
+	return skb->end - skb->tail;
+}
+
+#define CPSW_DEBUG	(NETIF_MSG_HW		| NETIF_MSG_WOL		| \
+			 NETIF_MSG_DRV		| NETIF_MSG_LINK	| \
+			 NETIF_MSG_IFUP		| NETIF_MSG_INTR	| \
+			 NETIF_MSG_PROBE	| NETIF_MSG_TIMER	| \
+			 NETIF_MSG_IFDOWN	| NETIF_MSG_RX_ERR	| \
+			 NETIF_MSG_TX_ERR	| NETIF_MSG_TX_DONE	| \
+			 NETIF_MSG_PKTDATA	| NETIF_MSG_TX_QUEUED	| \
+			 NETIF_MSG_RX_STATUS)
+
+#define cpsw_info(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_info(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define cpsw_err(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_err(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define cpsw_dbg(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_dbg(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define cpsw_notice(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_notice(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define ALE_ALL_PORTS		0x7
+
+#define CPSW_MAJOR_VERSION(reg)		(reg >> 8 & 0x7)
+#define CPSW_MINOR_VERSION(reg)		(reg & 0xff)
+#define CPSW_RTL_VERSION(reg)		((reg >> 11) & 0x1f)
+
+#define CPSW_VERSION_1		0x19010a
+#define CPSW_VERSION_2		0x19010c
+
+#define HOST_PORT_NUM		0
+#define SLIVER_SIZE		0x40
+
+#define CPSW1_HOST_PORT_OFFSET	0x028
+#define CPSW1_SLAVE_OFFSET	0x050
+#define CPSW1_SLAVE_SIZE	0x040
+#define CPSW1_CPDMA_OFFSET	0x100
+#define CPSW1_STATERAM_OFFSET	0x200
+#define CPSW1_CPTS_OFFSET	0x500
+#define CPSW1_ALE_OFFSET	0x600
+#define CPSW1_SLIVER_OFFSET	0x700
+
+#define CPSW2_HOST_PORT_OFFSET	0x108
+#define CPSW2_SLAVE_OFFSET	0x200
+#define CPSW2_SLAVE_SIZE	0x100
+#define CPSW2_CPDMA_OFFSET	0x800
+#define CPSW2_STATERAM_OFFSET	0xa00
+#define CPSW2_CPTS_OFFSET	0xc00
+#define CPSW2_ALE_OFFSET	0xd00
+#define CPSW2_SLIVER_OFFSET	0xd80
+#define CPSW2_BD_OFFSET		0x2000
+
+#define CPDMA_RXTHRESH		0x0c0
+#define CPDMA_RXFREE		0x0e0
+#define CPDMA_TXHDP		0x00
+#define CPDMA_RXHDP		0x20
+#define CPDMA_TXCP		0x40
+#define CPDMA_RXCP		0x60
+
+#define CPSW_POLL_WEIGHT	64
+#define CPSW_MIN_PACKET_SIZE	60
+#define CPSW_MAX_PACKET_SIZE	(1500 + 14 + 4 + 4)
+
+#define RX_PRIORITY_MAPPING	0x76543210
+#define TX_PRIORITY_MAPPING	0x33221100
+#define CPDMA_TX_PRIORITY_MAP	0x76543210
+
+#if 0
+#define cpsw_enable_irq(priv)	\
+	do {			\
+		u32 i;		\
+		for (i = 0; i < priv->num_irqs; i++) \
+			rtdm_irq_enable(&priv->irqs_table[i]); \
+	} while (0);
+#define cpsw_disable_irq(priv)	\
+	do {			\
+		u32 i;		\
+		for (i = 0; i < priv->num_irqs; i++) \
+			rtdm_irq_disable(&priv->irqs_table[i]); \
+	} while (0);
+#endif
+
+static int debug_level;
+module_param(debug_level, int, 0);
+MODULE_PARM_DESC(debug_level, "cpsw debug level (NETIF_MSG bits)");
+
+static int ale_ageout = 10;
+module_param(ale_ageout, int, 0);
+MODULE_PARM_DESC(ale_ageout, "cpsw ale ageout interval (seconds)");
+
+static int rx_packet_max = CPSW_MAX_PACKET_SIZE;
+module_param(rx_packet_max, int, 0);
+MODULE_PARM_DESC(rx_packet_max, "maximum receive packet size (bytes)");
+
+struct cpsw_wr_regs {
+	u32	id_ver;
+	u32	soft_reset;
+	u32	control;
+	u32	int_control;
+	u32	c0_rx_thresh_en;
+	u32	c0_rx_en;
+	u32	c0_tx_en;
+	u32	c0_misc_en;
+	u32	c1_rx_thresh_en;
+	u32	c1_rx_en;
+	u32	c1_tx_en;
+	u32	c1_misc_en;
+	u32	c2_rx_thresh_en;
+	u32	c2_rx_en;
+	u32	c2_tx_en;
+	u32	c2_misc_en;
+	u32	c0_rx_thresh_stat;
+	u32	c0_rx_stat;
+	u32	c0_tx_stat;
+	u32	c0_misc_stat;
+	u32	c1_rx_thresh_stat;
+	u32	c1_rx_stat;
+	u32	c1_tx_stat;
+	u32	c1_misc_stat;
+	u32	c2_rx_thresh_stat;
+	u32	c2_rx_stat;
+	u32	c2_tx_stat;
+	u32	c2_misc_stat;
+	u32	c0_rx_imax;
+	u32	c0_tx_imax;
+	u32	c1_rx_imax;
+	u32	c1_tx_imax;
+	u32	c2_rx_imax;
+	u32	c2_tx_imax;
+	u32	rgmii_ctl;
+};
+
+struct cpsw_ss_regs {
+	u32	id_ver;
+	u32	control;
+	u32	soft_reset;
+	u32	stat_port_en;
+	u32	ptype;
+	u32	soft_idle;
+	u32	thru_rate;
+	u32	gap_thresh;
+	u32	tx_start_wds;
+	u32	flow_control;
+	u32	vlan_ltype;
+	u32	ts_ltype;
+	u32	dlr_ltype;
+};
+
+/* CPSW_PORT_V1 */
+#define CPSW1_MAX_BLKS      0x00 /* Maximum FIFO Blocks */
+#define CPSW1_BLK_CNT       0x04 /* FIFO Block Usage Count (Read Only) */
+#define CPSW1_TX_IN_CTL     0x08 /* Transmit FIFO Control */
+#define CPSW1_PORT_VLAN     0x0c /* VLAN Register */
+#define CPSW1_TX_PRI_MAP    0x10 /* Tx Header Priority to Switch Pri Mapping */
+#define CPSW1_TS_CTL        0x14 /* Time Sync Control */
+#define CPSW1_TS_SEQ_LTYPE  0x18 /* Time Sync Sequence ID Offset and Msg Type */
+#define CPSW1_TS_VLAN       0x1c /* Time Sync VLAN1 and VLAN2 */
+
+/* CPSW_PORT_V2 */
+#define CPSW2_CONTROL       0x00 /* Control Register */
+#define CPSW2_MAX_BLKS      0x08 /* Maximum FIFO Blocks */
+#define CPSW2_BLK_CNT       0x0c /* FIFO Block Usage Count (Read Only) */
+#define CPSW2_TX_IN_CTL     0x10 /* Transmit FIFO Control */
+#define CPSW2_PORT_VLAN     0x14 /* VLAN Register */
+#define CPSW2_TX_PRI_MAP    0x18 /* Tx Header Priority to Switch Pri Mapping */
+#define CPSW2_TS_SEQ_MTYPE  0x1c /* Time Sync Sequence ID Offset and Msg Type */
+
+/* CPSW_PORT_V1 and V2 */
+#define SA_LO               0x20 /* CPGMAC_SL Source Address Low */
+#define SA_HI               0x24 /* CPGMAC_SL Source Address High */
+#define SEND_PERCENT        0x28 /* Transmit Queue Send Percentages */
+
+/* CPSW_PORT_V2 only */
+#define RX_DSCP_PRI_MAP0    0x30 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP1    0x34 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP2    0x38 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP3    0x3c /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP4    0x40 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP5    0x44 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP6    0x48 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP7    0x4c /* Rx DSCP Priority to Rx Packet Mapping */
+
+/* Bit definitions for the CPSW2_CONTROL register */
+#define PASS_PRI_TAGGED     (1<<24) /* Pass Priority Tagged */
+#define VLAN_LTYPE2_EN      (1<<21) /* VLAN LTYPE 2 enable */
+#define VLAN_LTYPE1_EN      (1<<20) /* VLAN LTYPE 1 enable */
+#define DSCP_PRI_EN         (1<<16) /* DSCP Priority Enable */
+#define TS_320              (1<<14) /* Time Sync Dest Port 320 enable */
+#define TS_319              (1<<13) /* Time Sync Dest Port 319 enable */
+#define TS_132              (1<<12) /* Time Sync Dest IP Addr 132 enable */
+#define TS_131              (1<<11) /* Time Sync Dest IP Addr 131 enable */
+#define TS_130              (1<<10) /* Time Sync Dest IP Addr 130 enable */
+#define TS_129              (1<<9)  /* Time Sync Dest IP Addr 129 enable */
+#define TS_BIT8             (1<<8)  /* ts_ttl_nonzero? */
+#define TS_ANNEX_D_EN       (1<<4)  /* Time Sync Annex D enable */
+#define TS_LTYPE2_EN        (1<<3)  /* Time Sync LTYPE 2 enable */
+#define TS_LTYPE1_EN        (1<<2)  /* Time Sync LTYPE 1 enable */
+#define TS_TX_EN            (1<<1)  /* Time Sync Transmit Enable */
+#define TS_RX_EN            (1<<0)  /* Time Sync Receive Enable */
+
+#define CTRL_TS_BITS \
+	(TS_320 | TS_319 | TS_132 | TS_131 | TS_130 | TS_129 | TS_BIT8 | \
+	 TS_ANNEX_D_EN | TS_LTYPE1_EN)
+
+#define CTRL_ALL_TS_MASK (CTRL_TS_BITS | TS_TX_EN | TS_RX_EN)
+#define CTRL_TX_TS_BITS  (CTRL_TS_BITS | TS_TX_EN)
+#define CTRL_RX_TS_BITS  (CTRL_TS_BITS | TS_RX_EN)
+
+/* Bit definitions for the CPSW2_TS_SEQ_MTYPE register */
+#define TS_SEQ_ID_OFFSET_SHIFT   (16)    /* Time Sync Sequence ID Offset */
+#define TS_SEQ_ID_OFFSET_MASK    (0x3f)
+#define TS_MSG_TYPE_EN_SHIFT     (0)     /* Time Sync Message Type Enable */
+#define TS_MSG_TYPE_EN_MASK      (0xffff)
+
+/* The PTP event messages - Sync, Delay_Req, Pdelay_Req, and Pdelay_Resp. */
+#define EVENT_MSG_BITS ((1<<0) | (1<<1) | (1<<2) | (1<<3))
+
+/* Bit definitions for the CPSW1_TS_CTL register */
+#define CPSW_V1_TS_RX_EN		BIT(0)
+#define CPSW_V1_TS_TX_EN		BIT(4)
+#define CPSW_V1_MSG_TYPE_OFS		16
+
+/* Bit definitions for the CPSW1_TS_SEQ_LTYPE register */
+#define CPSW_V1_SEQ_ID_OFS_SHIFT	16
+
+struct cpsw_host_regs {
+	u32	max_blks;
+	u32	blk_cnt;
+	u32	flow_thresh;
+	u32	port_vlan;
+	u32	tx_pri_map;
+	u32	cpdma_tx_pri_map;
+	u32	cpdma_rx_chan_map;
+};
+
+struct cpsw_sliver_regs {
+	u32	id_ver;
+	u32	mac_control;
+	u32	mac_status;
+	u32	soft_reset;
+	u32	rx_maxlen;
+	u32	__reserved_0;
+	u32	rx_pause;
+	u32	tx_pause;
+	u32	__reserved_1;
+	u32	rx_pri_map;
+};
+
+struct cpsw_slave {
+	void __iomem			*regs;
+	struct cpsw_sliver_regs __iomem	*sliver;
+	int				slave_num;
+	u32				mac_control;
+	struct cpsw_slave_data		*data;
+	struct phy_device		*phy;
+};
+
+static inline u32 slave_read(struct cpsw_slave *slave, u32 offset)
+{
+	return __raw_readl(slave->regs + offset);
+}
+
+static inline void slave_write(struct cpsw_slave *slave, u32 val, u32 offset)
+{
+	__raw_writel(val, slave->regs + offset);
+}
+
+struct cpsw_priv {
+	raw_spinlock_t			lock;
+	struct platform_device		*pdev;
+	struct rtnet_device		*ndev;
+	struct resource			*cpsw_res;
+	struct resource			*cpsw_wr_res;
+	struct device			*dev;
+	struct cpsw_platform_data	data;
+	struct cpsw_ss_regs __iomem	*regs;
+	struct cpsw_wr_regs __iomem	*wr_regs;
+	struct cpsw_host_regs __iomem	*host_port_regs;
+	u32				msg_enable;
+	u32				version;
+	struct net_device_stats		stats;
+	int				rx_packet_max;
+	int				host_port;
+	struct clk			*clk;
+	u8				mac_addr[ETH_ALEN];
+	struct cpsw_slave		*slaves;
+	struct cpdma_ctlr		*dma;
+	struct cpdma_chan		*txch, *rxch;
+	struct cpsw_ale			*ale;
+	/* snapshot of IRQ numbers */
+	int irqs_table[4];
+	u32 num_irqs;
+	bool irq_enabled;
+	struct cpts cpts;
+
+	struct rtskb_pool skb_pool;
+
+	struct net_device *phy_phony_net_device;
+};
+
+struct dummy_netdev_priv {
+	struct rtnet_device *rtdev;
+};
+
+static inline struct rtskb *dev_alloc_rtskb_ip_align(struct rtnet_device *ndev, unsigned int size){
+	struct cpsw_priv *priv = ndev->priv;
+	struct rtskb_pool *pool = &priv->skb_pool;
+	struct rtskb *skb = alloc_rtskb(size + NET_IP_ALIGN, pool);
+
+	if(skb)
+		skb->rtdev = ndev;
+	if(NET_IP_ALIGN && skb)
+		rtskb_reserve(skb, NET_IP_ALIGN);
+	return skb;
+}
+
+#define napi_to_priv(napi)	container_of(napi, struct cpsw_priv, napi)
+#define for_each_slave(priv, func, arg...)			\
+	do {							\
+		int idx;					\
+		for (idx = 0; idx < (priv)->data.slaves; idx++)	\
+			(func)((priv)->slaves + idx, ##arg);	\
+	} while (0)
+
+#if 0
+static void cpsw_ndo_set_rx_mode(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	if (ndev->flags & IFF_PROMISC) {
+		/* Enable promiscuous mode */
+		dev_err(priv->dev, "Ignoring Promiscuous mode\n");
+		return;
+	}
+
+	/* Clear all mcast from ALE */
+	cpsw_ale_flush_multicast(priv->ale, ALE_ALL_PORTS << priv->host_port);
+
+	if (!netdev_mc_empty(ndev)) {
+		struct netdev_hw_addr *ha;
+
+		/* program multicast address list into ALE register */
+		netdev_for_each_mc_addr(ha, ndev) {
+			cpsw_ale_add_mcast(priv->ale, (u8 *)ha->addr,
+				ALE_ALL_PORTS << priv->host_port, 0, 0);
+		}
+	}
+}
+#endif
+
+static void cpsw_intr_enable(struct cpsw_priv *priv)
+{
+	__raw_writel(0xFF, &priv->wr_regs->c0_tx_en);
+	__raw_writel(0xFF, &priv->wr_regs->c0_rx_en);
+
+	cpdma_ctlr_int_ctrl(priv->dma, true);
+	return;
+}
+
+static void cpsw_intr_disable(struct cpsw_priv *priv)
+{
+	__raw_writel(0, &priv->wr_regs->c0_tx_en);
+	__raw_writel(0, &priv->wr_regs->c0_rx_en);
+
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	return;
+}
+
+void cpsw_tx_handler(void *token, int len, int status)
+{
+	struct rtskb		*skb = token;
+	struct rtnet_device	*ndev = skb->rtdev;
+	struct cpsw_priv	*priv = ndev->priv;
+
+#if 0
+	trace_printk("cpsw_tx_handler(%x, %d, %d)\n", token, len, status);
+#endif
+	if (unlikely(rtnetif_queue_stopped(ndev)))
+		rtnetif_wake_queue(ndev);
+	cpts_tx_timestamp(&priv->cpts, skb);
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += len;
+	dev_kfree_rtskb(skb);
+}
+
+void cpsw_rx_handler(void *token, int len, int status)
+{
+	struct rtskb		*skb = token;
+	struct rtnet_device	*ndev = skb->rtdev;
+	struct cpsw_priv	*priv = ndev->priv;
+	int			ret = 0;
+	ktime_t time_stamp = ktime_get();
+
+	/* free and bail if we are shutting down */
+	if (unlikely(!rtnetif_running(ndev)) ||
+			unlikely(!rtnetif_carrier_ok(ndev))) {
+		dev_kfree_rtskb(skb);
+		return;
+	}
+
+	if (likely(status >= 0)) {
+		skb->time_stamp = time_stamp;
+		rtskb_put(skb, len);
+		cpts_rx_timestamp(&priv->cpts, skb);
+		skb->protocol = rt_eth_type_trans(skb, ndev);
+		rtnetif_rx(skb);
+		priv->stats.rx_bytes += len;
+		priv->stats.rx_packets++;
+		skb = NULL;
+	}
+
+	if (unlikely(!rtnetif_running(ndev))) {
+		if (skb)
+			dev_kfree_rtskb(skb);
+		return;
+	}
+
+	if (likely(!skb)) {
+		skb = dev_alloc_rtskb_ip_align(ndev, priv->rx_packet_max);
+		if (WARN_ON(!skb))
+			return;
+
+		ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
+					rtskb_tailroom(skb), GFP_KERNEL);
+	}
+	WARN_ON(ret < 0);
+}
+
+static inline int cpsw_get_slave_port(struct cpsw_priv *priv, u32 slave_num)
+{
+	if (priv->host_port == 0)
+		return slave_num + 1;
+	else
+		return slave_num;
+}
+
+#if 0
+static int cpsw_poll(struct cpsw_priv *priv, int budget)
+{
+	int			num_tx, num_rx, num_total_tx, num_total_rx;
+	int			budget_left;
+
+	budget_left = budget;
+
+	/* read status and throw away */
+	(void)__raw_readl(&priv->wr_regs->c0_tx_stat);
+
+	/* handle all transmits */
+	num_total_tx = 0;
+	while (budget_left > 0 &&
+		(num_tx = cpdma_chan_process(priv->txch, 128)) > 0) {
+		budget_left -= num_tx;
+		num_total_tx += num_tx;
+	}
+
+	if (num_total_tx > 0 && budget_left > 0)
+		cpdma_ctlr_eoi(priv->dma, 0x02);
+
+	/* read status and throw away */
+	(void)__raw_readl(&priv->wr_regs->c0_rx_stat);
+
+	/* handle all receives */
+	num_total_rx = 0;
+	while (budget_left > 0 &&
+		(num_rx = cpdma_chan_process(priv->rxch, budget_left)) > 0) {
+		budget_left -= num_rx;
+		num_total_rx += num_rx;
+	}
+
+	if (num_total_rx > 0 && budget_left > 0)
+		cpdma_ctlr_eoi(priv->dma, 0x01);
+
+	if ((num_total_rx + num_total_tx) < budget) {
+		cpsw_intr_enable(priv);
+		if (priv->irq_enabled == false) {
+			cpsw_enable_irq(priv);
+			priv->irq_enabled = true;
+		}
+	}
+
+	trace_printk("lost packets\n");
+	return num_total_rx + num_total_rx;
+}
+#endif
+
+#if 0
+static int cpsw_interrupt(rtdm_irq_t *irq_handle)
+{
+	struct cpsw_priv *priv = rtdm_irq_get_arg(irq_handle, struct cpsw_priv);
+
+	if (likely(rtnetif_running(priv->ndev))) {
+		cpsw_intr_disable(priv);
+		if (priv->irq_enabled == true) {
+			cpsw_disable_irq(priv);
+			priv->irq_enabled = false;
+		}
+		cpsw_poll(priv, CPSW_POLL_WEIGHT);
+	}
+
+	return IRQ_HANDLED;
+}
+#endif
+
+static irqreturn_t cpsw_rx_thresh_pend_irq(int irq, void *data)
+{
+	/* not handling this interrupt yet */
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cpsw_rx_pend_irq(int irq, void *data)
+{
+	struct cpsw_priv *priv = (struct cpsw_priv *)data;
+	int num_rx, total_rx;
+	u32 rx_stat;
+	unsigned long context;
+
+	rx_stat = __raw_readl(&priv->wr_regs->c0_rx_stat) & 0xff;
+	if (rx_stat == 0)
+		return IRQ_NONE;
+#if 0
+	trace_printk("cpsw_rx_pend_irq: %d\n", rx_stat);
+#endif
+	raw_spin_lock_irqsave(&priv->lock, context);
+
+	total_rx = 0;
+	while ((num_rx = cpdma_chan_process(priv->rxch, RX_RING_SIZE)) > 0)
+		total_rx += num_rx;
+
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+
+	raw_spin_unlock_irqrestore(&priv->lock, context);
+
+	if(total_rx > 0)
+		rt_mark_stack_mgr(priv->ndev);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cpsw_tx_pend_irq(int irq, void *data)
+{
+	struct cpsw_priv *priv = (struct cpsw_priv *) data;
+	int num_tx, total_tx;
+	u32 tx_stat;
+
+	tx_stat = __raw_readl(&priv->wr_regs->c0_tx_stat) & 0xff;
+	if (tx_stat == 0)
+		return IRQ_NONE;
+#if 0
+	trace_printk("cpsw_tx_pend_irq: %d\n", tx_stat);
+#endif
+	total_tx = 0;
+	while ((num_tx = cpdma_chan_process(priv->txch, 128)) > 0)
+		total_tx += num_tx;
+
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cpsw_misc_pend_irq(int irq, void *data)
+{
+	/* not handling this interrupt yet */
+	return IRQ_HANDLED;
+}
+
+static inline void soft_reset(const char *module, void __iomem *reg)
+{
+	unsigned long timeout = jiffies + HZ;
+
+	__raw_writel(1, reg);
+	do {
+		cpu_relax();
+	} while ((__raw_readl(reg) & 1) && time_after(timeout, jiffies));
+
+	WARN(__raw_readl(reg) & 1, "failed to soft-reset %s\n", module);
+}
+
+#define mac_hi(mac)	(((mac)[0] << 0) | ((mac)[1] << 8) |	\
+			 ((mac)[2] << 16) | ((mac)[3] << 24))
+#define mac_lo(mac)	(((mac)[4] << 0) | ((mac)[5] << 8))
+
+static void cpsw_set_slave_mac(struct cpsw_slave *slave,
+			       struct cpsw_priv *priv)
+{
+	slave_write(slave, mac_hi(priv->mac_addr), SA_HI);
+	slave_write(slave, mac_lo(priv->mac_addr), SA_LO);
+}
+
+static void _cpsw_adjust_link(struct cpsw_slave *slave,
+			      struct cpsw_priv *priv, bool *link)
+{
+	struct phy_device	*phy = slave->phy;
+	u32			mac_control = 0;
+	u32			slave_port;
+
+	if (!phy)
+		return;
+
+	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
+
+	if (phy->link) {
+		mac_control = priv->data.mac_control;
+
+		/* enable forwarding */
+		cpsw_ale_control_set(priv->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+		if (phy->speed == 1000)
+			mac_control |= BIT(7);	/* GIGABITEN	*/
+		if (phy->duplex)
+			mac_control |= BIT(0);	/* FULLDUPLEXEN	*/
+
+		/* set speed_in input in case RMII mode is used in 100Mbps */
+		if (phy->speed == 100)
+			mac_control |= BIT(15);
+
+		*link = true;
+	} else {
+		mac_control = 0;
+		/* disable forwarding */
+		cpsw_ale_control_set(priv->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_DISABLE);
+	}
+
+	if (mac_control != slave->mac_control) {
+		phy_print_status(phy);
+		__raw_writel(mac_control, &slave->sliver->mac_control);
+	}
+
+	slave->mac_control = mac_control;
+}
+
+static void cpsw_adjust_link(struct net_device *ndev_)
+{
+	struct dummy_netdev_priv *p = netdev_priv(ndev_);
+	struct rtnet_device	*ndev = (struct rtnet_device *)p->rtdev;
+	struct cpsw_priv	*priv = ndev->priv;
+	bool			link = false;
+
+	for_each_slave(priv, _cpsw_adjust_link, priv, &link);
+
+	if (link) {
+		rtnetif_carrier_on(ndev);
+		if (rtnetif_running(ndev))
+			rtnetif_wake_queue(ndev);
+	} else {
+		rtnetif_carrier_off(ndev);
+		rtnetif_stop_queue(ndev);
+	}
+}
+
+static inline int __show_stat(char *buf, int maxlen, const char *name, u32 val)
+{
+	static char *leader = "........................................";
+
+	if (!val)
+		return 0;
+	else
+		return snprintf(buf, maxlen, "%s %s %10d\n", name,
+				leader + strlen(name), val);
+}
+
+static void cpsw_slave_open(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	char name[32];
+	u32 slave_port;
+	struct net_device *dummy;
+	struct dummy_netdev_priv *p;
+
+	sprintf(name, "slave-%d", slave->slave_num);
+
+	soft_reset(name, &slave->sliver->soft_reset);
+
+	/* setup priority mapping */
+	__raw_writel(RX_PRIORITY_MAPPING, &slave->sliver->rx_pri_map);
+
+	switch (priv->version) {
+	case CPSW_VERSION_1:
+		slave_write(slave, TX_PRIORITY_MAPPING, CPSW1_TX_PRI_MAP);
+		break;
+	case CPSW_VERSION_2:
+		slave_write(slave, TX_PRIORITY_MAPPING, CPSW2_TX_PRI_MAP);
+		break;
+	}
+
+	/* setup max packet size, and mac address */
+	__raw_writel(priv->rx_packet_max, &slave->sliver->rx_maxlen);
+	cpsw_set_slave_mac(slave, priv);
+
+	slave->mac_control = 0;	/* no link yet */
+
+	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
+
+	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
+			   1 << slave_port, 0, ALE_MCAST_FWD_2);
+
+	/* Do the same as one did in macb.c rtnet driver
+	 * because the rtnet_device structure is much different than net_device.
+	 */
+	dummy = alloc_etherdev(sizeof(struct dummy_netdev_priv));
+	dev_alloc_name(dummy, "dummyeth%d");
+	p = netdev_priv(dummy);
+	p->rtdev = priv->ndev;
+	priv->phy_phony_net_device = dummy;
+	/* set dummy->dev.parent to priv->dev */
+	SET_NETDEV_DEV(dummy, priv->dev);
+	slave->phy = phy_connect(dummy, slave->data->phy_id,
+				 &cpsw_adjust_link, slave->data->phy_if);
+	if (IS_ERR(slave->phy)) {
+		dev_err(priv->dev, "phy %s not found on slave %d\n",
+			slave->data->phy_id, slave->slave_num);
+		slave->phy = NULL;
+	} else {
+		dev_info(priv->dev, "phy found : id is : 0x%x\n",
+			 slave->phy->phy_id);
+		phy_start(slave->phy);
+	}
+}
+
+static void cpsw_init_host_port(struct cpsw_priv *priv)
+{
+	/* soft reset the controller and initialize ale */
+	soft_reset("cpsw", &priv->regs->soft_reset);
+	cpsw_ale_start(priv->ale);
+
+	/* switch to vlan unaware mode */
+	cpsw_ale_control_set(priv->ale, 0, ALE_VLAN_AWARE, 0);
+
+	/* setup host port priority mapping */
+	__raw_writel(CPDMA_TX_PRIORITY_MAP,
+		     &priv->host_port_regs->cpdma_tx_pri_map);
+	__raw_writel(0, &priv->host_port_regs->cpdma_rx_chan_map);
+
+	cpsw_ale_control_set(priv->ale, priv->host_port,
+			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+	cpsw_ale_add_ucast(priv->ale, priv->mac_addr, priv->host_port, 0);
+	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
+			   1 << priv->host_port, 0, ALE_MCAST_FWD_2);
+}
+
+static int cpsw_ndo_open(struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+	int i, ret;
+	u32 reg;
+
+	cpsw_intr_disable(priv);
+	rtnetif_carrier_off(ndev);
+
+#if 0
+	pm_runtime_get_sync(&priv->pdev->dev);
+#endif
+	reg = priv->version;
+
+	dev_info(priv->dev, "initializing cpsw version %d.%d (%d)\n",
+		 CPSW_MAJOR_VERSION(reg), CPSW_MINOR_VERSION(reg),
+		 CPSW_RTL_VERSION(reg));
+
+	/* initialize host and slave ports */
+	cpsw_init_host_port(priv);
+	for_each_slave(priv, cpsw_slave_open, priv);
+
+	/* setup tx dma to fixed prio and zero offset */
+	cpdma_control_set(priv->dma, CPDMA_TX_PRIO_FIXED, 1);
+	cpdma_control_set(priv->dma, CPDMA_RX_BUFFER_OFFSET, 0);
+
+	/* disable priority elevation and enable statistics on all ports */
+	__raw_writel(0, &priv->regs->ptype);
+
+	/* enable statistics collection only on the host port */
+	__raw_writel(0x7, &priv->regs->stat_port_en);
+
+	for (i = 0; i < RX_RING_SIZE; i++) {
+		struct rtskb *skb;
+
+		ret = -ENOMEM;
+		skb = dev_alloc_rtskb_ip_align(ndev, priv->rx_packet_max);
+		if (!skb)
+			break;
+		ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
+					rtskb_tailroom(skb), GFP_KERNEL);
+		if (WARN_ON(ret < 0))
+			break;
+	}
+	/* continue even if we didn't manage to submit all receive descs */
+	cpsw_info(priv, ifup, "submitted %d rx descriptors\n", i);
+
+	cpdma_ctlr_start(priv->dma);
+	cpsw_intr_enable(priv);
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+
+	return 0;
+}
+
+static void cpsw_slave_stop(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	if (!slave->phy)
+		return;
+	phy_stop(slave->phy);
+	phy_disconnect(slave->phy);
+	slave->phy = NULL;
+}
+
+static int cpsw_ndo_stop(struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+
+	cpsw_info(priv, ifdown, "shutting down cpsw device\n");
+#if 0
+	if (priv->irq_enabled == true) {
+		cpsw_disable_irq(priv);
+		priv->irq_enabled = false;
+	}
+#endif
+	rtnetif_stop_queue(priv->ndev);
+	rtnetif_carrier_off(priv->ndev);
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	cpdma_ctlr_stop(priv->dma);
+	cpsw_ale_stop(priv->ale);
+	for_each_slave(priv, cpsw_slave_stop, priv);
+#if 0
+	pm_runtime_put_sync(&priv->pdev->dev);
+#endif
+	return 0;
+}
+
+static netdev_tx_t cpsw_ndo_start_xmit(struct rtskb *skb,
+				       struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+	int ret;
+
+	if (!rtskb_padto(skb, CPSW_MIN_PACKET_SIZE)) {
+		cpsw_err(priv, tx_err, "packet pad failed\n");
+		priv->stats.tx_dropped++;
+		return NETDEV_TX_OK;
+	}
+
+	#if 0
+	if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP && priv->cpts.tx_enable)
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+	#endif
+
+	rtskb_tx_timestamp(skb);
+
+	ret = cpdma_chan_submit(priv->txch, skb, skb->data,
+				skb->len, GFP_KERNEL);
+	if (unlikely(ret != 0)) {
+		cpsw_err(priv, tx_err, "desc submit failed\n");
+		goto fail;
+	}
+
+	return NETDEV_TX_OK;
+fail:
+	priv->stats.tx_dropped++;
+	rtnetif_stop_queue(ndev);
+	return NETDEV_TX_BUSY;
+}
+
+#if 0
+static void cpsw_ndo_change_rx_flags(struct net_device *ndev, int flags)
+{
+	/*
+	 * The switch cannot operate in promiscuous mode without substantial
+	 * headache.  For promiscuous mode to work, we would need to put the
+	 * ALE in bypass mode and route all traffic to the host port.
+	 * Subsequently, the host will need to operate as a "bridge", learn,
+	 * and flood as needed.  For now, we simply complain here and
+	 * do nothing about it :-)
+	 */
+	if ((flags & IFF_PROMISC) && (ndev->flags & IFF_PROMISC))
+		dev_err(&ndev->dev, "promiscuity ignored!\n");
+
+	/*
+	 * The switch cannot filter multicast traffic unless it is configured
+	 * in "VLAN Aware" mode.  Unfortunately, VLAN awareness requires a
+	 * whole bunch of additional logic that this driver does not implement
+	 * at present.
+	 */
+	if ((flags & IFF_ALLMULTI) && !(ndev->flags & IFF_ALLMULTI))
+		dev_err(&ndev->dev, "multicast traffic cannot be filtered!\n");
+}
+#endif
+
+#ifdef CONFIG_TI_CPTS
+
+static void cpsw_hwtstamp_v1(struct cpsw_priv *priv)
+{
+	struct cpsw_slave *slave = &priv->slaves[priv->data.cpts_active_slave];
+	u32 ts_en, seq_id;
+
+	if (!priv->cpts.tx_enable && !priv->cpts.rx_enable) {
+		slave_write(slave, 0, CPSW1_TS_CTL);
+		return;
+	}
+
+	seq_id = (30 << CPSW_V1_SEQ_ID_OFS_SHIFT) | ETH_P_1588;
+	ts_en = EVENT_MSG_BITS << CPSW_V1_MSG_TYPE_OFS;
+
+	if (priv->cpts.tx_enable)
+		ts_en |= CPSW_V1_TS_TX_EN;
+
+	if (priv->cpts.rx_enable)
+		ts_en |= CPSW_V1_TS_RX_EN;
+
+	slave_write(slave, ts_en, CPSW1_TS_CTL);
+	slave_write(slave, seq_id, CPSW1_TS_SEQ_LTYPE);
+}
+
+static void cpsw_hwtstamp_v2(struct cpsw_priv *priv)
+{
+	struct cpsw_slave *slave = &priv->slaves[priv->data.cpts_active_slave];
+	u32 ctrl, mtype;
+
+	ctrl = slave_read(slave, CPSW2_CONTROL);
+	ctrl &= ~CTRL_ALL_TS_MASK;
+
+	if (priv->cpts.tx_enable)
+		ctrl |= CTRL_TX_TS_BITS;
+
+	if (priv->cpts.rx_enable)
+		ctrl |= CTRL_RX_TS_BITS;
+
+	mtype = (30 << TS_SEQ_ID_OFFSET_SHIFT) | EVENT_MSG_BITS;
+
+	slave_write(slave, mtype, CPSW2_TS_SEQ_MTYPE);
+	slave_write(slave, ctrl, CPSW2_CONTROL);
+	__raw_writel(ETH_P_1588, &priv->regs->ts_ltype);
+}
+
+static int cpsw_hwtstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
+{
+	struct cpsw_priv *priv = netdev_priv(dev);
+	struct cpts *cpts = &priv->cpts;
+	struct hwtstamp_config cfg;
+
+	if (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))
+		return -EFAULT;
+
+	/* reserved for future extensions */
+	if (cfg.flags)
+		return -EINVAL;
+
+	switch (cfg.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		cpts->tx_enable = 0;
+		break;
+	case HWTSTAMP_TX_ON:
+		cpts->tx_enable = 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (cfg.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		cpts->rx_enable = 0;
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+		return -ERANGE;
+	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+		cpts->rx_enable = 1;
+		cfg.rx_filter = HWTSTAMP_FILTER_PTP_V2_EVENT;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (priv->version) {
+	case CPSW_VERSION_1:
+		cpsw_hwtstamp_v1(priv);
+		break;
+	case CPSW_VERSION_2:
+		cpsw_hwtstamp_v2(priv);
+		break;
+	default:
+		return -ENOTSUPP;
+	}
+
+	return copy_to_user(ifr->ifr_data, &cfg, sizeof(cfg)) ? -EFAULT : 0;
+}
+
+#endif /*CONFIG_TI_CPTS*/
+
+#if 0
+static int cpsw_ndo_ioctl(struct net_device *dev, struct ifreq *req, int cmd)
+{
+	if (!rtnetif_running(dev))
+		return -EINVAL;
+
+#ifdef CONFIG_TI_CPTS
+	if (cmd == SIOCSHWTSTAMP)
+		return cpsw_hwtstamp_ioctl(dev, req);
+#endif
+	return -ENOTSUPP;
+}
+#endif
+
+#if 0
+static void cpsw_ndo_tx_timeout(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	cpsw_err(priv, tx_err, "transmit timeout, restarting dma\n");
+	priv->stats.tx_errors++;
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	cpdma_chan_stop(priv->txch);
+	cpdma_chan_start(priv->txch);
+	cpdma_ctlr_int_ctrl(priv->dma, true);
+	cpsw_intr_enable(priv);
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+}
+#endif
+
+static struct net_device_stats *cpsw_ndo_get_stats(struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+	return &priv->stats;
+}
+
+#if 0
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void cpsw_ndo_poll_controller(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	if (!priv->data.disable_napi)
+		cpsw_interrupt(ndev->irq, priv);
+	else {
+		/* bah! */
+		cpsw_rx_pend_irq(ndev->irq, priv);
+		cpsw_tx_pend_irq(ndev->irq, priv);
+	}
+	cpdma_ctlr_int_ctrl(priv->dma, true);
+	cpsw_intr_enable(priv);
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+}
+#endif
+#endif
+
+#if 0
+static struct net_device_ops cpsw_netdev_ops = {
+	.ndo_open		= cpsw_ndo_open,
+	.ndo_stop		= cpsw_ndo_stop,
+	.ndo_start_xmit		= cpsw_ndo_start_xmit,
+	.ndo_change_rx_flags	= cpsw_ndo_change_rx_flags,
+	.ndo_do_ioctl		= cpsw_ndo_ioctl,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_change_mtu		= eth_change_mtu,
+	.ndo_tx_timeout		= cpsw_ndo_tx_timeout,
+	.ndo_get_stats		= cpsw_ndo_get_stats,
+	.ndo_set_rx_mode	= cpsw_ndo_set_rx_mode,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cpsw_ndo_poll_controller,
+#endif
+};
+#endif
+
+static void cpsw_get_drvinfo(struct net_device *ndev,
+			     struct ethtool_drvinfo *info)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+	strcpy(info->driver, "TI CPSW Driver v1.0");
+	strcpy(info->version, "1.0");
+	strcpy(info->bus_info, priv->pdev->name);
+}
+
+static u32 cpsw_get_msglevel(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+	return priv->msg_enable;
+}
+
+static void cpsw_set_msglevel(struct net_device *ndev, u32 value)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+	priv->msg_enable = value;
+}
+
+static int cpsw_get_ts_info(struct net_device *ndev,
+			    struct ethtool_ts_info *info)
+{
+#ifdef CONFIG_TI_CPTS
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	info->so_timestamping =
+		SOF_TIMESTAMPING_TX_HARDWARE |
+		SOF_TIMESTAMPING_TX_SOFTWARE |
+		SOF_TIMESTAMPING_RX_HARDWARE |
+		SOF_TIMESTAMPING_RX_SOFTWARE |
+		SOF_TIMESTAMPING_SOFTWARE |
+		SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->phc_index = priv->cpts.phc_index;
+	info->tx_types =
+		(1 << HWTSTAMP_TX_OFF) |
+		(1 << HWTSTAMP_TX_ON);
+	info->rx_filters =
+		(1 << HWTSTAMP_FILTER_NONE) |
+		(1 << HWTSTAMP_FILTER_PTP_V2_EVENT);
+#else
+	info->so_timestamping =
+		SOF_TIMESTAMPING_TX_SOFTWARE |
+		SOF_TIMESTAMPING_RX_SOFTWARE |
+		SOF_TIMESTAMPING_SOFTWARE;
+	info->phc_index = -1;
+	info->tx_types = 0;
+	info->rx_filters = 0;
+#endif
+	return 0;
+}
+
+static const struct ethtool_ops cpsw_ethtool_ops = {
+	.get_drvinfo	= cpsw_get_drvinfo,
+	.get_msglevel	= cpsw_get_msglevel,
+	.set_msglevel	= cpsw_set_msglevel,
+	.get_link	= ethtool_op_get_link,
+	.get_ts_info	= cpsw_get_ts_info,
+};
+
+static void cpsw_slave_init(struct cpsw_slave *slave, struct cpsw_priv *priv,
+			    u32 slave_reg_ofs, u32 sliver_reg_ofs)
+{
+	void __iomem		*regs = priv->regs;
+	int			slave_num = slave->slave_num;
+	struct cpsw_slave_data	*data = priv->data.slave_data + slave_num;
+
+	slave->data	= data;
+	slave->regs	= regs + slave_reg_ofs;
+	slave->sliver	= regs + sliver_reg_ofs;
+}
+
+static int cpsw_probe_dt(struct cpsw_platform_data *data,
+			 struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct device_node *slave_node;
+	int i = 0, ret;
+	u32 prop;
+
+	if (!node)
+		return -EINVAL;
+
+	if (of_property_read_u32(node, "slaves", &prop)) {
+		dev_err(&pdev->dev, "Missing slaves property in the DT.\n");
+		return -EINVAL;
+	}
+	data->slaves = prop;
+
+	if (of_property_read_u32(node, "active_slave", &prop)) {
+		dev_err(&pdev->dev, "Missing active_slave property in the DT.\n");
+		return -EINVAL;
+	}
+	data->active_slave = prop;
+
+	if (of_property_read_u32(node, "cpts_clock_mult", &prop)) {
+		dev_err(&pdev->dev, "Missing cpts_clock_mult property in the DT.\n");
+		return -EINVAL;
+	}
+	data->cpts_clock_mult = prop;
+
+	if (of_property_read_u32(node, "cpts_clock_shift", &prop)) {
+		dev_err(&pdev->dev, "Missing cpts_clock_shift property in the DT.\n");
+		return -EINVAL;
+	}
+	data->cpts_clock_shift = prop;
+
+	data->slave_data = devm_kzalloc(&pdev->dev, data->slaves
+					* sizeof(struct cpsw_slave_data),
+					GFP_KERNEL);
+	if (!data->slave_data)
+		return -ENOMEM;
+
+	if (of_property_read_u32(node, "cpdma_channels", &prop)) {
+		dev_err(&pdev->dev, "Missing cpdma_channels property in the DT.\n");
+		return -EINVAL;
+	}
+	data->channels = prop;
+
+	if (of_property_read_u32(node, "ale_entries", &prop)) {
+		dev_err(&pdev->dev, "Missing ale_entries property in the DT.\n");
+		return -EINVAL;
+	}
+	data->ale_entries = prop;
+
+	if (of_property_read_u32(node, "bd_ram_size", &prop)) {
+		dev_err(&pdev->dev, "Missing bd_ram_size property in the DT.\n");
+		return -EINVAL;
+	}
+	data->bd_ram_size = prop;
+
+	if (of_property_read_u32(node, "rx_descs", &prop)) {
+		dev_warn(&pdev->dev, "Missing rx_descs property in the DT; defaults to 64\n");
+		prop = 64;
+	}
+	data->rx_descs = prop;
+
+	if (of_property_read_u32(node, "mac_control", &prop)) {
+		dev_err(&pdev->dev, "Missing mac_control property in the DT.\n");
+		return -EINVAL;
+	}
+	data->mac_control = prop;
+
+	if (of_property_read_bool(node, "dual_emac"))
+		data->dual_emac = 1;
+
+	/*
+	 * Populate all the child nodes here...
+	 */
+	ret = of_platform_populate(node, NULL, NULL, &pdev->dev);
+	/* We do not want to force this, as in some cases may not have child */
+	if (ret)
+		dev_warn(&pdev->dev, "Doesn't have any child node\n");
+
+	for_each_child_of_node(node, slave_node) {
+		struct cpsw_slave_data *slave_data = data->slave_data + i;
+		const void *mac_addr = NULL;
+		u32 phyid;
+		int lenp;
+		const __be32 *parp;
+		struct device_node *mdio_node;
+		struct platform_device *mdio;
+
+		/* This is no slave child node, continue */
+		if (strcmp(slave_node->name, "slave"))
+			continue;
+
+		parp = of_get_property(slave_node, "phy_id", &lenp);
+		if ((parp == NULL) || (lenp != (sizeof(void *) * 2))) {
+			dev_err(&pdev->dev, "Missing slave[%d] phy_id property\n", i);
+			goto no_phy_slave;
+		}
+		mdio_node = of_find_node_by_phandle(be32_to_cpup(parp));
+		phyid = be32_to_cpup(parp+1);
+		mdio = of_find_device_by_node(mdio_node);
+	        of_node_put(mdio_node);
+		if (!mdio) {
+			dev_err(&pdev->dev, "Missing mdio platform device\n");
+			return -EINVAL;
+		}
+		snprintf(slave_data->phy_id, sizeof(slave_data->phy_id),
+			 PHY_ID_FMT, mdio->name, phyid);
+
+		 slave_data->phy_if = of_get_phy_mode(slave_node);
+		 if (slave_data->phy_if < 0) {
+			 dev_err(&pdev->dev, "Missing or malformed slave[%d] phy-mode property\n",
+				 i);
+			 return slave_data->phy_if;
+		 }
+
+no_phy_slave:
+		mac_addr = of_get_mac_address(slave_node);
+		if (mac_addr) {
+			memcpy(slave_data->mac_addr, mac_addr, ETH_ALEN);
+		} else {
+			if (of_machine_is_compatible("ti,am33xx")) {
+				ret = cpsw_am33xx_cm_get_macid(&pdev->dev,
+							0x630, i,
+							slave_data->mac_addr);
+				if (ret)
+					return ret;
+			}
+		}
+		if (data->dual_emac) {
+			if (of_property_read_u32(slave_node, "dual_emac_res_vlan",
+						 &prop)) {
+				dev_err(&pdev->dev, "Missing dual_emac_res_vlan in DT.\n");
+				slave_data->dual_emac_res_vlan = i+1;
+				dev_err(&pdev->dev, "Using %d as Reserved VLAN for %d slave\n",
+					slave_data->dual_emac_res_vlan, i);
+			} else {
+				slave_data->dual_emac_res_vlan = prop;
+			}
+		}
+
+		i++;
+		if (i == data->slaves)
+			break;
+		}
+
+ 	return 0;
+ }
+
+static irq_handler_t cpsw_get_irq_handler(struct cpsw_priv *priv, int irq_idx)
+{
+	static const irq_handler_t non_napi_irq_tab[4] = {
+		cpsw_rx_thresh_pend_irq, cpsw_rx_pend_irq,
+		cpsw_tx_pend_irq, cpsw_misc_pend_irq
+	};
+
+	if ((unsigned int)irq_idx >= 4)
+		return NULL;
+
+#if 0
+	if (!priv->data.disable_napi)
+		return cpsw_interrupt;
+#endif
+
+	return non_napi_irq_tab[irq_idx];
+}
+
+static int cpsw_probe(struct platform_device *pdev)
+{
+	struct cpsw_platform_data	*data = pdev->dev.platform_data;
+	struct rtnet_device		*ndev;
+	struct cpsw_priv		*priv;
+	struct cpdma_params		dma_params;
+	struct cpsw_ale_params		ale_params;
+	void __iomem			*ss_regs, *wr_regs;
+	struct resource			*res;
+	u32 slave_offset, sliver_offset, slave_size;
+	irq_handler_t		irqh;
+	int ret = 0, i, j, k = 0;
+
+	ndev = rt_alloc_etherdev(sizeof(struct cpsw_priv), RX_RING_SIZE*2);
+	if (ndev == NULL){
+		return -ENOMEM;
+	}
+	rtdev_alloc_name(ndev, "rteth%d");
+	rt_rtdev_connect(ndev, &RTDEV_manager);
+	ndev->vers = RTDEV_VERS_2_0;
+
+	priv = ndev->priv;
+	platform_set_drvdata(pdev, ndev);
+	raw_spin_lock_init(&priv->lock);
+	priv->pdev = pdev;
+	priv->ndev = ndev;
+	priv->dev  = &pdev->dev;
+	priv->msg_enable = netif_msg_init(debug_level, CPSW_DEBUG);
+	priv->rx_packet_max = max(rx_packet_max, 128);
+	priv->irq_enabled = false;
+	priv->phy_phony_net_device = NULL;
+
+	if (rtskb_module_pool_init(&priv->skb_pool, RX_RING_SIZE*2) < RX_RING_SIZE*2) {
+		rtskb_pool_release(&priv->skb_pool);
+		ret = -ENOMEM;
+		goto clean_real_ndev_ret;
+	}
+
+	pm_runtime_use_autosuspend(&pdev->dev);
+	/* if delay is negative and the use_autosuspend flag is set
+	 * then runtime suspends are prevented.
+	 */
+	pm_runtime_set_autosuspend_delay(&pdev->dev, PM_NEGATIVE_DELAY);
+	pm_runtime_enable(&pdev->dev);
+	ret = pm_runtime_get_sync(&pdev->dev);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "%s: pm_runtime_get_sync error %d\n",
+				__func__, ret);
+		return ret;
+	}
+
+
+	if (cpsw_probe_dt(&priv->data, pdev)) {
+		pr_err("cpsw: platform data missing\n");
+		ret = -ENODEV;
+		goto clean_ndev_ret;
+	}
+	data = &priv->data;
+
+	pr_info("DT probed OK\n");
+
+	if (is_valid_ether_addr(data->slave_data[0].mac_addr)) {
+		memcpy(priv->mac_addr, data->slave_data[0].mac_addr, ETH_ALEN);
+		pr_info("Detected MACID = %pM", priv->mac_addr);
+	} else {
+		eth_random_addr(priv->mac_addr);
+		pr_info("Random MACID = %pM", priv->mac_addr);
+	}
+
+	memcpy(ndev->dev_addr, priv->mac_addr, ETH_ALEN);
+
+	priv->slaves = kzalloc(sizeof(struct cpsw_slave) * data->slaves,
+			       GFP_KERNEL);
+	if (!priv->slaves) {
+		ret = -EBUSY;
+		goto clean_ndev_ret;
+	}
+	for (i = 0; i < data->slaves; i++)
+		priv->slaves[i].slave_num = i;
+
+	priv->clk = clk_get(&pdev->dev, "fck");
+	if (IS_ERR(priv->clk)) {
+		dev_err(&pdev->dev, "fck is not found\n");
+		ret = -ENODEV;
+		goto clean_slave_ret;
+	}
+
+	priv->cpsw_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!priv->cpsw_res) {
+		dev_err(priv->dev, "error getting i/o resource\n");
+		ret = -ENOENT;
+		goto clean_clk_ret;
+	}
+	if (!request_mem_region(priv->cpsw_res->start,
+				resource_size(priv->cpsw_res), ndev->name)) {
+		dev_err(priv->dev, "failed request i/o region\n");
+		ret = -ENXIO;
+		goto clean_clk_ret;
+	}
+	ss_regs = ioremap(priv->cpsw_res->start, resource_size(priv->cpsw_res));
+	if (!ss_regs) {
+		dev_err(priv->dev, "unable to map i/o region\n");
+		goto clean_cpsw_iores_ret;
+	}
+	priv->regs = ss_regs;
+	priv->version = __raw_readl(&priv->regs->id_ver);
+	priv->host_port = HOST_PORT_NUM;
+
+	priv->cpsw_wr_res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	if (!priv->cpsw_wr_res) {
+		dev_err(priv->dev, "error getting i/o resource\n");
+		ret = -ENOENT;
+		goto clean_iomap_ret;
+	}
+	if (!request_mem_region(priv->cpsw_wr_res->start,
+			resource_size(priv->cpsw_wr_res), ndev->name)) {
+		dev_err(priv->dev, "failed request i/o region\n");
+		ret = -ENXIO;
+		goto clean_iomap_ret;
+	}
+	wr_regs = ioremap(priv->cpsw_wr_res->start,
+				resource_size(priv->cpsw_wr_res));
+	if (!wr_regs) {
+		dev_err(priv->dev, "unable to map i/o region\n");
+		goto clean_cpsw_wr_iores_ret;
+	}
+	priv->wr_regs = wr_regs;
+
+	memset(&dma_params, 0, sizeof(dma_params));
+	memset(&ale_params, 0, sizeof(ale_params));
+
+	switch (priv->version) {
+	case CPSW_VERSION_1:
+		priv->host_port_regs = ss_regs + CPSW1_HOST_PORT_OFFSET;
+		priv->cpts.reg       = ss_regs + CPSW1_CPTS_OFFSET;
+		dma_params.dmaregs   = ss_regs + CPSW1_CPDMA_OFFSET;
+		dma_params.txhdp     = ss_regs + CPSW1_STATERAM_OFFSET;
+		ale_params.ale_regs  = ss_regs + CPSW1_ALE_OFFSET;
+		slave_offset         = CPSW1_SLAVE_OFFSET;
+		slave_size           = CPSW1_SLAVE_SIZE;
+		sliver_offset        = CPSW1_SLIVER_OFFSET;
+		dma_params.desc_mem_phys = 0;
+		break;
+	case CPSW_VERSION_2:
+		priv->host_port_regs = ss_regs + CPSW2_HOST_PORT_OFFSET;
+		priv->cpts.reg       = ss_regs + CPSW2_CPTS_OFFSET;
+		dma_params.dmaregs   = ss_regs + CPSW2_CPDMA_OFFSET;
+		dma_params.txhdp     = ss_regs + CPSW2_STATERAM_OFFSET;
+		ale_params.ale_regs  = ss_regs + CPSW2_ALE_OFFSET;
+		slave_offset         = CPSW2_SLAVE_OFFSET;
+		slave_size           = CPSW2_SLAVE_SIZE;
+		sliver_offset        = CPSW2_SLIVER_OFFSET;
+		dma_params.desc_mem_phys =
+			(u32 __force) priv->cpsw_res->start + CPSW2_BD_OFFSET;
+		break;
+	default:
+		dev_err(priv->dev, "unknown version 0x%08x\n", priv->version);
+		ret = -ENODEV;
+		goto clean_cpsw_wr_iores_ret;
+	}
+	for (i = 0; i < priv->data.slaves; i++) {
+		struct cpsw_slave *slave = &priv->slaves[i];
+		cpsw_slave_init(slave, priv, slave_offset, sliver_offset);
+		slave_offset  += slave_size;
+		sliver_offset += SLIVER_SIZE;
+	}
+
+	dma_params.dev		= &pdev->dev;
+	dma_params.rxthresh	= dma_params.dmaregs + CPDMA_RXTHRESH;
+	dma_params.rxfree	= dma_params.dmaregs + CPDMA_RXFREE;
+	dma_params.rxhdp	= dma_params.txhdp + CPDMA_RXHDP;
+	dma_params.txcp		= dma_params.txhdp + CPDMA_TXCP;
+	dma_params.rxcp		= dma_params.txhdp + CPDMA_RXCP;
+
+	dma_params.num_chan		= data->channels;
+	dma_params.has_soft_reset	= true;
+	dma_params.min_packet_size	= CPSW_MIN_PACKET_SIZE;
+	dma_params.desc_mem_size	= data->bd_ram_size;
+	dma_params.desc_align		= 16;
+	dma_params.has_ext_regs		= true;
+	dma_params.desc_hw_addr         = dma_params.desc_mem_phys;
+
+	priv->dma = cpdma_ctlr_create(&dma_params);
+	if (!priv->dma) {
+		dev_err(priv->dev, "error initializing dma\n");
+		ret = -ENOMEM;
+		goto clean_wr_iomap_ret;
+	}
+
+	priv->txch = cpdma_chan_create(priv->dma, tx_chan_num(0),
+				       cpsw_tx_handler);
+	priv->rxch = cpdma_chan_create(priv->dma, rx_chan_num(0),
+				       cpsw_rx_handler);
+
+	if (WARN_ON(!priv->txch || !priv->rxch)) {
+		dev_err(priv->dev, "error initializing dma channels\n");
+		ret = -ENOMEM;
+		goto clean_dma_ret;
+	}
+
+	ale_params.dev			= &pdev->dev;
+	ale_params.ale_ageout		= ale_ageout;
+	ale_params.ale_entries		= data->ale_entries;
+	ale_params.ale_ports		= data->slaves;
+
+	priv->ale = cpsw_ale_create(&ale_params);
+	if (!priv->ale) {
+		dev_err(priv->dev, "error initializing ale engine\n");
+		ret = -ENODEV;
+		goto clean_dma_ret;
+	}
+
+	ndev->irq = platform_get_irq(pdev, 0);
+	if (ndev->irq < 0) {
+		dev_err(priv->dev, "error getting irq resource\n");
+		ret = -ENOENT;
+		goto clean_ale_ret;
+	}
+
+#if 0
+	dev_info(&pdev->dev, "NAPI %s\n", priv->data.disable_napi ?
+			"disabled" : "enabled");
+#endif
+
+	rt_stack_connect(ndev, &STACK_manager);
+	/* get interrupts */
+	j = k = 0;
+	while ((res = platform_get_resource(pdev, IORESOURCE_IRQ, j++))) {
+		for (i = res->start; k < 4 && i <= res->end; i++) {
+			irqh = cpsw_get_irq_handler(priv, k);
+			if (irqh == NULL) {
+				dev_err(&pdev->dev, "Unable to get handler "
+						"for #%d (%d)\n", k, i);
+				goto clean_ale_ret;
+			}
+			priv->irqs_table[k] = i;
+			if (request_irq(i, irqh, IRQF_NO_THREAD | IRQF_SHARED,
+						dev_name(&pdev->dev), priv)) {
+				dev_err(priv->dev, "error attaching irq\n");
+				goto clean_ale_ret;
+			}
+			k++;
+		}
+	}
+	priv->num_irqs = k;
+
+	ndev->flags |= IFF_ALLMULTI;	/* see cpsw_ndo_change_rx_flags() */
+
+	#if 0
+	ndev->netdev_ops = &cpsw_netdev_ops;
+	SET_ETHTOOL_OPS(ndev, &cpsw_ethtool_ops);
+	#endif
+	ndev->open		= cpsw_ndo_open;
+	ndev->hard_start_xmit 	= cpsw_ndo_start_xmit;
+	ndev->get_stats    	= cpsw_ndo_get_stats;
+	ndev->stop 		= cpsw_ndo_stop;
+
+	/* register the network device */
+	ret = rt_register_rtnetdev(ndev);
+	if (ret) {
+		dev_err(priv->dev, "error registering net device\n");
+		ret = -ENODEV;
+		goto clean_irq_ret;
+	}
+
+	if (cpts_register(&pdev->dev, &priv->cpts,
+			  data->cpts_clock_mult, data->cpts_clock_shift))
+		dev_err(priv->dev, "error registering cpts device\n");
+
+	cpsw_notice(priv, probe, "initialized device (regs %x, irq %d)\n",
+		  priv->cpsw_res->start, ndev->irq);
+
+	return 0;
+
+clean_irq_ret:
+	for(i = 0; i < priv->num_irqs; i++)
+		free_irq(priv->irqs_table[i], priv);
+	rt_stack_disconnect(ndev);
+clean_ale_ret:
+	cpsw_ale_destroy(priv->ale);
+clean_dma_ret:
+	cpdma_chan_destroy(priv->txch);
+	cpdma_chan_destroy(priv->rxch);
+	cpdma_ctlr_destroy(priv->dma);
+clean_wr_iomap_ret:
+	iounmap(priv->wr_regs);
+clean_cpsw_wr_iores_ret:
+	release_mem_region(priv->cpsw_wr_res->start,
+			   resource_size(priv->cpsw_wr_res));
+clean_iomap_ret:
+	iounmap(priv->regs);
+clean_cpsw_iores_ret:
+	release_mem_region(priv->cpsw_res->start,
+			   resource_size(priv->cpsw_res));
+clean_clk_ret:
+	clk_put(priv->clk);
+clean_slave_ret:
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+	kfree(priv->slaves);
+clean_ndev_ret:
+	rtskb_pool_release(&priv->skb_pool);
+clean_real_ndev_ret:
+	rt_rtdev_disconnect(ndev);
+	rtdev_free(ndev);
+	return ret;
+}
+
+static int cpsw_remove(struct platform_device *pdev)
+{
+	struct rtnet_device *ndev = platform_get_drvdata(pdev);
+	struct cpsw_priv *priv = ndev->priv;
+	int i;
+
+	pr_info("removing cpsw device");
+	platform_set_drvdata(pdev, NULL);
+
+	cpts_unregister(&priv->cpts);
+	if (priv->phy_phony_net_device)
+		free_netdev(priv->phy_phony_net_device);
+	for(i = 0; i < priv->num_irqs; i++)
+		free_irq(priv->irqs_table[i], priv);
+	rt_stack_disconnect(ndev);
+	cpsw_ale_destroy(priv->ale);
+	cpdma_chan_destroy(priv->txch);
+	cpdma_chan_destroy(priv->rxch);
+	cpdma_ctlr_destroy(priv->dma);
+	iounmap(priv->regs);
+	release_mem_region(priv->cpsw_res->start,
+			   resource_size(priv->cpsw_res));
+	iounmap(priv->wr_regs);
+	release_mem_region(priv->cpsw_wr_res->start,
+			   resource_size(priv->cpsw_wr_res));
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	clk_put(priv->clk);
+	kfree(priv->slaves);
+	rtskb_pool_release(&priv->skb_pool);
+	rt_rtdev_disconnect(ndev);
+	rtdev_free(ndev);
+
+	return 0;
+}
+#if 0
+static int cpsw_suspend(struct device *dev)
+{
+	struct platform_device	*pdev = to_platform_device(dev);
+	struct rtnet_device	*ndev = platform_get_drvdata(pdev);
+
+	if (rtnetif_running(ndev))
+		cpsw_ndo_stop(ndev);
+	pm_runtime_put_sync(&pdev->dev);
+
+	return 0;
+}
+
+static int cpsw_resume(struct device *dev)
+{
+	struct platform_device	*pdev = to_platform_device(dev);
+	struct rtnet_device	*ndev = platform_get_drvdata(pdev);
+
+	pm_runtime_get_sync(&pdev->dev);
+	if (rtnetif_running(ndev))
+		cpsw_ndo_open(ndev);
+	return 0;
+}
+
+static const struct dev_pm_ops cpsw_pm_ops = {
+	.suspend	= cpsw_suspend,
+	.resume		= cpsw_resume,
+};
+#endif
+
+static const struct of_device_id cpsw_of_mtable[] = {
+	{ .compatible = "ti,cpsw", },
+	{ /* sentinel */ },
+};
+
+static struct platform_driver cpsw_driver = {
+	.driver = {
+		.name	 = "cpsw",
+		.owner	 = THIS_MODULE,
+#if 0
+		.pm	 = &cpsw_pm_ops,
+#endif
+		.of_match_table = of_match_ptr(cpsw_of_mtable),
+	},
+	.probe = cpsw_probe,
+	.remove = cpsw_remove,
+};
+
+static int __init cpsw_init(void)
+{
+	load_smsc_phy_module(0);
+	load_davinci_mdio_module(0);
+	return platform_driver_register(&cpsw_driver);
+}
+late_initcall(cpsw_init);
+
+static void __exit cpsw_exit(void)
+{
+	platform_driver_unregister(&cpsw_driver);
+}
+module_exit(cpsw_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cyril Chemparathy <cyril@ti.com>");
+MODULE_AUTHOR("Mugunthan V N <mugunthanvnm@ti.com>");
+MODULE_DESCRIPTION("RT TI CPSW Ethernet driver");
diff -Naur a/net/rtnet/drivers/ticpsw/cpsw.h b/net/rtnet/drivers/ticpsw/cpsw.h
--- a/net/rtnet/drivers/ticpsw/cpsw.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/cpsw.h	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,56 @@
+/*
+ * Texas Instruments Ethernet Switch Driver
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __CPSW_H__
+#define __CPSW_H__
+
+#include <linux/if_ether.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/ktime.h>
+#include <linux/timekeeping.h>
+
+struct cpsw_slave_data {
+	char		phy_id[MII_BUS_ID_SIZE];
+	int		phy_if;
+	u8		mac_addr[ETH_ALEN];
+	u16		dual_emac_res_vlan;	/* Reserved VLAN for DualEMAC */
+};
+
+struct cpsw_platform_data {
+	struct cpsw_slave_data	*slave_data;
+	u32	ss_reg_ofs;	/* Subsystem control register offset */
+	u32	channels;	/* number of cpdma channels (symmetric) */
+	u32	slaves;		/* number of slave cpgmac ports */
+	u32	active_slave; /* time stamping, ethtool and SIOCGMIIPHY slave */
+	u32	cpts_clock_mult;  /* convert input clock ticks to nanoseconds */
+	u32	cpts_clock_shift; /* convert input clock ticks to nanoseconds */
+	u32	ale_entries;	/* ale table size */
+	u32	bd_ram_size;  /*buffer descriptor ram size */
+	u32	rx_descs;	/* Number of Rx Descriptios */
+	u32	mac_control;	/* Mac control register */
+	u16	default_vlan;	/* Def VLAN for ALE lookup in VLAN aware mode*/
+	bool	dual_emac;	/* Enable Dual EMAC mode */
+};
+
+int cpsw_am33xx_cm_get_macid(struct device *dev, u16 offset, int slave,
+			     u8 *mac_addr);
+int load_davinci_mdio_module(int v);
+int load_smsc_phy_module(int v);
+
+#endif /* __CPSW_H__ */
diff -Naur a/net/rtnet/drivers/ticpsw/cpts.c b/net/rtnet/drivers/ticpsw/cpts.c
--- a/net/rtnet/drivers/ticpsw/cpts.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/cpts.c	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,424 @@
+/*
+ * TI Common Platform Time Sync
+ *
+ * Copyright (C) 2012 Richard Cochran <richardcochran@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#include <linux/err.h>
+#include <linux/if.h>
+#include <linux/hrtimer.h>
+#include <linux/module.h>
+#include <linux/net_tstamp.h>
+#include <linux/ptp_classify.h>
+#include <linux/time.h>
+#include <linux/uaccess.h>
+#include <linux/workqueue.h>
+
+#include "cpts.h"
+
+#ifdef CONFIG_TI_CPTS
+
+static struct sock_filter ptp_filter[] = {
+	PTP_FILTER
+};
+
+#define cpts_read32(c, r)	__raw_readl(&c->reg->r)
+#define cpts_write32(c, v, r)	__raw_writel(v, &c->reg->r)
+
+static int event_expired(struct cpts_event *event)
+{
+	return time_after(jiffies, event->tmo);
+}
+
+static int event_type(struct cpts_event *event)
+{
+	return (event->high >> EVENT_TYPE_SHIFT) & EVENT_TYPE_MASK;
+}
+
+static int cpts_fifo_pop(struct cpts *cpts, u32 *high, u32 *low)
+{
+	u32 r = cpts_read32(cpts, intstat_raw);
+
+	if (r & TS_PEND_RAW) {
+		*high = cpts_read32(cpts, event_high);
+		*low  = cpts_read32(cpts, event_low);
+		cpts_write32(cpts, EVENT_POP, event_pop);
+		return 0;
+	}
+	return -1;
+}
+
+/*
+ * Returns zero if matching event type was found.
+ */
+static int cpts_fifo_read(struct cpts *cpts, int match)
+{
+	int i, type = -1;
+	u32 hi, lo;
+	struct cpts_event *event;
+
+	for (i = 0; i < CPTS_FIFO_DEPTH; i++) {
+		if (cpts_fifo_pop(cpts, &hi, &lo))
+			break;
+		if (list_empty(&cpts->pool)) {
+			pr_err("cpts: event pool is empty\n");
+			return -1;
+		}
+		event = list_first_entry(&cpts->pool, struct cpts_event, list);
+		event->tmo = jiffies + 2;
+		event->high = hi;
+		event->low = lo;
+		type = event_type(event);
+		switch (type) {
+		case CPTS_EV_PUSH:
+		case CPTS_EV_RX:
+		case CPTS_EV_TX:
+			list_del_init(&event->list);
+			list_add_tail(&event->list, &cpts->events);
+			break;
+		case CPTS_EV_ROLL:
+		case CPTS_EV_HALF:
+		case CPTS_EV_HW:
+			break;
+		default:
+			pr_err("cpts: unkown event type\n");
+			break;
+		}
+		if (type == match)
+			break;
+	}
+	return type == match ? 0 : -1;
+}
+
+static cycle_t cpts_systim_read(const struct cyclecounter *cc)
+{
+	u64 val = 0;
+	struct cpts_event *event;
+	struct list_head *this, *next;
+	struct cpts *cpts = container_of(cc, struct cpts, cc);
+
+	cpts_write32(cpts, TS_PUSH, ts_push);
+	if (cpts_fifo_read(cpts, CPTS_EV_PUSH))
+		pr_err("cpts: unable to obtain a time stamp\n");
+
+	list_for_each_safe(this, next, &cpts->events) {
+		event = list_entry(this, struct cpts_event, list);
+		if (event_type(event) == CPTS_EV_PUSH) {
+			list_del_init(&event->list);
+			list_add(&event->list, &cpts->pool);
+			val = event->low;
+			break;
+		}
+	}
+
+	return val;
+}
+
+/* PTP clock operations */
+
+static int cpts_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)
+{
+	u64 adj;
+	u32 diff, mult;
+	int neg_adj = 0;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	if (ppb < 0) {
+		neg_adj = 1;
+		ppb = -ppb;
+	}
+	mult = cpts->cc_mult;
+	adj = mult;
+	adj *= ppb;
+	diff = div_u64(adj, 1000000000ULL);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+
+	timecounter_read(&cpts->tc);
+
+	cpts->cc.mult = neg_adj ? mult - diff : mult + diff;
+
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return 0;
+}
+
+static int cpts_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)
+{
+	s64 now;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	now = timecounter_read(&cpts->tc);
+	now += delta;
+	timecounter_init(&cpts->tc, &cpts->cc, now);
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return 0;
+}
+
+static int cpts_ptp_gettime(struct ptp_clock_info *ptp, struct timespec *ts)
+{
+	u64 ns;
+	u32 remainder;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	ns = timecounter_read(&cpts->tc);
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	ts->tv_sec = div_u64_rem(ns, 1000000000, &remainder);
+	ts->tv_nsec = remainder;
+
+	return 0;
+}
+
+static int cpts_ptp_settime(struct ptp_clock_info *ptp,
+			    const struct timespec *ts)
+{
+	u64 ns;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	ns = ts->tv_sec * 1000000000ULL;
+	ns += ts->tv_nsec;
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	timecounter_init(&cpts->tc, &cpts->cc, ns);
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return 0;
+}
+
+static int cpts_ptp_enable(struct ptp_clock_info *ptp,
+			   struct ptp_clock_request *rq, int on)
+{
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info cpts_info = {
+	.owner		= THIS_MODULE,
+	.name		= "CTPS timer",
+	.max_adj	= 1000000,
+	.n_ext_ts	= 0,
+	.pps		= 0,
+	.adjfreq	= cpts_ptp_adjfreq,
+	.adjtime	= cpts_ptp_adjtime,
+	.gettime	= cpts_ptp_gettime,
+	.settime	= cpts_ptp_settime,
+	.enable		= cpts_ptp_enable,
+};
+
+static void cpts_overflow_check(struct work_struct *work)
+{
+	struct timespec ts;
+	struct cpts *cpts = container_of(work, struct cpts, overflow_work.work);
+
+	cpts_write32(cpts, CPTS_EN, control);
+	cpts_write32(cpts, TS_PEND_EN, int_enable);
+	cpts_ptp_gettime(&cpts->info, &ts);
+	pr_debug("cpts overflow check at %ld.%09lu\n", ts.tv_sec, ts.tv_nsec);
+	schedule_delayed_work(&cpts->overflow_work, CPTS_OVERFLOW_PERIOD);
+}
+
+#define CPTS_REF_CLOCK_NAME "cpsw_cpts_rft_clk"
+
+static void cpts_clk_init(struct cpts *cpts)
+{
+	cpts->refclk = clk_get(NULL, CPTS_REF_CLOCK_NAME);
+	if (IS_ERR(cpts->refclk)) {
+		pr_err("Failed to clk_get %s\n", CPTS_REF_CLOCK_NAME);
+		cpts->refclk = NULL;
+		return;
+	}
+	clk_prepare_enable(cpts->refclk);
+}
+
+static void cpts_clk_release(struct cpts *cpts)
+{
+	clk_disable(cpts->refclk);
+	clk_put(cpts->refclk);
+}
+
+static int cpts_match(struct sk_buff *skb, unsigned int ptp_class,
+		      u16 ts_seqid, u8 ts_msgtype)
+{
+	u16 *seqid;
+	unsigned int offset;
+	u8 *msgtype, *data = skb->data;
+
+	switch (ptp_class) {
+	case PTP_CLASS_V1_IPV4:
+	case PTP_CLASS_V2_IPV4:
+		offset = ETH_HLEN + IPV4_HLEN(data) + UDP_HLEN;
+		break;
+	case PTP_CLASS_V1_IPV6:
+	case PTP_CLASS_V2_IPV6:
+		offset = OFF_PTP6;
+		break;
+	case PTP_CLASS_V2_L2:
+		offset = ETH_HLEN;
+		break;
+	case PTP_CLASS_V2_VLAN:
+		offset = ETH_HLEN + VLAN_HLEN;
+		break;
+	default:
+		return 0;
+	}
+
+	if (skb->len + ETH_HLEN < offset + OFF_PTP_SEQUENCE_ID + sizeof(*seqid))
+		return 0;
+
+	if (unlikely(ptp_class & PTP_CLASS_V1))
+		msgtype = data + offset + OFF_PTP_CONTROL;
+	else
+		msgtype = data + offset;
+
+	seqid = (u16 *)(data + offset + OFF_PTP_SEQUENCE_ID);
+
+	return (ts_msgtype == (*msgtype & 0xf) && ts_seqid == ntohs(*seqid));
+}
+
+static u64 cpts_find_ts(struct cpts *cpts, struct sk_buff *skb, int ev_type)
+{
+	u64 ns = 0;
+	struct cpts_event *event;
+	struct list_head *this, *next;
+	unsigned int class = sk_run_filter(skb, ptp_filter);
+	rtdm_lockctx_t context;
+	u16 seqid;
+	u8 mtype;
+
+	if (class == PTP_CLASS_NONE)
+		return 0;
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	cpts_fifo_read(cpts, CPTS_EV_PUSH);
+	list_for_each_safe(this, next, &cpts->events) {
+		event = list_entry(this, struct cpts_event, list);
+		if (event_expired(event)) {
+			list_del_init(&event->list);
+			list_add(&event->list, &cpts->pool);
+			continue;
+		}
+		mtype = (event->high >> MESSAGE_TYPE_SHIFT) & MESSAGE_TYPE_MASK;
+		seqid = (event->high >> SEQUENCE_ID_SHIFT) & SEQUENCE_ID_MASK;
+		if (ev_type == event_type(event) &&
+		    cpts_match(skb, class, seqid, mtype)) {
+			ns = timecounter_cyc2time(&cpts->tc, event->low);
+			list_del_init(&event->list);
+			list_add(&event->list, &cpts->pool);
+			break;
+		}
+	}
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return ns;
+}
+
+void cpts_rx_timestamp(struct cpts *cpts, struct sk_buff *skb)
+{
+	u64 ns;
+	struct skb_shared_hwtstamps *ssh;
+
+	if (!cpts->rx_enable)
+		return;
+	ns = cpts_find_ts(cpts, skb, CPTS_EV_RX);
+	if (!ns)
+		return;
+	ssh = skb_hwtstamps(skb);
+	memset(ssh, 0, sizeof(*ssh));
+	ssh->hwtstamp = ns_to_ktime(ns);
+}
+
+void cpts_tx_timestamp(struct cpts *cpts, struct sk_buff *skb)
+{
+	u64 ns;
+	struct skb_shared_hwtstamps ssh;
+
+	if (!(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS))
+		return;
+	ns = cpts_find_ts(cpts, skb, CPTS_EV_TX);
+	if (!ns)
+		return;
+	memset(&ssh, 0, sizeof(ssh));
+	ssh.hwtstamp = ns_to_ktime(ns);
+	skb_tstamp_tx(skb, &ssh);
+}
+
+#endif /*CONFIG_TI_CPTS*/
+
+int cpts_register(struct device *dev, struct cpts *cpts,
+		  u32 mult, u32 shift)
+{
+#ifdef CONFIG_TI_CPTS
+	int err, i;
+	rtdm_lockctx_t context;
+
+	if (ptp_filter_init(ptp_filter, ARRAY_SIZE(ptp_filter))) {
+		pr_err("cpts: bad ptp filter\n");
+		return -EINVAL;
+	}
+	cpts->info = cpts_info;
+	cpts->clock = ptp_clock_register(&cpts->info, dev);
+	if (IS_ERR(cpts->clock)) {
+		err = PTR_ERR(cpts->clock);
+		cpts->clock = NULL;
+		return err;
+	}
+	rtdm_lock_init(&cpts->lock);
+
+	cpts->cc.read = cpts_systim_read;
+	cpts->cc.mask = CLOCKSOURCE_MASK(32);
+	cpts->cc_mult = mult;
+	cpts->cc.mult = mult;
+	cpts->cc.shift = shift;
+
+	INIT_LIST_HEAD(&cpts->events);
+	INIT_LIST_HEAD(&cpts->pool);
+	for (i = 0; i < CPTS_MAX_EVENTS; i++)
+		list_add(&cpts->pool_data[i].list, &cpts->pool);
+
+	cpts_clk_init(cpts);
+	cpts_write32(cpts, CPTS_EN, control);
+	cpts_write32(cpts, TS_PEND_EN, int_enable);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	timecounter_init(&cpts->tc, &cpts->cc, ktime_to_ns(ktime_get_real()));
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	INIT_DELAYED_WORK(&cpts->overflow_work, cpts_overflow_check);
+	schedule_delayed_work(&cpts->overflow_work, CPTS_OVERFLOW_PERIOD);
+
+	cpts->phc_index = ptp_clock_index(cpts->clock);
+#endif
+	return 0;
+}
+
+void cpts_unregister(struct cpts *cpts)
+{
+#ifdef CONFIG_TI_CPTS
+	if (cpts->clock) {
+		ptp_clock_unregister(cpts->clock);
+		cancel_delayed_work_sync(&cpts->overflow_work);
+	}
+	if (cpts->refclk)
+		cpts_clk_release(cpts);
+#endif
+}
diff -Naur a/net/rtnet/drivers/ticpsw/cpts.h b/net/rtnet/drivers/ticpsw/cpts.h
--- a/net/rtnet/drivers/ticpsw/cpts.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/cpts.h	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,147 @@
+/*
+ * TI Common Platform Time Sync
+ *
+ * Copyright (C) 2012 Richard Cochran <richardcochran@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#ifndef _TI_CPTS_H_
+#define _TI_CPTS_H_
+
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/clocksource.h>
+#include <linux/device.h>
+#include <linux/list.h>
+#include <linux/ptp_clock_kernel.h>
+#include <linux/skbuff.h>
+
+#include <rtnet_port.h>
+
+struct cpsw_cpts {
+	u32 idver;                /* Identification and version */
+	u32 control;              /* Time sync control */
+	u32 res1;
+	u32 ts_push;              /* Time stamp event push */
+	u32 ts_load_val;          /* Time stamp load value */
+	u32 ts_load_en;           /* Time stamp load enable */
+	u32 res2[2];
+	u32 intstat_raw;          /* Time sync interrupt status raw */
+	u32 intstat_masked;       /* Time sync interrupt status masked */
+	u32 int_enable;           /* Time sync interrupt enable */
+	u32 res3;
+	u32 event_pop;            /* Event interrupt pop */
+	u32 event_low;            /* 32 Bit Event Time Stamp */
+	u32 event_high;           /* Event Type Fields */
+};
+
+/* Bit definitions for the IDVER register */
+#define TX_IDENT_SHIFT       (16)    /* TX Identification Value */
+#define TX_IDENT_MASK        (0xffff)
+#define RTL_VER_SHIFT        (11)    /* RTL Version Value */
+#define RTL_VER_MASK         (0x1f)
+#define MAJOR_VER_SHIFT      (8)     /* Major Version Value */
+#define MAJOR_VER_MASK       (0x7)
+#define MINOR_VER_SHIFT      (0)     /* Minor Version Value */
+#define MINOR_VER_MASK       (0xff)
+
+/* Bit definitions for the CONTROL register */
+#define HW4_TS_PUSH_EN       (1<<11) /* Hardware push 4 enable */
+#define HW3_TS_PUSH_EN       (1<<10) /* Hardware push 3 enable */
+#define HW2_TS_PUSH_EN       (1<<9)  /* Hardware push 2 enable */
+#define HW1_TS_PUSH_EN       (1<<8)  /* Hardware push 1 enable */
+#define INT_TEST             (1<<1)  /* Interrupt Test */
+#define CPTS_EN              (1<<0)  /* Time Sync Enable */
+
+/*
+ * Definitions for the single bit resisters:
+ * TS_PUSH TS_LOAD_EN  INTSTAT_RAW INTSTAT_MASKED INT_ENABLE EVENT_POP
+ */
+#define TS_PUSH             (1<<0)  /* Time stamp event push */
+#define TS_LOAD_EN          (1<<0)  /* Time Stamp Load */
+#define TS_PEND_RAW         (1<<0)  /* int read (before enable) */
+#define TS_PEND             (1<<0)  /* masked interrupt read (after enable) */
+#define TS_PEND_EN          (1<<0)  /* masked interrupt enable */
+#define EVENT_POP           (1<<0)  /* writing discards one event */
+
+/* Bit definitions for the EVENT_HIGH register */
+#define PORT_NUMBER_SHIFT    (24)    /* Indicates Ethernet port or HW pin */
+#define PORT_NUMBER_MASK     (0x1f)
+#define EVENT_TYPE_SHIFT     (20)    /* Time sync event type */
+#define EVENT_TYPE_MASK      (0xf)
+#define MESSAGE_TYPE_SHIFT   (16)    /* PTP message type */
+#define MESSAGE_TYPE_MASK    (0xf)
+#define SEQUENCE_ID_SHIFT    (0)     /* PTP message sequence ID */
+#define SEQUENCE_ID_MASK     (0xffff)
+
+enum {
+	CPTS_EV_PUSH, /* Time Stamp Push Event */
+	CPTS_EV_ROLL, /* Time Stamp Rollover Event */
+	CPTS_EV_HALF, /* Time Stamp Half Rollover Event */
+	CPTS_EV_HW,   /* Hardware Time Stamp Push Event */
+	CPTS_EV_RX,   /* Ethernet Receive Event */
+	CPTS_EV_TX,   /* Ethernet Transmit Event */
+};
+
+/* This covers any input clock up to about 500 MHz. */
+#define CPTS_OVERFLOW_PERIOD (HZ * 8)
+
+#define CPTS_FIFO_DEPTH 16
+#define CPTS_MAX_EVENTS 32
+
+struct cpts_event {
+	struct list_head list;
+	unsigned long tmo;
+	u32 high;
+	u32 low;
+};
+
+struct cpts {
+	struct cpsw_cpts __iomem *reg;
+	int tx_enable;
+	int rx_enable;
+#ifdef CONFIG_TI_CPTS
+	struct ptp_clock_info info;
+	struct ptp_clock *clock;
+	rtdm_lock_t lock; /* protects time registers */
+	u32 cc_mult; /* for the nominal frequency */
+	struct cyclecounter cc;
+	struct timecounter tc;
+	struct delayed_work overflow_work;
+	int phc_index;
+	struct clk *refclk;
+	struct list_head events;
+	struct list_head pool;
+	struct cpts_event pool_data[CPTS_MAX_EVENTS];
+#endif
+};
+
+#ifdef CONFIG_TI_CPTS
+extern void cpts_rx_timestamp(struct cpts *cpts, struct rtskb *skb);
+extern void cpts_tx_timestamp(struct cpts *cpts, struct rtskb *skb);
+#else
+static inline void cpts_rx_timestamp(struct cpts *cpts, struct rtskb *skb)
+{
+}
+static inline void cpts_tx_timestamp(struct cpts *cpts, struct rtskb *skb)
+{
+}
+#endif
+
+extern int cpts_register(struct device *dev, struct cpts *cpts,
+			 u32 mult, u32 shift);
+extern void cpts_unregister(struct cpts *cpts);
+
+#endif
diff -Naur a/net/rtnet/drivers/ticpsw/davinci_cpdma.c b/net/rtnet/drivers/ticpsw/davinci_cpdma.c
--- a/net/rtnet/drivers/ticpsw/davinci_cpdma.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/davinci_cpdma.c	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,1003 @@
+/*
+ * Texas Instruments CPDMA Driver
+ *
+ * Copyright (C) 2010 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/dma-mapping.h>
+#include <linux/io.h>
+
+#include "davinci_cpdma.h"
+
+#include <rtnet_port.h>
+
+/* DMA Registers */
+#define CPDMA_TXIDVER		0x00
+#define CPDMA_TXCONTROL		0x04
+#define CPDMA_TXTEARDOWN	0x08
+#define CPDMA_RXIDVER		0x10
+#define CPDMA_RXCONTROL		0x14
+#define CPDMA_SOFTRESET		0x1c
+#define CPDMA_RXTEARDOWN	0x18
+#define CPDMA_TXINTSTATRAW	0x80
+#define CPDMA_TXINTSTATMASKED	0x84
+#define CPDMA_TXINTMASKSET	0x88
+#define CPDMA_TXINTMASKCLEAR	0x8c
+#define CPDMA_MACINVECTOR	0x90
+#define CPDMA_MACEOIVECTOR	0x94
+#define CPDMA_RXINTSTATRAW	0xa0
+#define CPDMA_RXINTSTATMASKED	0xa4
+#define CPDMA_RXINTMASKSET	0xa8
+#define CPDMA_RXINTMASKCLEAR	0xac
+#define CPDMA_DMAINTSTATRAW	0xb0
+#define CPDMA_DMAINTSTATMASKED	0xb4
+#define CPDMA_DMAINTMASKSET	0xb8
+#define CPDMA_DMAINTMASKCLEAR	0xbc
+#define CPDMA_DMAINT_HOSTERR	BIT(1)
+
+/* the following exist only if has_ext_regs is set */
+#define CPDMA_DMACONTROL	0x20
+#define CPDMA_DMASTATUS		0x24
+#define CPDMA_RXBUFFOFS		0x28
+#define CPDMA_EM_CONTROL	0x2c
+
+/* Descriptor mode bits */
+#define CPDMA_DESC_SOP		BIT(31)
+#define CPDMA_DESC_EOP		BIT(30)
+#define CPDMA_DESC_OWNER	BIT(29)
+#define CPDMA_DESC_EOQ		BIT(28)
+#define CPDMA_DESC_TD_COMPLETE	BIT(27)
+#define CPDMA_DESC_PASS_CRC	BIT(26)
+
+#define CPDMA_TEARDOWN_VALUE	0xfffffffc
+
+struct cpdma_desc {
+	/* hardware fields */
+	u32			hw_next;
+	u32			hw_buffer;
+	u32			hw_len;
+	u32			hw_mode;
+	/* software fields */
+	void			*sw_token;
+	u32			sw_buffer;
+	u32			sw_len;
+};
+
+struct cpdma_desc_pool {
+	u32			phys;
+	u32			hw_addr;
+	void __iomem		*iomap;		/* ioremap map */
+	void			*cpumap;	/* dma_alloc map */
+	int			desc_size, mem_size;
+	int			num_desc, used_desc;
+	unsigned long		*bitmap;
+	struct device		*dev;
+	raw_spinlock_t		lock;
+};
+
+enum cpdma_state {
+	CPDMA_STATE_IDLE,
+	CPDMA_STATE_ACTIVE,
+	CPDMA_STATE_TEARDOWN,
+};
+
+static const char *cpdma_state_str[] = { "idle", "active", "teardown" };
+
+struct cpdma_ctlr {
+	enum cpdma_state	state;
+	struct cpdma_params	params;
+	struct device		*dev;
+	struct cpdma_desc_pool	*pool;
+	raw_spinlock_t		lock;
+	struct cpdma_chan	*channels[2 * CPDMA_MAX_CHANNELS];
+};
+
+struct cpdma_chan {
+	enum cpdma_state		state;
+	struct cpdma_ctlr		*ctlr;
+	int				chan_num;
+	raw_spinlock_t			lock;
+	struct cpdma_desc __iomem	*head, *tail;
+	int				count;
+	void __iomem			*hdp, *cp, *rxfree;
+	u32				mask;
+	cpdma_handler_fn		handler;
+	enum dma_data_direction		dir;
+	struct cpdma_chan_stats		stats;
+	/* offsets into dmaregs */
+	int	int_set, int_clear, td;
+};
+
+/* The following make access to common cpdma_ctlr params more readable */
+#define dmaregs		params.dmaregs
+#define num_chan	params.num_chan
+
+/* various accessors */
+#define dma_reg_read(ctlr, ofs)		__raw_readl((ctlr)->dmaregs + (ofs))
+#define chan_read(chan, fld)		__raw_readl((chan)->fld)
+#define desc_read(desc, fld)		__raw_readl(&(desc)->fld)
+#define dma_reg_write(ctlr, ofs, v)	__raw_writel(v, (ctlr)->dmaregs + (ofs))
+#define chan_write(chan, fld, v)	__raw_writel(v, (chan)->fld)
+#define desc_write(desc, fld, v)	__raw_writel((u32)(v), &(desc)->fld)
+
+/*
+ * Utility constructs for a cpdma descriptor pool.  Some devices (e.g. davinci
+ * emac) have dedicated on-chip memory for these descriptors.  Some other
+ * devices (e.g. cpsw switches) use plain old memory.  Descriptor pools
+ * abstract out these details
+ */
+static struct cpdma_desc_pool *
+cpdma_desc_pool_create(struct device *dev, u32 phys, u32 hw_addr,
+				int size, int align)
+{
+	int bitmap_size;
+	struct cpdma_desc_pool *pool;
+
+	pool = kzalloc(sizeof(*pool), GFP_KERNEL);
+	if (!pool)
+		return NULL;
+
+	raw_spin_lock_init(&pool->lock);
+
+	pool->dev	= dev;
+	pool->mem_size	= size;
+	pool->desc_size	= ALIGN(sizeof(struct cpdma_desc), align);
+	pool->num_desc	= size / pool->desc_size;
+
+	bitmap_size  = (pool->num_desc / BITS_PER_LONG) * sizeof(long);
+	pool->bitmap = kzalloc(bitmap_size, GFP_KERNEL);
+	if (!pool->bitmap)
+		goto fail;
+
+	if (phys) {
+		pool->phys  = phys;
+		pool->iomap = ioremap(phys, size);
+		pool->hw_addr = hw_addr;
+	} else {
+		pool->cpumap = dma_alloc_coherent(dev, size, &pool->phys,
+						  GFP_KERNEL);
+		pool->iomap = pool->cpumap;
+		pool->hw_addr = pool->phys;
+	}
+
+	if (pool->iomap)
+		return pool;
+
+fail:
+	kfree(pool->bitmap);
+	kfree(pool);
+	return NULL;
+}
+
+static void cpdma_desc_pool_destroy(struct cpdma_desc_pool *pool)
+{
+	unsigned long context;
+
+	if (!pool)
+		return;
+
+	raw_spin_lock_irqsave(&pool->lock, context);
+	WARN_ON(pool->used_desc);
+	kfree(pool->bitmap);
+	if (pool->cpumap) {
+		dma_free_coherent(pool->dev, pool->mem_size, pool->cpumap,
+				  pool->phys);
+	} else {
+		iounmap(pool->iomap);
+	}
+	raw_spin_unlock_irqrestore(&pool->lock, context);
+	kfree(pool);
+}
+
+static inline dma_addr_t desc_phys(struct cpdma_desc_pool *pool,
+		  struct cpdma_desc __iomem *desc)
+{
+	if (!desc)
+		return 0;
+	return pool->hw_addr + (__force dma_addr_t)desc -
+			    (__force dma_addr_t)pool->iomap;
+}
+
+static inline struct cpdma_desc __iomem *
+desc_from_phys(struct cpdma_desc_pool *pool, dma_addr_t dma)
+{
+	return dma ? pool->iomap + dma - pool->hw_addr : NULL;
+}
+
+static struct cpdma_desc __iomem *
+cpdma_desc_alloc(struct cpdma_desc_pool *pool, int num_desc)
+{
+	unsigned long context;
+	int index;
+	struct cpdma_desc __iomem *desc = NULL;
+
+	raw_spin_lock_irqsave(&pool->lock, context);
+
+	index = bitmap_find_next_zero_area(pool->bitmap, pool->num_desc, 0,
+					   num_desc, 0);
+	if (index < pool->num_desc) {
+		bitmap_set(pool->bitmap, index, num_desc);
+		desc = pool->iomap + pool->desc_size * index;
+		pool->used_desc++;
+	}
+
+	raw_spin_unlock_irqrestore(&pool->lock, context);
+	return desc;
+}
+
+static void cpdma_desc_free(struct cpdma_desc_pool *pool,
+			    struct cpdma_desc __iomem *desc, int num_desc)
+{
+	unsigned long index;
+	unsigned long context;
+
+	index = ((unsigned long)desc - (unsigned long)pool->iomap) /
+		pool->desc_size;
+	raw_spin_lock_irqsave(&pool->lock, context);
+	bitmap_clear(pool->bitmap, index, num_desc);
+	pool->used_desc--;
+	raw_spin_unlock_irqrestore(&pool->lock, context);
+}
+
+struct cpdma_ctlr *cpdma_ctlr_create(struct cpdma_params *params)
+{
+	struct cpdma_ctlr *ctlr;
+
+	ctlr = kzalloc(sizeof(*ctlr), GFP_KERNEL);
+	if (!ctlr)
+		return NULL;
+
+	ctlr->state = CPDMA_STATE_IDLE;
+	ctlr->params = *params;
+	ctlr->dev = params->dev;
+	raw_spin_lock_init(&ctlr->lock);
+
+	ctlr->pool = cpdma_desc_pool_create(ctlr->dev,
+					    ctlr->params.desc_mem_phys,
+					    ctlr->params.desc_hw_addr,
+					    ctlr->params.desc_mem_size,
+					    ctlr->params.desc_align);
+	if (!ctlr->pool) {
+		kfree(ctlr);
+		return NULL;
+	}
+
+	if (WARN_ON(ctlr->num_chan > CPDMA_MAX_CHANNELS))
+		ctlr->num_chan = CPDMA_MAX_CHANNELS;
+	return ctlr;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_create);
+
+int cpdma_ctlr_start(struct cpdma_ctlr *ctlr)
+{
+	unsigned long context;
+	int i;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_IDLE) {
+		raw_spin_unlock_irqrestore(&ctlr->lock, context);
+		return -EBUSY;
+	}
+
+	if (ctlr->params.has_soft_reset) {
+		unsigned long timeout = jiffies + HZ/10;
+
+		dma_reg_write(ctlr, CPDMA_SOFTRESET, 1);
+		while (time_before(jiffies, timeout)) {
+			if (dma_reg_read(ctlr, CPDMA_SOFTRESET) == 0)
+				break;
+		}
+		WARN_ON(!time_before(jiffies, timeout));
+	}
+
+	for (i = 0; i < ctlr->num_chan; i++) {
+		__raw_writel(0, ctlr->params.txhdp + 4 * i);
+		__raw_writel(0, ctlr->params.rxhdp + 4 * i);
+		__raw_writel(0, ctlr->params.txcp + 4 * i);
+		__raw_writel(0, ctlr->params.rxcp + 4 * i);
+	}
+
+	dma_reg_write(ctlr, CPDMA_RXINTMASKCLEAR, 0xffffffff);
+	dma_reg_write(ctlr, CPDMA_TXINTMASKCLEAR, 0xffffffff);
+
+	dma_reg_write(ctlr, CPDMA_TXCONTROL, 1);
+	dma_reg_write(ctlr, CPDMA_RXCONTROL, 1);
+
+	ctlr->state = CPDMA_STATE_ACTIVE;
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_start(ctlr->channels[i]);
+	}
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_start);
+
+int cpdma_ctlr_stop(struct cpdma_ctlr *ctlr)
+{
+	unsigned long context;
+	int i;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&ctlr->lock, context);
+		return -EINVAL;
+	}
+
+	ctlr->state = CPDMA_STATE_TEARDOWN;
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_stop(ctlr->channels[i]);
+	}
+
+	dma_reg_write(ctlr, CPDMA_RXINTMASKCLEAR, 0xffffffff);
+	dma_reg_write(ctlr, CPDMA_TXINTMASKCLEAR, 0xffffffff);
+
+	dma_reg_write(ctlr, CPDMA_TXCONTROL, 0);
+	dma_reg_write(ctlr, CPDMA_RXCONTROL, 0);
+
+	ctlr->state = CPDMA_STATE_IDLE;
+
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_stop);
+
+int cpdma_ctlr_dump(struct cpdma_ctlr *ctlr)
+{
+	struct device *dev = ctlr->dev;
+	unsigned long context;
+	int i;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+
+	dev_info(dev, "CPDMA: state: %s", cpdma_state_str[ctlr->state]);
+
+	dev_info(dev, "CPDMA: txidver: %x",
+		 dma_reg_read(ctlr, CPDMA_TXIDVER));
+	dev_info(dev, "CPDMA: txcontrol: %x",
+		 dma_reg_read(ctlr, CPDMA_TXCONTROL));
+	dev_info(dev, "CPDMA: txteardown: %x",
+		 dma_reg_read(ctlr, CPDMA_TXTEARDOWN));
+	dev_info(dev, "CPDMA: rxidver: %x",
+		 dma_reg_read(ctlr, CPDMA_RXIDVER));
+	dev_info(dev, "CPDMA: rxcontrol: %x",
+		 dma_reg_read(ctlr, CPDMA_RXCONTROL));
+	dev_info(dev, "CPDMA: softreset: %x",
+		 dma_reg_read(ctlr, CPDMA_SOFTRESET));
+	dev_info(dev, "CPDMA: rxteardown: %x",
+		 dma_reg_read(ctlr, CPDMA_RXTEARDOWN));
+	dev_info(dev, "CPDMA: txintstatraw: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTSTATRAW));
+	dev_info(dev, "CPDMA: txintstatmasked: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTSTATMASKED));
+	dev_info(dev, "CPDMA: txintmaskset: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTMASKSET));
+	dev_info(dev, "CPDMA: txintmaskclear: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTMASKCLEAR));
+	dev_info(dev, "CPDMA: macinvector: %x",
+		 dma_reg_read(ctlr, CPDMA_MACINVECTOR));
+	dev_info(dev, "CPDMA: maceoivector: %x",
+		 dma_reg_read(ctlr, CPDMA_MACEOIVECTOR));
+	dev_info(dev, "CPDMA: rxintstatraw: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTSTATRAW));
+	dev_info(dev, "CPDMA: rxintstatmasked: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTSTATMASKED));
+	dev_info(dev, "CPDMA: rxintmaskset: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTMASKSET));
+	dev_info(dev, "CPDMA: rxintmaskclear: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTMASKCLEAR));
+	dev_info(dev, "CPDMA: dmaintstatraw: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTSTATRAW));
+	dev_info(dev, "CPDMA: dmaintstatmasked: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTSTATMASKED));
+	dev_info(dev, "CPDMA: dmaintmaskset: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTMASKSET));
+	dev_info(dev, "CPDMA: dmaintmaskclear: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTMASKCLEAR));
+
+	if (!ctlr->params.has_ext_regs) {
+		dev_info(dev, "CPDMA: dmacontrol: %x",
+			 dma_reg_read(ctlr, CPDMA_DMACONTROL));
+		dev_info(dev, "CPDMA: dmastatus: %x",
+			 dma_reg_read(ctlr, CPDMA_DMASTATUS));
+		dev_info(dev, "CPDMA: rxbuffofs: %x",
+			 dma_reg_read(ctlr, CPDMA_RXBUFFOFS));
+	}
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++)
+		if (ctlr->channels[i])
+			cpdma_chan_dump(ctlr->channels[i]);
+
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_dump);
+
+int cpdma_ctlr_destroy(struct cpdma_ctlr *ctlr)
+{
+	unsigned long context;
+	int ret = 0, i;
+
+	if (!ctlr)
+		return -EINVAL;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_IDLE)
+		cpdma_ctlr_stop(ctlr);
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_destroy(ctlr->channels[i]);
+	}
+
+	cpdma_desc_pool_destroy(ctlr->pool);
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	kfree(ctlr);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_destroy);
+
+int cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable)
+{
+	unsigned long context;
+	int i, reg;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&ctlr->lock, context);
+		return -EINVAL;
+	}
+
+	reg = enable ? CPDMA_DMAINTMASKSET : CPDMA_DMAINTMASKCLEAR;
+	dma_reg_write(ctlr, reg, CPDMA_DMAINT_HOSTERR);
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_int_ctrl(ctlr->channels[i], enable);
+	}
+
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_int_ctrl);
+
+void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value)
+{
+	dma_reg_write(ctlr, CPDMA_MACEOIVECTOR, value);
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_eoi);
+
+struct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,
+				     cpdma_handler_fn handler)
+{
+	struct cpdma_chan *chan;
+	int ret, offset = (chan_num % CPDMA_MAX_CHANNELS) * 4;
+	unsigned long context;
+
+	if (__chan_linear(chan_num) >= ctlr->num_chan)
+		return NULL;
+
+	ret = -ENOMEM;
+	chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+	if (!chan)
+		goto err_chan_alloc;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	ret = -EBUSY;
+	if (ctlr->channels[chan_num])
+		goto err_chan_busy;
+
+	chan->ctlr	= ctlr;
+	chan->state	= CPDMA_STATE_IDLE;
+	chan->chan_num	= chan_num;
+	chan->handler	= handler;
+
+	if (is_rx_chan(chan)) {
+		chan->hdp	= ctlr->params.rxhdp + offset;
+		chan->cp	= ctlr->params.rxcp + offset;
+		chan->rxfree	= ctlr->params.rxfree + offset;
+		chan->int_set	= CPDMA_RXINTMASKSET;
+		chan->int_clear	= CPDMA_RXINTMASKCLEAR;
+		chan->td	= CPDMA_RXTEARDOWN;
+		chan->dir	= DMA_FROM_DEVICE;
+	} else {
+		chan->hdp	= ctlr->params.txhdp + offset;
+		chan->cp	= ctlr->params.txcp + offset;
+		chan->int_set	= CPDMA_TXINTMASKSET;
+		chan->int_clear	= CPDMA_TXINTMASKCLEAR;
+		chan->td	= CPDMA_TXTEARDOWN;
+		chan->dir	= DMA_TO_DEVICE;
+	}
+	chan->mask = BIT(chan_linear(chan));
+
+	raw_spin_lock_init(&chan->lock);
+
+	ctlr->channels[chan_num] = chan;
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return chan;
+
+err_chan_busy:
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	kfree(chan);
+err_chan_alloc:
+	return ERR_PTR(ret);
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_create);
+
+int cpdma_chan_destroy(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr *ctlr;
+	unsigned long context;
+
+	if (!chan)
+		return -EINVAL;
+	ctlr = chan->ctlr;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (chan->state != CPDMA_STATE_IDLE)
+		cpdma_chan_stop(chan);
+	ctlr->channels[chan->chan_num] = NULL;
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	kfree(chan);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_destroy);
+
+int cpdma_chan_get_stats(struct cpdma_chan *chan,
+			 struct cpdma_chan_stats *stats)
+{
+	unsigned long context;
+	if (!chan)
+		return -EINVAL;
+	raw_spin_lock_irqsave(&chan->lock, context);
+	memcpy(stats, &chan->stats, sizeof(*stats));
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+
+int cpdma_chan_dump(struct cpdma_chan *chan)
+{
+	unsigned long context;
+	struct device *dev = chan->ctlr->dev;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+
+	dev_info(dev, "channel %d (%s %d) state %s",
+		 chan->chan_num, is_rx_chan(chan) ? "rx" : "tx",
+		 chan_linear(chan), cpdma_state_str[chan->state]);
+	dev_info(dev, "\thdp: %x\n", chan_read(chan, hdp));
+	dev_info(dev, "\tcp: %x\n", chan_read(chan, cp));
+	if (chan->rxfree) {
+		dev_info(dev, "\trxfree: %x\n",
+			 chan_read(chan, rxfree));
+	}
+
+	dev_info(dev, "\tstats head_enqueue: %d\n",
+		 chan->stats.head_enqueue);
+	dev_info(dev, "\tstats tail_enqueue: %d\n",
+		 chan->stats.tail_enqueue);
+	dev_info(dev, "\tstats pad_enqueue: %d\n",
+		 chan->stats.pad_enqueue);
+	dev_info(dev, "\tstats misqueued: %d\n",
+		 chan->stats.misqueued);
+	dev_info(dev, "\tstats desc_alloc_fail: %d\n",
+		 chan->stats.desc_alloc_fail);
+	dev_info(dev, "\tstats pad_alloc_fail: %d\n",
+		 chan->stats.pad_alloc_fail);
+	dev_info(dev, "\tstats runt_receive_buff: %d\n",
+		 chan->stats.runt_receive_buff);
+	dev_info(dev, "\tstats runt_transmit_buff: %d\n",
+		 chan->stats.runt_transmit_buff);
+	dev_info(dev, "\tstats empty_dequeue: %d\n",
+		 chan->stats.empty_dequeue);
+	dev_info(dev, "\tstats busy_dequeue: %d\n",
+		 chan->stats.busy_dequeue);
+	dev_info(dev, "\tstats good_dequeue: %d\n",
+		 chan->stats.good_dequeue);
+	dev_info(dev, "\tstats requeue: %d\n",
+		 chan->stats.requeue);
+	dev_info(dev, "\tstats teardown_dequeue: %d\n",
+		 chan->stats.teardown_dequeue);
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+
+static void __cpdma_chan_submit(struct cpdma_chan *chan,
+				struct cpdma_desc __iomem *desc)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc __iomem	*prev = chan->tail;
+	struct cpdma_desc_pool		*pool = ctlr->pool;
+	dma_addr_t			desc_dma;
+	u32				mode;
+
+	desc_dma = desc_phys(pool, desc);
+
+	/* simple case - idle channel */
+	if (!chan->head) {
+		chan->stats.head_enqueue++;
+		chan->head = desc;
+		chan->tail = desc;
+		if (chan->state == CPDMA_STATE_ACTIVE)
+			chan_write(chan, hdp, desc_dma);
+		return;
+	}
+
+	/* first chain the descriptor at the tail of the list */
+	desc_write(prev, hw_next, desc_dma);
+	chan->tail = desc;
+	chan->stats.tail_enqueue++;
+
+	/* next check if EOQ has been triggered already */
+	mode = desc_read(prev, hw_mode);
+	if (((mode & (CPDMA_DESC_EOQ | CPDMA_DESC_OWNER)) == CPDMA_DESC_EOQ) &&
+	    (chan->state == CPDMA_STATE_ACTIVE)) {
+		desc_write(prev, hw_mode, mode & ~CPDMA_DESC_EOQ);
+		chan_write(chan, hdp, desc_dma);
+		chan->stats.misqueued++;
+	}
+}
+
+int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
+		      int len, gfp_t gfp_mask)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc __iomem	*desc;
+	dma_addr_t			buffer;
+	unsigned long			context;
+	u32				mode;
+	int				ret = 0;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+
+	if (chan->state == CPDMA_STATE_TEARDOWN) {
+		ret = -EINVAL;
+		goto unlock_ret;
+	}
+
+	desc = cpdma_desc_alloc(ctlr->pool, 1);
+	if (!desc) {
+		chan->stats.desc_alloc_fail++;
+		ret = -ENOMEM;
+		goto unlock_ret;
+	}
+
+	if (len < ctlr->params.min_packet_size) {
+		len = ctlr->params.min_packet_size;
+		chan->stats.runt_transmit_buff++;
+	}
+
+	buffer = dma_map_single(ctlr->dev, data, len, chan->dir);
+	mode = CPDMA_DESC_OWNER | CPDMA_DESC_SOP | CPDMA_DESC_EOP;
+
+	desc_write(desc, hw_next,   0);
+	desc_write(desc, hw_buffer, buffer);
+	desc_write(desc, hw_len,    len);
+	desc_write(desc, hw_mode,   mode | len);
+	desc_write(desc, sw_token,  token);
+	desc_write(desc, sw_buffer, buffer);
+	desc_write(desc, sw_len,    len);
+
+	__cpdma_chan_submit(chan, desc);
+
+	if (chan->state == CPDMA_STATE_ACTIVE && chan->rxfree)
+		chan_write(chan, rxfree, 1);
+
+	chan->count++;
+
+unlock_ret:
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_submit);
+
+static void __cpdma_chan_free(struct cpdma_chan *chan,
+			      struct cpdma_desc __iomem *desc,
+			      int outlen, int status)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc_pool		*pool = ctlr->pool;
+	dma_addr_t			buff_dma;
+	int				origlen;
+	void				*token;
+#if 0
+	trace_printk("__cpdma_chan_free(%x, %x, %d, %d)\n", chan, desc, outlen, status);
+#endif
+	token      = (void *)desc_read(desc, sw_token);
+	buff_dma   = desc_read(desc, sw_buffer);
+	origlen    = desc_read(desc, sw_len);
+
+
+	dma_unmap_single(ctlr->dev, buff_dma, origlen, chan->dir);
+	cpdma_desc_free(pool, desc, 1);
+	(*chan->handler)(token, outlen, status);
+}
+
+static int __cpdma_chan_process(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc __iomem	*desc;
+	int				status, outlen;
+	struct cpdma_desc_pool		*pool = ctlr->pool;
+	dma_addr_t			desc_dma;
+	unsigned long			context;
+#if 0
+	trace_printk("__cpdma_chan_process(%x)\n", chan);
+#endif	
+	raw_spin_lock_irqsave(&chan->lock, context);
+	desc = chan->head;
+	if (!desc) {
+		chan->stats.empty_dequeue++;
+		status = -ENOENT;
+		goto unlock_ret;
+	}
+	desc_dma = desc_phys(pool, desc);
+
+	status	= __raw_readl(&desc->hw_mode);
+	outlen	= status & 0x7ff;
+	if (status & CPDMA_DESC_OWNER) {
+		chan->stats.busy_dequeue++;
+		status = -EBUSY;
+		goto unlock_ret;
+	}
+	status	= status & (CPDMA_DESC_EOQ | CPDMA_DESC_TD_COMPLETE);
+
+	chan->head = desc_from_phys(pool, desc_read(desc, hw_next));
+	chan_write(chan, cp, desc_dma);
+	chan->count--;
+	chan->stats.good_dequeue++;
+
+	if (status & CPDMA_DESC_EOQ) {
+		chan->stats.requeue++;
+		chan_write(chan, hdp, desc_phys(pool, chan->head));
+	}
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+
+	__cpdma_chan_free(chan, desc, outlen, status);
+	return status;
+unlock_ret:
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+
+	return status;
+}
+
+int cpdma_chan_process(struct cpdma_chan *chan, int quota)
+{
+	int used = 0, ret = 0;
+#if 0
+	trace_printk("cpdma_chan_process(%x, %d)\n", chan, quota);
+#endif	
+	if (chan->state != CPDMA_STATE_ACTIVE)
+		return -EINVAL;
+
+	while (used < quota) {
+		ret = __cpdma_chan_process(chan);
+		if (ret < 0)
+			break;
+		used++;
+	}
+	return used;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_process);
+
+int cpdma_chan_start(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr	*ctlr = chan->ctlr;
+	struct cpdma_desc_pool	*pool = ctlr->pool;
+	unsigned long		context;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+	if (chan->state != CPDMA_STATE_IDLE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EBUSY;
+	}
+	if (ctlr->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EINVAL;
+	}
+	dma_reg_write(ctlr, chan->int_set, chan->mask);
+	chan->state = CPDMA_STATE_ACTIVE;
+	if (chan->head) {
+		chan_write(chan, hdp, desc_phys(pool, chan->head));
+		if (chan->rxfree)
+			chan_write(chan, rxfree, chan->count);
+	}
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_start);
+
+int cpdma_chan_stop(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr	*ctlr = chan->ctlr;
+	struct cpdma_desc_pool	*pool = ctlr->pool;
+	unsigned long		context;
+	int			ret;
+	unsigned long		timeout;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+	if (chan->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EINVAL;
+	}
+
+	chan->state = CPDMA_STATE_TEARDOWN;
+	dma_reg_write(ctlr, chan->int_clear, chan->mask);
+
+	/* trigger teardown */
+	dma_reg_write(ctlr, chan->td, chan_linear(chan));
+
+	/* wait for teardown complete */
+	timeout = jiffies + HZ/10;	/* 100 msec */
+	while (time_before(jiffies, timeout)) {
+		u32 cp = chan_read(chan, cp);
+		if ((cp & CPDMA_TEARDOWN_VALUE) == CPDMA_TEARDOWN_VALUE)
+			break;
+		cpu_relax();
+	}
+	WARN_ON(!time_before(jiffies, timeout));
+	chan_write(chan, cp, CPDMA_TEARDOWN_VALUE);
+
+	/* handle completed packets */
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	do {
+		ret = __cpdma_chan_process(chan);
+		if (ret < 0)
+			break;
+	} while ((ret & CPDMA_DESC_TD_COMPLETE) == 0);
+	raw_spin_lock_irqsave(&chan->lock, context);
+
+	/* remaining packets haven't been tx/rx'ed, clean them up */
+	while (chan->head) {
+		struct cpdma_desc __iomem *desc = chan->head;
+		dma_addr_t next_dma;
+
+		next_dma = desc_read(desc, hw_next);
+		chan->head = desc_from_phys(pool, next_dma);
+		chan->count--;
+		chan->stats.teardown_dequeue++;
+
+		/* issue callback without locks held */
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		__cpdma_chan_free(chan, desc, 0, -ENOSYS);
+		raw_spin_lock_irqsave(&chan->lock, context);
+	}
+
+	chan->state = CPDMA_STATE_IDLE;
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_stop);
+
+int cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable)
+{
+	unsigned long context;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+	if (chan->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EINVAL;
+	}
+
+	dma_reg_write(chan->ctlr, enable ? chan->int_set : chan->int_clear,
+		      chan->mask);
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+
+	return 0;
+}
+
+struct cpdma_control_info {
+	u32		reg;
+	u32		shift, mask;
+	int		access;
+#define ACCESS_RO	BIT(0)
+#define ACCESS_WO	BIT(1)
+#define ACCESS_RW	(ACCESS_RO | ACCESS_WO)
+};
+
+struct cpdma_control_info controls[] = {
+	[CPDMA_CMD_IDLE]	  = {CPDMA_DMACONTROL,	3,  1,      ACCESS_WO},
+	[CPDMA_COPY_ERROR_FRAMES] = {CPDMA_DMACONTROL,	4,  1,      ACCESS_RW},
+	[CPDMA_RX_OFF_LEN_UPDATE] = {CPDMA_DMACONTROL,	2,  1,      ACCESS_RW},
+	[CPDMA_RX_OWNERSHIP_FLIP] = {CPDMA_DMACONTROL,	1,  1,      ACCESS_RW},
+	[CPDMA_TX_PRIO_FIXED]	  = {CPDMA_DMACONTROL,	0,  1,      ACCESS_RW},
+	[CPDMA_STAT_IDLE]	  = {CPDMA_DMASTATUS,	31, 1,      ACCESS_RO},
+	[CPDMA_STAT_TX_ERR_CODE]  = {CPDMA_DMASTATUS,	20, 0xf,    ACCESS_RW},
+	[CPDMA_STAT_TX_ERR_CHAN]  = {CPDMA_DMASTATUS,	16, 0x7,    ACCESS_RW},
+	[CPDMA_STAT_RX_ERR_CODE]  = {CPDMA_DMASTATUS,	12, 0xf,    ACCESS_RW},
+	[CPDMA_STAT_RX_ERR_CHAN]  = {CPDMA_DMASTATUS,	8,  0x7,    ACCESS_RW},
+	[CPDMA_RX_BUFFER_OFFSET]  = {CPDMA_RXBUFFOFS,	0,  0xffff, ACCESS_RW},
+};
+
+int cpdma_control_get(struct cpdma_ctlr *ctlr, int control)
+{
+	unsigned long context;
+	struct cpdma_control_info *info = &controls[control];
+	int ret;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+
+	ret = -ENOTSUPP;
+	if (!ctlr->params.has_ext_regs)
+		goto unlock_ret;
+
+	ret = -EINVAL;
+	if (ctlr->state != CPDMA_STATE_ACTIVE)
+		goto unlock_ret;
+
+	ret = -ENOENT;
+	if (control < 0 || control >= ARRAY_SIZE(controls))
+		goto unlock_ret;
+
+	ret = -EPERM;
+	if ((info->access & ACCESS_RO) != ACCESS_RO)
+		goto unlock_ret;
+
+	ret = (dma_reg_read(ctlr, info->reg) >> info->shift) & info->mask;
+
+unlock_ret:
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return ret;
+}
+
+int cpdma_control_set(struct cpdma_ctlr *ctlr, int control, int value)
+{
+	unsigned long context;
+	struct cpdma_control_info *info = &controls[control];
+	int ret;
+	u32 val;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+
+	ret = -ENOTSUPP;
+	if (!ctlr->params.has_ext_regs)
+		goto unlock_ret;
+
+	ret = -EINVAL;
+	if (ctlr->state != CPDMA_STATE_ACTIVE)
+		goto unlock_ret;
+
+	ret = -ENOENT;
+	if (control < 0 || control >= ARRAY_SIZE(controls))
+		goto unlock_ret;
+
+	ret = -EPERM;
+	if ((info->access & ACCESS_WO) != ACCESS_WO)
+		goto unlock_ret;
+
+	val  = dma_reg_read(ctlr, info->reg);
+	val &= ~(info->mask << info->shift);
+	val |= (value & info->mask) << info->shift;
+	dma_reg_write(ctlr, info->reg, val);
+	ret = 0;
+
+unlock_ret:
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cpdma_control_set);
diff -Naur a/net/rtnet/drivers/ticpsw/davinci_cpdma.h b/net/rtnet/drivers/ticpsw/davinci_cpdma.h
--- a/net/rtnet/drivers/ticpsw/davinci_cpdma.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/davinci_cpdma.h	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,109 @@
+/*
+ * Texas Instruments CPDMA Driver
+ *
+ * Copyright (C) 2010 Texas Instruments
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __DAVINCI_CPDMA_H__
+#define __DAVINCI_CPDMA_H__
+
+#define CPDMA_MAX_CHANNELS	BITS_PER_LONG
+
+#define tx_chan_num(chan)	(chan)
+#define rx_chan_num(chan)	((chan) + CPDMA_MAX_CHANNELS)
+#define is_rx_chan(chan)	((chan)->chan_num >= CPDMA_MAX_CHANNELS)
+#define is_tx_chan(chan)	(!is_rx_chan(chan))
+#define __chan_linear(chan_num)	((chan_num) & (CPDMA_MAX_CHANNELS - 1))
+#define chan_linear(chan)	__chan_linear((chan)->chan_num)
+
+struct cpdma_params {
+	struct device		*dev;
+	void __iomem		*dmaregs;
+	void __iomem		*txhdp, *rxhdp, *txcp, *rxcp;
+	void __iomem		*rxthresh, *rxfree;
+	int			num_chan;
+	bool			has_soft_reset;
+	int			min_packet_size;
+	u32			desc_mem_phys;
+	u32			desc_hw_addr;
+	int			desc_mem_size;
+	int			desc_align;
+
+	/*
+	 * Some instances of embedded cpdma controllers have extra control and
+	 * status registers.  The following flag enables access to these
+	 * "extended" registers.
+	 */
+	bool			has_ext_regs;
+};
+
+struct cpdma_chan_stats {
+	u32			head_enqueue;
+	u32			tail_enqueue;
+	u32			pad_enqueue;
+	u32			misqueued;
+	u32			desc_alloc_fail;
+	u32			pad_alloc_fail;
+	u32			runt_receive_buff;
+	u32			runt_transmit_buff;
+	u32			empty_dequeue;
+	u32			busy_dequeue;
+	u32			good_dequeue;
+	u32			requeue;
+	u32			teardown_dequeue;
+};
+
+struct cpdma_ctlr;
+struct cpdma_chan;
+
+typedef void (*cpdma_handler_fn)(void *token, int len, int status);
+
+struct cpdma_ctlr *cpdma_ctlr_create(struct cpdma_params *params);
+int cpdma_ctlr_destroy(struct cpdma_ctlr *ctlr);
+int cpdma_ctlr_start(struct cpdma_ctlr *ctlr);
+int cpdma_ctlr_stop(struct cpdma_ctlr *ctlr);
+int cpdma_ctlr_dump(struct cpdma_ctlr *ctlr);
+
+struct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,
+				     cpdma_handler_fn handler);
+int cpdma_chan_destroy(struct cpdma_chan *chan);
+int cpdma_chan_start(struct cpdma_chan *chan);
+int cpdma_chan_stop(struct cpdma_chan *chan);
+int cpdma_chan_dump(struct cpdma_chan *chan);
+
+int cpdma_chan_get_stats(struct cpdma_chan *chan,
+			 struct cpdma_chan_stats *stats);
+int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
+		      int len, gfp_t gfp_mask);
+int cpdma_chan_process(struct cpdma_chan *chan, int quota);
+
+int cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable);
+void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value);
+int cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable);
+
+enum cpdma_control {
+	CPDMA_CMD_IDLE,			/* write-only */
+	CPDMA_COPY_ERROR_FRAMES,	/* read-write */
+	CPDMA_RX_OFF_LEN_UPDATE,	/* read-write */
+	CPDMA_RX_OWNERSHIP_FLIP,	/* read-write */
+	CPDMA_TX_PRIO_FIXED,		/* read-write */
+	CPDMA_STAT_IDLE,		/* read-only */
+	CPDMA_STAT_TX_ERR_CHAN,		/* read-only */
+	CPDMA_STAT_TX_ERR_CODE,		/* read-only */
+	CPDMA_STAT_RX_ERR_CHAN,		/* read-only */
+	CPDMA_STAT_RX_ERR_CODE,		/* read-only */
+	CPDMA_RX_BUFFER_OFFSET,		/* read-write */
+};
+
+int cpdma_control_get(struct cpdma_ctlr *ctlr, int control);
+int cpdma_control_set(struct cpdma_ctlr *ctlr, int control, int value);
+
+#endif
diff -Naur a/net/rtnet/drivers/ticpsw/Makefile b/net/rtnet/drivers/ticpsw/Makefile
--- a/net/rtnet/drivers/ticpsw/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/drivers/ticpsw/Makefile	2021-07-14 15:39:13.254125361 +0300
@@ -0,0 +1,10 @@
+ccflags-y += -Inet/rtnet/stack/include -g -DDEBUG
+
+obj-$(CONFIG_RTNET_DRV_TI_CPSW) += rt_ticpsw.o
+
+rt_ticpsw-y := \
+	cpsw.o \
+	cpsw_ale.o \
+	cpts.o \
+	davinci_cpdma.o
+
diff -Naur a/net/rtnet/Kconfig b/net/rtnet/Kconfig
--- a/net/rtnet/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/Kconfig	2021-07-14 15:39:13.314124941 +0300
@@ -0,0 +1,24 @@
+menu "RTnet"
+
+config RTNET
+    depends on NET
+    bool "RTnet, TCP/IP socket interface"
+
+if RTNET
+
+config RTNET_CHECKED
+    bool "Internal Bug Checks"
+    default n
+    help
+    Switch on if you face crashes when RTnet is running or if you suspect
+    any other RTnet-related issues. This feature will add a few sanity
+    checks at critical points that will produce warnings on the kernel
+    console in case certain internal bugs are detected.
+
+source "net/rtnet/stack/Kconfig"
+source "net/rtnet/drivers/Kconfig"
+source "net/rtnet/addons/Kconfig"
+
+endif
+
+endmenu
diff -Naur a/net/rtnet/Makefile b/net/rtnet/Makefile
--- a/net/rtnet/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/Makefile	2021-07-14 15:39:13.314124941 +0300
@@ -0,0 +1 @@
+obj-$(CONFIG_RTNET) += stack/ drivers/ addons/
diff -Naur a/net/rtnet/modules.builtin b/net/rtnet/modules.builtin
--- a/net/rtnet/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/modules.builtin	2021-07-14 15:39:13.314124941 +0300
@@ -0,0 +1,6 @@
+net/rtnet/drivers/rt_loopback.ko
+net/rtnet/drivers/microchip/enc28j60.ko
+net/rtnet/stack/rtnet.ko
+net/rtnet/stack/ipv4/rtipv4.ko
+net/rtnet/stack/ipv4/udp/rtudp.ko
+net/rtnet/stack/packet/rtpacket.ko
diff -Naur a/net/rtnet/README b/net/rtnet/README
--- a/net/rtnet/README	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/README	2021-07-14 15:39:13.246125417 +0300
@@ -0,0 +1,3 @@
+
+rtnet-preempt_rt-v66t.tgz
+- compiled orange pi one without warnings
diff -Naur a/net/rtnet/stack/eth.c b/net/rtnet/stack/eth.c
--- a/net/rtnet/stack/eth.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/eth.c	2021-07-14 15:39:13.334124801 +0300
@@ -0,0 +1,141 @@
+/***
+ *
+ *  stack/eth.c - Ethernet-specific functions
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+
+
+/*
+ *  Create the Ethernet MAC header for an arbitrary protocol layer
+ *
+ *  saddr=NULL  means use device source address
+ *  daddr=NULL  means leave destination address (eg unresolved arp)
+ */
+int rt_eth_header(struct rtskb *skb, struct rtnet_device *rtdev,
+                  unsigned short type, void *daddr, void *saddr, unsigned len)
+{
+    struct ethhdr *eth = (struct ethhdr *)rtskb_push(skb,ETH_HLEN);
+
+    /*
+     *  Set rtskb mac field
+     */
+
+    skb->mac.ethernet = eth;
+
+    /*
+     *  Set the protocol type. For a packet of type ETH_P_802_3 we put the length
+     *  in here instead. It is up to the 802.2 layer to carry protocol information.
+     */
+
+    if (type!=ETH_P_802_3)
+        eth->h_proto = htons(type);
+    else
+        eth->h_proto = htons(len);
+
+    /*
+     *  Set the source hardware address.
+     */
+
+    if(saddr)
+        memcpy(eth->h_source,saddr,rtdev->addr_len);
+    else
+        memcpy(eth->h_source,rtdev->dev_addr,rtdev->addr_len);
+
+    if (rtdev->flags & (IFF_LOOPBACK|IFF_NOARP))
+    {
+        memset(eth->h_dest, 0, rtdev->addr_len);
+        return rtdev->hard_header_len;
+    }
+
+    if (daddr)
+    {
+        memcpy(eth->h_dest,daddr,rtdev->addr_len);
+        return rtdev->hard_header_len;
+    }
+
+    return -rtdev->hard_header_len;
+}
+
+
+
+
+unsigned short rt_eth_type_trans(struct rtskb *skb, struct rtnet_device *rtdev)
+{
+    struct ethhdr *eth;
+    unsigned char *rawp;
+
+
+    rtcap_mark_incoming(skb);
+
+    skb->mac.raw = skb->data;
+    rtskb_pull(skb,rtdev->hard_header_len);
+    eth = skb->mac.ethernet;
+
+    if (*eth->h_dest & 1)
+    {
+        if (memcmp(eth->h_dest,rtdev->broadcast, ETH_ALEN) == 0)
+            skb->pkt_type = PACKET_BROADCAST;
+        else
+            skb->pkt_type = PACKET_MULTICAST;
+    }
+
+    /*
+     *  This ALLMULTI check should be redundant by 1.4
+     *  so don't forget to remove it.
+     *
+     *  Seems, you forgot to remove it. All silly devices
+     *  seems to set IFF_PROMISC.
+     */
+
+    else if (1 /*rtdev->flags&IFF_PROMISC*/)
+    {
+        if (memcmp(eth->h_dest,rtdev->dev_addr, ETH_ALEN))
+            skb->pkt_type = PACKET_OTHERHOST;
+    }
+
+    if (ntohs(eth->h_proto) >= 1536)
+        return eth->h_proto;
+
+    rawp = skb->data;
+
+    /*
+     *  This is a magic hack to spot IPX packets. Older Novell breaks
+     *  the protocol design and runs IPX over 802.3 without an 802.2 LLC
+     *  layer. We look for FFFF which isn't a used 802.2 SSAP/DSAP. This
+     *  won't work for fault tolerant netware but does for the rest.
+     */
+    if (*(unsigned short *)rawp == 0xFFFF)
+        return htons(ETH_P_802_3);
+
+    /*
+     *  Real 802.2 LLC
+     */
+    return htons(ETH_P_802_2);
+}
+
+
+EXPORT_SYMBOL_GPL(rt_eth_header);
+EXPORT_SYMBOL_GPL(rt_eth_type_trans);
diff -Naur a/net/rtnet/stack/include/ethernet/eth.h b/net/rtnet/stack/include/ethernet/eth.h
--- a/net/rtnet/stack/include/ethernet/eth.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ethernet/eth.h	2021-07-14 15:39:13.434124100 +0300
@@ -0,0 +1,32 @@
+/* ethernet/eth.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_ETH_H_
+#define __RTNET_ETH_H_
+
+
+#include <rtskb.h>
+#include <rtdev.h>
+
+extern int rt_eth_header(struct rtskb *skb,struct rtnet_device *rtdev, 
+			 unsigned short type,void *daddr,void *saddr,unsigned int len);
+extern unsigned short rt_eth_type_trans(struct rtskb *skb, struct rtnet_device *dev);
+
+
+#endif  /* __RTNET_ETH_H_ */
diff -Naur a/net/rtnet/stack/include/INCLUDE.policy b/net/rtnet/stack/include/INCLUDE.policy
--- a/net/rtnet/stack/include/INCLUDE.policy	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/INCLUDE.policy	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,15 @@
+RTnet Include Policy
+
+1. Every source file (/<module>/<source>.c) shall have an associated
+   header file (/include/<module>/<source>.h). This header shall contain
+   all required #defines, types, and function prototypes (except they are
+   API related). 
+
+2. API functions, types, etc. shall be placed in header files located in
+   the main include directory (/include/<module>.h>). The header files
+   shall be named after the associated module.
+
+3. The main include directory shall only contain API header files. 
+
+4. All header files shall be includable without requiring further header
+   file to be included beforehand. 
diff -Naur a/net/rtnet/stack/include/ipv4/af_inet.h b/net/rtnet/stack/include/ipv4/af_inet.h
--- a/net/rtnet/stack/include/ipv4/af_inet.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/af_inet.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,36 @@
+/***
+ *
+ *  include/ipv4/af_inet.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@wev.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_AF_INET_H_
+#define __RTNET_AF_INET_H_
+
+#include <rtnet_internal.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+extern struct xnvfile_directory ipv4_proc_root;
+#endif
+
+#endif  /* __RTNET_AF_INET_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/arp.h b/net/rtnet/stack/include/ipv4/arp.h
--- a/net/rtnet/stack/include/ipv4/arp.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/arp.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,54 @@
+/***
+ *
+ *  include/ipv4/arp.h - Adress Resolution Protocol for RTnet
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *                2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_ARP_H_
+#define __RTNET_ARP_H_
+
+#include <linux/if_arp.h>
+#include <linux/init.h>
+#include <linux/types.h>
+
+#include <ipv4/route.h>
+
+
+#define RT_ARP_SKB_PRIO     RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, \
+                                             RTSKB_DEF_NRT_CHANNEL)
+
+void rt_arp_send(int type, int ptype, u32 dest_ip,
+                 struct rtnet_device *rtdev, u32 src_ip,
+                 unsigned char *dest_hw, unsigned char *src_hw,
+                 unsigned char *target_hw);
+
+static inline void rt_arp_solicit(struct rtnet_device *rtdev, u32 target)
+{
+    rt_arp_send(ARPOP_REQUEST, ETH_P_ARP, target, rtdev, rtdev->local_ip,
+                NULL, NULL, NULL);
+}
+
+void __init rt_arp_init(void);
+void rt_arp_release(void);
+
+
+#endif  /* __RTNET_ARP_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/icmp.h b/net/rtnet/stack/include/ipv4/icmp.h
--- a/net/rtnet/stack/include/ipv4/icmp.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/icmp.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,56 @@
+/***
+ *
+ *  ipv4/icmp.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_ICMP_H_
+#define __RTNET_ICMP_H_
+
+#include <linux/init.h>
+
+#include <rtskb.h>
+#include <rtnet_rtpc.h>
+#include <ipv4/protocol.h>
+
+
+#define RT_ICMP_PRIO            RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, \
+                                                 RTSKB_DEF_NRT_CHANNEL)
+
+#define ICMP_REPLY_POOL_SIZE    8
+
+
+void rt_icmp_queue_echo_request(struct rt_proc_call *call);
+void rt_icmp_dequeue_echo_request(struct rt_proc_call *call);
+void rt_icmp_cleanup_echo_requests(void);
+int rt_icmp_send_echo(u32 daddr, u16 id, u16 sequence, size_t msg_size);
+
+#ifdef CONFIG_RTNET_RTIPV4_ICMP
+void __init rt_icmp_init(void);
+void rt_icmp_release(void);
+#else /* !CONFIG_RTNET_RTIPV4_ICMP */
+#define rt_icmp_init() do {} while (0)
+#define rt_icmp_release() do {} while (0)
+#endif /* CONFIG_RTNET_RTIPV4_ICMP */
+
+
+#endif  /* __RTNET_ICMP_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/ip_fragment.h b/net/rtnet/stack/include/ipv4/ip_fragment.h
--- a/net/rtnet/stack/include/ipv4/ip_fragment.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/ip_fragment.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,39 @@
+/* ipv4/ip_fragment.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *               2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_IP_FRAGMENT_H_
+#define __RTNET_IP_FRAGMENT_H_
+
+#include <linux/init.h>
+
+#include <rtskb.h>
+#include <ipv4/protocol.h>
+
+
+extern struct rtskb *rt_ip_defrag(struct rtskb *skb,
+                                  struct rtinet_protocol *ipprot);
+
+extern void rt_ip_frag_invalidate_socket(struct rtsocket *sock);
+
+extern int __init rt_ip_fragment_init(void);
+extern void rt_ip_fragment_cleanup(void);
+
+
+#endif  /* __RTNET_IP_FRAGMENT_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/ip_input.h b/net/rtnet/stack/include/ipv4/ip_input.h
--- a/net/rtnet/stack/include/ipv4/ip_input.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/ip_input.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,47 @@
+/* ipv4/ip_input.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *               2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_IP_INPUT_H_
+#define __RTNET_IP_INPUT_H_
+
+#include <rtskb.h>
+#include <stack_mgr.h>
+
+
+extern int rt_ip_rcv(struct rtskb *skb, struct rtpacket_type *pt);
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+typedef void (*rt_ip_fallback_handler_t)(struct rtskb *skb);
+
+/*
+ * This hook can be used to register a fallback handler for incoming
+ * IP packets. Typically this is done to move over to the standard Linux
+ * IP protocol (e.g. for handling TCP).
+ * Manipulating the fallback handler is expected to happen only when the
+ * RTnetinterfaces are shut down (avoiding race conditions).
+ *
+ * Note that merging RT and non-RT traffic this way most likely breaks hard
+ * real-time constraints!
+ */
+extern rt_ip_fallback_handler_t rt_ip_fallback_handler;
+#endif
+
+
+#endif  /* __RTNET_IP_INPUT_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/ip_output.h b/net/rtnet/stack/include/ipv4/ip_output.h
--- a/net/rtnet/stack/include/ipv4/ip_output.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/ip_output.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,42 @@
+/***
+ *
+ *  include/ipv4/ip_output.h - prepare outgoing IP packets
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *                2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_IP_OUTPUT_H_
+#define __RTNET_IP_OUTPUT_H_
+
+#include <linux/init.h>
+
+#include <rtdev.h>
+#include <ipv4/route.h>
+
+
+extern int rt_ip_build_xmit(struct rtsocket *sk,
+    int getfrag (const void *, unsigned char *, unsigned int, unsigned int, int),
+    const void *frag, unsigned length, struct dest_route *rt, int flags, int msg_in_userspace);
+
+extern void __init rt_ip_init(void);
+extern void rt_ip_release(void);
+
+
+#endif  /* __RTNET_IP_OUTPUT_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/ip_sock.h b/net/rtnet/stack/include/ipv4/ip_sock.h
--- a/net/rtnet/stack/include/ipv4/ip_sock.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/ip_sock.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,32 @@
+/***
+ *
+ *  include/ipv4/ip_sock.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_IP_SOCK_H_
+#define __RTNET_IP_SOCK_H_
+
+#include <rtnet_socket.h>
+
+
+extern int rt_ip_ioctl(struct rtsocket *sock, int request, void *arg);
+
+#endif  /* __RTNET_IP_SOCK_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/protocol.h b/net/rtnet/stack/include/ipv4/protocol.h
--- a/net/rtnet/stack/include/ipv4/protocol.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/protocol.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,71 @@
+/***
+ *
+ *  include/ipv4/protocol.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_PROTOCOL_H_
+#define __RTNET_PROTOCOL_H_
+
+#include <rtnet_socket.h>
+#include <rtskb.h>
+
+
+#define MAX_RT_INET_PROTOCOLS   32
+
+/***
+ * transport layer protocol
+ */
+struct rtinet_protocol {
+    char                *name;
+    unsigned short      protocol;
+
+    struct rtsocket     *(*dest_socket)(struct rtskb *);
+    void                (*rcv_handler)(struct rtskb *);
+    void                (*err_handler)(struct rtskb *);
+    int                 (*init_socket)(struct rtsocket **, int, int, int);
+};
+
+
+extern struct rtinet_protocol *rt_inet_protocols[];
+
+#define rt_inet_hashkey(id)  (id & (MAX_RT_INET_PROTOCOLS-1))
+extern void rt_inet_add_protocol(struct rtinet_protocol *prot);
+extern void rt_inet_del_protocol(struct rtinet_protocol *prot);
+extern int rt_inet_socket(struct rtsocket **psock, int family, int type, int protocol);
+
+extern int rt_udp_bind(struct rtsocket *sock,
+                const struct sockaddr __user *addr, socklen_t addrlen);
+extern int rt_udp_ioctl(struct rtsocket *sock, unsigned int request, void __user *arg);
+extern ssize_t rt_udp_recvmsg(struct rtsocket *sock, struct user_msghdr *u_msg, int msg_flags, int msg_in_userspace);
+extern ssize_t rt_udp_sendmsg(struct rtsocket *sock, const struct user_msghdr *msg, int msg_flags, int msg_in_userspace);
+extern void rt_udp_close(struct rtsocket *sock);
+
+extern int rt_packet_socket(struct rtsocket **psock, int family, int type, int protocol);
+extern int rt_packet_bind(struct rtsocket *sock,
+                          struct sockaddr __user *addr, socklen_t addrlen);
+extern ssize_t rt_packet_recvmsg(struct rtsocket *sock, struct user_msghdr *u_msg, int msg_flags, int msg_in_userspace);
+extern ssize_t rt_packet_sendmsg(struct rtsocket *sock, const struct user_msghdr *msg, int msg_flags, int msg_in_userspace);
+extern int rt_packet_ioctl(struct rtsocket *sock, unsigned int request, void __user *arg);
+extern void rt_packet_close(struct rtsocket *sock);
+
+#endif  /* __RTNET_PROTOCOL_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/route.h b/net/rtnet/stack/include/ipv4/route.h
--- a/net/rtnet/stack/include/ipv4/route.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/route.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,62 @@
+/***
+ *
+ *  include/ipv4/route.h - real-time routing
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  Rewritten version of the original route by David Schleef and Ulrich Marx
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_ROUTE_H_
+#define __RTNET_ROUTE_H_
+
+#include <linux/init.h>
+#include <linux/types.h>
+
+#include <rtdev.h>
+
+
+struct dest_route {
+    u32                 ip;
+    unsigned char       dev_addr[MAX_ADDR_LEN];
+    struct rtnet_device *rtdev;
+};
+
+
+int rt_ip_route_add_host(u32 addr, unsigned char *dev_addr,
+                         struct rtnet_device *rtdev);
+void rt_ip_route_del_all(struct rtnet_device *rtdev);
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+int rt_ip_route_add_net(u32 addr, u32 mask, u32 gw_addr);
+int rt_ip_route_del_net(u32 addr, u32 mask);
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_RTNET_RTIPV4_ROUTER
+int rt_ip_route_forward(struct rtskb *rtskb, u32 daddr);
+#endif /* CONFIG_RTNET_RTIPV4_ROUTER */
+
+int rt_ip_route_del_host(u32 addr, struct rtnet_device *rtdev);
+int rt_ip_route_get_host(u32 addr, char* if_name, unsigned char *dev_addr,
+                         struct rtnet_device *rtdev);
+int rt_ip_route_output(struct dest_route *rt_buf, u32 daddr, u32 saddr);
+
+int __init rt_ip_routing_init(void);
+void rt_ip_routing_release(void);
+
+#endif  /* __RTNET_ROUTE_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/tcp.h b/net/rtnet/stack/include/ipv4/tcp.h
--- a/net/rtnet/stack/include/ipv4/tcp.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/tcp.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,50 @@
+/***
+ *
+ *  include/ipv4/tcp.h
+ *
+ *  Copyright (C) 2009 Vladimir Zapolskiy <vladimir.zapolskiy@siemens.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License, version 2, as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+ 
+#ifndef __RTNET_TCP_H_
+#define __RTNET_TCP_H_
+
+#include <rtskb.h>
+#include <ipv4/protocol.h>
+
+/* Maximum number of active tcp sockets, must be power of 2 */
+#define RT_TCP_SOCKETS      32
+
+/*Maximum number of active tcp connections, must be power of 2 */
+#define RT_TCP_CONNECTIONS  64
+
+/* Maximum size of TCP input window */
+#define RT_TCP_WINDOW       4096
+
+/* Maximum number of retransmissions of invalid segments */
+#define RT_TCP_RETRANSMIT   3
+
+/* Number of milliseconds to wait for ACK */
+#define RT_TCP_WAIT_TIME    10
+
+/* Priority of RST|ACK replies (error condition => non-RT prio) */
+#define RT_TCP_RST_PRIO     RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, \
+                                             RTSKB_DEF_NRT_CHANNEL)
+
+/* rtskb pool for sending socket-less RST|ACK */
+#define RT_TCP_RST_POOL_SIZE 8
+
+#endif  /* __RTNET_TCP_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4/udp.h b/net/rtnet/stack/include/ipv4/udp.h
--- a/net/rtnet/stack/include/ipv4/udp.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4/udp.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,33 @@
+/***
+ *
+ *  include/ipv4/udp.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_UDP_H_
+#define __RTNET_UDP_H_
+
+/* Maximum number of active udp sockets
+   Only increase with care (look-up delays!), must be power of 2 */
+#define RT_UDP_SOCKETS      64
+
+#endif  /* __RTNET_UDP_H_ */
diff -Naur a/net/rtnet/stack/include/ipv4_chrdev.h b/net/rtnet/stack/include/ipv4_chrdev.h
--- a/net/rtnet/stack/include/ipv4_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/ipv4_chrdev.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,104 @@
+/***
+ *
+ *  include/ipv4.h
+ *
+ *  Real-Time IP/UDP/ICMP stack
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __IPV4_H_
+#define __RTCFG_H_
+
+#include <rtnet_chrdev.h>
+
+
+struct ipv4_cmd {
+    struct rtnet_ioctl_head head;
+
+    union {
+        /*** rtroute ***/
+        struct {
+            __u32       ip_addr;
+        } solicit;
+
+        struct {
+            __u8        dev_addr[DEV_ADDR_LEN];
+            __u32       ip_addr;
+        } gethost;
+
+        struct {
+            __u8        dev_addr[DEV_ADDR_LEN];
+            __u32       ip_addr;
+        } addhost;
+
+        struct {
+            __u32       ip_addr;
+        } delhost;
+
+        struct {
+            __u32       net_addr;
+            __u32       net_mask;
+            __u32       gw_addr;
+        } addnet;
+
+        struct {
+            __u32       net_addr;
+            __u32       net_mask;
+        } delnet;
+
+        /*** rtping ***/
+        struct {
+            __u32       ip_addr;
+            __u16       id;
+            __u16       sequence;
+            __u32       msg_size;
+            __u32       timeout;
+            __s64       rtt;
+        } ping;
+
+        __u64 __padding[8];
+    } args;
+};
+
+
+#define IOC_RT_HOST_ROUTE_ADD           _IOW(RTNET_IOC_TYPE_IPV4, 0,    \
+                                             struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_SOLICIT       _IOW(RTNET_IOC_TYPE_IPV4, 1,    \
+                                             struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_DELETE        _IOW(RTNET_IOC_TYPE_IPV4, 2 |   \
+                                             RTNET_IOC_NODEV_PARAM,     \
+                                             struct ipv4_cmd)
+#define IOC_RT_NET_ROUTE_ADD            _IOW(RTNET_IOC_TYPE_IPV4, 3 |   \
+                                             RTNET_IOC_NODEV_PARAM,     \
+                                             struct ipv4_cmd)
+#define IOC_RT_NET_ROUTE_DELETE         _IOW(RTNET_IOC_TYPE_IPV4, 4 |   \
+                                             RTNET_IOC_NODEV_PARAM,     \
+                                             struct ipv4_cmd)
+#define IOC_RT_PING                     _IOWR(RTNET_IOC_TYPE_IPV4, 5 |  \
+                                              RTNET_IOC_NODEV_PARAM,    \
+                                              struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_DELETE_DEV    _IOW(RTNET_IOC_TYPE_IPV4, 6,    \
+                                             struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_GET           _IOWR(RTNET_IOC_TYPE_IPV4, 7 |  \
+					      RTNET_IOC_NODEV_PARAM,    \
+					      struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_GET_DEV       _IOWR(RTNET_IOC_TYPE_IPV4, 8,   \
+					      struct ipv4_cmd)
+
+#endif  /* __IPV4_H_ */
diff -Naur a/net/rtnet/stack/include/nomac_chrdev.h b/net/rtnet/stack/include/nomac_chrdev.h
--- a/net/rtnet/stack/include/nomac_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/nomac_chrdev.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,41 @@
+/***
+ *
+ *  include/nomac_chrdev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_CHRDEV_H_
+#define __NOMAC_CHRDEV_H_
+
+#include <rtnet_chrdev.h>
+
+
+struct nomac_config {
+    struct rtnet_ioctl_head head;
+};
+
+
+#define NOMAC_IOC_ATTACH                _IOW(RTNET_IOC_TYPE_RTMAC_NOMAC, 0, \
+                                             struct nomac_config)
+#define NOMAC_IOC_DETACH                _IOW(RTNET_IOC_TYPE_RTMAC_NOMAC, 1, \
+                                             struct nomac_config)
+
+#endif /* __NOMAC_CHRDEV_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h b/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,46 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_client_event.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_CLIENT_EVENT_H_
+#define __RTCFG_CLIENT_EVENT_H_
+
+#include <rtcfg_chrdev.h>
+
+
+int rtcfg_main_state_client_0(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data);
+int rtcfg_main_state_client_1(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data);
+int rtcfg_main_state_client_announced(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data);
+int rtcfg_main_state_client_all_known(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data);
+int rtcfg_main_state_client_all_frames(int ifindex, RTCFG_EVENT event_id,
+                                       void* event_data);
+int rtcfg_main_state_client_2(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data);
+int rtcfg_main_state_client_ready(int ifindex, RTCFG_EVENT event_id,
+                                  void* event_data);
+
+#endif /* __RTCFG_CLIENT_EVENT_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h b/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,71 @@
+/***
+ *
+ *	include/rtcfg/rtcfg_conn_event.h
+ *
+ *	Real-Time Configuration Distribution Protocol
+ *
+ *	Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License as published by
+ *	the Free Software Foundation; either version 2 of the License, or
+ *	(at your option) any later version.
+ *
+ *	This program is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	You should have received a copy of the GNU General Public License
+ *	along with this program; if not, write to the Free Software
+ *	Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_CONN_EVENT_H_
+#define __RTCFG_CONN_EVENT_H_
+
+#include <linux/netdevice.h>
+
+#include <rtcfg_chrdev.h>
+#include <rtcfg/rtcfg_file.h>
+#include <rtnet_internal.h>
+
+
+typedef enum {
+	RTCFG_CONN_SEARCHING,
+	RTCFG_CONN_STAGE_1,
+	RTCFG_CONN_STAGE_2,
+	RTCFG_CONN_READY,
+	RTCFG_CONN_DEAD
+} RTCFG_CONN_STATE;
+
+struct rtcfg_connection {
+	struct list_head		entry;
+	int						ifindex;
+	RTCFG_CONN_STATE		state;
+	u8						mac_addr[MAX_ADDR_LEN];
+	unsigned int			addr_type;
+	union {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+		u32					ip_addr;
+#endif
+	} addr;
+	void					*stage1_data;
+	size_t					stage1_size;
+	struct rtcfg_file		*stage2_file;
+	u32						cfg_offs;
+	unsigned int			flags;
+	unsigned int			burstrate;
+	nanosecs_abs_t			last_frame;
+	u64						cfg_timeout;
+#ifdef CONFIG_XENO_OPT_VFILE
+	struct xnvfile_regular	proc_entry;
+#endif
+};
+
+
+int rtcfg_do_conn_event(struct rtcfg_connection *conn, RTCFG_EVENT event_id,
+						void* event_data);
+
+#endif /* __RTCFG_CONN_EVENT_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_event.h b/net/rtnet/stack/include/rtcfg/rtcfg_event.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_event.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_event.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,124 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_event.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_EVENT_H_
+#define __RTCFG_EVENT_H_
+
+#include <linux/if_ether.h>
+#include <linux/netdevice.h>
+
+#include <rtcfg_chrdev.h>
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_rtpc.h>
+
+
+#define FLAG_TIMER_STARTED          16
+#define FLAG_TIMER_SHUTDOWN         17
+#define FLAG_TIMER_PENDING          18
+
+#define _FLAG_TIMER_STARTED         (1 << FLAG_TIMER_STARTED)
+#define _FLAG_TIMER_SHUTDOWN        (1 << FLAG_TIMER_SHUTDOWN)
+#define _FLAG_TIMER_PENDING         (1 << FLAG_TIMER_PENDING)
+
+typedef enum {
+    RTCFG_MAIN_OFF,
+    RTCFG_MAIN_SERVER_RUNNING,
+    RTCFG_MAIN_CLIENT_0,
+    RTCFG_MAIN_CLIENT_1,
+    RTCFG_MAIN_CLIENT_ANNOUNCED,
+    RTCFG_MAIN_CLIENT_ALL_KNOWN,
+    RTCFG_MAIN_CLIENT_ALL_FRAMES,
+    RTCFG_MAIN_CLIENT_2,
+    RTCFG_MAIN_CLIENT_READY
+} RTCFG_MAIN_STATE;
+
+struct rtcfg_station {
+    u8 mac_addr[ETH_ALEN]; /* Ethernet-specific! */
+    u8 flags;
+};
+
+struct rtcfg_device {
+    RTCFG_MAIN_STATE                state;
+    u32                             other_stations;
+    u32                             stations_found;
+    u32                             stations_ready;
+    struct rt_mutex                    dev_mutex;
+    struct list_head                event_calls;
+    raw_spinlock_t                     event_calls_lock;
+    rtdm_timer_t                    timer;
+    unsigned long                   flags;
+    unsigned int                    burstrate;
+#ifdef CONFIG_XENO_OPT_VFILE
+    struct xnvfile_directory        proc_entry;
+    struct xnvfile_regular          proc_state_vfile;
+    struct xnvfile_regular	    proc_stations_vfile;
+#endif
+
+    union {
+	struct {
+	    unsigned int            addr_type;
+	    union {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+		u32                 ip_addr;
+#endif
+	    } srv_addr;
+	    u8                      srv_mac_addr[MAX_ADDR_LEN];
+	    u8                      *stage2_buffer;
+	    u32                     cfg_len;
+	    u32                     cfg_offs;
+	    unsigned int            packet_counter;
+	    u32                     chain_len;
+	    struct rtskb            *stage2_chain;
+	    u32                     max_stations;
+	    struct rtcfg_station    *station_addr_list;
+	} clt;
+
+	struct {
+	    u32                     clients_configured;
+	    struct list_head        conn_list;
+	    u16                     heartbeat;
+	    u64                     heartbeat_timeout;
+	} srv;
+    } spec;
+};
+
+
+extern struct rtcfg_device device[MAX_RT_DEVICES];
+extern const char *rtcfg_event[];
+extern const char *rtcfg_main_state[];
+
+
+int rtcfg_do_main_event(int ifindex, RTCFG_EVENT event_id, void* event_data);
+void rtcfg_next_main_state(int ifindex, RTCFG_MAIN_STATE state);
+
+void rtcfg_queue_blocking_call(int ifindex, struct rt_proc_call *call);
+struct rt_proc_call *rtcfg_dequeue_blocking_call(int ifindex);
+void rtcfg_complete_cmd(int ifindex, RTCFG_EVENT event_id, int result);
+void rtcfg_reset_device(int ifindex);
+
+void rtcfg_init_state_machines(void);
+void rtcfg_cleanup_state_machines(void);
+
+#endif /* __RTCFG_EVENT_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_file.h b/net/rtnet/stack/include/rtcfg/rtcfg_file.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_file.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_file.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,45 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_file.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_FILE_H_
+#define __RTCFG_FILE_H_
+
+#include <linux/list.h>
+#include <linux/types.h>
+
+
+struct rtcfg_file {
+    struct list_head entry;
+    int              ref_count;
+    const char*      name;
+    size_t           size;
+    void*            buffer;
+};
+
+
+struct rtcfg_file *rtcfg_get_file(const char *filename);
+void rtcfg_add_file(struct rtcfg_file *file);
+int rtcfg_release_file(struct rtcfg_file *file);
+
+#endif /* __RTCFG_FILE_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_frame.h b/net/rtnet/stack/include/rtcfg/rtcfg_frame.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_frame.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_frame.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,141 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_frame.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_FRAME_H_
+#define __RTCFG_FRAME_H_
+
+#include <linux/init.h>
+#include <linux/if_packet.h>
+#include <asm/byteorder.h>
+
+#include <rtcfg/rtcfg_event.h>
+
+
+#define ETH_RTCFG                   0x9022
+
+#define RTCFG_SKB_PRIO \
+    RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, RTSKB_DEF_NRT_CHANNEL)
+
+#define RTCFG_ID_STAGE_1_CFG        0
+#define RTCFG_ID_ANNOUNCE_NEW       1
+#define RTCFG_ID_ANNOUNCE_REPLY     2
+#define RTCFG_ID_STAGE_2_CFG        3
+#define RTCFG_ID_STAGE_2_CFG_FRAG   4
+#define RTCFG_ID_ACK_CFG            5
+#define RTCFG_ID_READY              6
+#define RTCFG_ID_HEARTBEAT          7
+#define RTCFG_ID_DEAD_STATION       8
+
+#define RTCFG_ADDRSIZE_MAC          0
+#define RTCFG_ADDRSIZE_IP           4
+#define RTCFG_MAX_ADDRSIZE          RTCFG_ADDRSIZE_IP
+
+#define RTCFG_FLAG_STAGE_2_DATA 0
+#define RTCFG_FLAG_READY        1
+
+#define _RTCFG_FLAG_STAGE_2_DATA (1 << RTCFG_FLAG_STAGE_2_DATA)
+#define _RTCFG_FLAG_READY        (1 << RTCFG_FLAG_READY)
+
+struct rtcfg_frm_head {
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+    u8 id:5;
+    u8 version:3;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+    u8 version:3;
+    u8 id:5;
+#else
+    #error unsupported byte order
+#endif
+} __attribute__((packed));
+
+struct rtcfg_frm_stage_1_cfg {
+    struct rtcfg_frm_head head;
+    u8                    addr_type;
+    u8                    client_addr[0];
+    u8                    server_addr[0];
+    u8                    burstrate;
+    u16                   cfg_len;
+    u8                    cfg_data[0];
+} __attribute__((packed));
+
+struct rtcfg_frm_announce {
+    struct rtcfg_frm_head head;
+    u8                    addr_type;
+    u8                    addr[0];
+    u8                    flags;
+    u8                    burstrate;
+} __attribute__((packed));
+
+struct rtcfg_frm_stage_2_cfg {
+    struct rtcfg_frm_head head;
+    u8                    flags;
+    u32                   stations;
+    u16                   heartbeat_period;
+    u32                   cfg_len;
+    u8                    cfg_data[0];
+} __attribute__((packed));
+
+struct rtcfg_frm_stage_2_cfg_frag {
+    struct rtcfg_frm_head head;
+    u32                   frag_offs;
+    u8                    cfg_data[0];
+} __attribute__((packed));
+
+struct rtcfg_frm_ack_cfg {
+    struct rtcfg_frm_head head;
+    u32                   ack_len;
+} __attribute__((packed));
+
+struct rtcfg_frm_simple {
+    struct rtcfg_frm_head head;
+} __attribute__((packed));
+
+struct rtcfg_frm_dead_station {
+    struct rtcfg_frm_head head;
+    u8                    addr_type;
+    u8                    logical_addr[0];
+    u8                    physical_addr[32];
+} __attribute__((packed));
+
+
+int rtcfg_send_stage_1(struct rtcfg_connection *conn);
+int rtcfg_send_stage_2(struct rtcfg_connection *conn, int send_data);
+int rtcfg_send_stage_2_frag(struct rtcfg_connection *conn);
+int rtcfg_send_announce_new(int ifindex);
+int rtcfg_send_announce_reply(int ifindex, u8 *dest_mac_addr);
+int rtcfg_send_ack(int ifindex);
+int rtcfg_send_dead_station(struct rtcfg_connection *conn);
+
+int rtcfg_send_simple_frame(int ifindex, int frame_id, u8 *dest_addr);
+
+#define rtcfg_send_ready(ifindex)                                   \
+    rtcfg_send_simple_frame(ifindex, RTCFG_ID_READY, NULL)
+#define rtcfg_send_heartbeat(ifindex)                               \
+    rtcfg_send_simple_frame(ifindex, RTCFG_ID_HEARTBEAT,            \
+                            device[ifindex].spec.clt.srv_mac_addr)
+
+int __init rtcfg_init_frames(void);
+void rtcfg_cleanup_frames(void);
+
+#endif /* __RTCFG_FRAME_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg.h b/net/rtnet/stack/include/rtcfg/rtcfg.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,49 @@
+/***
+ *
+ *  include/rtcfg/rtcfg.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_H_INTERNAL_
+#define __RTCFG_H_INTERNAL_
+
+#include <rtdm/driver.h>
+
+
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+
+
+/***
+ * RTcfg debugging
+ */
+#ifdef CONFIG_RTNET_RTCFG_DEBUG
+
+extern int rtcfg_debug;
+
+/* use 0 for production, 1 for verification, >2 for debug */
+#define RTCFG_DEFAULT_DEBUG_LEVEL    10
+
+#define RTCFG_DEBUG(n, args...) (rtcfg_debug >= (n)) ? (rtdm_printk(args)) : 0
+#else
+#define RTCFG_DEBUG(n, args...)
+#endif /* CONFIG_RTCFG_DEBUG */
+
+#endif /* __RTCFG_H_INTERNAL_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h b/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,34 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_ioctl.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_IOCTL_H_
+#define __RTCFG_IOCTL_H_
+
+
+extern struct rtnet_ioctls rtcfg_ioctls;
+
+#define rtcfg_init_ioctls()     rtnet_register_ioctls(&rtcfg_ioctls)
+#define rtcfg_cleanup_ioctls()  rtnet_unregister_ioctls(&rtcfg_ioctls)
+
+#endif /* __RTCFG_IOCTL_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_proc.h b/net/rtnet/stack/include/rtcfg/rtcfg_proc.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_proc.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_proc.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,61 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_proc.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_PROC_H_
+#define __RTCFG_PROC_H_
+
+#include <rtnet_internal.h>
+
+#ifdef CONFIG_XENO_OPT_VFILE
+
+extern struct mutex nrt_proc_lock;
+
+
+void rtcfg_update_conn_proc_entries(int ifindex);
+void rtcfg_remove_conn_proc_entries(int ifindex);
+
+int rtcfg_init_proc(void);
+void rtcfg_cleanup_proc(void);
+
+
+static inline void rtcfg_lockwr_proc(int ifindex)
+{
+    mutex_lock(&nrt_proc_lock);
+    rtcfg_remove_conn_proc_entries(ifindex);
+}
+
+static inline void rtcfg_unlockwr_proc(int ifindex)
+{
+    rtcfg_update_conn_proc_entries(ifindex);
+    mutex_unlock(&nrt_proc_lock);
+}
+
+#else
+
+#define rtcfg_lockwr_proc(x)    do {} while (0)
+#define rtcfg_unlockwr_proc(x)  do {} while (0)
+
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+#endif /* __RTCFG_PROC_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg/rtcfg_timer.h b/net/rtnet/stack/include/rtcfg/rtcfg_timer.h
--- a/net/rtnet/stack/include/rtcfg/rtcfg_timer.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg/rtcfg_timer.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,34 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_timer.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_TIMER_H_
+#define __RTCFG_TIMER_H_
+
+void rtcfg_timer(rtdm_timer_t *t);
+
+void rtcfg_timer_run(void);
+
+void rtcfg_thread_signal(void);
+
+#endif /* __RTCFG_TIMER_H_ */
diff -Naur a/net/rtnet/stack/include/rtcfg_chrdev.h b/net/rtnet/stack/include/rtcfg_chrdev.h
--- a/net/rtnet/stack/include/rtcfg_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtcfg_chrdev.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,179 @@
+/***
+ *
+ *  include/rtcfg.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_H_
+#define __RTCFG_H_
+
+#include <rtnet_chrdev.h>
+
+
+#define ERTCFG_START            0x0F00
+#define ESTAGE1SIZE             ERTCFG_START
+
+#define FLAG_STAGE_2_DATA       0x0001
+#define FLAG_READY              0x0002
+#define FLAG_ASSIGN_ADDR_BY_MAC 0x0100
+
+#define RTCFG_ADDR_MAC          0x00
+#define RTCFG_ADDR_IP           0x01
+#define RTCFG_ADDR_MASK         0xFF
+
+
+typedef enum {
+    RTCFG_CMD_SERVER,
+    RTCFG_CMD_ADD,
+    RTCFG_CMD_DEL,
+    RTCFG_CMD_WAIT,
+    RTCFG_CMD_CLIENT,
+    RTCFG_CMD_ANNOUNCE,
+    RTCFG_CMD_READY,
+    RTCFG_CMD_DETACH,
+
+    /* internal usage only */
+    RTCFG_TIMER,
+    RTCFG_FRM_STAGE_1_CFG,
+    RTCFG_FRM_ANNOUNCE_NEW,
+    RTCFG_FRM_ANNOUNCE_REPLY,
+    RTCFG_FRM_STAGE_2_CFG,
+    RTCFG_FRM_STAGE_2_CFG_FRAG,
+    RTCFG_FRM_ACK_CFG,
+    RTCFG_FRM_READY,
+    RTCFG_FRM_HEARTBEAT,
+    RTCFG_FRM_DEAD_STATION
+} RTCFG_EVENT;
+
+struct rtskb;
+struct rtcfg_station;
+struct rtcfg_connection;
+struct rtcfg_file;
+
+struct rtcfg_cmd {
+    struct rtnet_ioctl_head head;
+
+    union {
+        struct {
+            __u32                   period;
+            __u32                   burstrate;
+            __u32                   heartbeat;
+            __u32                   threshold;
+            __u32                   flags;
+        } server;
+
+        struct {
+            __u32                   addr_type;
+            __u32                   ip_addr;
+            __u8                    mac_addr[DEV_ADDR_LEN];
+            __u32                   timeout;
+            __u16                   stage1_size;
+            __u16                   __padding;
+            void                    *stage1_data;
+            const char              *stage2_filename;
+
+            /* internal usage only */
+            struct rtcfg_connection *conn_buf;
+            struct rtcfg_file       *stage2_file;
+        } add;
+
+        struct {
+            __u32                   addr_type;
+            __u32                   ip_addr;
+            __u8                    mac_addr[DEV_ADDR_LEN];
+
+            /* internal usage only */
+            struct rtcfg_connection *conn_buf;
+            struct rtcfg_file       *stage2_file;
+        } del;
+
+        struct {
+            __u32                   timeout;
+        } wait;
+
+        struct {
+            __u32                   timeout;
+            __u32                   max_stations;
+            __u64                   buffer_size;
+            void                    *buffer;
+
+            /* internal usage only */
+            struct rtcfg_station    *station_buf;
+            struct rtskb            *rtskb;
+        } client;
+
+        struct {
+            __u32                   timeout;
+            __u32                   flags;
+            __u32                   burstrate;
+            __u32                   __padding;
+            __u64                   buffer_size;
+            void                    *buffer;
+
+            /* internal usage only */
+            struct rtskb            *rtskb;
+        } announce;
+
+        struct {
+            __u32                   timeout;
+        } ready;
+
+        struct {
+            /* internal usage only */
+            struct rtcfg_connection *conn_buf;
+            struct rtcfg_file       *stage2_file;
+            struct rtcfg_station    *station_addr_list;
+            struct rtskb            *stage2_chain;
+        } detach;
+
+        __u64 __padding[16];
+    } args;
+
+    /* internal usage only */
+    union {
+        struct {
+            int         ifindex;
+            RTCFG_EVENT event_id;
+        } data;
+
+        __u64 __padding[2];
+    } internal;
+};
+
+
+#define RTCFG_IOC_SERVER        _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_SERVER,  \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_ADD           _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_ADD,     \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_DEL           _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_DEL,     \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_WAIT          _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_WAIT,    \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_CLIENT        _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_CLIENT,  \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_ANNOUNCE      _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_ANNOUNCE,\
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_READY         _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_READY,   \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_DETACH        _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_DETACH,  \
+                                     struct rtcfg_cmd)
+
+#endif /* __RTCFG_H_ */
diff -Naur a/net/rtnet/stack/include/rtdev.h b/net/rtnet/stack/include/rtdev.h
--- a/net/rtnet/stack/include/rtdev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtdev.h	2021-07-14 15:39:13.434124100 +0300
@@ -0,0 +1,285 @@
+/***
+ *
+ *  rtdev.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTDEV_H_
+#define __RTDEV_H_
+
+#define MAX_RT_DEVICES                  8
+
+
+#ifdef __KERNEL__
+
+#include <asm/atomic.h>
+#include <linux/netdevice.h>
+
+#include <rtskb.h>
+#include <rtnet_internal.h>
+
+#define RTDEV_VERS_2_0                  0x0200
+
+#define PRIV_FLAG_UP                    0
+#define PRIV_FLAG_ADDING_ROUTE          1
+
+#ifndef NETIF_F_LLTX
+#define NETIF_F_LLTX                    4096
+#endif
+
+#define RTDEV_TX_OK		0
+#define RTDEV_TX_BUSY	1
+
+enum rtnet_link_state {
+	__RTNET_LINK_STATE_XOFF = 0,
+	__RTNET_LINK_STATE_START,
+	__RTNET_LINK_STATE_PRESENT,
+	__RTNET_LINK_STATE_NOCARRIER,
+};
+#define RTNET_LINK_STATE_XOFF (1 << __RTNET_LINK_STATE_XOFF)
+#define RTNET_LINK_STATE_START (1 << __RTNET_LINK_STATE_START)
+#define RTNET_LINK_STATE_PRESENT (1 << __RTNET_LINK_STATE_PRESENT)
+#define RTNET_LINK_STATE_NOCARRIER (1 << __RTNET_LINK_STATE_NOCARRIER)
+
+/***
+ *  rtnet_device
+ */
+struct rtnet_device {
+    /* Many field are borrowed from struct net_device in
+     * <linux/netdevice.h> - WY
+     */
+    unsigned int        vers;
+
+    char                name[IFNAMSIZ];
+
+    unsigned long       rmem_end;   /* shmem "recv" end     */
+    unsigned long       rmem_start; /* shmem "recv" start   */
+    unsigned long       mem_end;    /* shared mem end       */
+    unsigned long       mem_start;  /* shared mem start     */
+    unsigned long       base_addr;  /* device I/O address   */
+    unsigned int        irq;        /* device IRQ number    */
+
+    /*
+     *  Some hardware also needs these fields, but they are not
+     *  part of the usual set specified in Space.c.
+     */
+    unsigned char       if_port;    /* Selectable AUI, TP,..*/
+    unsigned char       dma;        /* DMA channel          */
+    __u16               __padding;
+
+    unsigned long       link_state;
+    int                 ifindex;
+    atomic_t            refcount;
+
+    struct module       *rt_owner;  /* like classic owner, but      *
+				     * forces correct macro usage   */
+
+    unsigned int        flags;      /* interface flags (a la BSD)   */
+    unsigned long       priv_flags; /* internal flags               */
+    unsigned short      type;       /* interface hardware type      */
+    unsigned short      hard_header_len;    /* hardware hdr length  */
+    unsigned int        mtu;        /* eth = 1536, tr = 4...        */
+    void                *priv;      /* pointer to private data      */
+    netdev_features_t   features;   /* [RT]NETIF_F_*                */
+
+    /* Interface address info. */
+    unsigned char       broadcast[MAX_ADDR_LEN];    /* hw bcast add */
+    unsigned char       dev_addr[MAX_ADDR_LEN];     /* hw address   */
+    unsigned char       addr_len;   /* hardware address length      */
+
+    int                 promiscuity;
+    int                 allmulti;
+
+    __u32               local_ip;   /* IP address in network order  */
+    __u32               broadcast_ip; /* broadcast IP in network order */
+
+    rtdm_event_t        *stack_event;
+
+    struct rt_mutex        xmit_mutex; /* protects xmit routine        */
+    raw_spinlock_t         rtdev_lock; /* management lock              */
+    struct mutex        nrt_lock;   /* non-real-time locking        */
+
+    unsigned int        add_rtskbs; /* additionally allocated global rtskbs */
+
+    struct rtskb_pool   dev_pool;
+
+    /* RTmac related fields */
+    struct rtmac_disc   *mac_disc;
+    struct rtmac_priv   *mac_priv;
+    int                 (*mac_detach)(struct rtnet_device *rtdev);
+
+    /* Device operations */
+    int                 (*open)(struct rtnet_device *rtdev);
+    int                 (*stop)(struct rtnet_device *rtdev);
+    int                 (*hard_header)(struct rtskb *, struct rtnet_device *,
+				       unsigned short type, void *daddr,
+				       void *saddr, unsigned int len);
+    int                 (*rebuild_header)(struct rtskb *);
+    int                 (*hard_start_xmit)(struct rtskb *skb,
+					   struct rtnet_device *dev);
+    int                 (*hw_reset)(struct rtnet_device *rtdev);
+
+    /* Transmission hook, managed by the stack core, RTcap, and RTmac
+     *
+     * If xmit_lock is used, start_xmit points either to rtdev_locked_xmit or
+     * the RTmac discipline handler. If xmit_lock is not required, start_xmit
+     * points to hard_start_xmit or the discipline handler.
+     */
+    int                 (*start_xmit)(struct rtskb *skb,
+				      struct rtnet_device *dev);
+
+    /* MTU hook, managed by the stack core and RTmac */
+    unsigned int        (*get_mtu)(struct rtnet_device *rtdev,
+				   unsigned int priority);
+
+    int                 (*do_ioctl)(struct rtnet_device *rtdev,
+				    struct ifreq *ifr, int cmd);
+    struct net_device_stats *(*get_stats)(struct rtnet_device *rtdev);
+
+    /* DMA pre-mapping hooks */
+    dma_addr_t          (*map_rtskb)(struct rtnet_device *rtdev,
+				     struct rtskb *skb);
+    void                (*unmap_rtskb)(struct rtnet_device *rtdev,
+				       struct rtskb *skb);
+};
+
+
+struct dummy_rtnetdev_priv {
+	struct rtnet_device *rtdev;
+};
+
+struct rtnet_core_cmd;
+
+struct rtdev_event_hook {
+    struct list_head    entry;
+    void                (*register_device)(struct rtnet_device *rtdev);
+    void                (*unregister_device)(struct rtnet_device *rtdev);
+    void                (*ifup)(struct rtnet_device *rtdev,
+				struct rtnet_core_cmd *up_cmd);
+    void                (*ifdown)(struct rtnet_device *rtdev);
+};
+
+extern struct list_head event_hook_list;
+extern struct mutex rtnet_devices_nrt_lock;
+extern struct rtnet_device *rtnet_devices[];
+
+
+struct rtnet_device *__rt_alloc_etherdev(unsigned sizeof_priv,
+					unsigned dev_pool_size,
+					struct module *module);
+struct rtnet_device *__rt_alloc_etherdev_mqs(unsigned sizeof_priv,
+                                        unsigned dev_pool_size,
+					unsigned int txqs, unsigned int rxqs,
+                                        struct module *module);
+#define rt_alloc_etherdev(priv_size, rx_size) \
+    __rt_alloc_etherdev(priv_size, rx_size, THIS_MODULE)
+#define rt_alloc_etherdev_mqs(priv_size, rx_size, txqs, rxqs) \
+    __rt_alloc_etherdev_mqs(priv_size, rx_size, txqs, rxqs, THIS_MODULE)
+
+void rtdev_free(struct rtnet_device *rtdev);
+
+int rt_register_rtnetdev(struct rtnet_device *rtdev);
+int rt_unregister_rtnetdev(struct rtnet_device *rtdev);
+
+void rtdev_add_event_hook(struct rtdev_event_hook *hook);
+void rtdev_del_event_hook(struct rtdev_event_hook *hook);
+
+void rtdev_alloc_name (struct rtnet_device *rtdev, const char *name_mask);
+
+/**
+ *  __rtdev_get_by_index - find a rtnet_device by its ifindex
+ *  @ifindex: index of device
+ *  @note: caller must hold rtnet_devices_nrt_lock
+ */
+static inline struct rtnet_device *__rtdev_get_by_index(int ifindex)
+{
+    return rtnet_devices[ifindex-1];
+}
+
+struct rtnet_device *rtdev_get_by_name(const char *if_name);
+struct rtnet_device *rtdev_get_by_index(int ifindex);
+struct rtnet_device *rtdev_get_by_hwaddr(unsigned short type,char *ha);
+struct rtnet_device *rtdev_get_loopback(void);
+
+int rtdev_reference(struct rtnet_device *rtdev);
+
+static inline void rtdev_dereference(struct rtnet_device *rtdev)
+{
+    smp_mb__before_atomic();
+    if (rtdev->rt_owner && atomic_dec_and_test(&rtdev->refcount))
+	module_put(rtdev->rt_owner);
+}
+
+int rtdev_xmit(struct rtskb *skb);
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+int rtdev_xmit_proxy(struct rtskb *skb);
+#endif
+
+unsigned int rt_hard_mtu(struct rtnet_device *rtdev, unsigned int priority);
+
+int rtdev_open(struct rtnet_device *rtdev);
+int rtdev_close(struct rtnet_device *rtdev);
+
+int rtdev_map_rtskb(struct rtskb *skb);
+void rtdev_unmap_rtskb(struct rtskb *skb);
+
+struct rtskb *rtnetdev_alloc_rtskb(struct rtnet_device *dev, unsigned int size);
+
+static inline void rtnetdev_update_features(struct rtnet_device *dev)
+{
+}
+
+#define rtnetdev_priv(dev) ((dev)->priv)
+
+#define rtdev_emerg(__dev, format, args...) \
+	pr_emerg("%s: " format, (__dev)->name, ##args)
+#define rtdev_alert(__dev, format, args...) \
+	pr_alert("%s: " format, (__dev)->name, ##args)
+#define rtdev_crit(__dev, format, args...) \
+	pr_crit("%s: " format, (__dev)->name, ##args)
+#define rtdev_err(__dev, format, args...) \
+	pr_err("%s: " format, (__dev)->name, ##args)
+#define rtdev_warn(__dev, format, args...) \
+	pr_warn("%s: " format, (__dev)->name, ##args)
+#define rtdev_notice(__dev, format, args...) \
+	pr_notice("%s: " format, (__dev)->name, ##args)
+#define rtdev_info(__dev, format, args...) \
+	pr_info("%s: " format, (__dev)->name, ##args)
+#define rtdev_dbg(__dev, format, args...) \
+	pr_debug("%s: " format, (__dev)->name, ##args)
+
+#ifdef VERBOSE_DEBUG
+#define rtdev_vdbg rtdev_dbg
+#else
+#define rtdev_vdbg(__dev, format, args...)			\
+({								\
+	if (0)							\
+		pr_debug("%s: " format, (__dev)->name, ##args);	\
+								\
+	0;							\
+})
+#endif
+
+#endif  /* __KERNEL__ */
+
+#endif  /* __RTDEV_H_ */
diff -Naur a/net/rtnet/stack/include/rtdev_mgr.h b/net/rtnet/stack/include/rtdev_mgr.h
--- a/net/rtnet/stack/include/rtdev_mgr.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtdev_mgr.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,41 @@
+/* rtdev_mgr.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTDEV_MGR_H_
+#define __RTDEV_MGR_H_
+
+#ifdef __KERNEL__
+
+#include <rtnet_internal.h>
+
+
+extern void rtnetif_err_rx(struct rtnet_device *rtdev);
+extern void rtnetif_err_tx(struct rtnet_device *rtdev);
+
+extern void rt_rtdev_connect (struct rtnet_device *rtdev, struct rtnet_mgr *mgr);
+extern void rt_rtdev_disconnect (struct rtnet_device *rtdev);
+extern int rt_rtdev_mgr_init (struct rtnet_mgr *mgr);
+extern void rt_rtdev_mgr_delete (struct rtnet_mgr *mgr);
+extern int rt_rtdev_mgr_start (struct rtnet_mgr *mgr);
+extern int rt_rtdev_mgr_stop (struct rtnet_mgr *mgr);
+
+
+#endif  /* __KERNEL__ */
+
+#endif  /* __RTDEV_MGR_H_ */
diff -Naur a/net/rtnet/stack/include/rtdm_net.h b/net/rtnet/stack/include/rtdm_net.h
--- a/net/rtnet/stack/include/rtdm_net.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtdm_net.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,47 @@
+/*
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2005-2011 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#ifndef _COBALT_RTDM_NET_H
+#define _COBALT_RTDM_NET_H
+
+#include <rtdm_uapi_net.h>
+
+struct rtnet_callback {
+    void    (*func)(void *, void *);
+    void    *arg;
+};
+
+#define RTNET_RTIOC_CALLBACK    _IOW(RTIOC_TYPE_NETWORK, 0x12, \
+				     struct rtnet_callback)
+
+/* utility functions */
+
+/* provided by rt_ipv4 */
+unsigned long rt_inet_aton(const char *ip);
+
+/* provided by rt_packet */
+int rt_eth_aton(unsigned char *addr_buf, const char *mac);
+
+#define RTNET_RTDM_VER 914
+
+#endif  /* _COBALT_RTDM_NET_H */
diff -Naur a/net/rtnet/stack/include/rtdm_uapi_net.h b/net/rtnet/stack/include/rtdm_uapi_net.h
--- a/net/rtnet/stack/include/rtdm_uapi_net.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtdm_uapi_net.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,82 @@
+/***
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2005-2011 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *  As a special exception to the GNU General Public license, the RTnet
+ *  project allows you to use this header file in unmodified form to produce
+ *  application programs executing in user-space which use RTnet services by
+ *  normal system calls. The resulting executable will not be covered by the
+ *  GNU General Public License merely as a result of this header file use.
+ *  Instead, this header file use will be considered normal use of RTnet and
+ *  not a "derived work" in the sense of the GNU General Public License.
+ *
+ *  This exception does not apply when the application code is built as a
+ *  static or dynamically loadable portion of the Linux kernel nor does the
+ *  exception override other reasons justifying application of the GNU General
+ *  Public License.
+ *
+ *  This exception applies only to the code released by the RTnet project
+ *  under the name RTnet and bearing this exception notice. If you copy code
+ *  from other sources into a copy of RTnet, the exception does not apply to
+ *  the code that you add in this way.
+ *
+ */
+
+#ifndef _RTDM_UAPI_NET_H
+#define _RTDM_UAPI_NET_H
+
+#include <rtnet_rtdm.h>
+#include <uapi_rtdm.h>
+
+/* sub-classes: RTDM_CLASS_NETWORK */
+#define RTDM_SUBCLASS_RTNET     0
+
+#define RTIOC_TYPE_NETWORK      RTDM_CLASS_NETWORK
+
+/* RTnet-specific IOCTLs */
+#define RTNET_RTIOC_XMITPARAMS  _IOW(RTIOC_TYPE_NETWORK, 0x10, unsigned int)
+#define RTNET_RTIOC_PRIORITY    RTNET_RTIOC_XMITPARAMS  /* legacy */
+#define RTNET_RTIOC_TIMEOUT     _IOW(RTIOC_TYPE_NETWORK, 0x11, int64_t)
+/* RTNET_RTIOC_CALLBACK         _IOW(RTIOC_TYPE_NETWORK, 0x12, ...
+ * IOCTL only usable inside the kernel. */
+/* RTNET_RTIOC_NONBLOCK         _IOW(RTIOC_TYPE_NETWORK, 0x13, unsigned int)
+ * This IOCTL is no longer supported (and it was buggy anyway).
+ * Use RTNET_RTIOC_TIMEOUT with any negative timeout value instead. */
+#define RTNET_RTIOC_EXTPOOL     _IOW(RTIOC_TYPE_NETWORK, 0x14, unsigned int)
+#define RTNET_RTIOC_SHRPOOL     _IOW(RTIOC_TYPE_NETWORK, 0x15, unsigned int)
+
+/* socket transmission priorities */
+#define SOCK_MAX_PRIO           0
+#define SOCK_DEF_PRIO           SOCK_MAX_PRIO + \
+				    (SOCK_MIN_PRIO-SOCK_MAX_PRIO+1)/2
+#define SOCK_MIN_PRIO           SOCK_NRT_PRIO - 1
+#define SOCK_NRT_PRIO           31
+
+/* socket transmission channels */
+#define SOCK_DEF_RT_CHANNEL     0           /* default rt xmit channel     */
+#define SOCK_DEF_NRT_CHANNEL    1           /* default non-rt xmit channel */
+#define SOCK_USER_CHANNEL       2           /* first user-defined channel  */
+
+/* argument construction for RTNET_RTIOC_XMITPARAMS */
+#define SOCK_XMIT_PARAMS(priority, channel) ((priority) | ((channel) << 16))
+
+#endif  /* !_RTDM_UAPI_NET_H */
diff -Naur a/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h b/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h
--- a/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,39 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac_dev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_DEV_H_
+#define __NOMAC_DEV_H_
+
+#include <rtmac/nomac/nomac.h>
+
+
+int nomac_dev_init(struct rtnet_device *rtdev, struct nomac_priv *nomac);
+
+
+static inline void nomac_dev_release(struct nomac_priv *nomac)
+{
+    rtdm_dev_unregister(&nomac->api_device);
+}
+
+#endif /* __NOMAC_DEV_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/nomac/nomac.h b/net/rtnet/stack/include/rtmac/nomac/nomac.h
--- a/net/rtnet/stack/include/rtmac/nomac/nomac.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/nomac/nomac.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,54 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_H_
+#define __NOMAC_H_
+
+#include <rtdm/driver.h>
+
+#include <rtmac/rtmac_disc.h>
+
+
+#define RTMAC_TYPE_NOMAC        0
+
+#define NOMAC_MAGIC             0x004D0A0C
+
+
+struct nomac_priv {
+    unsigned int                magic;
+    struct rtnet_device         *rtdev;
+    char                        device_name[32];
+    struct rtdm_driver          api_driver;
+    struct rtdm_device          api_device;
+    /* ... */
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    struct list_head            list_entry;
+#endif
+};
+
+
+extern struct rtmac_disc        nomac_disc;
+
+#endif /* __NOMAC_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h b/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h
--- a/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,32 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac_ioctl.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_IOCTL_H_
+#define __NOMAC_IOCTL_H_
+
+
+int nomac_ioctl(struct rtnet_device *rtdev, unsigned int request,
+                unsigned long arg);
+
+#endif /* __NOMAC_IOCTL_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h b/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h
--- a/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,39 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac_proto.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_PROTO_H_
+#define __NOMAC_PROTO_H_
+
+#include <rtdev.h>
+
+
+int nomac_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev);
+int nomac_nrt_packet_tx(struct rtskb *rtskb);
+
+int nomac_packet_rx(struct rtskb *rtskb);
+
+int nomac_proto_init(void);
+void nomac_proto_cleanup(void);
+
+#endif /* __NOMAC_PROTO_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/rtmac_disc.h b/net/rtnet/stack/include/rtmac/rtmac_disc.h
--- a/net/rtnet/stack/include/rtmac/rtmac_disc.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/rtmac_disc.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,105 @@
+/***
+ *
+ *  include/rtmac/rtmac_disc.h
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTMAC_DISC_H_
+#define __RTMAC_DISC_H_
+
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/proc_fs.h>
+
+#include <rtdev.h>
+#include <rtnet_chrdev.h>
+
+
+#define RTMAC_NO_VNIC       NULL
+#define RTMAC_DEFAULT_VNIC  rtmac_vnic_xmit
+
+typedef int (*vnic_xmit_handler)(struct sk_buff *skb, struct net_device *dev);
+
+struct rtmac_priv {
+    int (*orig_start_xmit)(struct rtskb *skb, struct rtnet_device *dev);
+    struct net_device       *vnic;
+    struct net_device_stats vnic_stats;
+    struct rtskb_pool       vnic_skb_pool;
+    unsigned int            vnic_max_mtu;
+
+    u8                      disc_priv[0] __attribute__ ((aligned(16)));
+};
+
+struct rtmac_proc_entry {
+    const char *name;
+    //int (*handler)(struct xnvfile_regular_iterator *it, void *data);
+    //struct xnvfile_regular vfile;
+    struct proc_dir_entry *pde;
+};
+
+struct rtmac_disc {
+    struct list_head    list;
+
+    const char          *name;
+    unsigned int        priv_size;      /* size of rtmac_priv.disc_priv */
+    u16                 disc_type;
+
+    int                 (*packet_rx)(struct rtskb *skb);
+    /* rt_packet_tx prototype must be compatible with hard_start_xmit */
+    int                 (*rt_packet_tx)(struct rtskb *skb,
+					struct rtnet_device *dev);
+    int                 (*nrt_packet_tx)(struct rtskb *skb);
+
+    unsigned int        (*get_mtu)(struct rtnet_device *rtdev,
+				   unsigned int priority);
+
+    vnic_xmit_handler   vnic_xmit;
+
+    int                 (*attach)(struct rtnet_device *rtdev, void *disc_priv);
+    int                 (*detach)(struct rtnet_device *rtdev, void *disc_priv);
+
+    struct rtnet_ioctls ioctls;
+
+    struct rtmac_proc_entry *proc_entries;
+    unsigned nr_proc_entries;
+
+    struct module *owner;
+};
+
+
+int rtmac_disc_attach(struct rtnet_device *rtdev, struct rtmac_disc *disc);
+int rtmac_disc_detach(struct rtnet_device *rtdev);
+
+int __rtmac_disc_register(struct rtmac_disc *disc, struct module *module);
+#define rtmac_disc_register(disc) __rtmac_disc_register(disc, THIS_MODULE)
+
+void rtmac_disc_deregister(struct rtmac_disc *disc);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_disciplines_show(struct xnvfile_regular_iterator *it, void *d);
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+#endif /* __RTMAC_DISC_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/rtmac_proc.h b/net/rtnet/stack/include/rtmac/rtmac_proc.h
--- a/net/rtnet/stack/include/rtmac/rtmac_proc.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/rtmac_proc.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,36 @@
+/***
+ *
+ *  include/rtmac/rtmac_proc.h
+ *
+ *  rtmac - real-time networking medium access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTMAC_PROC_H_
+#define __RTMAC_PROC_H_
+
+
+int rtmac_disc_proc_register(struct rtmac_disc *disc);
+void rtmac_disc_proc_unregister(struct rtmac_disc *disc);
+
+int rtmac_proc_register(void);
+void rtmac_proc_release(void);
+
+
+#endif /* __RTMAC_PROC_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/rtmac_proto.h b/net/rtnet/stack/include/rtmac/rtmac_proto.h
--- a/net/rtnet/stack/include/rtmac/rtmac_proto.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/rtmac_proto.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,87 @@
+/***
+ *
+ *  include/rtmac/rtmac_proto.h
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTMAC_PROTO_H_
+#define __RTMAC_PROTO_H_
+
+#include <stack_mgr.h>
+
+
+#define RTMAC_VERSION           0x02
+#define ETH_RTMAC               0x9021
+
+#define RTMAC_FLAG_TUNNEL       0x01
+
+
+struct rtmac_hdr {
+    u16 type;
+    u8  ver;
+    u8  flags;
+} __attribute__ ((packed));
+
+
+
+static inline int rtmac_add_header(struct rtnet_device *rtdev, void *daddr,
+                                   struct rtskb *skb, u16 type, u8 flags)
+{
+    struct rtmac_hdr *hdr =
+        (struct rtmac_hdr *)rtskb_push(skb, sizeof(struct rtmac_hdr));
+
+
+    hdr->type  = htons(type);
+    hdr->ver   = RTMAC_VERSION;
+    hdr->flags = flags;
+
+    skb->rtdev = rtdev;
+
+    if (rtdev->hard_header &&
+        (rtdev->hard_header(skb, rtdev, ETH_RTMAC, daddr,
+                            rtdev->dev_addr, skb->len) < 0))
+        return -1;
+
+    return 0;
+}
+
+
+
+static inline int rtmac_xmit(struct rtskb *skb)
+{
+    struct rtnet_device *rtdev = skb->rtdev;
+    int ret;
+
+
+    ret = rtdev->hard_start_xmit(skb, rtdev);
+    if (ret != 0)
+        kfree_rtskb(skb);
+
+    return ret;
+}
+
+
+extern struct rtpacket_type rtmac_packet_type;
+
+#define rtmac_proto_init()  rtdev_add_pack(&rtmac_packet_type)
+void rtmac_proto_release(void);
+
+#endif /* __RTMAC_PROTO_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/rtmac_vnic.h b/net/rtnet/stack/include/rtmac/rtmac_vnic.h
--- a/net/rtnet/stack/include/rtmac/rtmac_vnic.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/rtmac_vnic.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,61 @@
+/* include/rtmac/rtmac_vnic.h
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *               2003 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#ifndef __RTMAC_VNIC_H_
+#define __RTMAC_VNIC_H_
+
+#ifdef __KERNEL__
+
+#include <linux/init.h>
+#include <linux/netdevice.h>
+
+#include <rtmac/rtmac_disc.h>
+
+#define DEFAULT_VNIC_RTSKBS     32
+
+
+int rtmac_vnic_rx(struct rtskb *skb, u16 type);
+
+int rtmac_vnic_xmit(struct sk_buff *skb, struct net_device *dev);
+
+void rtmac_vnic_set_max_mtu(struct rtnet_device *rtdev, unsigned int max_mtu);
+
+int rtmac_vnic_add(struct rtnet_device *rtdev, vnic_xmit_handler vnic_xmit);
+int rtmac_vnic_unregister(struct rtnet_device *rtdev);
+
+static inline void rtmac_vnic_cleanup(struct rtnet_device *rtdev)
+{
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+
+    rtskb_pool_release(&mac_priv->vnic_skb_pool);
+}
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_vnics_show(struct xnvfile_regular_iterator *it, void *data);
+#endif
+
+int __init rtmac_vnic_module_init(void);
+void rtmac_vnic_module_cleanup(void);
+
+
+#endif /* __KERNEL__ */
+
+#endif /* __RTMAC_VNIC_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h b/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h
--- a/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,39 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_dev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_DEV_H_
+#define __TDMA_DEV_H_
+
+#include <rtmac/tdma/tdma.h>
+
+
+int tdma_dev_init(struct rtnet_device *rtdev, struct tdma_priv *tdma);
+
+
+static inline void tdma_dev_release(struct tdma_priv *tdma)
+{
+    rtdm_dev_unregister(&tdma->api_device);
+}
+
+#endif /* __TDMA_DEV_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/tdma/tdma.h b/net/rtnet/stack/include/rtmac/tdma/tdma.h
--- a/net/rtnet/stack/include/rtmac/tdma/tdma.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/tdma/tdma.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,168 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_H_
+#define __TDMA_H_
+
+#include <rtdm/driver.h>
+
+#include <rtnet_rtpc.h>
+#include <rtmac/rtmac_disc.h>
+
+
+#define RTMAC_TYPE_TDMA         0x0001
+
+#define TDMA_MAGIC              0x3A0D4D0A
+
+#define TDMA_FLAG_CALIBRATED    1
+#define TDMA_FLAG_RECEIVED_SYNC 2
+#define TDMA_FLAG_MASTER        3   /* also set for backup masters */
+#define TDMA_FLAG_BACKUP_MASTER 4
+#define TDMA_FLAG_ATTACHED      5
+#define TDMA_FLAG_BACKUP_ACTIVE 6
+
+#define DEFAULT_SLOT            0
+#define DEFAULT_NRT_SLOT        1
+
+/* job IDs */
+#define WAIT_ON_SYNC            -1
+#define XMIT_SYNC               -2
+#define BACKUP_SYNC             -3
+#define XMIT_REQ_CAL            -4
+#define XMIT_RPL_CAL            -5
+
+
+struct tdma_priv;
+
+
+struct tdma_job {
+    struct list_head            entry;
+    int                         id;
+    unsigned int                ref_count;
+};
+
+
+#define SLOT_JOB(job)           ((struct tdma_slot *)(job))
+
+struct tdma_slot {
+    struct tdma_job             head;
+
+    u64                         offset;
+    unsigned int                period;
+    unsigned int                phasing;
+    unsigned int                mtu;
+    unsigned int                size;
+    struct rtskb_prio_queue     *queue;
+    struct rtskb_prio_queue     local_queue;
+};
+
+
+#define REQUEST_CAL_JOB(job)    ((struct tdma_request_cal *)(job))
+
+struct tdma_request_cal {
+    struct tdma_job             head;
+
+    struct tdma_priv            *tdma;
+    u64                         offset;
+    unsigned int                period;
+    unsigned int                phasing;
+    unsigned int                cal_rounds;
+    u64                         *cal_results;
+    u64                         *result_buffer;
+};
+
+
+#define REPLY_CAL_JOB(job)      ((struct tdma_reply_cal *)(job))
+
+struct tdma_reply_cal {
+    struct tdma_job             head;
+
+    u32                         reply_cycle;
+    u64                         reply_offset;
+    struct rtskb                *reply_rtskb;
+};
+
+struct tdma_priv {
+    unsigned int                magic;
+    struct rtnet_device         *rtdev;
+    char                        device_name[32];
+    struct rtdm_driver          api_driver;
+    struct rtdm_device          api_device;
+
+#ifdef ALIGN_RTOS_TASK
+    __u8                        __align[(ALIGN_RTOS_TASK -
+                                         ((sizeof(unsigned int) +
+                                           sizeof(struct rtnet_device *) +
+                                           sizeof(struct rtdm_device)
+                                          ) & (ALIGN_RTOS_TASK-1))
+                                         ) & (ALIGN_RTOS_TASK-1)];
+#endif
+    rtdm_task_t                 worker_task;
+    rtdm_event_t                worker_wakeup;
+    rtdm_event_t                xmit_event;
+    rtdm_event_t                sync_event;
+
+    unsigned long               flags;
+    unsigned int                cal_rounds;
+    u32                         current_cycle;
+    u64                         current_cycle_start;
+    u64                         master_packet_delay_ns;
+    nanosecs_rel_t              clock_offset;
+
+    struct tdma_job             sync_job;
+    struct tdma_job             *first_job;
+    struct tdma_job             *current_job;
+    volatile unsigned int       job_list_revision;
+
+    unsigned int                max_slot_id;
+    struct tdma_slot            **slot_table;
+
+    struct rt_proc_call         *calibration_call;
+    unsigned char               master_hw_addr[MAX_ADDR_LEN];
+
+    rtdm_lock_t                 lock;
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+    struct rtskb_pool           cal_rtskb_pool;
+    u64                         cycle_period;
+    u64                         backup_sync_inc;
+#endif
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    struct list_head            list_entry;
+#endif
+};
+
+
+extern struct rtmac_disc        tdma_disc;
+
+#define print_jobs()            do { \
+    struct tdma_job *entry; \
+    rtdm_printk("%s:%d - ", __FUNCTION__, __LINE__); \
+    list_for_each_entry(entry, &tdma->first_job->entry, entry) \
+        rtdm_printk("%d ", entry->id); \
+    rtdm_printk("\n"); \
+} while (0)
+
+#endif /* __TDMA_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h b/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h
--- a/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,36 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_ioctl.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_IOCTL_H_
+#define __TDMA_IOCTL_H_
+
+#include <rtmac/tdma/tdma.h>
+
+
+int tdma_cleanup_slot(struct tdma_priv *tdma, struct tdma_slot *slot);
+
+int tdma_ioctl(struct rtnet_device *rtdev, unsigned int request,
+               unsigned long arg);
+
+#endif /* __TDMA_IOCTL_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h b/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h
--- a/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,87 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_proto.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_PROTO_H_
+#define __TDMA_PROTO_H_
+
+#include <rtdev.h>
+
+#include <rtmac/tdma/tdma.h>
+
+
+#define TDMA_FRM_VERSION    0x0201
+
+#define TDMA_FRM_SYNC       0x0000
+#define TDMA_FRM_REQ_CAL    0x0010
+#define TDMA_FRM_RPL_CAL    0x0011
+
+
+struct tdma_frm_head {
+    u16                     version;
+    u16                     id;
+} __attribute__((packed));
+
+
+#define SYNC_FRM(head)      ((struct tdma_frm_sync *)(head))
+
+struct tdma_frm_sync {
+    struct tdma_frm_head    head;
+    u32                     cycle_no;
+    u64                     xmit_stamp;
+    u64                     sched_xmit_stamp;
+} __attribute__((packed));
+
+
+#define REQ_CAL_FRM(head)   ((struct tdma_frm_req_cal *)(head))
+
+struct tdma_frm_req_cal {
+    struct tdma_frm_head    head;
+    u64                     xmit_stamp;
+    u32                     reply_cycle;
+    u64                     reply_slot_offset;
+} __attribute__((packed));
+
+
+#define RPL_CAL_FRM(head)   ((struct tdma_frm_rpl_cal *)(head))
+
+struct tdma_frm_rpl_cal {
+    struct tdma_frm_head    head;
+    u64                     request_xmit_stamp;
+    u64                     reception_stamp;
+    u64                     xmit_stamp;
+} __attribute__((packed));
+
+
+void tdma_xmit_sync_frame(struct tdma_priv *tdma);
+int tdma_xmit_request_cal_frame(struct tdma_priv *tdma, u32 reply_cycle,
+                                u64 reply_slot_offset);
+
+int tdma_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev);
+int tdma_nrt_packet_tx(struct rtskb *rtskb);
+
+int tdma_packet_rx(struct rtskb *rtskb);
+
+unsigned int tdma_get_mtu(struct rtnet_device *rtdev, unsigned int priority);
+
+#endif /* __TDMA_PROTO_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h b/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h
--- a/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,35 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_worker.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_WORKER_H_
+#define __TDMA_WORKER_H_
+
+#include <rtdm/driver.h>
+
+
+#define DEF_WORKER_PRIO         RTDM_TASK_HIGHEST_PRIORITY
+
+void tdma_worker(void *arg);
+
+#endif /* __TDMA_WORKER_H_ */
diff -Naur a/net/rtnet/stack/include/rtmac.h b/net/rtnet/stack/include/rtmac.h
--- a/net/rtnet/stack/include/rtmac.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtmac.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,96 @@
+/***
+ *
+ *  include/rtmac.h
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2004-2006 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *  As a special exception to the GNU General Public license, the RTnet
+ *  project allows you to use this header file in unmodified form to produce
+ *  application programs executing in user-space which use RTnet services by
+ *  normal system calls. The resulting executable will not be covered by the
+ *  GNU General Public License merely as a result of this header file use.
+ *  Instead, this header file use will be considered normal use of RTnet and
+ *  not a "derived work" in the sense of the GNU General Public License.
+ *
+ *  This exception does not apply when the application code is built as a
+ *  static or dynamically loadable portion of the Linux kernel nor does the
+ *  exception override other reasons justifying application of the GNU General
+ *  Public License.
+ *
+ *  This exception applies only to the code released by the RTnet project
+ *  under the name RTnet and bearing this exception notice. If you copy code
+ *  from other sources into a copy of RTnet, the exception does not apply to
+ *  the code that you add in this way.
+ *
+ */
+
+#ifndef __RTMAC_H_
+#define __RTMAC_H_
+
+#include <rtdm/rtdm.h>
+
+
+/* sub-classes: RTDM_CLASS_RTMAC */
+#define RTDM_SUBCLASS_TDMA          0
+#define RTDM_SUBCLASS_UNMANAGED     1
+
+#define RTIOC_TYPE_RTMAC            RTDM_CLASS_RTMAC
+
+
+/* ** Common Cycle Event Types ** */
+/* standard event, wake up once per cycle */
+#define RTMAC_WAIT_ON_DEFAULT       0x00
+/* wake up on media access of the station, may trigger multiple times per
+   cycle */
+#define RTMAC_WAIT_ON_XMIT          0x01
+
+/* ** TDMA-specific Cycle Event Types ** */
+/* tigger on on SYNC frame reception/transmission */
+#define TDMA_WAIT_ON_SYNC           RTMAC_WAIT_ON_DEFAULT
+#define TDMA_WAIT_ON_SOF            TDMA_WAIT_ON_SYNC /* legacy support */
+
+
+/* RTMAC_RTIOC_WAITONCYCLE_EX control and status data */
+struct rtmac_waitinfo {
+    /** Set to wait type before invoking the service */
+    unsigned int    type;
+
+    /** Set to sizeof(struct rtmac_waitinfo) before invoking the service */
+    size_t          size;
+
+    /** Counter of elementary cycles of the underlying RTmac discipline
+        (if applicable) */
+    unsigned long   cycle_no;
+
+    /** Date (in local time) of the last elementary cycle start of the RTmac
+        discipline (if applicable) */
+    nanosecs_abs_t  cycle_start;
+
+    /** Offset of the local clock to the global clock provided by the RTmac
+        discipline (if applicable): t_global = t_local + clock_offset */
+    nanosecs_rel_t  clock_offset;
+};
+
+
+/* RTmac Discipline IOCTLs */
+#define RTMAC_RTIOC_TIMEOFFSET      _IOR(RTIOC_TYPE_RTMAC, 0x00, int64_t)
+#define RTMAC_RTIOC_WAITONCYCLE     _IOW(RTIOC_TYPE_RTMAC, 0x01, unsigned int)
+#define RTMAC_RTIOC_WAITONCYCLE_EX  _IOWR(RTIOC_TYPE_RTMAC, 0x02, \
+                                          struct rtmac_waitinfo)
+
+#endif /* __RTMAC_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_chrdev.h b/net/rtnet/stack/include/rtnet_chrdev.h
--- a/net/rtnet/stack/include/rtnet_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_chrdev.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,123 @@
+/***
+ *
+ *  include/rtnet_chrdev.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999    Lineo, Inc
+ *                1999,2002 David A. Schleef <ds@schleef.org>
+ *                2002 Ulrich Marx <marx@fet.uni-hannover.de>
+ *                2003,2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_CHRDEV_H_
+#define __RTNET_CHRDEV_H_
+
+#include <rtdev.h>
+
+
+#ifdef __KERNEL__
+
+#include <linux/list.h>
+#include <linux/init.h>
+#include <linux/ioctl.h>
+#include <linux/netdevice.h>
+#include <linux/types.h>
+
+
+/* new extensible interface */
+struct rtnet_ioctls {
+    /* internal usage only */
+    struct list_head entry;
+    atomic_t         ref_count;
+
+    /* provider specification */
+    const char       *service_name;
+    unsigned int     ioctl_type;
+    int              (*handler)(struct rtnet_device *rtdev,
+                                unsigned int request, unsigned long arg);
+};
+
+extern int rtnet_register_ioctls(struct rtnet_ioctls *ioctls);
+extern void rtnet_unregister_ioctls(struct rtnet_ioctls *ioctls);
+
+extern int __init rtnet_chrdev_init(void);
+extern void rtnet_chrdev_release(void);
+
+#else   /* ifndef __KERNEL__ */
+
+#include <net/if.h>             /* IFNAMSIZ */
+#include <linux/types.h>
+
+#endif  /* __KERNEL__ */
+
+
+#define RTNET_MINOR             240 /* user interface for /dev/rtnet */
+#define DEV_ADDR_LEN            32  /* avoids inconsistent MAX_ADDR_LEN */
+
+
+struct rtnet_ioctl_head {
+    char if_name[IFNAMSIZ];
+};
+
+struct rtnet_core_cmd {
+    struct rtnet_ioctl_head head;
+
+    union {
+        /*** rtifconfig **/
+        struct {
+            __u32       ip_addr;
+            __u32       broadcast_ip;
+            __u32       set_dev_flags;
+            __u32       clear_dev_flags;
+            __u32       dev_addr_type;
+            __u32       __padding;
+            __u8        dev_addr[DEV_ADDR_LEN];
+        } up;
+
+        struct {
+            __u32       ifindex;
+            __u32       type;
+            __u32       ip_addr;
+            __u32       broadcast_ip;
+            __u32       mtu;
+            __u32       flags;
+            __u8        dev_addr[DEV_ADDR_LEN];
+        } info;
+
+        __u64 __padding[8];
+    } args;
+};
+
+
+#define RTNET_IOC_NODEV_PARAM           0x80
+
+#define RTNET_IOC_TYPE_CORE             0
+#define RTNET_IOC_TYPE_RTCFG            1
+#define RTNET_IOC_TYPE_IPV4             2
+#define RTNET_IOC_TYPE_RTMAC_NOMAC      100
+#define RTNET_IOC_TYPE_RTMAC_TDMA       110
+
+#define IOC_RT_IFUP                     _IOW(RTNET_IOC_TYPE_CORE, 0,    \
+                                             struct rtnet_core_cmd)
+#define IOC_RT_IFDOWN                   _IOW(RTNET_IOC_TYPE_CORE, 1,    \
+                                             struct rtnet_core_cmd)
+#define IOC_RT_IFINFO                   _IOWR(RTNET_IOC_TYPE_CORE, 2 |  \
+                                              RTNET_IOC_NODEV_PARAM,    \
+                                              struct rtnet_core_cmd)
+
+#endif  /* __RTNET_CHRDEV_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_internal.h b/net/rtnet/stack/include/rtnet_internal.h
--- a/net/rtnet/stack/include/rtnet_internal.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_internal.h	2021-07-14 15:39:13.426124156 +0300
@@ -0,0 +1,79 @@
+/***
+ *
+ *  rtnet_internal.h - internal declarations
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_INTERNAL_H_
+#define __RTNET_INTERNAL_H_
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <rtnet_rtdm.h>
+
+#define CONFIG_RTNET_CHECKED 1
+
+#ifdef CONFIG_RTNET_CHECKED
+#define RTNET_ASSERT(expr, func) \
+    if (!(expr)) \
+    { \
+	printk(KERN_ERR	"Assertion failed! %s:%s:%d %s\n", \
+	__FILE__, __FUNCTION__, __LINE__, (#expr)); \
+	func \
+    }
+#else
+#define RTNET_ASSERT(expr, func)
+#endif /* CONFIG_RTNET_CHECKED */
+
+/* some configurables */
+
+#define RTNET_DEF_STACK_PRIORITY \
+    RTDM_TASK_HIGHEST_PRIORITY + RTDM_TASK_LOWER_PRIORITY
+/*#define RTNET_RTDEV_PRIORITY        5*/
+
+
+struct rtnet_device;
+
+struct rtnet_mgr {
+    struct task_struct     *task;
+    rtdm_event_t    event;
+};
+
+
+extern struct rtnet_mgr STACK_manager;
+extern struct rtnet_mgr RTDEV_manager;
+
+extern const char rtnet_rtdm_provider_name[];
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+extern struct xnvfile_directory rtnet_proc_root;
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+#include <linux/mutex.h>
+
+#endif /* __RTNET_INTERNAL_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_iovec.h b/net/rtnet/stack/include/rtnet_iovec.h
--- a/net/rtnet/stack/include/rtnet_iovec.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_iovec.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,45 @@
+/* rtnet_iovec.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *               2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_IOVEC_H_
+#define __RTNET_IOVEC_H_
+
+#ifdef __KERNEL__
+
+#include <linux/uio.h>
+#include <rtnet_socket.h>
+
+struct user_msghdr;
+struct rtdm_fd;
+
+ssize_t rtnet_write_to_iov(struct rtsocket *sock,
+			   struct iovec *iov, int iovlen,
+			   const void *data, size_t len, int msg_in_userspace);
+
+ssize_t rtnet_read_from_iov(struct rtsocket *sock,
+			    struct iovec *iov, int iovlen,
+			    void *data, size_t len, int msg_in_userspace);
+#endif  /* __KERNEL__ */
+
+#endif  /* __RTNET_IOVEC_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_multiple_queues.h b/net/rtnet/stack/include/rtnet_multiple_queues.h
--- a/net/rtnet/stack/include/rtnet_multiple_queues.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_multiple_queues.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,110 @@
+/* include/rtnet_port.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2003      Wittawat Yamwong
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_MULTIPLE_QUEUES_H_
+#define __RTNET_MULTIPLE_QUEUES_H_
+
+#include <rtnet_port.h>
+
+#ifdef __KERNEL__
+
+static inline void rtnetif_tx_start_all_queues(struct rtnet_device *rtdev)
+{
+    /* TODO: allow multiple netdev queues */
+     rtnetif_start_queue(rtdev);
+}
+
+static inline void rtnetif_tx_start_queue(struct rtnet_device *rtdev, struct netdev_queue* txq)
+{
+    /* TODO: allow multiple netdev queues */
+     rtnetif_start_queue(rtdev);
+}
+
+static inline void rtnetif_tx_wake_queue(struct rtnet_device *rtdev, struct netdev_queue* txq)
+{
+    /* TODO: allow multiple netdev queues */
+    rtnetif_wake_queue(rtdev);
+}
+
+static inline void rtnetif_tx_wake_all_queues(struct rtnet_device *rtdev)
+{
+    /* TODO: allow multiple netdev queues */
+    rtnetif_wake_queue(rtdev);
+}
+
+static inline struct netdev_queue* rtnetdev_get_tx_queue(struct rtnet_device *rtdev, unsigned int queue)
+{
+    /* TODO: allow multiple netdev queues */
+    return NULL;
+}
+
+static inline int rtnetif_is_multiqueue(struct rtnet_device *rtdev)
+{
+    /* TODO: allow multiple netdev queues */
+    return 0;
+}
+
+static inline int rtnetif_tx_queue_stopped(struct rtnet_device *rtdev, struct netdev_queue* txq)
+{
+    /* TODO: allow multiple netdev queues */
+    return test_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline void rtnetif_tx_stop_queue(struct rtnet_device *rtdev, struct netdev_queue* txq)
+{
+    /* TODO: allow multiple netdev queues */
+    rtnetif_stop_queue(rtdev);
+}
+
+static inline void rtnetif_tx_disable(struct rtnet_device *rtdev)
+{
+    /* TODO: allow multiple netdev queues */
+    rtnetif_stop_queue(rtdev);
+}
+
+static inline void rtnetdev_tx_completed_queue(struct rtnet_device *rtdev, struct netdev_queue* txq, int pkts_compl, int bytes_compl)
+{
+	return;
+}
+
+static inline int rtnetif_set_real_num_tx_queues(struct rtnet_device *rtdev, unsigned int txq)
+{
+	return 0;
+}
+
+static inline int rtnetif_set_real_num_rx_queues(struct rtnet_device *rtdev, unsigned int txq)
+{
+	return 0;
+}
+
+static inline void rtnetdev_tx_sent_queue(struct netdev_queue* txq, unsigned int bytes_sent)
+{
+}
+
+static inline void rtnetdev_tx_reset_queue(struct netdev_queue *q)
+{
+}
+
+#endif /* __KERNEL__ */
+
+#endif /* __RTNET_MULTIPLE_QUEUES_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_port.h b/net/rtnet/stack/include/rtnet_port.h
--- a/net/rtnet/stack/include/rtnet_port.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_port.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,128 @@
+/* include/rtnet_port.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2003      Wittawat Yamwong
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_PORT_H_
+#define __RTNET_PORT_H_
+
+#ifdef __KERNEL__
+
+#include <linux/bitops.h>
+#include <linux/moduleparam.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/vmalloc.h>
+#include <linux/bitops.h>
+
+#include <rtdev.h>
+#include <rtdev_mgr.h>
+#include <stack_mgr.h>
+#include <ethernet/eth.h>
+
+static inline void rtnetif_start_queue(struct rtnet_device *rtdev)
+{
+    clear_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline void rtnetif_wake_queue(struct rtnet_device *rtdev)
+{
+    if (test_and_clear_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state))
+    	/*TODO __netif_schedule(dev); */ ;
+}
+
+static inline void rtnetif_stop_queue(struct rtnet_device *rtdev)
+{
+    set_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline int rtnetif_running(struct rtnet_device *rtdev)
+{
+    return test_bit(__RTNET_LINK_STATE_START, &rtdev->link_state);
+}
+
+static inline int rtnetif_queue_stopped(struct rtnet_device *rtdev)
+{
+    return test_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline int rtnetif_xmit_stopped(struct rtnet_device *rtdev, struct netdev_queue* txq)
+{
+    return test_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline void rtnetif_trans_update(struct rtnet_device *rtdev)
+{
+}
+
+static inline int rtnetif_device_present(struct rtnet_device *rtdev)
+{
+    return test_bit(__RTNET_LINK_STATE_PRESENT, &rtdev->link_state);
+}
+
+static inline void rtnetif_device_detach(struct rtnet_device *rtdev)
+{
+	if (test_and_clear_bit(__RTNET_LINK_STATE_PRESENT,
+			       &rtdev->link_state) &&
+	    rtnetif_running(rtdev)) {
+		rtnetif_stop_queue(rtdev);
+	}
+}
+
+static inline void rtnetif_device_attach(struct rtnet_device *rtdev)
+{
+	if (!test_and_set_bit(__RTNET_LINK_STATE_PRESENT,
+			      &rtdev->link_state) &&
+	    rtnetif_running(rtdev)) {
+		rtnetif_wake_queue(rtdev);
+		/* __netdev_watchdog_up(rtdev); */
+	}
+}
+
+static inline void rtnetif_carrier_on(struct rtnet_device *rtdev)
+{
+    clear_bit(__RTNET_LINK_STATE_NOCARRIER, &rtdev->link_state);
+    /*
+    if (netif_running(dev))
+	__netdev_watchdog_up(dev);
+    */
+}
+
+static inline void rtnetif_carrier_off(struct rtnet_device *rtdev)
+{
+    set_bit(__RTNET_LINK_STATE_NOCARRIER, &rtdev->link_state);
+}
+
+static inline int rtnetif_carrier_ok(struct rtnet_device *rtdev)
+{
+    return !test_bit(__RTNET_LINK_STATE_NOCARRIER, &rtdev->link_state);
+}
+
+#define NIPQUAD(addr) \
+	((unsigned char *)&addr)[0],	\
+	((unsigned char *)&addr)[1],	\
+	((unsigned char *)&addr)[2],	\
+	((unsigned char *)&addr)[3]
+#define NIPQUAD_FMT "%u.%u.%u.%u"
+
+#endif /* __KERNEL__ */
+
+#endif /* __RTNET_PORT_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_rtdm.h b/net/rtnet/stack/include/rtnet_rtdm.h
--- a/net/rtnet/stack/include/rtnet_rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_rtdm.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#ifndef __RTNET_RTDM_H_
+#define __RTNET_RTDM_H_
+
+#include <linux/slab.h>
+#include <linux/swait.h>
+#include <linux/socket.h>
+
+#define RTNET_TX_THREAD_RT_PRIO (MAX_RT_PRIO - 10)
+#define RTNET_RX_THREAD_RT_PRIO (MAX_RT_PRIO - 9)
+
+typedef struct {
+	struct swait_queue_head head_swait;
+	unsigned char condition;
+} rtdm_event_t;
+
+#define rtdm_event_init(event, flags)	\
+	do { \
+		__rtdm_event_init(event); \
+	} while (0) 
+
+/* TODO: rtdm_event_pulse should not be 100% rtdm_event_signal */
+#define rtdm_event_pulse(event) \
+	rtdm_event_signal(event)
+
+#define rtdm_event_clear(event) \
+	do { \
+		(event)->condition = 0; \
+		smp_mb(); \
+	} while (0)
+
+int rtdm_event_timedwait(rtdm_event_t *event, ktime_t timeout, void *not_used);
+
+void __rtdm_event_init(rtdm_event_t *event);
+void rtdm_event_signal_one(rtdm_event_t *event);
+int rtdm_event_wait_one(rtdm_event_t *event);
+void rtdm_event_signal(rtdm_event_t *event);
+int rtdm_event_wait(rtdm_event_t *event);
+
+#define rtdm_event_destroy(event) rtdm_event_signal(event)
+
+#define rtdm_fd_to_private(fd) NULL
+//#define rtdm_fd_is_user(fd) 1
+#define rtdm_in_rt_context(x) \
+	((current->rt_priority > (MAX_RT_PRIO / 2)) && \
+	((current->policy == SCHED_FIFO) || \
+	 (current->policy == SCHED_RR)   || \
+	 (current->policy == SCHED_DEADLINE)))
+
+typedef __u32 socklen_t;
+
+#define RTDM_TASK_LOWEST_PRIORITY (1 + MAX_RT_PRIO / 2)
+#define RTDM_TASK_HIGHEST_PRIORITY (MAX_RT_PRIO - 1)
+#define RTDM_TASK_LOWER_PRIORITY	(-1)
+
+#define RTDM_IOV_FASTMAX  16
+int rtdm_get_iovec(struct iovec **iovp,
+                   const struct user_msghdr *msg,
+                   struct iovec *iov_fast, int msg_in_userspace);
+ssize_t rtdm_get_iov_flatlen(struct iovec *iov, int iovlen);
+static inline
+void rtdm_drop_iovec(struct iovec *iov, struct iovec *iov_fast)
+{
+        if (iov != iov_fast)
+                kfree(iov);
+}
+
+#endif  /* __RTNET_RTDM_H_ */
+
diff -Naur a/net/rtnet/stack/include/rtnet_rtpc.h b/net/rtnet/stack/include/rtnet_rtpc.h
--- a/net/rtnet/stack/include/rtnet_rtpc.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_rtpc.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,77 @@
+/***
+ *
+ *  include/rtnet_rtpc.h
+ *
+ *  RTnet - real-time networking subsystem
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_RTPC_H_
+#define __RTNET_RTPC_H_
+
+#include <linux/init.h>
+
+#include <rtnet_internal.h>
+
+
+struct rt_proc_call;
+
+typedef int (*rtpc_proc)(struct rt_proc_call *call);
+typedef void (*rtpc_copy_back_proc)(struct rt_proc_call *call,
+                                    void *priv_data);
+typedef void (*rtpc_cleanup_proc)(void *priv_data);
+
+struct rt_proc_call {
+    struct list_head    list_entry;
+    int                 processed;
+    rtpc_proc           proc;
+    int                 result;
+    atomic_t            ref_count;
+    wait_queue_head_t   call_wq;
+    rtpc_cleanup_proc   cleanup_handler;
+    char                priv_data[0] __attribute__ ((aligned(8)));
+};
+
+#define CALL_PENDING    1000 /* result value for blocked calls */
+
+
+int rtnet_rtpc_dispatch_call(rtpc_proc rt_proc, unsigned int timeout,
+                             void *priv_data, size_t priv_data_size,
+                             rtpc_copy_back_proc copy_back_handler,
+                             rtpc_cleanup_proc cleanup_handler);
+
+
+void rtnet_rtpc_complete_call(struct rt_proc_call *call, int result);
+void rtnet_rtpc_complete_call_nrt(struct rt_proc_call *call, int result);
+
+#define rtpc_dispatch_call                  rtnet_rtpc_dispatch_call
+#define rtpc_complete_call                  rtnet_rtpc_complete_call
+#define rtpc_complete_call_nrt              rtnet_rtpc_complete_call_nrt
+
+#define rtpc_get_priv(call, type)           (type *)(call->priv_data)
+#define rtpc_get_result(call)               call->result
+#define rtpc_set_result(call, new_result)   call->result = new_result
+#define rtpc_set_cleanup_handler(call, handler) \
+    call->cleanup_handler = handler;
+
+
+int __init rtpc_init(void);
+void rtpc_cleanup(void);
+
+#endif /* __RTNET_RTPC_H_ */
diff -Naur a/net/rtnet/stack/include/rtnet_socket.h b/net/rtnet/stack/include/rtnet_socket.h
--- a/net/rtnet/stack/include/rtnet_socket.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtnet_socket.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,146 @@
+/***
+ *
+ *  include/rtnet_socket.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_SOCKET_H_
+#define __RTNET_SOCKET_H_
+
+#include <asm/atomic.h>
+#include <linux/list.h>
+#include <linux/semaphore.h>
+#include <linux/wait.h>
+#include <linux/file.h>
+
+#include <rtdev.h>
+#include <stack_mgr.h>
+
+
+struct rtsocket {
+    int fd;
+    int fd_refs;
+    struct file *file;
+    wait_queue_head_rtnet_t wq_head_rtnet;
+    int family;
+    int type;
+    unsigned short          protocol;
+
+    struct rtskb_pool       skb_pool;
+    unsigned int            pool_size;
+    struct mutex            pool_nrt_lock;
+
+    struct rtskb_queue      incoming;
+
+    raw_spinlock_t             param_lock;
+
+    unsigned int            priority;
+    ktime_t                 timeout;      /* receive timeout, 0 for infinite */
+
+    struct semaphore        pending_sem;
+
+    void                    (*callback_func)(void *, void *);
+    void                    *callback_arg;
+
+    unsigned long           flags;
+
+    union {
+	/* IP specific */
+	struct {
+	    u32             saddr;      /* source ip-addr (bind) */
+	    u32             daddr;      /* destination ip-addr */
+	    u16             sport;      /* source port */
+	    u16             dport;      /* destination port */
+
+	    int             reg_index;  /* index in port registry */
+	    u8              tos;
+	    u8              state;
+	} inet;
+
+	/* packet socket specific */
+	struct {
+	    struct rtpacket_type packet_type;
+	    int                  ifindex;
+	} packet;
+    } prot;
+
+    struct module *owner;
+};
+
+void rtdm_sem_destroy(struct semaphore *sem);
+
+static inline int rt_socket_fd(struct rtsocket *sock)
+{
+    return sock->fd;
+}
+
+void *rtnet_get_arg(struct rtsocket *sock, void *tmp,
+		    const void *src, size_t len, int msg_in_userspace);
+
+int rtnet_put_arg(struct rtsocket *sock, void *dst,
+		  const void *src, size_t len, int msg_in_userspace);
+
+int rtdm_fd_lock(struct rtsocket *sock);
+int rtdm_fd_unlock(struct rtsocket *sock);
+
+#define rt_socket_reference(sock)   \
+    rtdm_fd_lock(sock)
+#define rt_socket_dereference(sock) \
+    rtdm_fd_unlock(sock)
+
+int __rt_socket_init(struct rtsocket *sock, unsigned short protocol,
+		struct module *module);
+#define rt_socket_init(sock, proto) \
+    __rt_socket_init(sock, proto, THIS_MODULE)
+
+void rt_socket_cleanup(struct rtsocket *sock);
+int rt_socket_common_ioctl(struct rtsocket *sock, int request, void __user *arg);
+int rt_socket_if_ioctl(struct rtsocket *sock, int request, void __user *arg);
+#if 0
+int rt_socket_select_bind(int fd,
+			  rtdm_selector_t *selector,
+			  enum rtdm_selecttype type,
+			  unsigned fd_index);
+#endif
+int __rt_bare_socket_init_icmp(struct rtsocket *sock, unsigned short protocol,
+                        unsigned int priority, unsigned int pool_size,
+                        struct module *module);
+#define rt_bare_socket_init_icmp(sock, proto, prio, pool_sz) \
+    __rt_bare_socket_init_icmp(sock, proto, prio, pool_sz, THIS_MODULE)
+int __rt_bare_socket_init(struct rtsocket *sock, unsigned short protocol,
+			unsigned int priority, unsigned int pool_size,
+			struct module *module);
+#define rt_bare_socket_init(sock, proto, prio, pool_sz) \
+    __rt_bare_socket_init(sock, proto, prio, pool_sz, THIS_MODULE)
+
+static inline void rt_bare_socket_cleanup(struct rtsocket *sock)
+{
+    rtskb_pool_release(&sock->skb_pool);
+    module_put(sock->owner);
+}
+
+#endif  /* __RTNET_SOCKET_H_ */
diff -Naur a/net/rtnet/stack/include/rtskb_fifo.h b/net/rtnet/stack/include/rtskb_fifo.h
--- a/net/rtnet/stack/include/rtskb_fifo.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtskb_fifo.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,151 @@
+/***
+ *
+ *  include/rtskb_fifo.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2006 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTSKB_FIFO_H_
+#define __RTSKB_FIFO_H_
+
+#include <rtskb.h>
+
+
+struct rtskb_fifo {
+    unsigned long       read_pos ____cacheline_aligned_in_smp;
+    raw_spinlock_t         read_lock;
+    unsigned long       size_mask;
+    unsigned long       write_pos ____cacheline_aligned_in_smp;
+    raw_spinlock_t         write_lock;
+    struct rtskb        *buffer[0];
+};
+
+#define DECLARE_RTSKB_FIFO(name_prefix, size)   \
+struct {                                        \
+    struct rtskb_fifo   fifo;                   \
+    struct rtskb        *__buffer[(size)];      \
+} name_prefix                                   \
+
+
+static inline int __rtskb_fifo_insert(struct rtskb_fifo *fifo,
+                                      struct rtskb *rtskb)
+{
+    unsigned long pos = fifo->write_pos;
+    unsigned long new_pos = (pos + 1) & fifo->size_mask;
+
+    if (unlikely(new_pos == fifo->read_pos))
+        return -EAGAIN;
+
+    fifo->buffer[pos] = rtskb;
+
+    /* rtskb must have been written before write_pos update */
+    smp_wmb();
+
+    fifo->write_pos = new_pos;
+
+    return 0;
+}
+
+static inline int rtskb_fifo_insert(struct rtskb_fifo *fifo,
+                                    struct rtskb *rtskb)
+{
+    unsigned long context;
+    int result;
+
+    raw_spin_lock_irqsave(&fifo->write_lock, context);
+    result = __rtskb_fifo_insert(fifo, rtskb);
+    raw_spin_unlock_irqrestore(&fifo->write_lock, context);
+
+    return result;
+}
+
+static inline int rtskb_fifo_insert_inirq(struct rtskb_fifo *fifo,
+                                          struct rtskb *rtskb)
+{
+    int result;
+
+    raw_spin_lock(&fifo->write_lock);
+    result = __rtskb_fifo_insert(fifo, rtskb);
+    raw_spin_unlock(&fifo->write_lock);
+
+    return result;
+}
+
+static inline struct rtskb *__rtskb_fifo_remove(struct rtskb_fifo *fifo)
+{
+    unsigned long pos = fifo->read_pos;
+    struct rtskb *result;
+
+    /* check FIFO status first */
+    if (unlikely(pos == fifo->write_pos))
+        return NULL;
+
+    /* at least one rtskb is enqueued, so get the next one */
+    result = fifo->buffer[pos];
+
+    /* result must have been read before read_pos update */
+    smp_rmb();
+
+    fifo->read_pos = (pos + 1) & fifo->size_mask;
+
+    /* read_pos must have been written for a consitent fifo state on exit */
+    smp_wmb();
+
+    return result;
+}
+
+static inline struct rtskb *rtskb_fifo_remove(struct rtskb_fifo *fifo)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&fifo->read_lock, context);
+    result = __rtskb_fifo_remove(fifo);
+    raw_spin_unlock_irqrestore(&fifo->read_lock, context);
+
+    return result;
+}
+
+static inline struct rtskb *rtskb_fifo_remove_inirq(struct rtskb_fifo *fifo)
+{
+    struct rtskb *result;
+
+    raw_spin_lock(&fifo->read_lock);
+    result = __rtskb_fifo_remove(fifo);
+    raw_spin_unlock(&fifo->read_lock);
+
+    return result;
+}
+
+/* for now inlined... */
+static inline void rtskb_fifo_init(struct rtskb_fifo *fifo,
+                                   unsigned long size)
+{
+    fifo->read_pos  = 0;
+    fifo->write_pos = 0;
+    fifo->size_mask = size - 1;
+    raw_spin_lock_init(&fifo->read_lock);
+    raw_spin_lock_init(&fifo->write_lock);
+}
+
+#endif  /* __RTSKB_FIFO_H_ */
diff -Naur a/net/rtnet/stack/include/rtskb.h b/net/rtnet/stack/include/rtskb.h
--- a/net/rtnet/stack/include/rtskb.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtskb.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,842 @@
+/***
+ *
+ *  include/rtskb.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>,
+ *                2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTSKB_H_
+#define __RTSKB_H_
+
+#ifdef __KERNEL__
+
+#include <linux/skbuff.h>
+
+#include <rtdm_net.h>
+#include <rtnet_internal.h>
+
+
+/***
+
+rtskb Management - A Short Introduction
+---------------------------------------
+
+1. rtskbs (Real-Time Socket Buffers)
+
+A rtskb consists of a management structure (struct rtskb) and a fixed-sized
+(RTSKB_SIZE) data buffer. It is used to store network packets on their way from
+the API routines through the stack to the NICs or vice versa. rtskbs are
+allocated as one chunk of memory which contains both the managment structure
+and the buffer memory itself.
+
+
+2. rtskb Queues
+
+A rtskb queue is described by struct rtskb_queue. A queue can contain an
+unlimited number of rtskbs in an ordered way. A rtskb can either be added to
+the head (rtskb_queue_head()) or the tail of a queue (rtskb_queue_tail()). When
+a rtskb is removed from a queue (rtskb_dequeue()), it is always taken from the
+head. Queues are normally spin lock protected unless the __variants of the
+queuing functions are used.
+
+
+3. Prioritized rtskb Queues
+
+A prioritized queue contains a number of normal rtskb queues within an array.
+The array index of a sub-queue correspond to the priority of the rtskbs within
+this queue. For enqueuing a rtskb (rtskb_prio_queue_head()), its priority field
+is evaluated and the rtskb is then placed into the appropriate sub-queue. When
+dequeuing a rtskb, the first rtskb of the first non-empty sub-queue with the
+highest priority is returned. The current implementation supports 32 different
+priority levels, the lowest if defined by QUEUE_MIN_PRIO, the highest by
+QUEUE_MAX_PRIO.
+
+
+4. rtskb Pools
+
+As rtskbs must not be allocated by a normal memory manager during runtime,
+preallocated rtskbs are kept ready in several pools. Most packet producers
+(NICs, sockets, etc.) have their own pools in order to be independent of the
+load situation of other parts of the stack.
+
+When a pool is created (rtskb_pool_init()), the required rtskbs are allocated
+from a Linux slab cache. Pools can be extended (rtskb_pool_extend()) or
+shrinked (rtskb_pool_shrink()) during runtime. When shutting down the
+program/module, every pool has to be released (rtskb_pool_release()). All these
+commands demand to be executed within a non real-time context.
+
+Pools are organized as normal rtskb queues (struct rtskb_queue). When a rtskb
+is allocated (alloc_rtskb()), it is actually dequeued from the pool's queue.
+When freeing a rtskb (kfree_rtskb()), the rtskb is enqueued to its owning pool.
+rtskbs can be exchanged between pools (rtskb_acquire()). In this case, the
+passed rtskb switches over to from its owning pool to a given pool, but only if
+this pool can pass an empty rtskb from its own queue back.
+
+
+5. rtskb Chains
+
+To ease the defragmentation of larger IP packets, several rtskbs can form a
+chain. For these purposes, the first rtskb (and only the first!) provides a
+pointer to the last rtskb in the chain. When enqueuing the first rtskb of a
+chain, the whole chain is automatically placed into the destined queue. But,
+to dequeue a complete chain specialized calls are required (postfix: _chain).
+While chains also get freed en bloc (kfree_rtskb()) when passing the first
+rtskbs, it is not possible to allocate a chain from a pool (alloc_rtskb()); a
+newly allocated rtskb is always reset to a "single rtskb chain". Furthermore,
+the acquisition of complete chains is NOT supported (rtskb_acquire()).
+
+
+6. Capturing Support (Optional)
+
+When incoming or outgoing packets are captured, the assigned rtskb needs to be
+shared between the stack, the driver, and the capturing service. In contrast to
+many other network stacks, RTnet does not create a new rtskb head and
+re-references the payload. Instead, additional fields at the end of the rtskb
+structure are use for sharing a rtskb with a capturing service. If the sharing
+bit (RTSKB_CAP_SHARED) in cap_flags is set, the rtskb will not be returned to
+the owning pool upon the call of kfree_rtskb. Instead this bit will be reset,
+and a compensation rtskb stored in cap_comp_skb will be returned to the owning
+pool. cap_start and cap_len can be used to mirror the dimension of the full
+packet. This is required because the data and len fields will be modified while
+walking through the stack. cap_next allows to add a rtskb to a separate queue
+which is independent of any queue described in 2.
+
+Certain setup tasks for capturing packets can not become part of a capturing
+module, they have to be embedded into the stack. For this purpose, several
+inline functions are provided. rtcap_mark_incoming() is used to save the packet
+dimension right before it is modifed by the stack. rtcap_report_incoming()
+calls the capturing handler, if present, in order to let it process the
+received rtskb (e.g. allocate compensation rtskb, mark original rtskb as
+shared, and enqueue it).
+
+Outgoing rtskb have to be captured by adding a hook function to the chain of
+hard_start_xmit functions of a device. To measure the delay caused by RTmac
+between the request and the actual transmission, a time stamp can be taken using
+rtcap_mark_rtmac_enqueue(). This function is typically called by RTmac
+disciplines when they add a rtskb to their internal transmission queue. In such
+a case, the RTSKB_CAP_RTMAC_STAMP bit is set in cap_flags to indicate that the
+cap_rtmac_stamp field now contains valid data.
+
+ ***/
+
+
+#ifndef CHECKSUM_PARTIAL
+#define CHECKSUM_PARTIAL        CHECKSUM_HW
+#endif
+
+#define RTSKB_CAP_SHARED        1   /* rtskb shared between stack and RTcap */
+#define RTSKB_CAP_RTMAC_STAMP   2   /* cap_rtmac_stamp is valid             */
+
+#define RTSKB_UNMAPPED          0
+
+struct rtskb_queue;
+struct rtsocket;
+struct rtnet_device;
+
+/***
+ *  rtskb - realtime socket buffer
+ */
+struct rtskb {
+    struct rtskb        *next;      /* used for queuing rtskbs */
+    struct rtskb        *chain_end; /* marks the end of a rtskb chain starting
+				       with this very rtskb */
+
+    struct rtskb_pool   *pool;      /* owning pool */
+
+    unsigned int        priority;   /* bit 0..15: prio, 16..31: user-defined */
+
+    struct rtsocket     *sk;        /* assigned socket */
+    struct rtnet_device *rtdev;     /* source or destination device */
+
+    ktime_t      time_stamp; /* arrival or transmission (RTcap) time */
+
+    /* patch address of the transmission time stamp, can be NULL
+     * calculation: *xmit_stamp = cpu_to_be64(time_in_ns + *xmit_stamp)
+     */
+    ktime_t      *xmit_stamp;
+
+    /* transport layer */
+    union
+    {
+	struct tcphdr   *th;
+	struct udphdr   *uh;
+	struct icmphdr  *icmph;
+	struct iphdr    *ipihdr;
+	unsigned char   *raw;
+    } h;
+
+    /* network layer */
+    union
+    {
+	struct iphdr    *iph;
+	struct arphdr   *arph;
+	unsigned char   *raw;
+    } nh;
+
+    /* link layer */
+    union
+    {
+	struct ethhdr   *ethernet;
+	unsigned char   *raw;
+    } mac;
+
+    unsigned short      protocol;
+    unsigned char       pkt_type;
+
+    unsigned char       ip_summed;
+    unsigned int        csum;
+
+    unsigned char	*head;
+    unsigned char       *data;
+    unsigned char       *tail;
+    unsigned char       *end;
+    /* control block */
+    char		cb[40];
+    unsigned int        len, data_len;
+
+    dma_addr_t          buf_dma_addr;
+
+    unsigned char       *buf_start;
+
+#ifdef CONFIG_RTNET_CHECKED
+    unsigned char       *buf_end;
+#endif
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+    int                 cap_flags;  /* see RTSKB_CAP_xxx                    */
+    struct rtskb        *cap_comp_skb; /* compensation rtskb                */
+    struct rtskb        *cap_next;  /* used for capture queue               */
+    unsigned char       *cap_start; /* start offset for capturing           */
+    unsigned int        cap_len;    /* capture length of this rtskb         */
+    nanosecs_abs_t      cap_rtmac_stamp; /* RTmac enqueuing time            */
+#endif
+
+    struct list_head    entry; /* for global rtskb list */
+};
+
+struct rtskb_queue {
+    struct rtskb        *first;
+    struct rtskb        *last;
+    raw_spinlock_t         lock;
+};
+
+struct rtskb_pool_lock_ops {
+    int (*trylock)(void *cookie);
+    void (*unlock)(void *cookie);
+};
+
+struct rtskb_pool {
+    struct rtskb_queue queue;
+    const struct rtskb_pool_lock_ops *lock_ops;
+    void *lock_cookie;
+};
+
+#define QUEUE_MAX_PRIO          0
+#define QUEUE_MIN_PRIO          31
+
+struct rtskb_prio_queue {
+    raw_spinlock_t lock;
+    unsigned long       usage;  /* bit array encoding non-empty sub-queues */
+    struct rtskb_queue  queue[QUEUE_MIN_PRIO+1];
+};
+
+#define RTSKB_PRIO_MASK         0x0000FFFF  /* bits  0..15: xmit prio    */
+#define RTSKB_CHANNEL_MASK      0xFFFF0000  /* bits 16..31: xmit channel */
+#define RTSKB_CHANNEL_SHIFT     16
+
+#define RTSKB_DEF_RT_CHANNEL    SOCK_DEF_RT_CHANNEL
+#define RTSKB_DEF_NRT_CHANNEL   SOCK_DEF_NRT_CHANNEL
+#define RTSKB_USER_CHANNEL      SOCK_USER_CHANNEL
+
+/* Note: always keep SOCK_XMIT_PARAMS consistent with definitions above! */
+#define RTSKB_PRIO_VALUE        SOCK_XMIT_PARAMS
+
+
+/* default values for the module parameter */
+#define DEFAULT_GLOBAL_RTSKBS       0       /* default number of rtskb's in global pool */
+#define DEFAULT_DEVICE_RTSKBS       16      /* default additional rtskbs per network adapter */
+#define DEFAULT_SOCKET_RTSKBS       16      /* default number of rtskb's in socket pools */
+
+#define ALIGN_RTSKB_STRUCT_LEN      SKB_DATA_ALIGN(sizeof(struct rtskb))
+#define RTSKB_SIZE                  (2048 + 32)   /* maximum needed by rpi-4 bcmgenet driver. Adjusted from 1544. */
+/* Needed by rpi-4 */
+#define ALIGN_RTSKB_CONTROL_BLOCK_SIZE SKB_DATA_ALIGN(64)
+
+extern unsigned int rtskb_pools;        /* current number of rtskb pools      */
+extern unsigned int rtskb_pools_max;    /* maximum number of rtskb pools      */
+extern unsigned int rtskb_amount;       /* current number of allocated rtskbs */
+extern unsigned int rtskb_amount_max;   /* maximum number of allocated rtskbs */
+
+#ifdef CONFIG_RTNET_CHECKED
+extern void rtskb_over_panic(struct rtskb *skb, int len, void *here);
+extern void rtskb_under_panic(struct rtskb *skb, int len, void *here);
+#endif
+
+extern struct rtskb *rtskb_pool_dequeue(struct rtskb_pool *pool);
+
+extern void rtskb_pool_queue_tail(struct rtskb_pool *pool, struct rtskb *skb);
+
+extern struct rtskb *alloc_rtskb(unsigned int size, struct rtskb_pool *pool);
+
+extern void kfree_rtskb(struct rtskb *skb);
+#define dev_kfree_rtskb(a)  kfree_rtskb(a)
+
+
+#define rtskb_checksum_none_assert(skb) (skb->ip_summed = CHECKSUM_NONE)
+
+extern struct rtskb* rtskb_realloc_headroom(struct rtnet_device *rtdev, struct rtskb *skb, unsigned int headroom);
+
+static inline int rtskb_is_gso(struct rtskb *skb)
+{
+	return 0;
+}
+
+static inline int rtskb_get_queue_mapping(struct rtskb *skb)
+{
+	/* TODO: support multiple tx queues */ 
+	return 0;
+}
+
+static inline void rtskb_record_rx_queue(struct rtskb *skb, u16 rx_queue)
+{
+}
+
+static inline void rtskb_copy_to_linear_data(struct rtskb *skb,
+					   const void *from,
+					   const unsigned int len)
+{
+	memcpy(skb->data, from, len);
+}
+
+static inline void rtskb_tx_timestamp(struct rtskb *skb)
+{
+	ktime_t *ts = skb->xmit_stamp;
+
+	if (!ts)
+		return;
+
+	*ts = cpu_to_be64(ktime_get() + *ts);
+}
+
+/***
+ *  rtskb_queue_init - initialize the queue
+ *  @queue
+ */
+static inline void rtskb_queue_init(struct rtskb_queue *queue)
+{
+    raw_spin_lock_init(&queue->lock);
+    queue->first = NULL;
+    queue->last  = NULL;
+}
+
+/***
+ *  rtskb_prio_queue_init - initialize the prioritized queue
+ *  @prioqueue
+ */
+static inline void rtskb_prio_queue_init(struct rtskb_prio_queue *prioqueue)
+{
+    memset(prioqueue, 0, sizeof(struct rtskb_prio_queue));
+    raw_spin_lock_init(&prioqueue->lock);
+}
+
+/***
+ *  rtskb_queue_empty
+ *  @queue
+ */
+static inline int rtskb_queue_empty(struct rtskb_queue *queue)
+{
+    return (queue->first == NULL);
+}
+
+/***
+ *  rtskb__prio_queue_empty
+ *  @queue
+ */
+static inline int rtskb_prio_queue_empty(struct rtskb_prio_queue *prioqueue)
+{
+    return (prioqueue->usage == 0);
+}
+
+/***
+ *  __rtskb_queue_head - insert a buffer at the queue head (w/o locks)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_queue_head(struct rtskb_queue *queue,
+				      struct rtskb *skb)
+{
+    struct rtskb *chain_end = skb->chain_end;
+
+    chain_end->next = queue->first;
+
+    if (queue->first == NULL)
+	queue->last = chain_end;
+    queue->first = skb;
+}
+
+/***
+ *  rtskb_queue_head - insert a buffer at the queue head (lock protected)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_queue_head(struct rtskb_queue *queue, struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    __rtskb_queue_head(queue, skb);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+}
+
+/***
+ *  __rtskb_prio_queue_head - insert a buffer at the prioritized queue head
+ *                            (w/o locks)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_prio_queue_head(struct rtskb_prio_queue *prioqueue,
+					   struct rtskb *skb)
+{
+    unsigned int prio = skb->priority & RTSKB_PRIO_MASK;
+
+    RTNET_ASSERT(prio <= 31, prio = 31;);
+
+    __rtskb_queue_head(&prioqueue->queue[prio], skb);
+    __set_bit(prio, &prioqueue->usage);
+}
+
+/***
+ *  rtskb_prio_queue_head - insert a buffer at the prioritized queue head
+ *                          (lock protected)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_prio_queue_head(struct rtskb_prio_queue *prioqueue,
+					 struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    __rtskb_prio_queue_head(prioqueue, skb);
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+}
+
+/***
+ *  __rtskb_queue_tail - insert a buffer at the queue tail (w/o locks)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_queue_tail(struct rtskb_queue *queue,
+				      struct rtskb *skb)
+{
+    struct rtskb *chain_end = skb->chain_end;
+
+    chain_end->next = NULL;
+
+    if (queue->first == NULL)
+	queue->first = skb;
+    else {
+        if(queue->last == NULL) {
+	        trace_printk("queue->last is NULL\n");
+        	return;
+    	}
+	queue->last->next = skb;
+    }
+    queue->last = chain_end;
+}
+
+/***
+ *  rtskb_queue_tail - insert a buffer at the queue tail (lock protected)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_queue_tail(struct rtskb_queue *queue,
+				    struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    __rtskb_queue_tail(queue, skb);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+}
+
+/***
+ *  __rtskb_prio_queue_tail - insert a buffer at the prioritized queue tail
+ *                            (w/o locks)
+ *  @prioqueue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_prio_queue_tail(struct rtskb_prio_queue *prioqueue,
+					   struct rtskb *skb)
+{
+    unsigned int prio = skb->priority & RTSKB_PRIO_MASK;
+
+    RTNET_ASSERT(prio <= 31, prio = 31;);
+
+    __rtskb_queue_tail(&prioqueue->queue[prio], skb);
+    __set_bit(prio, &prioqueue->usage);
+}
+
+/***
+ *  rtskb_prio_queue_tail - insert a buffer at the prioritized queue tail
+ *                          (lock protected)
+ *  @prioqueue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_prio_queue_tail(struct rtskb_prio_queue *prioqueue,
+					 struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    __rtskb_prio_queue_tail(prioqueue, skb);
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+}
+
+/***
+ *  __rtskb_dequeue - remove from the head of the queue (w/o locks)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *__rtskb_dequeue(struct rtskb_queue *queue)
+{
+    struct rtskb *result;
+
+    if ((result = queue->first) != NULL) {
+	queue->first = result->next;
+	result->next = NULL;
+    }
+
+    return result;
+}
+
+/***
+ *  rtskb_dequeue - remove from the head of the queue (lock protected)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *rtskb_dequeue(struct rtskb_queue *queue)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    result = __rtskb_dequeue(queue);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+
+    return result;
+}
+
+/***
+ *  __rtskb_prio_dequeue - remove from the head of the prioritized queue
+ *                         (w/o locks)
+ *  @prioqueue: queue to remove from
+ */
+static inline struct rtskb *
+    __rtskb_prio_dequeue(struct rtskb_prio_queue *prioqueue)
+{
+    int prio;
+    struct rtskb *result = NULL;
+    struct rtskb_queue *sub_queue;
+
+    if (prioqueue->usage) {
+	prio      = ffz(~prioqueue->usage);
+	sub_queue = &prioqueue->queue[prio];
+	result    = __rtskb_dequeue(sub_queue);
+	if (rtskb_queue_empty(sub_queue))
+	    __change_bit(prio, &prioqueue->usage);
+    }
+
+    return result;
+}
+
+/***
+ *  rtskb_prio_dequeue - remove from the head of the prioritized queue
+ *                       (lock protected)
+ *  @prioqueue: queue to remove from
+ */
+static inline struct rtskb *
+    rtskb_prio_dequeue(struct rtskb_prio_queue *prioqueue)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    result = __rtskb_prio_dequeue(prioqueue);
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+
+    return result;
+}
+
+/***
+ *  __rtskb_dequeue_chain - remove a chain from the head of the queue
+ *                          (w/o locks)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *__rtskb_dequeue_chain(struct rtskb_queue *queue)
+{
+    struct rtskb *result;
+    struct rtskb *chain_end;
+
+    if (likely((result = queue->first) != NULL)) {
+	chain_end = result->chain_end;
+	queue->first = chain_end->next;
+	chain_end->next = NULL;
+    } else
+	printk(KERN_ERR "%s result is null\n", __func__);
+
+    return result;
+}
+
+/***
+ *  rtskb_dequeue_chain - remove a chain from the head of the queue
+ *                        (lock protected)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *rtskb_dequeue_chain(struct rtskb_queue *queue)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    result = __rtskb_dequeue_chain(queue);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+
+    return result;
+}
+
+/***
+ *  rtskb_prio_dequeue_chain - remove a chain from the head of the
+ *                             prioritized queue
+ *  @prioqueue: queue to remove from
+ */
+static inline
+    struct rtskb *rtskb_prio_dequeue_chain(struct rtskb_prio_queue *prioqueue)
+{
+    unsigned long context;
+    int prio;
+    struct rtskb *result = NULL;
+    struct rtskb_queue *sub_queue;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    if (prioqueue->usage) {
+	prio      = ffz(~prioqueue->usage);
+	sub_queue = &prioqueue->queue[prio];
+	result    = __rtskb_dequeue_chain(sub_queue);
+	if (rtskb_queue_empty(sub_queue))
+	    __change_bit(prio, &prioqueue->usage);
+    }
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+
+    return result;
+}
+
+/***
+ *  rtskb_queue_purge - clean the queue
+ *  @queue
+ */
+static inline void rtskb_queue_purge(struct rtskb_queue *queue)
+{
+    struct rtskb *skb;
+    while ( (skb=rtskb_dequeue(queue))!=NULL )
+	kfree_rtskb(skb);
+}
+
+static inline int rtskb_headlen(const struct rtskb *skb)
+{
+    return skb->len - skb->data_len;
+}
+
+static inline unsigned int rtskb_headroom(const struct rtskb *skb)
+{
+	return skb->data - skb->head;
+}
+
+static inline void rtskb_reserve(struct rtskb *skb, unsigned int len)
+{
+    skb->data+=len;
+    skb->tail+=len;
+}
+
+static inline unsigned char *__rtskb_put(struct rtskb *skb, unsigned int len)
+{
+    unsigned char *tmp=skb->tail;
+
+    skb->tail+=len;
+    skb->len+=len;
+    return tmp;
+}
+
+#define rtskb_put(skb, length) \
+({ \
+    struct rtskb *__rtskb = (skb); \
+    unsigned int __len = (length); \
+    unsigned char *tmp=__rtskb->tail; \
+\
+    __rtskb->tail += __len; \
+    __rtskb->len  += __len; \
+\
+    if(unlikely(__rtskb->tail > __rtskb->buf_end)) \
+	rtskb_over_panic(__rtskb, __len, (void*)_THIS_IP_); \
+\
+    tmp; \
+})
+
+static inline unsigned char *__rtskb_push(struct rtskb *skb, unsigned int len)
+{
+    skb->data-=len;
+    skb->len+=len;
+    return skb->data;
+}
+
+#define rtskb_push(skb, length) \
+({ \
+    struct rtskb *__rtskb = (skb); \
+    unsigned int __len = (length); \
+\
+    __rtskb->data -= __len; \
+    __rtskb->len  += __len; \
+\
+    RTNET_ASSERT(__rtskb->data >= __rtskb->buf_start, \
+	rtskb_under_panic(__rtskb, __len, (void*)_THIS_IP_);); \
+\
+    __rtskb->data; \
+})
+
+static inline unsigned char *__rtskb_pull(struct rtskb *skb, unsigned int len)
+{
+    RTNET_ASSERT(len <= skb->len, return NULL;);
+
+    skb->len -= len;
+
+    return skb->data += len;
+}
+
+static inline unsigned char *rtskb_pull(struct rtskb *skb, unsigned int len)
+{
+    if (len > skb->len)
+	return NULL;
+
+    skb->len -= len;
+
+    return skb->data += len;
+}
+
+static inline void rtskb_trim(struct rtskb *skb, unsigned int len)
+{
+    if (skb->len>len) {
+	skb->len = len;
+	skb->tail = skb->data+len;
+    }
+}
+
+static inline struct rtskb *rtskb_padto(struct rtskb *rtskb, unsigned int len)
+{
+    RTNET_ASSERT(len <= (unsigned int)(rtskb->buf_end + 1 - rtskb->data),
+		 return NULL;);
+
+    memset(rtskb->data + rtskb->len, 0, len - rtskb->len);
+
+    return rtskb;
+}
+
+static inline dma_addr_t rtskb_data_dma_addr(struct rtskb *rtskb,
+					     unsigned int offset)
+{
+    return rtskb->buf_dma_addr + rtskb->data - rtskb->buf_start + offset;
+}
+
+extern struct rtskb_pool global_pool;
+
+extern unsigned int rtskb_pool_init(struct rtskb_pool *pool,
+				    unsigned int initial_size,
+				    const struct rtskb_pool_lock_ops *lock_ops,
+				    void *lock_cookie);
+
+extern unsigned int __rtskb_module_pool_init(struct rtskb_pool *pool,
+					    unsigned int initial_size,
+					    struct module *module);
+
+#define rtskb_module_pool_init(pool, size) \
+    __rtskb_module_pool_init(pool, size, THIS_MODULE)
+
+extern void rtskb_pool_release(struct rtskb_pool *pool);
+
+extern unsigned int rtskb_pool_extend(struct rtskb_pool *pool,
+				      unsigned int add_rtskbs);
+extern unsigned int rtskb_pool_shrink(struct rtskb_pool *pool,
+				      unsigned int rem_rtskbs);
+extern int rtskb_acquire(struct rtskb *rtskb, struct rtskb_pool *comp_pool);
+extern struct rtskb* rtskb_clone(struct rtskb *rtskb,
+				 struct rtskb_pool *pool);
+
+extern int rtskb_pools_init(void);
+extern void rtskb_pools_release(void);
+
+extern unsigned int rtskb_copy_and_csum_bits(const struct rtskb *skb,
+					     int offset, u8 *to, int len,
+					     unsigned int csum);
+extern void rtskb_copy_and_csum_dev(const struct rtskb *skb, u8 *to);
+
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+
+extern rtdm_lock_t rtcap_lock;
+extern void (*rtcap_handler)(struct rtskb *skb);
+
+static inline void rtcap_mark_incoming(struct rtskb *skb)
+{
+    skb->cap_start = skb->data;
+    skb->cap_len   = skb->len;
+}
+
+static inline void rtcap_report_incoming(struct rtskb *skb)
+{
+    unsigned long context;
+
+
+    raw_spin_lock_irqsave(&rtcap_lock, context);
+    if (rtcap_handler != NULL)
+	rtcap_handler(skb);
+
+    raw_spin_unlock_irqrestore(&rtcap_lock, context);
+}
+
+static inline void rtcap_mark_rtmac_enqueue(struct rtskb *skb)
+{
+    /* rtskb start and length are probably not valid yet */
+    skb->cap_flags |= RTSKB_CAP_RTMAC_STAMP;
+    skb->cap_rtmac_stamp = rtdm_clock_read();
+}
+
+#else /* ifndef CONFIG_RTNET_ADDON_RTCAP */
+
+#define rtcap_mark_incoming(skb)
+#define rtcap_report_incoming(skb)
+#define rtcap_mark_rtmac_enqueue(skb)
+
+#endif /* CONFIG_RTNET_ADDON_RTCAP */
+
+
+#endif /* __KERNEL__ */
+
+#endif  /* __RTSKB_H_ */
diff -Naur a/net/rtnet/stack/include/rtwlan.h b/net/rtnet/stack/include/rtwlan.h
--- a/net/rtnet/stack/include/rtwlan.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtwlan.h	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,256 @@
+/* rtwlan.h
+ *
+ * This file is a rtnet adaption from ieee80211/ieee80211.h used by the
+ * rt2x00-2.0.0-b3 sourceforge project
+ *
+ * Merged with mainline ieee80211.h in Aug 2004.  Original ieee802_11
+ * remains copyright by the original authors
+ *
+ * Portions of the merged code are based on Host AP (software wireless
+ * LAN access point) driver for Intersil Prism2/2.5/3.
+ *
+ * Copyright (c) 2001-2002, SSH Communications Security Corp and Jouni Malinen
+ * <jkmaline@cc.hut.fi>
+ * Copyright (c) 2002-2003, Jouni Malinen <jkmaline@cc.hut.fi>
+ *
+ * Adaption to a generic IEEE 802.11 stack by James Ketrenos
+ * <jketreno@linux.intel.com>
+ * Copyright (c) 2004-2005, Intel Corporation
+ *
+ * Adaption to rtnet
+ * Copyright (c) 2006, Daniel Gregorek <dxg@gmx.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef RTWLAN_H
+#define RTWLAN_H
+
+#include <linux/if_ether.h>	/* ETH_ALEN */
+#include <linux/kernel.h>	/* ARRAY_SIZE */
+
+#include <rtskb.h>
+#include <rtwlan_io.h>
+
+#define IEEE80211_1ADDR_LEN 10
+#define IEEE80211_2ADDR_LEN 16
+#define IEEE80211_3ADDR_LEN 24
+#define IEEE80211_4ADDR_LEN 30
+#define IEEE80211_FCS_LEN    4
+#define IEEE80211_HLEN			(IEEE80211_4ADDR_LEN)
+#define IEEE80211_FRAME_LEN		(IEEE80211_DATA_LEN + IEEE80211_HLEN)
+
+#define MIN_FRAG_THRESHOLD     256U
+#define	MAX_FRAG_THRESHOLD     2346U
+
+/* Frame control field constants */
+#define IEEE80211_FCTL_VERS		0x0003
+#define IEEE80211_FCTL_FTYPE		0x000c
+#define IEEE80211_FCTL_STYPE		0x00f0
+#define IEEE80211_FCTL_TODS		0x0100
+#define IEEE80211_FCTL_FROMDS		0x0200
+#define IEEE80211_FCTL_MOREFRAGS	0x0400
+#define IEEE80211_FCTL_RETRY		0x0800
+#define IEEE80211_FCTL_PM		0x1000
+#define IEEE80211_FCTL_MOREDATA		0x2000
+#define IEEE80211_FCTL_PROTECTED	0x4000
+#define IEEE80211_FCTL_ORDER		0x8000
+
+#define IEEE80211_FTYPE_MGMT		0x0000
+#define IEEE80211_FTYPE_CTL		0x0004
+#define IEEE80211_FTYPE_DATA		0x0008
+
+/* management */
+#define IEEE80211_STYPE_ASSOC_REQ	0x0000
+#define IEEE80211_STYPE_ASSOC_RESP      0x0010
+#define IEEE80211_STYPE_REASSOC_REQ	0x0020
+#define IEEE80211_STYPE_REASSOC_RESP	0x0030
+#define IEEE80211_STYPE_PROBE_REQ	0x0040
+#define IEEE80211_STYPE_PROBE_RESP	0x0050
+#define IEEE80211_STYPE_BEACON		0x0080
+#define IEEE80211_STYPE_ATIM		0x0090
+#define IEEE80211_STYPE_DISASSOC	0x00A0
+#define IEEE80211_STYPE_AUTH		0x00B0
+#define IEEE80211_STYPE_DEAUTH		0x00C0
+#define IEEE80211_STYPE_ACTION		0x00D0
+
+/* control */
+#define IEEE80211_STYPE_PSPOLL		0x00A0
+#define IEEE80211_STYPE_RTS		0x00B0
+#define IEEE80211_STYPE_CTS		0x00C0
+#define IEEE80211_STYPE_ACK		0x00D0
+#define IEEE80211_STYPE_CFEND		0x00E0
+#define IEEE80211_STYPE_CFENDACK	0x00F0
+
+/* data */
+#define IEEE80211_STYPE_DATA		0x0000
+#define IEEE80211_STYPE_DATA_CFACK	0x0010
+#define IEEE80211_STYPE_DATA_CFPOLL	0x0020
+#define IEEE80211_STYPE_DATA_CFACKPOLL	0x0030
+#define IEEE80211_STYPE_NULLFUNC	0x0040
+#define IEEE80211_STYPE_CFACK		0x0050
+#define IEEE80211_STYPE_CFPOLL		0x0060
+#define IEEE80211_STYPE_CFACKPOLL	0x0070
+#define IEEE80211_STYPE_QOS_DATA        0x0080
+
+#define RTWLAN_SCTL_SEQ		0xFFF0
+
+#define WLAN_FC_GET_VERS(fc) ((fc) & IEEE80211_FCTL_VERS)
+#define WLAN_FC_GET_TYPE(fc) ((fc) & IEEE80211_FCTL_FTYPE)
+#define WLAN_FC_GET_STYPE(fc) ((fc) & IEEE80211_FCTL_STYPE)
+
+#define IEEE80211_DSSS_RATE_1MB                 0x02
+#define IEEE80211_DSSS_RATE_2MB                 0x04
+#define IEEE80211_DSSS_RATE_5MB                 0x0B
+#define IEEE80211_DSSS_RATE_11MB                0x16
+#define IEEE80211_OFDM_RATE_6MB                 0x0C
+#define IEEE80211_OFDM_RATE_9MB                 0x12
+#define IEEE80211_OFDM_RATE_12MB		0x18
+#define IEEE80211_OFDM_RATE_18MB		0x24
+#define IEEE80211_OFDM_RATE_24MB		0x30
+#define IEEE80211_OFDM_RATE_36MB		0x48
+#define IEEE80211_OFDM_RATE_48MB		0x60
+#define IEEE80211_OFDM_RATE_54MB		0x6C
+#define IEEE80211_BASIC_RATE_MASK		0x80
+
+#define MAC_FMT "%02x:%02x:%02x:%02x:%02x:%02x"
+#define MAC_ARG(x) ((u8*)(x))[0],((u8*)(x))[1],((u8*)(x))[2],((u8*)(x))[3],((u8*)(x))[4],((u8*)(x))[5]
+
+#ifdef CONFIG_RTWLAN_DEBUG
+#define RTWLAN_DEBUG_PRINTK(__message...)	do{ rtdm_printk(__message); }while(0)
+#define RTWLAN_DEBUG(__message,__args...)	RTWLAN_DEBUG_PRINTK(KERN_DEBUG  "rtwlan->%s: Debug - " __message,__FUNCTION__,##__args);
+#else
+#define RTWLAN_DEBUG(__message...)	do{  }while(0)
+#endif
+
+struct rtwlan_stats {
+    unsigned long	rx_packets;		/* total packets received	*/
+    unsigned long	tx_packets;		/* total packets transmitted	*/
+    unsigned long       tx_retry;               /* total packets transmitted with retry */
+};
+
+struct rtwlan_device {
+
+    struct rtwlan_stats stats;
+
+    struct rtskb_pool skb_pool;
+
+    int mode;
+
+    int (*hard_start_xmit)(struct rtskb *rtskb, struct rtnet_device * rtnet_dev);
+
+    /* This must be the last item */
+    u8 priv[0];
+};
+
+/* Minimal header; can be used for passing 802.11 frames with sufficient
+ * information to determine what type of underlying data type is actually
+ * stored in the data. */
+struct ieee80211_hdr {
+    u16 frame_ctl;
+    u16 duration_id;
+    u8 payload[0];
+} __attribute__ ((packed));
+
+struct ieee80211_hdr_3addr {
+    u16 frame_ctl;
+    u16 duration_id;
+    u8 addr1[ETH_ALEN];
+    u8 addr2[ETH_ALEN];
+    u8 addr3[ETH_ALEN];
+    u16 seq_ctl;
+    u8 payload[0];
+} __attribute__ ((packed));
+
+
+static inline int ieee80211_get_hdrlen(u16 fc)
+{
+    int hdrlen = IEEE80211_3ADDR_LEN;
+    u16 stype = WLAN_FC_GET_STYPE(fc);
+
+    switch (WLAN_FC_GET_TYPE(fc)) {
+	case IEEE80211_FTYPE_DATA:
+	    if ((fc & IEEE80211_FCTL_FROMDS) && (fc & IEEE80211_FCTL_TODS))
+		hdrlen = IEEE80211_4ADDR_LEN;
+	    if (stype & IEEE80211_STYPE_QOS_DATA)
+		hdrlen += 2;
+	    break;
+
+	case IEEE80211_FTYPE_CTL:
+	    switch (WLAN_FC_GET_STYPE(fc)) {
+		case IEEE80211_STYPE_CTS:
+		case IEEE80211_STYPE_ACK:
+		    hdrlen = IEEE80211_1ADDR_LEN;
+		    break;
+
+		default:
+		    hdrlen = IEEE80211_2ADDR_LEN;
+		    break;
+	    }
+	    break;
+    }
+
+    return hdrlen;
+}
+
+
+static inline int ieee80211_is_ofdm_rate(u8 rate)
+{
+    switch (rate & ~IEEE80211_BASIC_RATE_MASK) {
+	case IEEE80211_OFDM_RATE_6MB:
+	case IEEE80211_OFDM_RATE_9MB:
+	case IEEE80211_OFDM_RATE_12MB:
+	case IEEE80211_OFDM_RATE_18MB:
+	case IEEE80211_OFDM_RATE_24MB:
+	case IEEE80211_OFDM_RATE_36MB:
+	case IEEE80211_OFDM_RATE_48MB:
+	case IEEE80211_OFDM_RATE_54MB:
+	    return 1;
+    }
+    return 0;
+}
+
+static inline int ieee80211_is_dsss_rate(u8 rate)
+{
+    switch (rate & ~IEEE80211_BASIC_RATE_MASK) {
+	case IEEE80211_DSSS_RATE_1MB:
+	case IEEE80211_DSSS_RATE_2MB:
+	case IEEE80211_DSSS_RATE_5MB:
+	case IEEE80211_DSSS_RATE_11MB:
+	    return 1;
+    }
+    return 0;
+}
+
+
+static inline void * rtwlan_priv(struct rtwlan_device *rtwlan_dev)
+{
+    return (void *)rtwlan_dev + sizeof(struct rtwlan_device);
+}
+
+struct rtnet_device * rtwlan_alloc_dev(unsigned sizeof_priv, unsigned dev_pool_size);
+int rtwlan_rx(struct rtskb * rtskb, struct rtnet_device * rtnet_dev);
+int rtwlan_tx(struct rtskb * rtskb, struct rtnet_device * rtnet_dev);
+
+#ifdef CONFIG_RTNET_RTWLAN
+int __init rtwlan_init(void);
+void rtwlan_exit(void);
+#else /* !CONFIG_RTNET_RTWLAN */
+#define rtwlan_init()   0
+#define rtwlan_exit()
+#endif /* CONFIG_RTNET_RTWLAN */
+
+#endif
diff -Naur a/net/rtnet/stack/include/rtwlan_io.h b/net/rtnet/stack/include/rtwlan_io.h
--- a/net/rtnet/stack/include/rtwlan_io.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/rtwlan_io.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,120 @@
+/* rtwlan_io.h
+ *
+ * Copyright (C) 2006      Daniel Gregorek <dxg@gmx.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef RTWLAN_IO
+#define RTWLAN_IO
+
+#include <rtnet_chrdev.h>
+
+#define RTWLAN_TXMODE_RAW 0
+#define RTWLAN_TXMODE_ACK 1
+#define RTWLAN_TXMODE_MCAST 2
+
+#define ENORTWLANDEV 0xff08
+
+struct rtwlan_cmd {
+
+    struct rtnet_ioctl_head head;
+
+    union {
+
+        struct {
+            unsigned int    bitrate;
+            unsigned int    channel;
+            unsigned int    retry;
+            unsigned int    txpower;
+            unsigned int    mode;
+            unsigned int    autoresponder;
+            unsigned int    dropbcast;
+            unsigned int    dropmcast;
+            unsigned int    bbpsens;
+        }set;
+
+        struct {
+            unsigned int    address;
+            unsigned int    value;
+        }reg;
+
+        struct {
+            int            ifindex;
+            unsigned int   flags;
+            unsigned int   bitrate;
+            unsigned int   channel;
+            unsigned int   retry;
+            unsigned int   txpower;
+            unsigned int   bbpsens; 
+            unsigned int   mode;
+            unsigned int   autoresponder;
+            unsigned int   dropbcast;
+            unsigned int   dropmcast;
+            unsigned int   rx_packets;
+            unsigned int   tx_packets;
+            unsigned int   tx_retry;
+        }info;
+    }args;
+};
+
+#define RTNET_IOC_TYPE_RTWLAN 8
+
+#define IOC_RTWLAN_IFINFO    _IOWR(RTNET_IOC_TYPE_RTWLAN,	\
+                                   0 | RTNET_IOC_NODEV_PARAM,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BITRATE   _IOWR(RTNET_IOC_TYPE_RTWLAN, 1,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_CHANNEL   _IOWR(RTNET_IOC_TYPE_RTWLAN, 2,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_TXPOWER   _IOWR(RTNET_IOC_TYPE_RTWLAN, 3,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_RETRY     _IOWR(RTNET_IOC_TYPE_RTWLAN, 4,    \
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_TXMODE      _IOWR(RTNET_IOC_TYPE_RTWLAN, 5,    \
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_DROPBCAST _IOWR(RTNET_IOC_TYPE_RTWLAN, 6,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_DROPMCAST _IOWR(RTNET_IOC_TYPE_RTWLAN, 7,    \
+				   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_REGREAD   _IOWR(RTNET_IOC_TYPE_RTWLAN, 8,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_REGWRITE  _IOWR(RTNET_IOC_TYPE_RTWLAN, 9,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BBPWRITE  _IOWR(RTNET_IOC_TYPE_RTWLAN, 10,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BBPREAD   _IOWR(RTNET_IOC_TYPE_RTWLAN, 11,   \
+				   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BBPSENS   _IOWR(RTNET_IOC_TYPE_RTWLAN, 12,   \
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_AUTORESP  _IOWR(RTNET_IOC_TYPE_RTWLAN, 13,   \
+                                   struct rtwlan_cmd)
+
+
+#endif
diff -Naur a/net/rtnet/stack/include/stack_mgr.h b/net/rtnet/stack/include/stack_mgr.h
--- a/net/rtnet/stack/include/stack_mgr.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/stack_mgr.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,107 @@
+/***
+ *
+ *  stack_mgr.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2002      Ulrich Marx <marx@fet.uni-hannover.de>
+ *                2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __STACK_MGR_H_
+#define __STACK_MGR_H_
+
+#ifdef __KERNEL__
+
+#include <linux/list.h>
+
+#include <rtnet_internal.h>
+#include <rtdev.h>
+
+
+/***
+ * network layer protocol (layer 3)
+ */
+
+#define RTPACKET_HASH_TBL_SIZE  64
+#define RTPACKET_HASH_KEY_MASK  (RTPACKET_HASH_TBL_SIZE-1)
+
+struct rtpacket_type {
+    struct list_head    list_entry;
+
+    unsigned short      type;
+    short               refcount;
+
+    int                 (*handler)(struct rtskb *, struct rtpacket_type *);
+    int                 (*err_handler)(struct rtskb *, struct rtnet_device *,
+                                       struct rtpacket_type *);
+    bool                (*trylock)(struct rtpacket_type *);
+    void                (*unlock)(struct rtpacket_type *);
+
+    struct module	*owner;
+};
+
+
+int __rtdev_add_pack(struct rtpacket_type *pt, struct module *module);
+#define rtdev_add_pack(pt) \
+    __rtdev_add_pack(pt, THIS_MODULE)
+
+void rtdev_remove_pack(struct rtpacket_type *pt);
+
+static inline bool rtdev_lock_pack(struct rtpacket_type *pt)
+{
+    return try_module_get(pt->owner);
+}
+
+static inline void rtdev_unlock_pack(struct rtpacket_type *pt)
+{
+    module_put(pt->owner);
+}
+
+void rt_stack_connect(struct rtnet_device *rtdev, struct rtnet_mgr *mgr);
+void rt_stack_disconnect(struct rtnet_device *rtdev);
+
+#if IS_ENABLED(CONFIG_RTNET_DRV_LOOPBACK)
+void rt_stack_deliver(struct rtskb *rtskb);
+#endif /* CONFIG_RTNET_DRV_LOOPBACK */
+
+int rt_stack_mgr_init(struct rtnet_mgr *mgr);
+void rt_stack_mgr_delete(struct rtnet_mgr *mgr);
+
+void rtnetif_rx(struct rtskb *skb);
+
+static inline void rtnetif_tx(struct rtnet_device *rtdev)
+{
+}
+
+static inline void rt_mark_stack_mgr(struct rtnet_device *rtdev)
+{
+    if (rtdev->stack_event->condition == 1) {
+	/* warn */
+	trace_printk("rtdev->stack_event->condition already true\n");
+	return;
+    }
+    rtdm_event_signal_one(rtdev->stack_event);
+}
+
+#endif /* __KERNEL__ */
+
+#endif  /* __STACK_MGR_H_ */
diff -Naur a/net/rtnet/stack/include/tdma_chrdev.h b/net/rtnet/stack/include/tdma_chrdev.h
--- a/net/rtnet/stack/include/tdma_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/tdma_chrdev.h	2021-07-14 15:39:13.422124185 +0300
@@ -0,0 +1,88 @@
+/***
+ *
+ *  include/tdma_chrdev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_CHRDEV_H_
+#define __TDMA_CHRDEV_H_
+
+#ifndef __KERNEL__
+# include <inttypes.h>
+#endif
+
+#include <rtnet_chrdev.h>
+
+
+#define MIN_SLOT_SIZE       60
+
+
+struct tdma_config {
+    struct rtnet_ioctl_head head;
+
+    union {
+        struct {
+            __u64       cycle_period;
+            __u64       backup_sync_offset;
+            __u32       cal_rounds;
+            __u32       max_cal_requests;
+            __u32       max_slot_id;
+        } master;
+
+        struct {
+            __u32       cal_rounds;
+            __u32       max_slot_id;
+        } slave;
+
+        struct {
+            __s32       id;
+            __u32       period;
+            __u64       offset;
+            __u32       phasing;
+            __u32       size;
+            __s32       joint_slot;
+            __u32       cal_timeout;
+            __u64       *cal_results;
+        } set_slot;
+
+        struct {
+            __s32       id;
+        } remove_slot;
+
+        __u64 __padding[8];
+    } args;
+};
+
+
+#define TDMA_IOC_MASTER                 _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 0, \
+                                             struct tdma_config)
+#define TDMA_IOC_SLAVE                  _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 1, \
+                                             struct tdma_config)
+#define TDMA_IOC_CAL_RESULT_SIZE        _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 2, \
+                                             struct tdma_config)
+#define TDMA_IOC_SET_SLOT               _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 3, \
+                                             struct tdma_config)
+#define TDMA_IOC_REMOVE_SLOT            _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 4, \
+                                             struct tdma_config)
+#define TDMA_IOC_DETACH                 _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 5, \
+                                             struct tdma_config)
+
+#endif /* __TDMA_CHRDEV_H_ */
diff -Naur a/net/rtnet/stack/include/uapi_rtdm.h b/net/rtnet/stack/include/uapi_rtdm.h
--- a/net/rtnet/stack/include/uapi_rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/include/uapi_rtdm.h	2021-07-14 15:39:13.430124128 +0300
@@ -0,0 +1,209 @@
+/**
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * @file
+ * Real-Time Driver Model for Xenomai, user API header.
+ *
+ * @note Copyright (C) 2005, 2006 Jan Kiszka <jan.kiszka@web.de>
+ * @note Copyright (C) 2005 Joerg Langenberg <joerg.langenberg@gmx.net>
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
+ * @ingroup rtdm_user_api
+ */
+#ifndef _RTDM_UAPI_RTDM_H
+#define _RTDM_UAPI_RTDM_H
+
+#include <linux/ktime.h>
+
+/*!
+ * @addtogroup rtdm
+ * @{
+ */
+
+/*!
+ * @anchor rtdm_api_versioning @name API Versioning
+ * @{ */
+/** Common user and driver API version */
+#define RTDM_API_VER			9
+
+/** Minimum API revision compatible with the current release */
+#define RTDM_API_MIN_COMPAT_VER		9
+/** @} API Versioning */
+
+/** RTDM type for representing absolute dates. Its base type is a 64 bit
+ *  unsigned integer. The unit is 1 nanosecond. */
+typedef uint64_t nanosecs_abs_t;
+
+/** RTDM type for representing relative intervals. Its base type is a 64 bit
+ *  signed integer. The unit is 1 nanosecond. Relative intervals can also
+ *  encode the special timeouts "infinite" and "non-blocking", see
+ *  @ref RTDM_TIMEOUT_xxx. */
+//typedef int64_t nanosecs_rel_t;
+typedef ktime_t nanosecs_rel_t;
+
+/*!
+ * @anchor RTDM_TIMEOUT_xxx @name RTDM_TIMEOUT_xxx
+ * Special timeout values
+ * @{ */
+/** Block forever. */
+#define RTDM_TIMEOUT_INFINITE		0
+
+/** Any negative timeout means non-blocking. */
+#define RTDM_TIMEOUT_NONE		(-1)
+/** @} RTDM_TIMEOUT_xxx */
+/** @} rtdm */
+
+/*!
+ * @addtogroup rtdm_profiles
+ * @{
+ */
+
+/*!
+ * @anchor RTDM_CLASS_xxx   @name RTDM_CLASS_xxx
+ * Device classes
+ * @{ */
+#define RTDM_CLASS_PARPORT		1
+#define RTDM_CLASS_SERIAL		2
+#define RTDM_CLASS_CAN			3
+#define RTDM_CLASS_NETWORK		4
+#define RTDM_CLASS_RTMAC		5
+#define RTDM_CLASS_TESTING		6
+#define RTDM_CLASS_RTIPC		7
+#define RTDM_CLASS_COBALT		8
+#define RTDM_CLASS_UDD			9
+#define RTDM_CLASS_MEMORY		10
+#define RTDM_CLASS_GPIO			11
+#define RTDM_CLASS_SPI			12
+
+#define RTDM_CLASS_MISC			223
+#define RTDM_CLASS_EXPERIMENTAL		224
+#define RTDM_CLASS_MAX			255
+/** @} RTDM_CLASS_xxx */
+
+#define RTDM_SUBCLASS_GENERIC		(-1)
+
+#define RTIOC_TYPE_COMMON		0
+
+/*!
+ * @anchor device_naming    @name Device Naming
+ * Maximum length of device names (excluding the final null character)
+ * @{
+ */
+#define RTDM_MAX_DEVNAME_LEN		31
+/** @} Device Naming */
+
+/**
+ * Device information
+ */
+typedef struct rtdm_device_info {
+	/** Device flags, see @ref dev_flags "Device Flags" for details */
+	int device_flags;
+
+	/** Device class ID, see @ref RTDM_CLASS_xxx */
+	int device_class;
+
+	/** Device sub-class, either RTDM_SUBCLASS_GENERIC or a
+	 *  RTDM_SUBCLASS_xxx definition of the related @ref rtdm_profiles
+	 *  "Device Profile" */
+	int device_sub_class;
+
+	/** Supported device profile version */
+	int profile_version;
+} rtdm_device_info_t;
+
+/*!
+ * @anchor RTDM_PURGE_xxx_BUFFER    @name RTDM_PURGE_xxx_BUFFER
+ * Flags selecting buffers to be purged
+ * @{ */
+#define RTDM_PURGE_RX_BUFFER		0x0001
+#define RTDM_PURGE_TX_BUFFER		0x0002
+/** @} RTDM_PURGE_xxx_BUFFER*/
+
+/*!
+ * @anchor common_IOCTLs    @name Common IOCTLs
+ * The following IOCTLs are common to all device rtdm_profiles.
+ * @{
+ */
+
+/**
+ * Retrieve information about a device or socket.
+ * @param[out] arg Pointer to information buffer (struct rtdm_device_info)
+ */
+#define RTIOC_DEVICE_INFO \
+	_IOR(RTIOC_TYPE_COMMON, 0x00, struct rtdm_device_info)
+
+/**
+ * Purge internal device or socket buffers.
+ * @param[in] arg Purge mask, see @ref RTDM_PURGE_xxx_BUFFER
+ */
+#define RTIOC_PURGE		_IOW(RTIOC_TYPE_COMMON, 0x10, int)
+/** @} Common IOCTLs */
+/** @} rtdm */
+
+/* Internally used for mapping socket functions on IOCTLs */
+struct _rtdm_getsockopt_args {
+	int level;
+	int optname;
+	void *optval;
+	socklen_t *optlen;
+};
+
+struct _rtdm_setsockopt_args {
+	int level;
+	int optname;
+	const void *optval;
+	socklen_t optlen;
+};
+
+struct _rtdm_getsockaddr_args {
+	struct sockaddr *addr;
+	socklen_t *addrlen;
+};
+
+struct _rtdm_setsockaddr_args {
+	const struct sockaddr *addr;
+	socklen_t addrlen;
+};
+
+#define _RTIOC_GETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x20,		\
+				     struct _rtdm_getsockopt_args)
+#define _RTIOC_SETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x21,		\
+				     struct _rtdm_setsockopt_args)
+#define _RTIOC_BIND		_IOW(RTIOC_TYPE_COMMON, 0x22,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_CONNECT		_IOW(RTIOC_TYPE_COMMON, 0x23,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_LISTEN		_IOW(RTIOC_TYPE_COMMON, 0x24,		\
+				     int)
+#define _RTIOC_ACCEPT		_IOW(RTIOC_TYPE_COMMON, 0x25,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETSOCKNAME	_IOW(RTIOC_TYPE_COMMON, 0x26,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETPEERNAME	_IOW(RTIOC_TYPE_COMMON, 0x27,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_SHUTDOWN		_IOW(RTIOC_TYPE_COMMON, 0x28,		\
+				     int)
+
+/* Internally used for mmap() */
+struct _rtdm_mmap_request {
+	__u64 offset;
+	size_t length;
+	int prot;
+	int flags;
+};
+
+#endif /* !_RTDM_UAPI_RTDM_H */
diff -Naur a/net/rtnet/stack/iovec.c b/net/rtnet/stack/iovec.c
--- a/net/rtnet/stack/iovec.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/iovec.c	2021-07-14 15:39:13.334124801 +0300
@@ -0,0 +1,109 @@
+/***
+ *
+ *  stack/iovec.c
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *                2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *  
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+
+#include <rtnet_rtdm.h>
+
+ssize_t rtnet_write_to_iov(struct rtsocket *sock,
+			   struct iovec *iov, int iovlen,
+			   const void *data, size_t len, int msg_in_userspace)
+{
+	ssize_t ret = 0;
+	size_t nbytes;
+	int n;
+
+	for (n = 0; len > 0 && n < iovlen; n++, iov++) {
+		if (iov->iov_len == 0)
+			continue;
+
+		nbytes = iov->iov_len;
+		if (nbytes > len)
+			nbytes = len;
+
+		ret = rtnet_put_arg(sock, iov->iov_base, data, nbytes, msg_in_userspace);
+		if (ret)
+			break;
+	
+		len -= nbytes;
+		data += nbytes;
+		iov->iov_len -= nbytes;
+		iov->iov_base += nbytes;
+		ret += nbytes;
+		if (ret < 0) {
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rtnet_write_to_iov);
+
+ssize_t rtnet_read_from_iov(struct rtsocket *sock,
+			    struct iovec *iov, int iovlen,
+			    void *data, size_t len, int msg_in_userspace)
+{
+	ssize_t ret = 0;
+	size_t nbytes;
+	int n;
+
+	for (n = 0; len > 0 && n < iovlen; n++, iov++) {
+		if (iov->iov_len == 0)
+			continue;
+
+		nbytes = iov->iov_len;
+		if (nbytes > len)
+			nbytes = len;
+
+		if (!msg_in_userspace)
+			memcpy(data, iov->iov_base, nbytes);
+		else {
+			ret = copy_from_user(data, iov->iov_base, nbytes);
+			if (ret)
+				break;
+		}
+	
+		len -= nbytes;
+		data += nbytes;
+		iov->iov_len -= nbytes;
+		iov->iov_base += nbytes;
+		ret += nbytes;
+		if (ret < 0) {
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rtnet_read_from_iov);
diff -Naur a/net/rtnet/stack/ipv4/af_inet.c b/net/rtnet/stack/ipv4/af_inet.c
--- a/net/rtnet/stack/ipv4/af_inet.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/af_inet.c	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,367 @@
+/***
+ *
+ *  ipv4/af_inet.c
+ *
+ *  rtnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/uaccess.h>
+
+#include <ipv4_chrdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_rtpc.h>
+#include <ipv4/arp.h>
+#include <ipv4/icmp.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+
+
+MODULE_LICENSE("GPL");
+
+struct route_solicit_params {
+    struct rtnet_device *rtdev;
+    __u32               ip_addr;
+};
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct xnvfile_directory ipv4_proc_root;
+EXPORT_SYMBOL_GPL(ipv4_proc_root);
+#endif
+
+
+static int route_solicit_handler(struct rt_proc_call *call)
+{
+    struct route_solicit_params *param;
+    struct rtnet_device         *rtdev;
+
+
+    param = rtpc_get_priv(call, struct route_solicit_params);
+    rtdev = param->rtdev;
+
+    if ((rtdev->flags & IFF_UP) == 0)
+	return -ENODEV;
+
+    rt_arp_solicit(rtdev, param->ip_addr);
+
+    return 0;
+}
+
+
+
+static void cleanup_route_solicit(void *priv_data)
+{
+    struct route_solicit_params *param;
+
+
+    param = (struct route_solicit_params *)priv_data;
+    rtdev_dereference(param->rtdev);
+}
+
+
+
+#ifdef CONFIG_RTNET_RTIPV4_ICMP
+static int ping_handler(struct rt_proc_call *call)
+{
+    struct ipv4_cmd *cmd;
+    int             err;
+
+
+    cmd = rtpc_get_priv(call, struct ipv4_cmd);
+
+    rt_icmp_queue_echo_request(call);
+
+    err = rt_icmp_send_echo(cmd->args.ping.ip_addr, cmd->args.ping.id,
+			    cmd->args.ping.sequence, cmd->args.ping.msg_size);
+    if (err < 0) {
+	rt_icmp_dequeue_echo_request(call);
+	return err;
+    }
+
+    return -CALL_PENDING;
+}
+
+
+
+static void ping_complete_handler(struct rt_proc_call *call, void *priv_data)
+{
+    struct ipv4_cmd *cmd;
+    struct ipv4_cmd *usr_cmd = (struct ipv4_cmd *)priv_data;
+
+
+    if (rtpc_get_result(call) < 0)
+	return;
+
+    cmd = rtpc_get_priv(call, struct ipv4_cmd);
+    usr_cmd->args.ping.ip_addr = cmd->args.ping.ip_addr;
+    usr_cmd->args.ping.rtt     = cmd->args.ping.rtt;
+}
+#endif /* CONFIG_RTNET_RTIPV4_ICMP */
+
+
+
+static int ipv4_ioctl(struct rtnet_device *rtdev, unsigned int request,
+		      unsigned long arg)
+{
+    struct ipv4_cmd             cmd;
+    struct route_solicit_params params;
+    int                         ret;
+
+
+    ret = copy_from_user(&cmd, (void *)arg, sizeof(cmd));
+    if (ret != 0)
+	return -EFAULT;
+
+    switch (request) {
+	case IOC_RT_HOST_ROUTE_ADD:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    ret = rt_ip_route_add_host(cmd.args.addhost.ip_addr,
+				       cmd.args.addhost.dev_addr, rtdev);
+
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_HOST_ROUTE_SOLICIT:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    if (!rtdev_reference(rtdev)) {
+                mutex_unlock(&rtdev->nrt_lock);
+                return -EIDRM;
+            }
+
+	    params.rtdev   = rtdev;
+	    params.ip_addr = cmd.args.solicit.ip_addr;
+
+	    /* We need the rtpc wrapping because rt_arp_solicit can block on a
+	     * real-time lock in the NIC's xmit routine. */
+	    ret = rtpc_dispatch_call(route_solicit_handler, 0, &params,
+				     sizeof(params), NULL,
+				     cleanup_route_solicit);
+
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_HOST_ROUTE_DELETE:
+	case IOC_RT_HOST_ROUTE_DELETE_DEV:
+	    ret = rt_ip_route_del_host(cmd.args.delhost.ip_addr, rtdev);
+	    break;
+
+	case IOC_RT_HOST_ROUTE_GET:
+	case IOC_RT_HOST_ROUTE_GET_DEV:
+	    ret = rt_ip_route_get_host(cmd.args.gethost.ip_addr,
+				       cmd.head.if_name,
+				       cmd.args.gethost.dev_addr, rtdev);
+	    if (ret >= 0) {
+		if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+		    ret = -EFAULT;
+	    }
+	    break;
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+	case IOC_RT_NET_ROUTE_ADD:
+	    ret = rt_ip_route_add_net(cmd.args.addnet.net_addr,
+				      cmd.args.addnet.net_mask,
+				      cmd.args.addnet.gw_addr);
+	    break;
+
+	case IOC_RT_NET_ROUTE_DELETE:
+	    ret = rt_ip_route_del_net(cmd.args.delnet.net_addr,
+				      cmd.args.delnet.net_mask);
+	    break;
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_RTNET_RTIPV4_ICMP
+	case IOC_RT_PING:
+	    ret = rtpc_dispatch_call(ping_handler, cmd.args.ping.timeout, &cmd,
+				     sizeof(cmd), ping_complete_handler, NULL);
+	    if (ret >= 0) {
+		if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+		    ret = -EFAULT;
+	    }
+	    if (ret < 0)
+		rt_icmp_cleanup_echo_requests();
+	    break;
+#endif /* CONFIG_RTNET_RTIPV4_ICMP */
+
+	default:
+	    ret = -ENOTTY;
+    }
+
+    return ret;
+}
+
+
+
+unsigned long rt_inet_aton(const char *ip)
+{
+    int p, n, c;
+    union { unsigned long l; char c[4]; } u;
+    p = n = 0;
+    while ((c = *ip++)) {
+	if (c != '.') {
+	    n = n*10 + c-'0';
+	} else {
+	    if (n > 0xFF) {
+		return 0;
+	    }
+	    u.c[p++] = n;
+	    n = 0;
+	}
+    }
+    u.c[3] = n;
+    return u.l;
+}
+
+
+
+static void rt_ip_ifup(struct rtnet_device *rtdev,
+		       struct rtnet_core_cmd *up_cmd)
+{
+    struct rtnet_device *tmp;
+    int                 i;
+
+
+    rt_ip_route_del_all(rtdev); /* cleanup routing table */
+
+    if (up_cmd->args.up.ip_addr != 0xFFFFFFFF) {
+	rtdev->local_ip     = up_cmd->args.up.ip_addr;
+	rtdev->broadcast_ip = up_cmd->args.up.broadcast_ip;
+    }
+
+    if (rtdev->local_ip != 0) {
+	if (rtdev->flags & IFF_LOOPBACK) {
+	    for (i = 0; i < MAX_RT_DEVICES; i++)
+		if ((tmp = rtdev_get_by_index(i)) != NULL) {
+		    rt_ip_route_add_host(tmp->local_ip,
+					 rtdev->dev_addr, rtdev);
+		    rtdev_dereference(tmp);
+		}
+	} else if ((tmp = rtdev_get_loopback()) != NULL) {
+	    rt_ip_route_add_host(rtdev->local_ip,
+				 tmp->dev_addr, tmp);
+	    rtdev_dereference(tmp);
+	}
+
+	if (rtdev->flags & IFF_BROADCAST)
+	    rt_ip_route_add_host(up_cmd->args.up.broadcast_ip,
+				 rtdev->broadcast, rtdev);
+    }
+}
+
+
+
+static void rt_ip_ifdown(struct rtnet_device *rtdev)
+{
+    rt_ip_route_del_all(rtdev);
+}
+
+
+
+static struct rtdev_event_hook  rtdev_hook = {
+    .unregister_device = rt_ip_ifdown,
+    .ifup =              rt_ip_ifup,
+    .ifdown =            rt_ip_ifdown
+};
+
+static struct rtnet_ioctls ipv4_ioctls = {
+    .service_name =     "IPv4",
+    .ioctl_type =       RTNET_IOC_TYPE_IPV4,
+    .handler =          ipv4_ioctl
+};
+
+
+static int __init rt_ipv4_proto_init(void)
+{
+    int i;
+    int result;
+
+
+    /* Network-Layer */
+    rt_ip_init();
+    rt_arp_init();
+
+    /* Transport-Layer */
+    for (i=0; i<MAX_RT_INET_PROTOCOLS; i++)
+	rt_inet_protocols[i]=NULL;
+
+    rt_icmp_init();
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    result = xnvfile_init_dir("ipv4", &ipv4_proc_root, &rtnet_proc_root);
+    if (result < 0)
+	goto err1;
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    if ((result = rt_ip_routing_init()) < 0)
+	goto err2;
+    if ((result = rtnet_register_ioctls(&ipv4_ioctls)) < 0)
+	goto err3;
+
+    rtdev_add_event_hook(&rtdev_hook);
+
+    return 0;
+
+  err3:
+    rt_ip_routing_release();
+
+  err2:
+#ifdef CONFIG_XENO_OPT_VFILE
+    xnvfile_destroy_dir(&ipv4_proc_root);
+  err1:
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    rt_icmp_release();
+    rt_arp_release();
+    rt_ip_release();
+
+    return result;
+}
+
+
+static void __exit rt_ipv4_proto_release(void)
+{
+    rt_ip_release();
+
+    rtdev_del_event_hook(&rtdev_hook);
+    rtnet_unregister_ioctls(&ipv4_ioctls);
+    rt_ip_routing_release();
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    xnvfile_destroy_dir(&ipv4_proc_root);
+#endif
+
+    /* Transport-Layer */
+    rt_icmp_release();
+
+    /* Network-Layer */
+    rt_arp_release();
+}
+
+
+module_init(rt_ipv4_proto_init);
+module_exit(rt_ipv4_proto_release);
+
+
+EXPORT_SYMBOL_GPL(rt_inet_aton);
diff -Naur a/net/rtnet/stack/ipv4/arp.c b/net/rtnet/stack/ipv4/arp.c
--- a/net/rtnet/stack/ipv4/arp.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/arp.c	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,224 @@
+/***
+ *
+ *  ipv4/arp.h - Adress Resolution Protocol for RTnet
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <rtdev.h>
+#include <stack_mgr.h>
+#include <ipv4/arp.h>
+
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+#include <ipv4/ip_input.h>
+#endif /* CONFIG_RTNET_ADDON_PROXY_ARP */
+
+/***
+ *  arp_send:   Create and send an arp packet. If (dest_hw == NULL),
+ *              we create a broadcast message.
+ */
+void rt_arp_send(int type, int ptype, u32 dest_ip, struct rtnet_device *rtdev,
+		 u32 src_ip, unsigned char *dest_hw, unsigned char *src_hw,
+		 unsigned char *target_hw)
+{
+    struct rtskb *skb;
+    struct arphdr *arp;
+    unsigned char *arp_ptr;
+
+    if (rtdev->flags & IFF_NOARP)
+	return;
+
+    if (!(skb=alloc_rtskb(sizeof(struct arphdr) + 2*(rtdev->addr_len+4) +
+			   rtdev->hard_header_len+15, &global_pool))) {
+	trace_printk("%s %d\n", __func__, __LINE__);
+	return;
+    }
+
+    rtskb_reserve(skb, (rtdev->hard_header_len+15)&~15);
+
+    skb->nh.raw = skb->data;
+    arp = (struct arphdr *)rtskb_put(skb, sizeof(struct arphdr) +
+				     2*(rtdev->addr_len+4));
+
+    skb->rtdev = rtdev;
+    skb->protocol = __constant_htons (ETH_P_ARP);
+    skb->priority = RT_ARP_SKB_PRIO;
+    if (src_hw == NULL)
+	src_hw = rtdev->dev_addr;
+    if (dest_hw == NULL)
+	dest_hw = rtdev->broadcast;
+
+    /*
+     *  Fill the device header for the ARP frame
+     */
+    if (rtdev->hard_header &&
+	(rtdev->hard_header(skb,rtdev,ptype,dest_hw,src_hw,skb->len) < 0)) {
+	trace_printk("%s %d\n", __func__, __LINE__);
+	goto out;
+    }
+
+    arp->ar_hrd = htons(rtdev->type);
+    arp->ar_pro = __constant_htons(ETH_P_IP);
+    arp->ar_hln = rtdev->addr_len;
+    arp->ar_pln = 4;
+    arp->ar_op = htons(type);
+
+    arp_ptr=(unsigned char *)(arp+1);
+
+    memcpy(arp_ptr, src_hw, rtdev->addr_len);
+    arp_ptr+=rtdev->addr_len;
+
+    memcpy(arp_ptr, &src_ip,4);
+    arp_ptr+=4;
+
+    if (target_hw != NULL)
+	memcpy(arp_ptr, target_hw, rtdev->addr_len);
+    else
+	memset(arp_ptr, 0, rtdev->addr_len);
+    arp_ptr+=rtdev->addr_len;
+
+    memcpy(arp_ptr, &dest_ip, 4);
+
+
+    /* send the frame */
+    rtdev_xmit(skb);
+
+    return;
+
+  out:
+    kfree_rtskb(skb);
+}
+
+
+
+/***
+ *  arp_rcv:    Receive an arp request by the device layer.
+ */
+int rt_arp_rcv(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct rtnet_device *rtdev = skb->rtdev;
+    struct arphdr       *arp = skb->nh.arph;
+    unsigned char       *arp_ptr= (unsigned char *)(arp+1);
+    unsigned char       *sha;
+    u32                 sip, tip;
+    u16                 dev_type = rtdev->type;
+
+    /*
+     *  The hardware length of the packet should match the hardware length
+     *  of the device.  Similarly, the hardware types should match.  The
+     *  device should be ARP-able.  Also, if pln is not 4, then the lookup
+     *  is not from an IP number.  We can't currently handle this, so toss
+     *  it.
+     */
+    if ((arp->ar_hln != rtdev->addr_len) ||
+	(rtdev->flags & IFF_NOARP) ||
+	(skb->pkt_type == PACKET_OTHERHOST) ||
+	(skb->pkt_type == PACKET_LOOPBACK) ||
+	(arp->ar_pln != 4))
+	goto out;
+
+    switch (dev_type) {
+	default:
+	    if ((arp->ar_pro != __constant_htons(ETH_P_IP)) &&
+		(htons(dev_type) != arp->ar_hrd))
+		goto out;
+	    break;
+	case ARPHRD_ETHER:
+	    /*
+	     * ETHERNET devices will accept ARP hardware types of either
+	     * 1 (Ethernet) or 6 (IEEE 802.2).
+	     */
+	    if ((arp->ar_hrd != __constant_htons(ARPHRD_ETHER)) &&
+		(arp->ar_hrd != __constant_htons(ARPHRD_IEEE802))) {
+		goto out;
+	    }
+	    if (arp->ar_pro != __constant_htons(ETH_P_IP)) {
+		goto out;
+	    }
+	    break;
+    }
+
+    /* Understand only these message types */
+    if ((arp->ar_op != __constant_htons(ARPOP_REPLY)) &&
+	(arp->ar_op != __constant_htons(ARPOP_REQUEST)))
+	goto out;
+
+    /*
+     *  Extract fields
+     */
+    sha=arp_ptr;
+    arp_ptr += rtdev->addr_len;
+    memcpy(&sip, arp_ptr, 4);
+
+    arp_ptr += 4;
+    arp_ptr += rtdev->addr_len;
+    memcpy(&tip, arp_ptr, 4);
+
+    /* process only requests/replies directed to us */
+    if (tip == rtdev->local_ip) {
+	rt_ip_route_add_host(sip, sha, rtdev);
+
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+	if (!rt_ip_fallback_handler)
+#endif /* CONFIG_RTNET_ADDON_PROXY_ARP */
+		if (arp->ar_op == __constant_htons(ARPOP_REQUEST)) {
+			rt_arp_send(ARPOP_REPLY, ETH_P_ARP, sip, rtdev, tip, sha,
+				rtdev->dev_addr, sha);
+			goto out1;
+		}
+    }
+
+out:
+#ifdef CONFIG_RTNET_ADDON_PROXY_ARP
+    if (rt_ip_fallback_handler) {
+	    rt_ip_fallback_handler(skb);
+	    return 0;
+    }
+#endif /* CONFIG_RTNET_ADDON_PROXY_ARP */
+out1:
+    kfree_rtskb(skb);
+    return 0;
+}
+
+
+
+static struct rtpacket_type arp_packet_type = {
+    type:       __constant_htons(ETH_P_ARP),
+    handler:    &rt_arp_rcv
+};
+
+
+
+/***
+ *  rt_arp_init
+ */
+void __init rt_arp_init(void)
+{
+    rtdev_add_pack(&arp_packet_type);
+}
+
+
+
+/***
+ *  rt_arp_release
+ */
+void rt_arp_release(void)
+{
+    rtdev_remove_pack(&arp_packet_type);
+}
diff -Naur a/net/rtnet/stack/ipv4/icmp.c b/net/rtnet/stack/ipv4/icmp.c
--- a/net/rtnet/stack/ipv4/icmp.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/icmp.c	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,562 @@
+/***
+ *
+ *  ipv4/icmp.c
+ *
+ *  rtnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2002       Vinay Sridhara <vinaysridhara@yahoo.com>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/icmp.h>
+#include <net/checksum.h>
+
+#include <rtskb.h>
+#include <rtnet_socket.h>
+#include <ipv4_chrdev.h>
+#include <ipv4/icmp.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+
+
+/***
+ * Structure for sending the icmp packets
+ */
+struct icmp_bxm
+{
+    unsigned int        csum;
+    size_t              head_len;
+    size_t              data_len;
+    off_t               offset;
+    struct {
+	struct icmphdr  icmph;
+	ktime_t  timestamp;
+    } head;
+    union {
+	struct rtskb    *skb;
+	void            *buf;
+    } data;
+};
+
+struct rt_icmp_control
+{
+    void    (*handler)(struct rtskb *skb);
+    short   error;      /* This ICMP is classed as an error message */
+};
+
+
+raw_spinlock_t echo_calls_lock;
+LIST_HEAD(echo_calls);
+
+#if 0
+static struct {
+    /*
+     * Scratch pad, provided so that rt_socket_dereference(&icmp_socket);
+     * remains legal.
+     */
+    struct rtdm_dev_context dummy;
+
+    /*
+     *  Socket for icmp replies
+     *  It is not part of the socket pool. It may furthermore be used
+     *  concurrently by multiple tasks because all fields are static excect
+     *  skb_pool, but that one is spinlock protected.
+     */
+    struct rtsocket socket;
+} icmp_socket_container;
+#endif
+
+static struct rtsocket icmp_rtsocket;
+
+//#define icmp_fd		(&icmp_socket_container.dummy.fd)
+//#define icmp_socket     ((struct rtsocket *)rtdm_fd_to_private(icmp_fd))
+#define icmp_socket (&icmp_rtsocket)
+
+
+void rt_icmp_queue_echo_request(struct rt_proc_call *call)
+{
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+    list_add_tail(&call->list_entry, &echo_calls);
+    raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+}
+
+
+
+void rt_icmp_dequeue_echo_request(struct rt_proc_call *call)
+{
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+    list_del(&call->list_entry);
+    raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+}
+
+
+
+void rt_icmp_cleanup_echo_requests(void)
+{
+    unsigned long      context;
+    struct list_head    *entry;
+    struct list_head    *next;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+    entry = echo_calls.next;
+    INIT_LIST_HEAD(&echo_calls);
+    raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+
+    while (entry != &echo_calls) {
+	next = entry->next;
+	rtpc_complete_call_nrt((struct rt_proc_call *)entry, -EINTR);
+	entry = next;
+    }
+
+    /* purge any pending ICMP fragments */
+    rt_ip_frag_invalidate_socket(icmp_socket);
+}
+
+
+
+/***
+ *  rt_icmp_discard - dummy function
+ */
+static void rt_icmp_discard(struct rtskb *skb)
+{
+}
+
+
+
+static int rt_icmp_glue_reply_bits(const void *p, unsigned char *to,
+				   unsigned int offset, unsigned int fraglen, int msg_in_userspace)
+{
+    struct icmp_bxm *icmp_param = (struct icmp_bxm *)p;
+    struct icmphdr  *icmph;
+    unsigned long   csum;
+
+
+    /* TODO: add support for fragmented ICMP packets */
+    if (offset != 0)
+	return -EMSGSIZE;
+
+    csum = csum_partial_copy_nocheck((void *)&icmp_param->head, to,
+				     icmp_param->head_len, icmp_param->csum);
+
+    csum = rtskb_copy_and_csum_bits(icmp_param->data.skb,
+				    icmp_param->offset,
+				    to + icmp_param->head_len,
+				    fraglen - icmp_param->head_len,
+				    csum);
+
+    icmph = (struct icmphdr *)to;
+
+    icmph->checksum = csum_fold(csum);
+
+    return 0;
+}
+
+
+
+/***
+ *  common reply function
+ */
+static void rt_icmp_send_reply(struct icmp_bxm *icmp_param, struct rtskb *skb)
+{
+    struct dest_route   rt;
+    int                 err;
+
+
+    icmp_param->head.icmph.checksum = 0;
+    icmp_param->csum = 0;
+
+    /* route back to the source address via the incoming device */
+    if (rt_ip_route_output(&rt, skb->nh.iph->saddr,
+			   skb->rtdev->local_ip) != 0)
+	return;
+
+    rt_socket_reference(icmp_socket);
+    err = rt_ip_build_xmit(icmp_socket, rt_icmp_glue_reply_bits, icmp_param,
+			   sizeof(struct icmphdr) + icmp_param->data_len,
+			   &rt, MSG_DONTWAIT, 0);
+    if (err)
+	    rt_socket_dereference(icmp_socket);
+
+    rtdev_dereference(rt.rtdev);
+
+    RTNET_ASSERT(err == 0,
+		 printk(KERN_ERR "RTnet: %s() error in xmit\n", __FUNCTION__););
+    (void)err;
+}
+
+
+
+/***
+ *  rt_icmp_echo - handles echo replies on our previously sent requests
+ */
+static void rt_icmp_echo_reply(struct rtskb *skb)
+{
+    unsigned long      context;
+    struct rt_proc_call *call;
+    struct ipv4_cmd     *cmd;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+
+    if (!list_empty(&echo_calls)) {
+	call = (struct rt_proc_call *)echo_calls.next;
+	list_del(&call->list_entry);
+
+	raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+    } else {
+	raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+	return;
+    }
+
+    cmd = rtpc_get_priv(call, struct ipv4_cmd);
+
+    cmd->args.ping.ip_addr = skb->nh.iph->saddr;
+    cmd->args.ping.rtt     = 0;
+
+    if ((skb->h.icmph->un.echo.id == cmd->args.ping.id) &&
+	(ntohs(skb->h.icmph->un.echo.sequence) == cmd->args.ping.sequence) &&
+	skb->len == cmd->args.ping.msg_size) {
+	if (skb->len >= sizeof(ktime_t))
+	    cmd->args.ping.rtt =
+		ktime_get() - *((ktime_t *)skb->data);
+	rtpc_complete_call(call, sizeof(struct icmphdr) + skb->len);
+    } else
+	rtpc_complete_call(call, 0);
+}
+
+
+
+/***
+ *  rt_icmp_echo_request - handles echo requests sent by other stations
+ */
+static void rt_icmp_echo_request(struct rtskb *skb)
+{
+    struct icmp_bxm icmp_param;
+
+
+    icmp_param.head.icmph = *skb->h.icmph;
+    icmp_param.head.icmph.type = ICMP_ECHOREPLY;
+    icmp_param.data.skb = skb;
+    icmp_param.offset = 0;
+    icmp_param.data_len = skb->len;
+    icmp_param.head_len = sizeof(struct icmphdr);
+
+    rt_icmp_send_reply(&icmp_param, skb);
+
+    return;
+}
+
+
+
+static int rt_icmp_glue_request_bits(const void *p, unsigned char *to,
+				     unsigned int offset, unsigned int fraglen, int msg_in_userspace)
+{
+    struct icmp_bxm *icmp_param = (struct icmp_bxm *)p;
+    struct icmphdr  *icmph;
+    unsigned long   csum;
+
+
+    /* TODO: add support for fragmented ICMP packets */
+    RTNET_ASSERT(offset == 0,
+		 printk(KERN_WARNING "RTnet: %s() does not support fragmentation.\n",
+			     __FUNCTION__);
+		 return -1;);
+
+    csum = csum_partial_copy_nocheck((void *)&icmp_param->head, to,
+				     icmp_param->head_len, icmp_param->csum);
+
+    csum = csum_partial_copy_nocheck(icmp_param->data.buf,
+				     to + icmp_param->head_len,
+				     fraglen - icmp_param->head_len,
+				     csum);
+
+    icmph = (struct icmphdr *)to;
+
+    icmph->checksum = csum_fold(csum);
+
+    return 0;
+}
+
+
+
+/***
+ *  common request function
+ */
+static int rt_icmp_send_request(u32 daddr, struct icmp_bxm *icmp_param)
+{
+    struct dest_route   rt;
+    unsigned int        size;
+    int                 err;
+
+
+    icmp_param->head.icmph.checksum = 0;
+    icmp_param->csum = 0;
+
+    if ((err = rt_ip_route_output(&rt, daddr, INADDR_ANY)) < 0)
+	return err;
+
+    /* TODO: add support for fragmented ICMP packets */
+    size = icmp_param->head_len + icmp_param->data_len;
+    if (size + 20 /* ip header */ > rt.rtdev->get_mtu(rt.rtdev, RT_ICMP_PRIO))
+	err = -EMSGSIZE;
+    else {
+	rt_socket_reference(icmp_socket);
+	err = rt_ip_build_xmit(icmp_socket, rt_icmp_glue_request_bits,
+			       icmp_param, size, &rt, MSG_DONTWAIT, 0);
+	if (err)
+	    rt_socket_dereference(icmp_socket);
+    }
+
+    rtdev_dereference(rt.rtdev);
+
+    return err;
+}
+
+
+
+/***
+ *  rt_icmp_echo_request - sends an echo request to the specified address
+ */
+int rt_icmp_send_echo(u32 daddr, u16 id, u16 sequence, size_t msg_size)
+{
+    struct icmp_bxm icmp_param;
+    unsigned char   pattern_buf[msg_size];
+    off_t           pos;
+
+
+    /* first purge any potentially pending ICMP fragments */
+    rt_ip_frag_invalidate_socket(icmp_socket);
+
+    icmp_param.head.icmph.type = ICMP_ECHO;
+    icmp_param.head.icmph.code = 0;
+    icmp_param.head.icmph.un.echo.id       = id;
+    icmp_param.head.icmph.un.echo.sequence = htons(sequence);
+    icmp_param.offset = 0;
+
+    if (msg_size >= sizeof(ktime_t)) {
+	icmp_param.head_len = sizeof(struct icmphdr) + sizeof(ktime_t);
+	icmp_param.data_len = msg_size - sizeof(ktime_t);
+
+	for (pos = 0; pos < icmp_param.data_len; pos++)
+	    pattern_buf[pos] = pos & 0xFF;
+
+	icmp_param.head.timestamp = ktime_get();
+    } else {
+	icmp_param.head_len = sizeof(struct icmphdr) + msg_size;
+	icmp_param.data_len = 0;
+
+	for (pos = 0; pos < msg_size; pos++)
+	    pattern_buf[pos] = pos & 0xFF;
+    }
+    icmp_param.data.buf = pattern_buf;
+
+    return rt_icmp_send_request(daddr, &icmp_param);
+}
+
+
+
+/***
+ *  rt_icmp_socket
+ */
+int rt_icmp_socket(struct rtsocket **psock, int family, int type, int protocol)
+{
+    /* we don't support user-created ICMP sockets */
+    return -ENOPROTOOPT;
+}
+
+
+
+static struct rt_icmp_control rt_icmp_pointers[NR_ICMP_TYPES+1] =
+{
+    /* ECHO REPLY (0) */
+    { rt_icmp_echo_reply,       0 },
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+
+    /* DEST UNREACH (3) */
+    { rt_icmp_discard,          1 },
+
+    /* SOURCE QUENCH (4) */
+    { rt_icmp_discard,          1 },
+
+    /* REDIRECT (5) */
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+
+    /* ECHO (8) */
+    { rt_icmp_echo_request,     0 },
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+
+    /* TIME EXCEEDED (11) */
+    { rt_icmp_discard,          1 },
+
+    /* PARAMETER PROBLEM (12) */
+    { rt_icmp_discard,          1 },
+
+    /* TIMESTAMP (13) */
+    { rt_icmp_discard,          0 },
+
+    /* TIMESTAMP REPLY (14) */
+    { rt_icmp_discard,          0 },
+
+    /* INFO (15) */
+    { rt_icmp_discard,          0 },
+
+    /* INFO REPLY (16) */
+    { rt_icmp_discard,          0 },
+
+    /* ADDR MASK (17) */
+    { rt_icmp_discard,          0 },
+
+    /* ADDR MASK REPLY (18) */
+    { rt_icmp_discard,          0 }
+};
+
+
+
+/***
+ *  rt_icmp_dest_pool
+ */
+struct rtsocket *rt_icmp_dest_socket(struct rtskb *skb)
+{
+    rt_socket_reference(icmp_socket);
+    return icmp_socket;
+}
+
+
+
+/***
+ *  rt_icmp_rcv
+ */
+void rt_icmp_rcv(struct rtskb *skb)
+{
+    struct icmphdr  *icmpHdr = skb->h.icmph;
+    unsigned int    length   = skb->len;
+
+
+    /* check header sanity and don't accept fragmented packets */
+    if ((length < sizeof(struct icmphdr)) || (skb->next != NULL))
+    {
+	printk(KERN_WARNING "RTnet: improper length in icmp packet\n");
+	goto cleanup;
+    }
+
+    if (ip_compute_csum((unsigned char *)icmpHdr, length))
+    {
+	printk(KERN_WARNING "RTnet: invalid checksum in icmp packet %d\n", length);
+	goto cleanup;
+    }
+
+    if (!rtskb_pull(skb, sizeof(struct icmphdr)))
+    {
+	printk(KERN_WARNING "RTnet: pull failed %p\n", (skb->sk));
+	goto cleanup;
+    }
+
+
+    if (icmpHdr->type > NR_ICMP_TYPES)
+    {
+	printk(KERN_WARNING "RTnet: invalid icmp type\n");
+	goto cleanup;
+    }
+
+    /* sane packet, process it */
+    rt_icmp_pointers[icmpHdr->type].handler(skb);
+
+  cleanup:
+    kfree_rtskb(skb);
+}
+
+
+
+/***
+ *  rt_icmp_rcv_err
+ */
+void rt_icmp_rcv_err(struct rtskb *skb)
+{
+    printk(KERN_ERR "RTnet: rt_icmp_rcv err\n");
+}
+
+
+
+/***
+ *  ICMP-Initialisation
+ */
+static struct rtinet_protocol icmp_protocol = {
+    .protocol =     IPPROTO_ICMP,
+    .dest_socket =  &rt_icmp_dest_socket,
+    .rcv_handler =  &rt_icmp_rcv,
+    .err_handler =  &rt_icmp_rcv_err,
+    .init_socket =  &rt_icmp_socket
+};
+
+
+
+/***
+ *  rt_icmp_init
+ */
+void __init rt_icmp_init(void)
+{
+    int skbs;
+
+    raw_spin_lock_init(&echo_calls_lock);
+
+    icmp_socket->fd = 1;
+    skbs = rt_bare_socket_init_icmp(icmp_socket, IPPROTO_ICMP, RT_ICMP_PRIO,
+			    ICMP_REPLY_POOL_SIZE);
+    BUG_ON(skbs < 0);
+    if (skbs < ICMP_REPLY_POOL_SIZE)
+	printk(KERN_WARNING "RTnet: allocated only %d icmp rtskbs\n", skbs);
+
+    icmp_socket->prot.inet.tos = 0;
+    icmp_socket->fd_refs = 1;
+
+    rt_inet_add_protocol(&icmp_protocol);
+}
+
+
+
+/***
+ *  rt_icmp_release
+ */
+void rt_icmp_release(void)
+{
+    rt_icmp_cleanup_echo_requests();
+    rt_inet_del_protocol(&icmp_protocol);
+    rt_bare_socket_cleanup(icmp_socket);
+}
diff -Naur a/net/rtnet/stack/ipv4/ip_fragment.c b/net/rtnet/stack/ipv4/ip_fragment.c
--- a/net/rtnet/stack/ipv4/ip_fragment.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/ip_fragment.c	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,356 @@
+/* ip_fragment.c
+ *
+ * Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *               2003      Mathias Koehrer <mathias_koehrer@yahoo.de>
+ *               2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#include <linux/module.h>
+#include <net/checksum.h>
+#include <net/ip.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_socket.h>
+
+#include <linux/ip.h>
+#include <linux/in.h>
+
+#include <ipv4/ip_fragment.h>
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+#include <ipv4/ip_input.h>
+#endif /* CONFIG_RTNET_ADDON_PROXY */
+
+/*
+ * This defined sets the number of incoming fragmented IP messages that
+ * can be handled in parallel.
+ */
+#define COLLECTOR_COUNT 10
+
+struct ip_collector
+{
+    int   in_use;
+    __u32 saddr;
+    __u32 daddr;
+    __u16 id;
+    __u8  protocol;
+
+    struct rtskb_queue frags;
+    struct rtsocket *sock;
+    unsigned int buf_size;
+};
+
+static struct ip_collector collector[COLLECTOR_COUNT];
+
+
+static void alloc_collector(struct rtskb *skb, struct rtsocket *sock)
+{
+    int                 i;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+    struct iphdr        *iph = skb->nh.iph;
+
+
+    /*
+     * Find a free collector
+     *
+     * Note: We once used to clean up probably outdated chains, but the
+     * algorithm was not stable enough and could cause incorrect drops even
+     * under medium load. If we run in overload, we will loose data anyhow.
+     * What we should do in the future is to account collectors per socket or
+     * socket owner and set quotations.
+     * Garbage collection is now performed only on socket close.
+     */
+    for (i = 0; i < COLLECTOR_COUNT; i++) {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if (!p_coll->in_use) {
+            p_coll->in_use        = 1;
+            p_coll->buf_size      = skb->len;
+            p_coll->frags.first   = skb;
+            p_coll->frags.last    = skb;
+            p_coll->saddr         = iph->saddr;
+            p_coll->daddr         = iph->daddr;
+            p_coll->id            = iph->id;
+            p_coll->protocol      = iph->protocol;
+            p_coll->sock          = sock;
+
+            raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+
+            return;
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+
+    printk(KERN_ERR "RTnet: IP fragmentation - no collector available\n");
+    kfree_rtskb(skb);
+}
+
+
+
+/*
+ * Return a pointer to the collector that holds the message which
+ * fits to the iphdr of the passed rtskb.
+ * */
+static struct rtskb *add_to_collector(struct rtskb *skb, unsigned int offset, int more_frags)
+{
+	int                 i, err;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+    struct iphdr        *iph = skb->nh.iph;
+    struct rtskb        *first_skb;
+
+
+    /* Search in existing collectors */
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+    {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if (p_coll->in_use  &&
+            (iph->saddr    == p_coll->saddr) &&
+            (iph->daddr    == p_coll->daddr) &&
+            (iph->id       == p_coll->id) &&
+            (iph->protocol == p_coll->protocol))
+        {
+            first_skb = p_coll->frags.first;
+
+            /* Acquire the rtskb at the expense of the protocol pool */
+            if (rtskb_acquire(skb, &p_coll->sock->skb_pool) != 0) {
+                /* We have to drop this fragment => clean up the whole chain */
+                p_coll->in_use = 0;
+
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+
+#ifdef FRAG_DBG
+                printk(KERN_WARNING "RTnet: Compensation pool empty - IP fragments "
+                            "dropped (saddr:%x, daddr:%x)\n",
+                            iph->saddr, iph->daddr);
+#endif
+
+                kfree_rtskb(first_skb);
+                kfree_rtskb(skb);
+                return NULL;
+            }
+
+            /* Optimized version of __rtskb_queue_tail */
+            skb->next = NULL;
+            p_coll->frags.last->next = skb;
+            p_coll->frags.last = skb;
+
+            /* Extend the chain */
+            first_skb->chain_end = skb;
+
+            /* Sanity check: unordered fragments are not allowed! */
+            if (offset != p_coll->buf_size) {
+                /* We have to drop this fragment => clean up the whole chain */
+                p_coll->in_use = 0;
+                skb = first_skb;
+
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+                break; /* leave the for loop */
+            }
+
+            p_coll->buf_size += skb->len;
+
+            if (!more_frags) {
+                p_coll->in_use = 0;
+
+		err = rt_socket_reference(p_coll->sock);
+
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+
+		if (err < 0) {
+			kfree_rtskb(first_skb);
+			return NULL;
+		}
+
+                return first_skb;
+            } else {
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+                return NULL;
+            }
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+    if (rt_ip_fallback_handler) {
+            __rtskb_push(skb, iph->ihl*4);
+            rt_ip_fallback_handler(skb);
+            return NULL;
+    }
+#endif
+
+#ifdef FRAG_DBG
+    printk(KERN_WARNING "RTnet: Unordered IP fragment (saddr:%x, daddr:%x)"
+                " - dropped\n", iph->saddr, iph->daddr);
+#endif
+
+    kfree_rtskb(skb);
+    return NULL;
+}
+
+
+
+/*
+ * Cleans up all collectors referring to the specified socket.
+ * This is now the only kind of garbage collection we do.
+ */
+void rt_ip_frag_invalidate_socket(struct rtsocket *sock)
+{
+    int                 i;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+
+
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+    {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if ((p_coll->in_use) && (p_coll->sock == sock))
+        {
+            p_coll->in_use = 0;
+            kfree_rtskb(p_coll->frags.first);
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+}
+EXPORT_SYMBOL_GPL(rt_ip_frag_invalidate_socket);
+
+
+
+/*
+ * Cleans up all existing collectors
+ */
+static void cleanup_all_collectors(void)
+{
+    int                 i;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+
+
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+    {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if (p_coll->in_use)
+        {
+            p_coll->in_use = 0;
+            kfree_rtskb(p_coll->frags.first);
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+}
+
+
+
+/*
+ * This function returns an rtskb that contains the complete, accumulated IP message.
+ * If not all fragments of the IP message have been received yet, it returns NULL
+ * Note: the IP header must have already been pulled from the rtskb!
+ * */
+struct rtskb *rt_ip_defrag(struct rtskb *skb, struct rtinet_protocol *ipprot)
+{
+    unsigned int    more_frags;
+    unsigned int    offset;
+    struct rtsocket *sock;
+    struct iphdr    *iph = skb->nh.iph;
+    int             ret;
+
+
+    /* Parse the IP header */
+    offset = ntohs(iph->frag_off);
+    more_frags = offset & IP_MF;
+    offset &= IP_OFFSET;
+    offset <<= 3;   /* offset is in 8-byte chunks */
+
+    /* First fragment? */
+    if (offset == 0)
+    {
+        /* Get the destination socket */
+        if ((sock = ipprot->dest_socket(skb)) == NULL) {
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+                if (rt_ip_fallback_handler) {
+                    __rtskb_push(skb, iph->ihl*4);
+                    rt_ip_fallback_handler(skb);
+                    return NULL;
+                }
+#endif
+            /* Drop the rtskb */
+            kfree_rtskb(skb);
+            return NULL;
+        }
+
+	/* Acquire the rtskb, to unlock the device skb pool */
+        ret = rtskb_acquire(skb, &sock->skb_pool);
+
+        if (ret != 0) {
+            /* Drop the rtskb */
+            kfree_rtskb(skb);
+        } else {
+            /* Allocates a new collector */
+            alloc_collector(skb, sock);
+        }
+
+        /* Packet is queued or freed, socket can be released */
+        rt_socket_dereference(sock);
+
+        return NULL;
+    }
+    else
+    {
+        /* Add to an existing collector */
+        return add_to_collector(skb, offset, more_frags);
+    }
+}
+
+
+
+int __init rt_ip_fragment_init(void)
+{
+    int i;
+
+
+    /* Probably not needed (static variable...) */
+    memset(collector, 0, sizeof(collector));
+
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+        raw_spin_lock_init(&collector[i].frags.lock);
+
+    return 0;
+}
+
+
+
+void rt_ip_fragment_cleanup(void)
+{
+    cleanup_all_collectors();
+}
diff -Naur a/net/rtnet/stack/ipv4/ip_input.c b/net/rtnet/stack/ipv4/ip_input.c
--- a/net/rtnet/stack/ipv4/ip_input.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/ip_input.c	2021-07-14 15:39:13.410124269 +0300
@@ -0,0 +1,169 @@
+/***
+ *
+ *  ipv4/ip_input.c - process incoming IP packets
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <net/checksum.h>
+#include <net/ip.h>
+
+#include <rtskb.h>
+#include <rtnet_socket.h>
+#include <stack_mgr.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+#include <ipv4/ip_input.h>
+
+rt_ip_fallback_handler_t rt_ip_fallback_handler = NULL;
+EXPORT_SYMBOL_GPL(rt_ip_fallback_handler);
+#endif /* CONFIG_RTNET_ADDON_PROXY */
+
+
+
+/***
+ *  rt_ip_local_deliver
+ */
+static inline void rt_ip_local_deliver(struct rtskb *skb)
+{
+    struct iphdr *iph       = skb->nh.iph;
+    unsigned short protocol = iph->protocol;
+    struct rtinet_protocol *ipprot;
+    struct rtsocket *sock;
+    int err;
+
+
+    ipprot = rt_inet_protocols[rt_inet_hashkey(protocol)];
+
+    /* Check if we are supporting the protocol */
+    if ((ipprot != NULL) && (ipprot->protocol == protocol))
+    {
+        __rtskb_pull(skb, iph->ihl*4);
+
+        /* Point into the IP datagram, just past the header. */
+        skb->h.raw = skb->data;
+
+        /* Reassemble IP fragments */
+        if (iph->frag_off & htons(IP_MF|IP_OFFSET)) {
+            skb = rt_ip_defrag(skb, ipprot);
+            if (!skb)
+                return;
+
+	    sock = skb->sk;
+        } else {
+            /* Get the destination socket */
+            if ((sock = ipprot->dest_socket(skb)) == NULL) {
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+                if (rt_ip_fallback_handler) {
+                    __rtskb_push(skb, iph->ihl*4);
+                    rt_ip_fallback_handler(skb);
+                    return;
+                }
+#endif
+                kfree_rtskb(skb);
+                return;
+            }
+
+            /* Acquire the rtskb, to unlock the device skb pool */
+            err = rtskb_acquire(skb, &sock->skb_pool);
+
+            if (err) {
+                kfree_rtskb(skb);
+		rt_socket_dereference(sock);
+                return;
+            }
+        }
+
+        /* Deliver the packet to the next layer */
+        ipprot->rcv_handler(skb);
+
+	/* Packet is queued, socket can be released */
+	rt_socket_dereference(sock);
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+    } else if (rt_ip_fallback_handler) {
+        /* If a fallback handler for IP protocol has been installed,
+         * call it. */
+        rt_ip_fallback_handler(skb);
+#endif /* CONFIG_RTNET_ADDON_PROXY */
+    } else {
+	if (IS_ENABLED(CONFIG_RTNET_RTIPV4_DEBUG))
+		printk(KERN_ERR "RTnet: no protocol found\n");
+        kfree_rtskb(skb);
+    }
+}
+
+
+
+/***
+ *  rt_ip_rcv
+ */
+int rt_ip_rcv(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct iphdr *iph;
+    __u32 len;
+
+    /* When the interface is in promisc. mode, drop all the crap
+     * that it receives, do not try to analyse it.
+     */
+    if (skb->pkt_type == PACKET_OTHERHOST)
+        goto drop;
+
+    iph = skb->nh.iph;
+
+    /*
+     *  RFC1122: 3.1.2.2 MUST silently discard any IP frame that fails the checksum.
+     *
+     *  Is the datagram acceptable?
+     *
+     *  1.  Length at least the size of an ip header
+     *  2.  Version of 4
+     *  3.  Checksums correctly. [Speed optimisation for later, skip loopback checksums]
+     *  4.  Doesn't have a bogus length
+     */
+    if (iph->ihl < 5 || iph->version != 4)
+        goto drop;
+
+    if ( ip_fast_csum((u8 *)iph, iph->ihl)!=0 )
+        goto drop;
+
+    len = ntohs(iph->tot_len);
+    if ( (skb->len<len) || (len<((__u32)iph->ihl<<2)) )
+        goto drop;
+
+    rtskb_trim(skb, len);
+
+#ifdef CONFIG_RTNET_RTIPV4_ROUTER
+    if (rt_ip_route_forward(skb, iph->daddr))
+        return 0;
+#endif /* CONFIG_RTNET_RTIPV4_ROUTER */
+
+    rt_ip_local_deliver(skb);
+    return 0;
+
+  drop:
+    kfree_rtskb(skb);
+    return 0;
+}
diff -Naur a/net/rtnet/stack/ipv4/ip_output.c b/net/rtnet/stack/ipv4/ip_output.c
--- a/net/rtnet/stack/ipv4/ip_output.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/ip_output.c	2021-07-14 15:39:13.418124212 +0300
@@ -0,0 +1,286 @@
+/***
+ *
+ *  ipv4/ip_output.c - prepare outgoing IP packets
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/ip.h>
+#include <net/checksum.h>
+#include <net/ip.h>
+
+#include <rtnet_socket.h>
+#include <stack_mgr.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/ip_input.h>
+#include <ipv4/route.h>
+
+
+raw_spinlock_t rt_ip_id_lock;
+static u16          rt_ip_id_count = 0;
+
+/***
+ *  Slow path for fragmented packets
+ */
+int rt_ip_build_xmit_slow(struct rtsocket *sk,
+	int getfrag(const void *, char *, unsigned int, unsigned int, int),
+	const void *frag, unsigned length, struct dest_route *rt,
+	int msg_flags, unsigned int mtu, unsigned int prio, int msg_in_userspace)
+
+{
+    int             err, next_err;
+    struct rtskb    *skb;
+    struct rtskb    *next_skb;
+    struct          iphdr *iph;
+    struct          rtnet_device *rtdev = rt->rtdev;
+    unsigned int    fragdatalen;
+    unsigned int    offset = 0;
+    u16             msg_rt_ip_id;
+    unsigned long  context;
+    unsigned int    rtskb_size;
+    int             hh_len = (rtdev->hard_header_len + 15) & ~15;
+
+
+    #define FRAGHEADERLEN sizeof(struct iphdr)
+
+    fragdatalen  = ((mtu - FRAGHEADERLEN) & ~7);
+
+    /* Store id in local variable */
+    raw_spin_lock_irqsave(&rt_ip_id_lock, context);
+    msg_rt_ip_id = rt_ip_id_count++;
+    raw_spin_unlock_irqrestore(&rt_ip_id_lock, context);
+
+    rtskb_size = mtu + hh_len + 15;
+
+    /* TODO: delay previous skb until ALL errors are catched which may occure
+	     during next skb setup */
+
+    /* Preallocate first rtskb */
+    skb = alloc_rtskb(rtskb_size, &sk->skb_pool);
+    if (skb == NULL)
+	return -ENOBUFS;
+
+    for (offset = 0; offset < length; offset += fragdatalen)
+    {
+	int fraglen; /* The length (IP, including ip-header) of this
+			very fragment */
+	__u16 frag_off = offset >> 3 ;
+
+
+	next_err = 0;
+	if (offset >= length - fragdatalen)
+	{
+	    /* last fragment */
+	    fraglen  = FRAGHEADERLEN + length - offset ;
+	    next_skb = NULL;
+	}
+	else
+	{
+	    fraglen = FRAGHEADERLEN + fragdatalen;
+	    frag_off |= IP_MF;
+
+	    next_skb = alloc_rtskb(rtskb_size, &sk->skb_pool);
+	    if (next_skb == NULL) {
+		frag_off &= ~IP_MF; /* cut the chain */
+		next_err = -ENOBUFS;
+	    }
+	}
+
+	rtskb_reserve(skb, hh_len);
+
+	skb->rtdev    = rtdev;
+	skb->nh.iph   = iph = (struct iphdr *)rtskb_put(skb, fraglen);
+	skb->priority = prio;
+
+	iph->version  = 4;
+	iph->ihl      = 5;    /* 20 byte header - no options */
+	iph->tos      = sk->prot.inet.tos;
+	iph->tot_len  = htons(fraglen);
+	iph->id       = htons(msg_rt_ip_id);
+	iph->frag_off = htons(frag_off);
+	iph->ttl      = 255;
+	iph->protocol = sk->protocol;
+	iph->saddr    = rtdev->local_ip;
+	iph->daddr    = rt->ip;
+	iph->check    = 0; /* required! */
+	iph->check    = ip_fast_csum((unsigned char *)iph, 5 /*iph->ihl*/);
+
+	if ( (err=getfrag(frag, ((char *)iph) + 5 /*iph->ihl*/ * 4, offset,
+			  fraglen - FRAGHEADERLEN, msg_in_userspace)) )
+	    goto error;
+
+	if (rtdev->hard_header) {
+	    err = rtdev->hard_header(skb, rtdev, ETH_P_IP, rt->dev_addr,
+				     rtdev->dev_addr, skb->len);
+	    if (err < 0)
+		goto error;
+	}
+
+	err = rtdev_xmit(skb);
+
+	skb = next_skb;
+
+	if (err != 0) {
+	    err = -EAGAIN;
+	    goto error;
+	}
+
+	if (next_err != 0)
+	    return next_err;
+    }
+    return 0;
+
+  error:
+    if (skb != NULL) {
+	kfree_rtskb(skb);
+
+	if (next_skb != NULL)
+	    kfree_rtskb(next_skb);
+    }
+    return err;
+}
+
+
+
+/***
+ *  Fast path for unfragmented packets.
+ */
+int rt_ip_build_xmit(struct rtsocket *sk,
+	int getfrag(const void *, char *, unsigned int, unsigned int, int),
+	const void *frag, unsigned length, struct dest_route *rt,
+	int msg_flags, int msg_in_userspace)
+{
+    int                     err = 0;
+    struct rtskb            *skb;
+    struct iphdr            *iph;
+    int                     hh_len;
+    u16                     msg_rt_ip_id;
+    unsigned long          context;
+    struct  rtnet_device    *rtdev = rt->rtdev;
+    unsigned int            prio;
+    unsigned int            mtu;
+
+
+    /* sk->priority may encode both priority and output channel. Make sure
+       we use a consitent value, also for the MTU which is derived from the
+       channel. */
+    prio = (volatile unsigned int)sk->priority;
+    mtu = rtdev->get_mtu(rtdev, prio);
+
+    /*
+     *  Try the simple case first. This leaves fragmented frames, and by choice
+     *  RAW frames within 20 bytes of maximum size(rare) to the long path
+     */
+    length += sizeof(struct iphdr);
+
+    if (length > mtu)
+	return rt_ip_build_xmit_slow(sk, getfrag, frag,
+				     length - sizeof(struct iphdr),
+				     rt, msg_flags, mtu, prio, msg_in_userspace);
+
+    /* Store id in local variable */
+    raw_spin_lock_irqsave(&rt_ip_id_lock, context);
+    msg_rt_ip_id = rt_ip_id_count++;
+    raw_spin_unlock_irqrestore(&rt_ip_id_lock, context);
+
+    hh_len = (rtdev->hard_header_len+15)&~15;
+
+    skb = alloc_rtskb(length+hh_len+15, &sk->skb_pool);
+    if (skb==NULL)
+	return -ENOBUFS;
+
+    rtskb_reserve(skb, hh_len);
+
+    skb->rtdev    = rtdev;
+    skb->nh.iph   = iph = (struct iphdr *) rtskb_put(skb, length);
+    skb->priority = prio;
+
+    iph->version  = 4;
+    iph->ihl      = 5;
+    iph->tos      = sk->prot.inet.tos;
+    iph->tot_len  = htons(length);
+    iph->id       = htons(msg_rt_ip_id);
+    iph->frag_off = htons(IP_DF);
+    iph->ttl      = 255;
+    iph->protocol = sk->protocol;
+    iph->saddr    = rtdev->local_ip;
+    iph->daddr    = rt->ip;
+    iph->check    = 0; /* required! */
+    iph->check    = ip_fast_csum((unsigned char *)iph, 5 /*iph->ihl*/);
+
+    if ( (err=getfrag(frag, ((char *)iph) + 5 /*iph->ihl*/ * 4, 0,
+		      length - 5 /*iph->ihl*/ * 4, msg_in_userspace)) )
+	goto error;
+
+    if (rtdev->hard_header) {
+	err = rtdev->hard_header(skb, rtdev, ETH_P_IP, rt->dev_addr,
+				 rtdev->dev_addr, skb->len);
+	if (err < 0)
+	    goto error;
+    }
+
+    err = rtdev_xmit(skb);
+
+    if (err)
+	return -EAGAIN;
+    else
+	return 0;
+
+  error:
+    kfree_rtskb(skb);
+    return err;
+}
+EXPORT_SYMBOL_GPL(rt_ip_build_xmit);
+
+
+
+/***
+ *  IP protocol layer initialiser
+ */
+static struct rtpacket_type ip_packet_type = {
+    .type =     __constant_htons(ETH_P_IP),
+    .handler =  &rt_ip_rcv
+};
+
+
+
+/***
+ *  ip_init
+ */
+void __init rt_ip_init(void)
+{
+    raw_spin_lock_init(&rt_ip_id_lock);
+    rtdev_add_pack(&ip_packet_type);
+    rt_ip_fragment_init();
+}
+
+
+
+/***
+ *  ip_release
+ */
+void rt_ip_release(void)
+{
+    rtdev_remove_pack(&ip_packet_type);
+    rt_ip_fragment_cleanup();
+}
diff -Naur a/net/rtnet/stack/ipv4/ip_sock.c b/net/rtnet/stack/ipv4/ip_sock.c
--- a/net/rtnet/stack/ipv4/ip_sock.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/ip_sock.c	2021-07-14 15:39:13.410124269 +0300
@@ -0,0 +1,206 @@
+/***
+ *
+ *  ipv4/ip_sock.c
+ *
+ *  Copyright (C) 2003       Hans-Peter Bock <hpbock@avaapgh.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *                2019       Sebastian Smolorz <sebastian.smolorz@gmx.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/errno.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+
+#include <rtnet_socket.h>
+#include <uapi_rtdm.h>
+
+int rt_ip_setsockopt(int fd, struct rtsocket *s, int level,
+		     int optname, const void __user *optval, socklen_t optlen)
+{
+    int err = 0;
+    unsigned int _tos, *tos;
+
+    if (level != SOL_IP)
+	return -ENOPROTOOPT;
+
+    if (optlen < sizeof(unsigned int))
+	return -EINVAL;
+
+    switch (optname) {
+	case IP_TOS:
+	    tos = rtnet_get_arg(s, &_tos, optval, sizeof(_tos), 1);
+	    if (IS_ERR(tos))
+		return PTR_ERR(tos);
+	    else
+		s->prot.inet.tos = *tos;
+	    break;
+
+	default:
+	    err = -ENOPROTOOPT;
+	    break;
+    }
+
+    return err;
+}
+
+
+
+int rt_ip_getsockopt(int fd, struct rtsocket *s, int level,
+		     int optname, void __user *optval,
+		     socklen_t __user *optlen)
+{
+    int err = 0;
+    unsigned int tos;
+    socklen_t _len, *len;
+
+    len = rtnet_get_arg(s, &_len, optlen, sizeof(_len), 1);
+    if (IS_ERR(len))
+	return PTR_ERR(len);
+
+    if (*len < sizeof(unsigned int))
+	return -EINVAL;
+
+    switch (optname) {
+	case IP_TOS:
+	    tos = s->prot.inet.tos;
+	    err = rtnet_put_arg(s, optval, &tos, sizeof(tos), 1);
+	    if (!err) {
+		*len = sizeof(unsigned int);
+		err = rtnet_put_arg(s, optlen, len, sizeof(socklen_t), 1);
+	    }
+	    break;
+
+	default:
+	    err = -ENOPROTOOPT;
+	    break;
+    }
+
+    return err;
+}
+
+
+
+int rt_ip_getsockname(int fd, struct rtsocket *s,
+		      struct sockaddr __user *addr,
+		      socklen_t __user *addrlen)
+{
+    struct sockaddr_in _sin;
+    socklen_t *len, _len;
+    int ret;
+
+    len = rtnet_get_arg(s, &_len, addrlen, sizeof(_len), 1);
+    if (IS_ERR(len))
+	return PTR_ERR(len);
+
+    if (*len < sizeof(struct sockaddr_in))
+	return -EINVAL;
+
+    _sin.sin_family      = AF_INET;
+    _sin.sin_addr.s_addr = s->prot.inet.saddr;
+    _sin.sin_port        = s->prot.inet.sport;
+    memset(&_sin.sin_zero, 0, sizeof(_sin.sin_zero));
+    ret = rtnet_put_arg(s, addr, &_sin, sizeof(_sin), 1);
+    if (ret)
+	return ret;
+
+    *len = sizeof(struct sockaddr_in);
+    ret = rtnet_put_arg(s, addrlen, len, sizeof(socklen_t), 1);
+
+    return ret;
+}
+
+
+
+int rt_ip_getpeername(int fd, struct rtsocket *s,
+		      struct sockaddr __user *addr,
+		      socklen_t __user *addrlen)
+{
+    struct sockaddr_in _sin;
+    socklen_t *len, _len;
+    int ret;
+
+    len = rtnet_get_arg(s, &_len, addrlen, sizeof(_len), 1);
+    if (IS_ERR(len))
+	return PTR_ERR(len);
+
+    if (*len < sizeof(struct sockaddr_in))
+	return -EINVAL;
+
+    _sin.sin_family      = AF_INET;
+    _sin.sin_addr.s_addr = s->prot.inet.daddr;
+    _sin.sin_port        = s->prot.inet.dport;
+    memset(&_sin.sin_zero, 0, sizeof(_sin.sin_zero));
+    ret = rtnet_put_arg(s, addr, &_sin, sizeof(_sin), 1);
+    if (ret)
+	return ret;
+
+    *len = sizeof(struct sockaddr_in);
+    ret = rtnet_put_arg(s, addrlen, len, sizeof(socklen_t), 1);
+
+    return ret;
+}
+
+
+
+int rt_ip_ioctl(struct rtsocket *sock, int request, void __user *arg)
+{
+    struct _rtdm_getsockaddr_args   _getaddr, *getaddr;
+    struct _rtdm_getsockopt_args    _getopt, *getopt;
+    struct _rtdm_setsockopt_args    _setopt, *setopt;
+
+
+    switch (request) {
+	case _RTIOC_SETSOCKOPT:
+	    setopt = rtnet_get_arg(sock, &_setopt, arg, sizeof(_setopt), 1);
+	    if (IS_ERR(setopt))
+		return PTR_ERR(setopt);
+
+	    return rt_ip_setsockopt(sock->fd, sock, setopt->level, setopt->optname,
+				    setopt->optval, setopt->optlen);
+
+	case _RTIOC_GETSOCKOPT:
+	    getopt = rtnet_get_arg(sock, &_getopt, arg, sizeof(_getopt), 1);
+	    if (IS_ERR(getopt))
+		return PTR_ERR(getopt);
+
+	    return rt_ip_getsockopt(sock->fd, sock, getopt->level, getopt->optname,
+				    getopt->optval, getopt->optlen);
+
+	case _RTIOC_GETSOCKNAME:
+	    getaddr = rtnet_get_arg(sock, &_getaddr, arg, sizeof(_getaddr), 1);
+	    if (IS_ERR(getaddr))
+		return PTR_ERR(getaddr);
+
+	    return rt_ip_getsockname(sock->fd, sock, getaddr->addr, getaddr->addrlen);
+
+	case _RTIOC_GETPEERNAME:
+	    getaddr = rtnet_get_arg(sock, &_getaddr, arg, sizeof(_getaddr), 1);
+	    if (IS_ERR(getaddr))
+		return PTR_ERR(getaddr);
+
+	    return rt_ip_getpeername(sock->fd, sock, getaddr->addr, getaddr->addrlen);
+
+	default:
+	    return rt_socket_if_ioctl(sock, request, arg);
+    }
+}
+EXPORT_SYMBOL_GPL(rt_ip_ioctl);
diff -Naur a/net/rtnet/stack/ipv4/Kconfig b/net/rtnet/stack/ipv4/Kconfig
--- a/net/rtnet/stack/ipv4/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/Kconfig	2021-07-14 15:39:13.410124269 +0300
@@ -0,0 +1,76 @@
+config RTNET_RTIPV4
+    depends on RTNET
+    tristate "Real-Time IPv4"
+    default y
+    help
+    Enables the real-time capable IPv4 support of RTnet. The protocol is
+    implemented as a separate module. Supplementing tools (rtroute,
+    rtping) and examples are provided as well. Moreover, RTcfg will
+    include IPv4 support when this option is switched on.
+
+    For further information see also Documentation/README.routing and
+    Documentation/README.ipfragmentation.
+
+config RTNET_RTIPV4_ICMP
+    bool "ICMP support"
+    depends on RTNET_RTIPV4
+    default y
+    help
+    Enables ICMP support of the RTnet Real-Time IPv4 protocol.
+
+    When the RTnet-Proxy is enabled while this feature is disabled, ICMP
+    will be forwarded to the Linux network stack.
+
+config RTNET_RTIPV4_HOST_ROUTES
+    int "Maximum host routing table entries"
+    depends on RTNET_RTIPV4
+    default 32
+    help
+    Each IPv4 supporting interface and each remote host that is directly
+    reachable via via some output interface requires a host routing table
+    entry. If you run larger networks with may hosts per subnet, you may
+    have to increase this limit. Must be power of 2!
+
+config RTNET_RTIPV4_NETROUTING
+    bool "IP Network Routing"
+    depends on RTNET_RTIPV4
+    help
+    Enables routing across IPv4 real-time networks. You will only require
+    this feature in complex networks, while switching it off for flat,
+    single-segment networks improves code size and the worst-case routing
+    decision delay.
+
+    See Documentation/README.routing for further information.
+
+config RTNET_RTIPV4_NET_ROUTES
+    int "Maximum network routing table entries"
+    depends on RTNET_RTIPV4_NETROUTING
+    default 16
+    help
+    Each route describing a target network reachable via a router
+    requires an entry in the network routing table. If you run very
+    complex realtime networks, you may have to increase this limit. Must
+    be power of 2!
+
+config RTNET_RTIPV4_ROUTER
+    bool "IP Router"
+    depends on RTNET_RTIPV4
+    help
+    When switched on, the RTnet station will be able to forward IPv4
+    packets that are not directed to the station itself. Typically used in
+    combination with CONFIG_RTNET_RTIPV4_NETROUTING.
+
+    See Documentation/README.routing for further information.
+
+config RTNET_RTIPV4_DEBUG
+    bool "RTipv4 Debugging"
+    depends on RTNET_RTIPV4
+    default n
+    
+    help
+    Enables debug message output of the RTipv4 layer. Typically, you
+    may want to turn this on for tracing issues in packet delivery.
+
+source "net/rtnet/stack/ipv4/udp/Kconfig"
+#source "net/rtnet/stack/ipv4/tcp/Kconfig"
+
diff -Naur a/net/rtnet/stack/ipv4/Makefile b/net/rtnet/stack/ipv4/Makefile
--- a/net/rtnet/stack/ipv4/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/Makefile	2021-07-14 15:39:13.410124269 +0300
@@ -0,0 +1,19 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_RTIPV4) += rtipv4.o
+
+obj-$(CONFIG_RTNET_RTIPV4_UDP) += udp/
+
+#obj-$(CONFIG_RTNET_RTIPV4_TCP) += tcp/
+
+rtipv4-y := \
+	route.o \
+	protocol.o \
+	arp.o \
+	af_inet.o \
+	ip_input.o \
+	ip_sock.o \
+	ip_output.o \
+	ip_fragment.o
+
+rtipv4-$(CONFIG_RTNET_RTIPV4_ICMP) += icmp.o
diff -Naur a/net/rtnet/stack/ipv4/modules.builtin b/net/rtnet/stack/ipv4/modules.builtin
--- a/net/rtnet/stack/ipv4/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/modules.builtin	2021-07-14 15:39:13.410124269 +0300
@@ -0,0 +1,2 @@
+net/rtnet/stack/ipv4/rtipv4.ko
+net/rtnet/stack/ipv4/udp/rtudp.ko
diff -Naur a/net/rtnet/stack/ipv4/protocol.c b/net/rtnet/stack/ipv4/protocol.c
--- a/net/rtnet/stack/ipv4/protocol.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/protocol.c	2021-07-14 15:39:13.414124240 +0300
@@ -0,0 +1,100 @@
+/***
+ *
+ *  ipv4/protocol.c
+ *
+ *  rtnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/socket.h>
+#include <linux/in.h>
+
+#include <rtnet_socket.h>
+#include <ipv4/protocol.h>
+
+
+
+
+struct rtinet_protocol *rt_inet_protocols[MAX_RT_INET_PROTOCOLS];
+
+/***
+ * rt_inet_add_protocol
+ */
+void rt_inet_add_protocol(struct rtinet_protocol *prot)
+{
+    unsigned char hash = rt_inet_hashkey(prot->protocol);
+
+
+    if ( rt_inet_protocols[hash]==NULL )
+	rt_inet_protocols[hash] = prot;
+}
+EXPORT_SYMBOL_GPL(rt_inet_add_protocol);
+
+
+/***
+ * rt_inet_del_protocol
+ */
+void rt_inet_del_protocol(struct rtinet_protocol *prot)
+{
+    unsigned char hash = rt_inet_hashkey(prot->protocol);
+
+
+    if ( prot==rt_inet_protocols[hash] )
+	rt_inet_protocols[hash] = NULL;
+}
+EXPORT_SYMBOL_GPL(rt_inet_del_protocol);
+
+
+
+/***
+ * rt_inet_socket - initialize an Internet socket
+ * @sock: socket structure
+ * @protocol: protocol id
+ */
+int rt_inet_socket(struct rtsocket **psock, int family, int type, int protocol)
+{
+    struct rtinet_protocol  *prot;
+
+    if (protocol == 0)
+	switch (type) {
+	case SOCK_DGRAM:
+	    protocol = IPPROTO_UDP;
+	    break;
+	case SOCK_STREAM:
+	    protocol = IPPROTO_TCP;
+	    break;
+	}
+
+    prot = rt_inet_protocols[rt_inet_hashkey(protocol)];
+
+    /* create the socket (call the socket creator) */
+    if ((prot != NULL) && (prot->protocol == protocol))
+	return prot->init_socket(psock, family, type, protocol);
+    else {
+	printk(KERN_ERR "RTnet: protocol with id %d not found\n", protocol);
+
+	return -ENOPROTOOPT;
+    }
+}
+EXPORT_SYMBOL_GPL(rt_inet_socket);
diff -Naur a/net/rtnet/stack/ipv4/route.c b/net/rtnet/stack/ipv4/route.c
--- a/net/rtnet/stack/ipv4/route.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/route.c	2021-07-14 15:39:13.410124269 +0300
@@ -0,0 +1,1085 @@
+/***
+ *
+ *  ipv4/route.c - real-time routing
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  Rewritten version of the original route by David Schleef and Ulrich Marx
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <net/ip.h>
+
+#include <rtnet_internal.h>
+#include <rtnet_port.h>
+#include <rtnet_chrdev.h>
+#include <ipv4/af_inet.h>
+#include <ipv4/route.h>
+
+
+struct xnvfile_rev_tag {
+        int rev;
+};
+struct xnvfile_rev_tag host_route_tag;
+static inline void xnvfile_touch_tag(struct xnvfile_rev_tag *tag)
+{
+         tag->rev++;
+}
+
+/* FIXME: should also become some tunable parameter */
+#define ROUTER_FORWARD_PRIO \
+    RTSKB_PRIO_VALUE(QUEUE_MAX_PRIO+(QUEUE_MIN_PRIO-QUEUE_MAX_PRIO+1)/2, \
+		     RTSKB_DEF_RT_CHANNEL)
+
+
+/* First-level routing: explicite host routes */
+struct host_route {
+    struct host_route       *next;
+    struct dest_route       dest_host;
+};
+
+/* Second-level routing: routes to other networks */
+struct net_route {
+    struct net_route        *next;
+    u32                     dest_net_ip;
+    u32                     dest_net_mask;
+    u32                     gw_ip;
+};
+
+#if (CONFIG_RTNET_RTIPV4_HOST_ROUTES & (CONFIG_RTNET_RTIPV4_HOST_ROUTES - 1))
+# error CONFIG_RTNET_RTIPV4_HOST_ROUTES must be power of 2
+#endif
+#if CONFIG_RTNET_RTIPV4_HOST_ROUTES < 256
+# define HOST_HASH_TBL_SIZE 64
+#else
+# define HOST_HASH_TBL_SIZE ((CONFIG_RTNET_RTIPV4_HOST_ROUTES / 256) * 64)
+#endif
+#define HOST_HASH_KEY_MASK  (HOST_HASH_TBL_SIZE-1)
+
+static struct host_route    host_routes[CONFIG_RTNET_RTIPV4_HOST_ROUTES];
+static struct host_route    *free_host_route;
+static int                  allocated_host_routes;
+static struct host_route    *host_hash_tbl[HOST_HASH_TBL_SIZE];
+static raw_spinlock_t host_table_lock;
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+#if (CONFIG_RTNET_RTIPV4_NET_ROUTES & (CONFIG_RTNET_RTIPV4_NET_ROUTES - 1))
+# error CONFIG_RTNET_RTIPV4_NET_ROUTES must be power of 2
+#endif
+#if CONFIG_RTNET_RTIPV4_NET_ROUTES < 256
+# define NET_HASH_TBL_SIZE  64
+#else
+# define NET_HASH_TBL_SIZE  ((CONFIG_RTNET_RTIPV4_NET_ROUTES / 256) * 64)
+#endif
+#define NET_HASH_KEY_MASK   (NET_HASH_TBL_SIZE-1)
+#define NET_HASH_KEY_SHIFT  8
+
+static struct net_route     net_routes[CONFIG_RTNET_RTIPV4_NET_ROUTES];
+static struct net_route     *free_net_route;
+static int                  allocated_net_routes;
+static struct net_route     *net_hash_tbl[NET_HASH_TBL_SIZE + 1];
+static unsigned int         net_hash_key_shift = NET_HASH_KEY_SHIFT;
+static DEFINE_RTDM_LOCK(net_table_lock);
+
+module_param(net_hash_key_shift, uint, 0444);
+MODULE_PARM_DESC(net_hash_key_shift, "destination right shift for "
+		 "network hash key (default: 8)");
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+
+
+/***
+ *  proc filesystem section
+ */
+#ifdef CONFIG_XENO_OPT_VFILE
+static int rtnet_ipv4_route_show(struct xnvfile_regular_iterator *it, void *d)
+{
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+    u32 mask;
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+    xnvfile_printf(it, "Host routes allocated/total:\t%d/%d\n"
+	    "Host hash table size:\t\t%d\n",
+	    allocated_host_routes,
+	    CONFIG_RTNET_RTIPV4_HOST_ROUTES,
+	    HOST_HASH_TBL_SIZE);
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+    mask = NET_HASH_KEY_MASK << net_hash_key_shift;
+    xnvfile_printf(it, "Network routes allocated/total:\t%d/%d\n"
+	    "Network hash table size:\t%d\n"
+	    "Network hash key shift/mask:\t%d/%08X\n",
+	    allocated_net_routes,
+	    CONFIG_RTNET_RTIPV4_NET_ROUTES, NET_HASH_TBL_SIZE,
+	    net_hash_key_shift, mask);
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_RTNET_RTIPV4_ROUTER
+    xnvfile_printf(it, "IP Router:\t\t\tyes\n");
+#else
+    xnvfile_printf(it, "IP Router:\t\t\tno\n");
+#endif
+
+    return 0;
+}
+
+static int rtnet_ipv4_module_lock(struct xnvfile *vfile)
+{
+    bool res = try_module_get(THIS_MODULE);
+    if (!res)
+	return -EIDRM;
+
+    return 0;
+}
+
+static void rtnet_ipv4_module_unlock(struct xnvfile *vfile)
+{
+    module_put(THIS_MODULE);
+}
+
+static struct xnvfile_lock_ops rtnet_ipv4_module_lock_ops = {
+    .get = rtnet_ipv4_module_lock,
+    .put = rtnet_ipv4_module_unlock,
+};
+
+static struct xnvfile_regular_ops rtnet_ipv4_route_vfile_ops = {
+    .show = rtnet_ipv4_route_show,
+};
+
+static struct xnvfile_regular rtnet_ipv4_route_vfile = {
+    .entry = {
+	.lockops = &rtnet_ipv4_module_lock_ops,
+    },
+    .ops = &rtnet_ipv4_route_vfile_ops,
+};
+
+static unsigned long rtnet_ipv4_host_route_lock_ctx;
+
+static int rtnet_ipv4_host_route_lock(struct xnvfile *vfile)
+{
+    raw_spin_lock_irqsave(&host_table_lock, rtnet_ipv4_host_route_lock_ctx);
+    return 0;
+}
+
+static void rtnet_ipv4_host_route_unlock(struct xnvfile *vfile)
+{
+    raw_spin_unlock_irqrestore(&host_table_lock, rtnet_ipv4_host_route_lock_ctx);
+}
+
+static struct xnvfile_lock_ops rtnet_ipv4_host_route_lock_ops = {
+    .get = rtnet_ipv4_host_route_lock,
+    .put = rtnet_ipv4_host_route_unlock,
+};
+
+struct rtnet_ipv4_host_route_priv {
+    unsigned key;
+    struct host_route *entry_ptr;
+};
+
+struct rtnet_ipv4_host_route_data {
+    int key;
+    char name[IFNAMSIZ];
+    struct dest_route dest_host;
+};
+
+static void *rtnet_ipv4_host_route_begin(struct xnvfile_snapshot_iterator *it)
+{
+    struct rtnet_ipv4_host_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_host_route_data *data;
+    unsigned routes;
+    int err;
+
+    routes = allocated_host_routes;
+    if (!routes)
+	return VFILE_SEQ_EMPTY;
+
+    data = kmalloc(sizeof(*data) * routes, GFP_KERNEL);
+    if (data == NULL)
+	return NULL;
+
+    err = rtnet_ipv4_module_lock(NULL);
+    if (err < 0) {
+	kfree(data);
+	return VFILE_SEQ_EMPTY;
+    }
+
+    priv->key = -1;
+    priv->entry_ptr = NULL;
+    return data;
+}
+
+static void rtnet_ipv4_host_route_end(struct xnvfile_snapshot_iterator *it,
+				    void *buf)
+{
+    rtnet_ipv4_module_unlock(NULL);
+    kfree(buf);
+}
+
+static int rtnet_ipv4_host_route_next(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_host_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_host_route_data *p = data;
+    struct rtnet_device *rtdev;
+
+    if (priv->entry_ptr == NULL) {
+	if (++priv->key >= HOST_HASH_TBL_SIZE)
+	    return 0;
+
+	priv->entry_ptr = host_hash_tbl[priv->key];
+	if (priv->entry_ptr == NULL)
+	    return VFILE_SEQ_SKIP;
+    }
+
+    rtdev = priv->entry_ptr->dest_host.rtdev;
+
+    if (!rtdev_reference(rtdev))
+	return -EIDRM;
+
+    memcpy(&p->name, rtdev->name, sizeof(p->name));
+
+    rtdev_dereference(rtdev);
+
+    p->key = priv->key;
+
+    memcpy(&p->dest_host, &priv->entry_ptr->dest_host, sizeof(p->dest_host));
+
+    priv->entry_ptr = priv->entry_ptr->next;
+
+    return 1;
+}
+
+static int rtnet_ipv4_host_route_show(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_host_route_data *p = data;
+
+    if (p == NULL) {
+	xnvfile_printf(it, "Hash\tDestination\tHW Address\t\tDevice\n");
+	return 0;
+    }
+
+    xnvfile_printf(it, "%02X\t%u.%u.%u.%-3u\t"
+		"%02X:%02X:%02X:%02X:%02X:%02X\t%s\n",
+		p->key, NIPQUAD(p->dest_host.ip),
+		p->dest_host.dev_addr[0], p->dest_host.dev_addr[1],
+		p->dest_host.dev_addr[2], p->dest_host.dev_addr[3],
+		p->dest_host.dev_addr[4], p->dest_host.dev_addr[5],
+		p->name);
+    return 0;
+}
+
+static struct xnvfile_snapshot_ops rtnet_ipv4_host_route_vfile_ops = {
+    .begin = rtnet_ipv4_host_route_begin,
+    .end = rtnet_ipv4_host_route_end,
+    .next = rtnet_ipv4_host_route_next,
+    .show = rtnet_ipv4_host_route_show,
+};
+
+static struct xnvfile_snapshot rtnet_ipv4_host_route_vfile = {
+    .entry = {
+	.lockops = &rtnet_ipv4_host_route_lock_ops,
+    },
+    .privsz = sizeof(struct rtnet_ipv4_host_route_priv),
+    .datasz = sizeof(struct rtnet_ipv4_host_route_data),
+    .tag = &host_route_tag,
+    .ops = &rtnet_ipv4_host_route_vfile_ops,
+};
+
+static struct xnvfile_link rtnet_ipv4_arp_vfile;
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+static unsigned long rtnet_ipv4_net_route_lock_ctx;
+
+static int rtnet_ipv4_net_route_lock(struct xnvfile *vfile)
+{
+    raw_spin_lock_irqsave(&net_table_lock, rtnet_ipv4_net_route_lock_ctx);
+    return 0;
+}
+
+static void rtnet_ipv4_net_route_unlock(struct xnvfile *vfile)
+{
+    raw_spin_unlock_irqrestore(&net_table_lock, rtnet_ipv4_net_route_lock_ctx);
+}
+
+static struct xnvfile_lock_ops rtnet_ipv4_net_route_lock_ops = {
+    .get = rtnet_ipv4_net_route_lock,
+    .put = rtnet_ipv4_net_route_unlock,
+};
+
+struct rtnet_ipv4_net_route_priv {
+    unsigned key;
+    struct net_route *entry_ptr;
+};
+
+struct rtnet_ipv4_net_route_data {
+    int key;
+    u32 dest_net_ip;
+    u32 dest_net_mask;
+    u32 gw_ip;
+};
+
+struct xnvfile_rev_tag net_route_tag;
+
+static void *rtnet_ipv4_net_route_begin(struct xnvfile_snapshot_iterator *it)
+{
+    struct rtnet_ipv4_net_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_net_route_data *data;
+    unsigned routes;
+    int err;
+
+    routes = allocated_net_routes;
+    if (!routes)
+	return VFILE_SEQ_EMPTY;
+
+    data = kmalloc(sizeof(*data) * routes, GFP_KERNEL);
+    if (data == NULL)
+	return NULL;
+
+    err = rtnet_ipv4_module_lock(NULL);
+    if (err < 0) {
+	kfree(data);
+	return VFILE_SEQ_EMPTY;
+    }
+
+    priv->key = -1;
+    priv->entry_ptr = NULL;
+    return data;
+}
+
+static void rtnet_ipv4_net_route_end(struct xnvfile_snapshot_iterator *it,
+				    void *buf)
+{
+    rtnet_ipv4_module_unlock(NULL);
+    kfree(buf);
+}
+
+static int rtnet_ipv4_net_route_next(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_net_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_net_route_data *p = data;
+
+    if (priv->entry_ptr == NULL) {
+	if (++priv->key >= NET_HASH_TBL_SIZE + 1)
+	    return 0;
+
+	priv->entry_ptr = net_hash_tbl[priv->key];
+	if (priv->entry_ptr == NULL)
+	    return VFILE_SEQ_SKIP;
+    }
+
+    p->key = priv->key;
+    p->dest_net_ip = priv->entry_ptr->dest_net_ip;
+    p->dest_net_mask = priv->entry_ptr->dest_net_mask;
+    p->gw_ip = priv->entry_ptr->gw_ip;
+
+    priv->entry_ptr = priv->entry_ptr->next;
+
+    return 1;
+}
+
+static int rtnet_ipv4_net_route_show(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_net_route_data *p = data;
+
+    if (p == NULL) {
+	xnvfile_printf(it, "Hash\tDestination\tMask\t\t\tGateway\n");
+	return 0;
+    }
+
+    if (p->key < NET_HASH_TBL_SIZE)
+	xnvfile_printf(it, "%02X\t%u.%u.%u.%-3u\t%u.%u.%u.%-3u"
+		    "\t\t%u.%u.%u.%-3u\n",
+		    p->key, NIPQUAD(p->dest_net_ip),
+		    NIPQUAD(p->dest_net_mask),
+		    NIPQUAD(p->gw_ip));
+    else
+	xnvfile_printf(it, "*\t%u.%u.%u.%-3u\t%u.%u.%u.%-3u\t\t"
+		    "%u.%u.%u.%-3u\n",
+		    NIPQUAD(p->dest_net_ip),
+		    NIPQUAD(p->dest_net_mask),
+		    NIPQUAD(p->gw_ip));
+
+    return 0;
+}
+
+static struct xnvfile_snapshot_ops rtnet_ipv4_net_route_vfile_ops = {
+    .begin = rtnet_ipv4_net_route_begin,
+    .end = rtnet_ipv4_net_route_end,
+    .next = rtnet_ipv4_net_route_next,
+    .show = rtnet_ipv4_net_route_show,
+};
+
+static struct xnvfile_snapshot rtnet_ipv4_net_route_vfile = {
+    .entry = {
+	.lockops = &rtnet_ipv4_net_route_lock_ops,
+    },
+    .privsz = sizeof(struct rtnet_ipv4_net_route_priv),
+    .datasz = sizeof(struct rtnet_ipv4_net_route_data),
+    .tag = &net_route_tag,
+    .ops = &rtnet_ipv4_net_route_vfile_ops,
+};
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+
+
+static int __init rt_route_proc_register(void)
+{
+    int err;
+
+    err = xnvfile_init_regular("route",
+			    &rtnet_ipv4_route_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err1;
+
+    err = xnvfile_init_snapshot("host_route",
+				&rtnet_ipv4_host_route_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err2;
+
+    /* create "arp" as an alias for "host_route" */
+    err = xnvfile_init_link("arp", "host_route",
+			    &rtnet_ipv4_arp_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err3;
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+    err = xnvfile_init_snapshot("net_route",
+				&rtnet_ipv4_net_route_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err4;
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+    return 0;
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+  err4:
+    xnvfile_destroy_link(&rtnet_ipv4_arp_vfile);
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+  err3:
+    xnvfile_destroy_snapshot(&rtnet_ipv4_host_route_vfile);
+
+  err2:
+    xnvfile_destroy_regular(&rtnet_ipv4_route_vfile);
+
+  err1:
+    printk(KERN_ERR "RTnet: unable to initialize /proc entries (route)\n");
+    return err;
+}
+
+
+
+static void rt_route_proc_unregister(void)
+{
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+    xnvfile_destroy_snapshot(&rtnet_ipv4_net_route_vfile);
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+    xnvfile_destroy_link(&rtnet_ipv4_arp_vfile);
+    xnvfile_destroy_snapshot(&rtnet_ipv4_host_route_vfile);
+    xnvfile_destroy_regular(&rtnet_ipv4_route_vfile);
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+/***
+ *  rt_alloc_host_route - allocates new host route
+ */
+static inline struct host_route *rt_alloc_host_route(void)
+{
+    unsigned long      context;
+    struct host_route   *rt;
+
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    if ((rt = free_host_route) != NULL) {
+	free_host_route = rt->next;
+	allocated_host_routes++;
+    }
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+    return rt;
+}
+
+
+
+/***
+ *  rt_free_host_route - releases host route
+ *
+ *  Note: must be called with host_table_lock held
+ */
+static inline void rt_free_host_route(struct host_route *rt)
+{
+    rt->next        = free_host_route;
+    free_host_route = rt;
+    allocated_host_routes--;
+}
+
+
+
+/***
+ *  rt_ip_route_add_host: add or update host route
+ */
+int rt_ip_route_add_host(u32 addr, unsigned char *dev_addr,
+			 struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *new_route;
+    struct host_route   *rt;
+    unsigned int        key;
+    int                 ret = 0;
+
+
+    raw_spin_lock_irqsave(&rtdev->rtdev_lock, context);
+
+    if ((!test_bit(PRIV_FLAG_UP, &rtdev->priv_flags) ||
+	test_and_set_bit(PRIV_FLAG_ADDING_ROUTE, &rtdev->priv_flags))) {
+	raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+	return -EBUSY;
+    }
+
+    raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+
+    if ((new_route = rt_alloc_host_route()) != NULL) {
+	new_route->dest_host.ip    = addr;
+	new_route->dest_host.rtdev = rtdev;
+	memcpy(new_route->dest_host.dev_addr, dev_addr, rtdev->addr_len);
+    }
+
+    key = ntohl(addr) & HOST_HASH_KEY_MASK;
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    xnvfile_touch_tag(&host_route_tag);
+
+    rt = host_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_host.ip == addr) &&
+	    (rt->dest_host.rtdev->local_ip == rtdev->local_ip)) {
+	    rt->dest_host.rtdev = rtdev;
+	    memcpy(rt->dest_host.dev_addr, dev_addr, rtdev->addr_len);
+
+	    if (new_route)
+		rt_free_host_route(new_route);
+
+	    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+	    goto out;
+	}
+
+	rt = rt->next;
+    }
+
+    if (new_route) {
+	new_route->next    = host_hash_tbl[key];
+	host_hash_tbl[key] = new_route;
+
+	raw_spin_unlock_irqrestore(&host_table_lock, context);
+    } else {
+	raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+	/*ERRMSG*/printk(KERN_ERR "RTnet: no more host routes available\n");
+	ret = -ENOBUFS;
+    }
+
+  out:
+    clear_bit(PRIV_FLAG_ADDING_ROUTE, &rtdev->priv_flags);
+
+    return ret;
+}
+
+
+
+/***
+ *  rt_ip_route_del_host - deletes specified host route
+ */
+int rt_ip_route_del_host(u32 addr, struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *rt;
+    struct host_route   **last_ptr;
+    unsigned int        key;
+
+
+    key = ntohl(addr) & HOST_HASH_KEY_MASK;
+    last_ptr = &host_hash_tbl[key];
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    rt = host_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_host.ip == addr) &&
+	    (!rtdev || (rt->dest_host.rtdev->local_ip == rtdev->local_ip))) {
+	    *last_ptr = rt->next;
+
+	    rt_free_host_route(rt);
+
+	    xnvfile_touch_tag(&host_route_tag);
+
+	    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+	    return 0;
+	}
+
+	last_ptr = &rt->next;
+	rt = rt->next;
+    }
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+    return -ENOENT;
+}
+
+
+
+/***
+ *  rt_ip_route_del_all - deletes all routes associated with a specified device
+ */
+void rt_ip_route_del_all(struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *host_rt;
+    struct host_route   **last_host_ptr;
+    unsigned int        key;
+    u32                 ip;
+
+
+    for (key = 0; key < HOST_HASH_TBL_SIZE; key++) {
+      host_start_over:
+	last_host_ptr = &host_hash_tbl[key];
+
+	raw_spin_lock_irqsave(&host_table_lock, context);
+
+	host_rt = host_hash_tbl[key];
+	while (host_rt != NULL) {
+	    if (host_rt->dest_host.rtdev == rtdev) {
+		*last_host_ptr = host_rt->next;
+
+		rt_free_host_route(host_rt);
+
+		raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+		goto host_start_over;
+	    }
+
+	    last_host_ptr = &host_rt->next;
+	    host_rt = host_rt->next;
+	}
+
+	raw_spin_unlock_irqrestore(&host_table_lock, context);
+    }
+
+    if ((ip = rtdev->local_ip) != 0)
+	rt_ip_route_del_host(ip, rtdev);
+}
+
+
+/***
+ *  rt_ip_route_get_host - check if specified host route is resolved
+ */
+int rt_ip_route_get_host(u32 addr, char *if_name, unsigned char *dev_addr,
+			 struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *rt;
+    unsigned int        key;
+
+
+    key = ntohl(addr) & HOST_HASH_KEY_MASK;
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    rt = host_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_host.ip == addr) &&
+	    (!rtdev || rt->dest_host.rtdev->local_ip == rtdev->local_ip)) {
+	    memcpy(dev_addr, rt->dest_host.dev_addr,
+		   rt->dest_host.rtdev->addr_len);
+	    strncpy(if_name, rt->dest_host.rtdev->name, IFNAMSIZ);
+
+	    raw_spin_unlock_irqrestore(&host_table_lock, context);
+	    return 0;
+	}
+
+	rt = rt->next;
+    }
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+    return -ENOENT;
+}
+
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+/***
+ *  rt_alloc_net_route - allocates new network route
+ */
+static inline struct net_route *rt_alloc_net_route(void)
+{
+    unsigned long      context;
+    struct net_route    *rt;
+
+
+    raw_spin_lock_irqsave(&net_table_lock, context);
+
+    if ((rt = free_net_route) != NULL) {
+	free_net_route = rt->next;
+	allocated_net_routes++;
+    }
+
+    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+    return rt;
+}
+
+
+
+/***
+ *  rt_free_net_route - releases network route
+ *
+ *  Note: must be called with net_table_lock held
+ */
+static inline void rt_free_net_route(struct net_route *rt)
+{
+    rt->next       = free_net_route;
+    free_net_route = rt;
+    allocated_host_routes--;
+}
+
+
+
+/***
+ *  rt_ip_route_add_net: add or update network route
+ */
+int rt_ip_route_add_net(u32 addr, u32 mask, u32 gw_addr)
+{
+    unsigned long      context;
+    struct net_route    *new_route;
+    struct net_route    *rt;
+    struct net_route    **last_ptr;
+    unsigned int        key;
+    u32                 shifted_mask;
+
+
+    addr &= mask;
+
+    if ((new_route = rt_alloc_net_route()) != NULL) {
+	new_route->dest_net_ip   = addr;
+	new_route->dest_net_mask = mask;
+	new_route->gw_ip         = gw_addr;
+    }
+
+    shifted_mask = NET_HASH_KEY_MASK << net_hash_key_shift;
+    if ((mask & shifted_mask) == shifted_mask)
+	key = (ntohl(addr) >> net_hash_key_shift) & NET_HASH_KEY_MASK;
+    else
+	key = NET_HASH_TBL_SIZE;
+    last_ptr = &net_hash_tbl[key];
+
+    raw_spin_lock_irqsave(&net_table_lock, context);
+
+    xnvfile_touch_tag(&net_route_tag);
+
+    rt = net_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_net_ip == addr) && (rt->dest_net_mask == mask)) {
+	    rt->gw_ip = gw_addr;
+
+	    if (new_route)
+		rt_free_net_route(new_route);
+
+	    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	    return 0;
+	}
+
+	last_ptr = &rt->next;
+	rt = rt->next;
+    }
+
+    if (new_route) {
+	new_route->next = *last_ptr;
+	*last_ptr       = new_route;
+
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	return 0;
+    } else {
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	/*ERRMSG*/printk(KERN_ERR "RTnet: no more network routes available\n");
+	return -ENOBUFS;
+    }
+}
+
+
+
+/***
+ *  rt_ip_route_del_net - deletes specified network route
+ */
+int rt_ip_route_del_net(u32 addr, u32 mask)
+{
+    unsigned long      context;
+    struct net_route    *rt;
+    struct net_route    **last_ptr;
+    unsigned int        key;
+    u32                 shifted_mask;
+
+
+    addr &= mask;
+
+    shifted_mask = NET_HASH_KEY_MASK << net_hash_key_shift;
+    if ((mask & shifted_mask) == shifted_mask)
+	key = (ntohl(addr) >> net_hash_key_shift) & NET_HASH_KEY_MASK;
+    else
+	key = NET_HASH_TBL_SIZE;
+    last_ptr = &net_hash_tbl[key];
+
+    raw_spin_lock_irqsave(&net_table_lock, context);
+
+    rt = net_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_net_ip == addr) && (rt->dest_net_mask == mask)) {
+	    *last_ptr = rt->next;
+
+	    rt_free_net_route(rt);
+
+	    xnvfile_touch_tag(&net_route_tag);
+
+	    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	    return 0;
+	}
+
+	last_ptr = &rt->next;
+	rt = rt->next;
+    }
+
+    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+    return -ENOENT;
+}
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+
+
+/***
+ *  rt_ip_route_output - looks up output route
+ *
+ *  Note: increments refcount on returned rtdev in rt_buf
+ */
+int rt_ip_route_output(struct dest_route *rt_buf, u32 daddr, u32 saddr)
+{
+    unsigned long      context;
+    struct host_route   *host_rt;
+    unsigned int        key;
+
+#ifndef CONFIG_RTNET_RTIPV4_NETROUTING
+    #define DADDR       daddr
+#else
+    #define DADDR       real_daddr
+
+    struct net_route    *net_rt;
+    int                 lookup_gw  = 1;
+    u32                 real_daddr = daddr;
+
+
+  restart:
+#endif /* !CONFIG_RTNET_RTIPV4_NETROUTING */
+
+    key = ntohl(daddr) & HOST_HASH_KEY_MASK;
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    host_rt = host_hash_tbl[key];
+    if (likely(saddr == INADDR_ANY))
+	while (host_rt != NULL) {
+	    if (host_rt->dest_host.ip == daddr) {
+	      host_route_found:
+		if (!rtdev_reference(host_rt->dest_host.rtdev)) {
+		    raw_spin_unlock_irqrestore(&host_table_lock, context);
+		    goto next;
+		}
+
+		memcpy(rt_buf->dev_addr, &host_rt->dest_host.dev_addr,
+		       sizeof(rt_buf->dev_addr));
+		rt_buf->rtdev = host_rt->dest_host.rtdev;
+
+		raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+		rt_buf->ip = DADDR;
+
+		return 0;
+	    }
+	  next:
+	    host_rt = host_rt->next;
+	}
+    else
+	while (host_rt != NULL) {
+	    if ((host_rt->dest_host.ip == daddr) &&
+		(host_rt->dest_host.rtdev->local_ip == saddr))
+		goto host_route_found;
+	    host_rt = host_rt->next;
+	}
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+    if (lookup_gw) {
+	lookup_gw = 0;
+	key = (ntohl(daddr) >> net_hash_key_shift) & NET_HASH_KEY_MASK;
+
+	raw_spin_lock_irqsave(&net_table_lock, context);
+
+	net_rt = net_hash_tbl[key];
+	while (net_rt != NULL) {
+	    if (net_rt->dest_net_ip == (daddr & net_rt->dest_net_mask)) {
+		daddr = net_rt->gw_ip;
+
+		raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+		/* start over, now using the gateway ip as destination */
+		goto restart;
+	    }
+
+	    net_rt = net_rt->next;
+	}
+
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	/* last try: no hash key */
+	raw_spin_lock_irqsave(&net_table_lock, context);
+
+	net_rt = net_hash_tbl[NET_HASH_TBL_SIZE];
+	while (net_rt != NULL) {
+	    if (net_rt->dest_net_ip == (daddr & net_rt->dest_net_mask)) {
+		daddr = net_rt->gw_ip;
+
+		raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+		/* start over, now using the gateway ip as destination */
+		goto restart;
+	    }
+
+	    net_rt = net_rt->next;
+	}
+
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+    }
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+    /*ERRMSG*/printk(KERN_WARNING "RTnet: host %u.%u.%u.%u unreachable\n", NIPQUAD(daddr));
+    return -EHOSTUNREACH;
+}
+
+
+
+#ifdef CONFIG_RTNET_RTIPV4_ROUTER
+int rt_ip_route_forward(struct rtskb *rtskb, u32 daddr)
+{
+    struct rtnet_device *rtdev = rtskb->rtdev;
+    struct dest_route   dest;
+
+
+    if (likely((daddr == rtdev->local_ip) || (daddr == rtdev->broadcast_ip) ||
+	(rtdev->flags & IFF_LOOPBACK)))
+	return 0;
+
+    if (rtskb_acquire(rtskb, &global_pool) != 0) {
+	/*ERRMSG*/printk(KERN_WARNING "RTnet: router overloaded, dropping packet\n");
+	goto error;
+    }
+
+    if (rt_ip_route_output(&dest, daddr, INADDR_ANY) < 0) {
+	/*ERRMSG*/printk(KERN_WARNING "RTnet: unable to forward packet from %u.%u.%u.%u\n",
+			      NIPQUAD(rtskb->nh.iph->saddr));
+	goto error;
+    }
+
+    rtskb->rtdev    = dest.rtdev;
+    rtskb->priority = ROUTER_FORWARD_PRIO;
+
+    if ((dest.rtdev->hard_header) &&
+	(dest.rtdev->hard_header(rtskb, dest.rtdev, ETH_P_IP, dest.dev_addr,
+				 dest.rtdev->dev_addr, rtskb->len) < 0))
+	goto error;
+
+    rtdev_xmit(rtskb);
+
+    return 1;
+
+  error:
+    kfree_rtskb(rtskb);
+    return 1;
+}
+#endif /* CONFIG_RTNET_RTIPV4_ROUTER */
+
+
+
+/***
+ *  rt_ip_routing_init: initialize
+ */
+int __init rt_ip_routing_init(void)
+{
+    int i;
+
+    raw_spin_lock_init(&host_table_lock);
+
+    for (i = 0; i < CONFIG_RTNET_RTIPV4_HOST_ROUTES-2; i++)
+	host_routes[i].next = &host_routes[i+1];
+    free_host_route = &host_routes[0];
+
+#ifdef CONFIG_RTNET_RTIPV4_NETROUTING
+    for (i = 0; i < CONFIG_RTNET_RTIPV4_NET_ROUTES-2; i++)
+	net_routes[i].next = &net_routes[i+1];
+    free_net_route = &net_routes[0];
+#endif /* CONFIG_RTNET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    return rt_route_proc_register();
+#else /* !CONFIG_XENO_OPT_VFILE */
+    return 0;
+#endif /* CONFIG_XENO_OPT_VFILE */
+}
+
+
+
+/***
+ *  rt_ip_routing_realease
+ */
+void rt_ip_routing_release(void)
+{
+#ifdef CONFIG_XENO_OPT_VFILE
+    rt_route_proc_unregister();
+#endif /* CONFIG_XENO_OPT_VFILE */
+}
+
+
+EXPORT_SYMBOL_GPL(rt_ip_route_add_host);
+EXPORT_SYMBOL_GPL(rt_ip_route_del_host);
+EXPORT_SYMBOL_GPL(rt_ip_route_del_all);
+EXPORT_SYMBOL_GPL(rt_ip_route_output);
diff -Naur a/net/rtnet/stack/ipv4/tcp/Kconfig b/net/rtnet/stack/ipv4/tcp/Kconfig
--- a/net/rtnet/stack/ipv4/tcp/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/tcp/Kconfig	2020-11-23 14:47:05.148825450 +0200
@@ -0,0 +1,18 @@
+config RTNET_RTIPV4_TCP
+    tristate "TCP support"
+    depends on RTNET_RTIPV4
+    help
+    Enables TCP support of the RTnet Real-Time IPv4 protocol.
+
+    When the RTnet IPv4 is enabled while this feature is disabled, TCP
+    will be forwarded to the Linux network stack.
+
+config RTNET_RTIPV4_TCP_ERROR_INJECTION
+    bool "TCP error injection"
+    depends on RTNET_RTIPV4_TCP
+    help
+    Enables error injection for incoming TCP packets. This can be used
+    to test both protocol as well as application behavior under error
+    conditions. The per-socket error rate is 0 by default and can be
+    tuned during runtime via the error_rate and multi_error module
+    parameters.
diff -Naur a/net/rtnet/stack/ipv4/tcp/Makefile b/net/rtnet/stack/ipv4/tcp/Makefile
--- a/net/rtnet/stack/ipv4/tcp/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/tcp/Makefile	2020-11-23 14:47:05.148825450 +0200
@@ -0,0 +1,7 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_RTIPV4_TCP) += rttcp.o
+
+rttcp-y := \
+	tcp.o \
+	timerwheel.o
diff -Naur a/net/rtnet/stack/ipv4/tcp/tcp.c b/net/rtnet/stack/ipv4/tcp/tcp.c
--- a/net/rtnet/stack/ipv4/tcp/tcp.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/tcp/tcp.c	2021-07-14 15:27:45.159310821 +0300
@@ -0,0 +1,2490 @@
+/***
+ *
+ *  ipv4/tcp/tcp.c - TCP implementation for RTnet
+ *
+ *  Copyright (C) 2009 Vladimir Zapolskiy <vladimir.zapolskiy@siemens.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License, version 2, as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <uapi/asm-generic/socket.h>
+#include <linux/moduleparam.h>
+#include <linux/list.h>
+#include <linux/skbuff.h>
+#include <linux/err.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <net/tcp_states.h>
+#include <net/tcp.h>
+
+#include <rtnet_rtdm.h>
+#include <rtnet_rtpc.h>
+#include <rtskb.h>
+#include <rtdev.h>
+#include <rtnet_port.h>
+#include <ipv4/tcp.h>
+#include <ipv4/ip_sock.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/route.h>
+#include <ipv4/af_inet.h>
+
+#include "timerwheel.h"
+
+#ifdef CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION
+
+static unsigned int error_rate;
+module_param(error_rate, uint, 0664);
+MODULE_PARM_DESC(error_rate, "simulate packet loss after every n packets");
+
+static unsigned int multi_error = 1;
+module_param(multi_error, uint, 0664);
+MODULE_PARM_DESC(multi_error, "on simulated error, drop n packets in a row");
+
+static unsigned int counter_start = 1234;
+module_param(counter_start, uint, 0664);
+MODULE_PARM_DESC(counter_start, "start value of per-socket packet counter "
+		 "(used for error injection)");
+
+#endif /* CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION */
+
+struct tcp_sync {
+    u32 seq;
+    u32 ack_seq;
+
+    /* Local window size sent to peer  */
+    u16 window;
+    /* Last received destination peer window size */
+    u16 dst_window;
+};
+
+/*
+  connection timeout
+*/
+/* 5 second */
+static const nanosecs_rel_t rt_tcp_connection_timeout = 1000000000ull;
+
+/* retransmission timerwheel timeout */
+static const u64 rt_tcp_retransmit_timeout = 100000000ull;
+
+/*
+  keepalive constants
+*/
+/* 75 second */
+static const u64 rt_tcp_keepalive_intvl   = 75000000000ull;
+/* 9 probes to send */
+static const u8  rt_tcp_keepalive_probes  = 9;
+/* 2 hour */
+static const u64 rt_tcp_keepalive_timeout = 7200000000000ull;
+
+/*
+  retransmission timeout
+*/
+/* 50 millisecond */
+static const nanosecs_rel_t rt_tcp_retransmission_timeout = 50000000ull;
+/*
+  maximum allowed number of retransmissions
+*/
+static const unsigned int max_retransmits = 3;
+
+struct tcp_keepalive {
+    u8 enabled;
+    u32 probes;
+    struct hrtimer timer;
+};
+
+/***
+ *  This structure is used to register a TCP socket for reception. All
+ *  structures are kept in the port_registry array to increase the cache
+ *  locality during the critical port lookup in rt_tcp_v4_lookup().
+ */
+
+/* if dport & daddr are zeroes, it means a listening socket */
+/* otherwise this is a data structure, which describes a connection */
+
+/* NB: sock->prot.inet.saddr & sock->prot.inet.sport values are not used */
+struct tcp_socket {
+    struct rtsocket sock;  /* set up by rt_socket_init() implicitly */
+    u16             sport; /* local port */
+    u32             saddr; /* local ip-addr */
+    u16             dport; /* destination port */
+    u32             daddr; /* destination ip-addr */
+
+    u8 tcp_state;          /* tcp connection state */
+
+    u8 is_binding;         /* if set, tcp socket is in port binding progress */
+    u8 is_bound;           /* if set, tcp socket is already port bound */
+    u8 is_valid;           /* if set, read() and write() can process */
+    u8 is_accepting;       /* if set, accept() is in progress */
+    u8 is_accepted;        /* if set, accept() is already called */
+    u8 is_closed;          /* close() call for resource deallocation follows */
+
+    rtdm_event_t send_evt; /* write request is permissible */
+    rtdm_event_t conn_evt; /* connection event */
+
+    struct dest_route rt;
+    struct tcp_sync sync;
+    struct tcp_keepalive keepalive;
+    raw_spinlock_t socket_lock;
+
+    struct hlist_node link;
+
+    nanosecs_rel_t sk_sndtimeo;
+
+    /* retransmission routine data */
+    u32                nacked_first;
+    unsigned int       timer_state;
+    struct rtskb_queue retransmit_queue;
+    struct timerwheel_timer timer;
+
+#ifdef CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION
+    unsigned int packet_counter;
+    unsigned int error_rate;
+    unsigned int multi_error;
+#endif /* CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION */
+};
+
+struct rt_tcp_dispatched_packet_send_cmd {
+    __be32 flags; /* packet flags value */
+    struct tcp_socket *ts;
+};
+
+/***
+ *  Automatic port number assignment
+
+ *  The automatic assignment of port numbers to unbound sockets is realised as
+ *  a simple addition of two values:
+ *   - the socket ID (lower 8 bits of file descriptor) which is set during
+ *     initialisation and left unchanged afterwards
+ *   - the start value tcp_auto_port_start which is a module parameter
+
+ *  tcp_auto_port_mask, also a module parameter, is used to define the range of
+ *  port numbers which are used for automatic assignment. Any number within
+ *  this range will be rejected when passed to bind_rt().
+
+ */
+
+MODULE_LICENSE("GPL");
+
+#if 0
+static struct {
+	struct rtdm_dev_context dummy;
+	struct tcp_socket rst_socket;
+} rst_socket_container;
+
+#define rst_fd		(&rst_socket_container.dummy.fd)
+#define rst_socket	(*(struct tcp_socket *)rtdm_private_to_fd(rst_fd))
+#endif
+static struct tcp_socket internal_tcp_socket;
+#define rst_socket internal_tcp_socket
+
+static u32 tcp_auto_port_start = 1024;
+static u32 tcp_auto_port_mask  = ~(RT_TCP_SOCKETS-1);
+static u32 free_ports          = RT_TCP_SOCKETS;
+#define RT_PORT_BITMAP_WORDS                                    \
+    ((RT_TCP_SOCKETS + BITS_PER_LONG - 1) / BITS_PER_LONG)
+static unsigned long      port_bitmap[RT_PORT_BITMAP_WORDS];
+
+static struct tcp_socket* port_registry[RT_TCP_SOCKETS];
+static raw_spinlock_t tcp_socket_base_lock;
+
+static struct hlist_head port_hash[RT_TCP_SOCKETS * 2];
+#define port_hash_mask (RT_TCP_SOCKETS * 2 - 1)
+
+module_param(tcp_auto_port_start, uint, 0444);
+module_param(tcp_auto_port_mask, uint, 0444);
+MODULE_PARM_DESC(tcp_auto_port_start, "Start of automatically assigned "
+		 "port range for TCP");
+MODULE_PARM_DESC(tcp_auto_port_mask, "Mask that defines port range for TCP "
+		 "for automatic assignment");
+
+static inline struct tcp_socket *port_hash_search(u32 saddr, u16 sport)
+{
+    u32 bucket = sport & port_hash_mask;
+    struct tcp_socket *ts;
+
+    hlist_for_each_entry(ts, &port_hash[bucket], link)
+	if (ts->sport == sport &&
+	    (saddr == INADDR_ANY
+	     || ts->saddr == saddr
+	     || ts->saddr == INADDR_ANY))
+	    return ts;
+
+    return NULL;
+}
+
+static int port_hash_insert(struct tcp_socket *ts, u32 saddr, u16 sport)
+{
+    u32 bucket;
+
+    if (port_hash_search(saddr, sport))
+	return -EADDRINUSE;
+
+    bucket = sport & port_hash_mask;
+    ts->saddr = saddr;
+    ts->sport = sport;
+    ts->daddr = 0;
+    ts->dport = 0;
+
+    hlist_add_head(&ts->link, &port_hash[bucket]);
+
+    return 0;
+}
+
+static inline void port_hash_del(struct tcp_socket *ts)
+{
+    hlist_del(&ts->link);
+}
+
+/***
+ *  rt_tcp_v4_lookup
+ */
+static struct rtsocket *rt_tcp_v4_lookup(u32 daddr, u16 dport)
+{
+    unsigned long  context;
+    struct tcp_socket *ts;
+
+    raw_spin_lock_irqsave(&tcp_socket_base_lock, context);
+    ts = port_hash_search(daddr, dport);
+
+    if (ts && rt_socket_reference(&ts->sock) == 0) {
+
+	raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+
+	return &ts->sock;
+    }
+
+    raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+
+    return NULL;
+}
+
+/* test seq1 <= seq2 */
+static inline int rt_tcp_before(__u32 seq1, __u32 seq2)
+{
+    return (__s32)(seq1-seq2) <= 0;
+}
+
+/* test seq1 => seq2 */
+static inline int rt_tcp_after(__u32 seq1, __u32 seq2)
+{
+    return (__s32)(seq2-seq1) <= 0;
+}
+
+static inline void rt_tcp_set_flags(struct tcphdr* th, __be32 flags)
+{
+    __be16 *tf = ((__be16*)th) + 6;
+    *tf = flags;
+}
+
+static inline u32 rt_tcp_compute_ack_seq(struct tcphdr* th, u32 len)
+{
+    u32 ack_seq = ntohl(th->seq) + len;
+
+    if (unlikely(th->syn || th->fin))
+	ack_seq++;
+
+    return ack_seq;
+}
+
+static void rt_tcp_keepalive_start(struct tcp_socket *ts)
+{
+    if (ts->tcp_state == TCP_ESTABLISHED) {
+	hrtimer_start(&ts->keepalive.timer, rt_tcp_keepalive_timeout,
+			HRTIMER_MODE_REL_HARD);
+    }
+}
+
+static void rt_tcp_keepalive_stop(struct tcp_socket *ts)
+{
+    if (ts->tcp_state == TCP_ESTABLISHED) {
+	hrtimer_cancel(&ts->keepalive.timer);
+    }
+}
+
+#ifdef YET_UNUSED
+static void rt_tcp_keepalive_timer(rtdm_timer_t *timer);
+
+static void rt_tcp_keepalive_enable(struct tcp_socket *ts)
+{
+    unsigned long  context;
+    struct tcp_keepalive *keepalive;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    keepalive = &ts->keepalive;
+
+    if (keepalive->enabled) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+    keepalive->probes = rt_tcp_keepalive_probes;
+
+    hrtimer_init(&keepalive->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
+    keepalive->timer.function = &rt_tcp_keepalive_timer;
+
+    rt_tcp_keepalive_start(ts);
+
+    keepalive->enabled = 1;
+
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+}
+#endif
+
+static void rt_tcp_keepalive_disable(struct tcp_socket *ts)
+{
+    struct tcp_keepalive *keepalive;
+
+    keepalive = &ts->keepalive;
+
+    if (!keepalive->enabled) {
+	return;
+    }
+
+    rt_tcp_keepalive_stop(ts);
+    hrtimer_cancel(&keepalive->timer);
+
+    keepalive->enabled = 0;
+}
+
+static void rt_tcp_keepalive_feed(struct tcp_socket *ts)
+{
+    unsigned long  context;
+    struct tcp_keepalive *keepalive;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    keepalive = &ts->keepalive;
+
+    if (ts->tcp_state == TCP_ESTABLISHED && ts->keepalive.enabled) {
+
+	keepalive->probes = rt_tcp_keepalive_probes;
+
+	/* Restart keepalive timer */
+	hrtimer_cancel(&keepalive->timer);
+	hrtimer_start(&keepalive->timer, rt_tcp_keepalive_timeout,
+			 HRTIMER_MODE_REL_HARD);
+
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+    } else {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+    }
+}
+
+static int rt_tcp_socket_invalidate(struct tcp_socket *ts, u8 to_state)
+{
+    int signal = ts->is_valid;
+
+    ts->tcp_state = to_state;
+
+    /*
+      multiple invalidation could happen without fuss,
+      see rt_tcp_close(), rt_tcp_rcv(), timeout expiration etc.
+    */
+    if (ts->is_valid) {
+	ts->is_valid = 0;
+
+	if (ts->keepalive.enabled) {
+	    rt_tcp_keepalive_stop(ts);
+	}
+    }
+
+    return signal;
+}
+
+static void rt_tcp_socket_invalidate_signal(struct tcp_socket *ts)
+{
+    /* awake all readers and writers destroying events */
+    rtdm_sem_destroy(&ts->sock.pending_sem);
+    rtdm_event_destroy(&ts->send_evt);
+}
+
+static void rt_tcp_socket_validate(struct tcp_socket *ts)
+{
+    ts->tcp_state = TCP_ESTABLISHED;
+
+    ts->is_valid = 1;
+
+    if (ts->keepalive.enabled) {
+	rt_tcp_keepalive_start(ts);
+    }
+
+    rtdm_event_init(&ts->send_evt, 0);
+}
+
+/***
+ *  rt_tcp_retransmit_handler - timerwheel handler to process a retransmission
+ *  @data: pointer to a rttcp socket structure
+ */
+static void rt_tcp_retransmit_handler(void *data)
+{
+    struct tcp_socket *ts = (struct tcp_socket *)data;
+    struct rtskb* skb;
+    unsigned long context;
+    int signal;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    if (unlikely(rtskb_queue_empty(&ts->retransmit_queue))) {
+	/* handled, but retransmission queue is empty */
+	raw_spin_lock_irqsave(&ts->socket_lock, context);
+	printk(KERN_ERR "rttcp: bug in RT TCP retransmission routine\n");
+	return;
+    }
+
+    if (ts->tcp_state == TCP_CLOSE) {
+	/* socket is already closed */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+    if (ts->timer_state) {
+	/* more tries */
+	ts->timer_state--;
+	timerwheel_add_timer(&ts->timer, rt_tcp_retransmission_timeout);
+
+	/* warning, rtskb_clone is under lock */
+	skb = rtskb_clone(ts->retransmit_queue.first, &ts->sock.skb_pool);
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	/* BUG, window changes are not respected */
+	if (unlikely(rtdev_xmit(skb)) != 0) {
+	    kfree_rtskb(skb);
+	    printk(KERN_ERR "rttcp: packet retransmission from timer failed\n");
+	}
+    } else {
+	ts->timer_state = max_retransmits;
+
+	/* report about connection lost */
+	signal = rt_tcp_socket_invalidate(ts, TCP_CLOSE);
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	if (signal)
+	    rt_tcp_socket_invalidate_signal(ts);
+
+	/* retransmission queue will be cleaned up in rt_tcp_socket_destruct */
+	printk(KERN_ERR "rttcp: connection is lost by NACK timeout\n");
+    }
+}
+
+/***
+ *  rt_tcp_retransmit_ack - remove skbs from retransmission queue on ACK
+ *  @ts: rttcp socket
+ *  @ack_seq: received ACK sequence value
+ */
+static void rt_tcp_retransmit_ack(struct tcp_socket *ts, u32 ack_seq)
+{
+    struct rtskb* skb;
+    unsigned long  context;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    /*
+      ACK, but retransmission queue is empty
+      This could happen on repeated ACKs
+    */
+    if (rtskb_queue_empty(&ts->retransmit_queue)) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+    /*
+      Check ts->nacked_first value firstly to ensure that
+      skb for retransmission is present in the queue, otherwise
+      retransmission queue will be drained completely
+    */
+    if (!rt_tcp_before(ts->nacked_first, ack_seq)) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+    if (timerwheel_remove_timer(&ts->timer) != 0) {
+	/* already timed out */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+ dequeue_loop:
+    if (ts->tcp_state == TCP_CLOSE) {
+	/* warn about queue safety in race with anyone,
+	   who closes the socket */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+    if ((skb = __rtskb_dequeue(&ts->retransmit_queue)) == NULL) {
+	ts->timer_state = max_retransmits;
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return;
+    }
+
+    if (rt_tcp_before(ts->nacked_first, ack_seq)) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	kfree_rtskb(skb);
+	raw_spin_lock_irqsave(&ts->socket_lock, context);
+	goto dequeue_loop;
+    }
+
+    /* Put NACKed skb back to queue */
+    /* BUG, need to respect half-acknowledged packets */
+    ts->nacked_first = ntohl(skb->h.th->seq) + 1;
+
+    __rtskb_queue_head(&ts->retransmit_queue, skb);
+
+    /* Have more packages in retransmission queue, restart the timer */
+    timerwheel_add_timer(&ts->timer, rt_tcp_retransmission_timeout);
+
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+}
+
+/***
+ *  rt_tcp_retransmit_send - enqueue a skb to retransmission queue (not locked)
+ *  @ts: rttcp socket
+ *  @skb: a copied skb for enqueueing
+ */
+static void rt_tcp_retransmit_send(struct tcp_socket *ts, struct rtskb *skb)
+{
+    if (rtskb_queue_empty(&ts->retransmit_queue)) {
+	/* retransmission queue is empty */
+	ts->nacked_first = ntohl(skb->h.th->seq) + 1;
+
+	__rtskb_queue_tail(&ts->retransmit_queue, skb);
+
+	timerwheel_add_timer(&ts->timer, rt_tcp_retransmission_timeout);
+    } else {
+	/* retransmission queue is not empty */
+	__rtskb_queue_tail(&ts->retransmit_queue, skb);
+    }
+}
+
+static int rt_ip_build_frame(struct rtskb *skb, struct rtsocket *sk,
+			     struct dest_route *rt, struct iphdr *iph)
+{
+    int             ret;
+    struct          rtnet_device *rtdev = rt->rtdev;
+
+
+    RTNET_ASSERT(rtdev->hard_header, return -EBADF;);
+
+    if (!rtdev_reference(rt->rtdev))
+	return -EIDRM;
+
+    iph->ihl      = 5;    /* 20 byte header only - no TCP options */
+
+    skb->nh.iph   = iph;
+
+    iph->version  = 4;
+    iph->tos      = sk->prot.inet.tos;
+    iph->tot_len  = htons(skb->len); /* length of IP header and IP payload */
+    iph->id       = htons(0x00); /* zero IP frame id */
+    iph->frag_off = htons(IP_DF); /* and no more frames */
+    iph->ttl      = 255;
+    iph->protocol = sk->protocol;
+    iph->saddr    = rtdev->local_ip;
+    iph->daddr    = rt->ip;
+    iph->check    = 0; /* required to compute correct checksum */
+    iph->check    = ip_fast_csum((u8 *)iph, 5 /*iph->ihl*/);
+
+    ret = rtdev->hard_header(skb, rtdev, ETH_P_IP, rt->dev_addr,
+			     rtdev->dev_addr, skb->len);
+    rtdev_dereference(rt->rtdev);
+
+    if (ret != rtdev->hard_header_len) {
+	printk(KERN_ERR "rttcp: rt_ip_build_frame: error on lower level\n");
+	return -EINVAL;
+    }
+
+    return 0;
+}
+
+static void rt_tcp_build_header(struct tcp_socket *ts, struct rtskb *skb,
+				__be32 flags, u8 is_keepalive)
+{
+    u32 wcheck;
+    u8 tcphdrlen = 20;
+    u8 iphdrlen  = 20;
+    struct tcphdr *th;
+
+    th = skb->h.th;
+    th->source  = ts->sport;
+    th->dest    = ts->dport;
+
+    th->seq = htonl(ts->sync.seq);
+
+    if (unlikely(is_keepalive))
+	th->seq--;
+
+    th->ack_seq = htonl(ts->sync.ack_seq);
+    th->window  = htons(ts->sync.window);
+
+    rt_tcp_set_flags(th, flags);
+
+    th->doff = tcphdrlen >> 2; /* No options for now */
+    th->res1 = 0;
+    th->check   = 0;
+    th->urg_ptr = 0;
+
+    /* compute checksum */
+    wcheck = csum_partial(th, tcphdrlen, 0);
+
+    if (skb->len - tcphdrlen - iphdrlen) {
+	wcheck = csum_partial(skb->data + tcphdrlen + iphdrlen,
+			      skb->len - tcphdrlen - iphdrlen, wcheck);
+    }
+
+    th->check = tcp_v4_check(skb->len - iphdrlen, ts->saddr, ts->daddr, wcheck);
+}
+
+static int
+rt_tcp_segment(struct dest_route *rt, struct tcp_socket *ts, __be32 flags,
+	       u32 data_len, u8 *data_ptr, u8 is_keepalive)
+{
+    struct tcphdr       *th;
+    struct rtsocket     *sk    = &ts->sock;
+    struct rtnet_device *rtdev = rt->rtdev;
+    struct rtskb        *skb;
+    struct iphdr        *iph;
+    struct rtskb* cloned_skb;
+    unsigned long  context;
+
+    int ret;
+
+    u32 hh_len = (rtdev->hard_header_len + 15) & ~15;
+    u32 prio = (volatile unsigned int)sk->priority;
+    u32 mtu = rtdev->get_mtu(rtdev, prio);
+
+    u8 *data = NULL;
+
+    if ((skb = alloc_rtskb(mtu + hh_len + 15, &sk->skb_pool)) == NULL) {
+	printk(KERN_ERR "rttcp: no more elements in skb_pool for allocation\n");
+	return -ENOBUFS;
+    }
+
+    /* rtskb_reserve(skb, hh_len + 20); */
+    rtskb_reserve(skb, hh_len);
+
+    iph = (struct iphdr*)rtskb_put(skb, 20); /* length of IP header */
+    skb->nh.iph = iph;
+
+    th = (struct tcphdr*)rtskb_put(skb, 20); /* length of TCP header */
+    skb->h.th = th;
+
+    if (data_len) { /* check for available place */
+	data = (u8*)rtskb_put(skb, data_len); /* length of TCP payload */
+	if (!memcpy(data, (void*)data_ptr, data_len)) {
+	    ret = -EFAULT;
+	    goto error;
+	}
+    }
+
+    /* used local phy MTU value */
+    if (data_len > mtu)
+	data_len = mtu;
+
+    skb->rtdev    = rtdev;
+    skb->priority = prio;
+
+    /* do not validate socket connection on xmit
+       this should be done at upper level */
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+    rt_tcp_build_header(ts, skb, flags, is_keepalive);
+
+    if ((ret = rt_ip_build_frame(skb, sk, rt, iph)) != 0) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	goto error;
+    }
+
+    /* add rtskb entry to the socket retransmission queue */
+    if (ts->tcp_state != TCP_CLOSE &&
+	((flags & (TCP_FLAG_SYN|TCP_FLAG_FIN)) || data_len)) {
+	/* rtskb_clone below is called under lock, this is an admission,
+	   because for now there is no rtskb copy by reference */
+	cloned_skb = rtskb_clone(skb, &ts->sock.skb_pool);
+	if (!cloned_skb) {
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    printk(KERN_ERR "rttcp: cann't clone skb\n");
+	    ret = -ENOMEM;
+	    goto error;
+	}
+
+	rt_tcp_retransmit_send(ts, cloned_skb);
+    }
+
+    /* need to update sync here, because it is safe way in
+       comparison with races on fast ACK response */
+    if (flags & (TCP_FLAG_FIN|TCP_FLAG_SYN))
+	ts->sync.seq++;
+
+    ts->sync.seq += data_len;
+    ts->sync.dst_window -= data_len;
+
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    /* ignore return value from rtdev_xmit */
+    /* the packet was enqueued and on error will be retransmitted later */
+    /* on critical error after retransmission timeout the connection will
+       be closed by connection lost */
+    rtdev_xmit(skb);
+
+    return data_len;
+
+ error:
+    kfree_rtskb(skb);
+    return ret;
+}
+
+static int rt_tcp_send(struct tcp_socket *ts, __be32 flags)
+{
+    struct dest_route rt;
+    int ret;
+
+    /*
+     * We may not have a route yet during setup. But once it is set, it stays
+     * until the socket died.
+     */
+    if (likely(ts->rt.rtdev)) {
+	ret = rt_tcp_segment(&ts->rt, ts, flags, 0, NULL, 0);
+    } else {
+	ret = rt_ip_route_output(&rt, ts->daddr, ts->saddr);
+	if (ret == 0) {
+	    ret = rt_tcp_segment(&rt, ts, flags, 0, NULL, 0);
+	    rtdev_dereference(rt.rtdev);
+	}
+    }
+    if (ret < 0)
+	printk(KERN_ERR "rttcp: can't send a packet: err %d\n", -ret);
+    return ret;
+}
+
+#ifdef YET_UNUSED
+static enum hrtimer_restart rt_tcp_keepalive_timer(struct hrtimer *timer)
+{
+    unsigned long  context;
+    struct tcp_keepalive *keepalive = container_of(timer,
+						   struct tcp_keepalive, timer);
+
+    struct tcp_socket *ts = container_of(keepalive, struct tcp_socket, keepalive);
+    int signal = 0;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    if (keepalive->probes) {
+	/* Send a probe */
+	if (rt_tcp_segment(&ts->rt, ts, 0, 0, NULL, 1) < 0) {
+	    /* data receiving and sending is not possible anymore */
+	    signal = rt_tcp_socket_invalidate(ts, TCP_TIME_WAIT);
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	}
+
+	keepalive->probes--;
+	rtdm_timer_start_in_handler(&keepalive->timer,
+				    rt_tcp_keepalive_intvl,
+				    0, RTDM_TIMERMODE_RELATIVE);
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+    } else {
+	/* data receiving and sending is not possible anymore */
+
+	signal = rt_tcp_socket_invalidate(ts, TCP_TIME_WAIT);
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+    }
+
+    if (signal)
+	rt_tcp_socket_invalidate_signal(ts);
+
+    return HRTIMER_NORESTART;
+}
+#endif
+
+static inline u32 rt_tcp_initial_seq(void)
+{
+    uint64_t clock_val = ktime_get();
+    return (u32)(clock_val ^ (clock_val >> 32));
+}
+
+/***
+ *  rt_tcp_dest_socket
+ */
+static struct rtsocket *rt_tcp_dest_socket(struct rtskb *skb)
+{
+    struct tcphdr    *th = skb->h.th;
+
+    u32 saddr = skb->nh.iph->saddr;
+    u32 daddr = skb->nh.iph->daddr;
+    u32 sport = th->source;
+    u32 dport = th->dest;
+
+    u32 data_len;
+
+    if (tcp_v4_check(skb->len, saddr, daddr,
+		     csum_partial(skb->data, skb->len, 0))) {
+	printk(KERN_ERR "rttcp: invalid TCP packet checksum, dropped\n");
+	return NULL; /* Invalid checksum, drop the packet */
+    }
+
+    /* find the destination socket */
+    if ((skb->sk = rt_tcp_v4_lookup(daddr, dport)) == NULL) {
+	/*
+	  printk(KERN_ERR "Not found addr:0x%08x, port: 0x%04x\n", daddr, dport);
+	*/
+	if (!th->rst) {
+	    /* No listening socket found, send RST|ACK */
+	    rst_socket.saddr = daddr;
+	    rst_socket.daddr = saddr;
+	    rst_socket.sport = dport;
+	    rst_socket.dport = sport;
+
+	    data_len = skb->len - (th->doff << 2);
+
+	    rst_socket.sync.seq = 0;
+	    rst_socket.sync.ack_seq = rt_tcp_compute_ack_seq(th, data_len);
+
+	    if (rt_ip_route_output(&rst_socket.rt, daddr, saddr) == 0) {
+		rt_socket_reference(&rst_socket.sock);
+		rt_tcp_send(&rst_socket, TCP_FLAG_ACK|TCP_FLAG_RST);
+		rtdev_dereference(rst_socket.rt.rtdev);
+	    }
+	}
+    }
+
+    return skb->sk;
+}
+
+static void rt_tcp_window_update(struct tcp_socket *ts, u16 window)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    if (ts->sync.dst_window) {
+	ts->sync.dst_window = window;
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	if (!window) {
+	    /* clear send event status */
+	    rtdm_event_clear(&ts->send_evt);
+	}
+    } else {
+	if (window) {
+	    ts->sync.dst_window = window;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    /* set send event status */
+	    rtdm_event_signal(&ts->send_evt);
+	} else {
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	}
+    }
+}
+
+
+/***
+ *  rt_tcp_rcv
+ */
+static void rt_tcp_rcv(struct rtskb *skb)
+{
+    unsigned long  context;
+    struct tcp_socket* ts;
+    struct tcphdr* th = skb->h.th;
+    unsigned int data_len = skb->len - (th->doff << 2);
+    u32 seq = ntohl(th->seq);
+    int signal;
+
+    ts = container_of(skb->sk, struct tcp_socket, sock);
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+#ifdef CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION
+    if (ts->error_rate > 0) {
+	if ((ts->packet_counter++ % error_rate) < ts->multi_error) {
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    goto drop;
+	}
+    }
+#endif /* CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION */
+
+    /* Check for daddr/dport correspondence to values stored in
+       selected socket from hash */
+    if (ts->tcp_state != TCP_LISTEN &&
+	(ts->daddr != skb->nh.iph->saddr || ts->dport != skb->h.th->source)) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	goto drop;
+    }
+
+    /* Check if it is a keepalive probe */
+    if (ts->sync.ack_seq == (seq + 1) &&
+	ts->tcp_state == TCP_ESTABLISHED) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	rt_tcp_send(ts, TCP_FLAG_ACK);
+	goto feed;
+    }
+
+    if (ts->tcp_state == TCP_SYN_SENT) {
+	ts->sync.ack_seq = rt_tcp_compute_ack_seq(th, data_len);
+
+	if (th->syn && th->ack) {
+	    rt_tcp_socket_validate(ts);
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    rtdm_event_signal(&ts->conn_evt);
+	    /* Send ACK */
+	    rt_tcp_send(ts, TCP_FLAG_ACK);
+	    goto feed;
+	}
+
+	ts->tcp_state = TCP_CLOSE;
+	ts->sync.seq = ntohl(th->ack_seq);
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	/* Send RST|ACK */
+	rtdm_event_signal(&ts->conn_evt);
+	rt_tcp_send(ts, TCP_FLAG_RST|TCP_FLAG_ACK);
+	goto drop;
+    }
+
+    /* Check for SEQ correspondence to determine the connection relevance */
+
+    /* OR-list of conditions to be satisfied:
+     *
+     * th->ack && rt_tcp_after(ts->nacked_first, ntohl(th->ack_seq))
+     * th->ack && th->rst && ...
+     * th->syn && (ts->tcp_state == TCP_LISTEN ||
+		   ts->tcp_state == TCP_SYN_SENT)
+     * rt_tcp_after(seq, ts->sync.ack_seq) &&
+	   rt_tcp_before(seq, ts->sync.ack_seq + ts->sync.window)
+     */
+
+    if ((rt_tcp_after(seq, ts->sync.ack_seq) &&
+	 rt_tcp_before(seq, ts->sync.ack_seq + ts->sync.window)) ||
+	th->rst ||
+	(th->syn && (ts->tcp_state == TCP_LISTEN ||
+		     ts->tcp_state == TCP_SYN_SENT))) {
+	/* everything is ok */
+    } else if (rt_tcp_after(seq, ts->sync.ack_seq - data_len)) {
+	/* retransmission of data we already acked */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	rt_tcp_send(ts, TCP_FLAG_ACK);
+	goto drop;
+    } else {
+	/* drop forward ack */
+	if (th->ack &&
+	    /* but reset ack from old connection */
+	    ts->tcp_state == TCP_ESTABLISHED) {
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    printk(KERN_ERR "rttcp: dropped unappropriate ACK packet %u\n",
+			ts->sync.ack_seq);
+	    goto drop;
+	}
+
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	printk(KERN_ERR "rttcp: sequence number is not in window, "
+		    "dropped (failed: %u <= %u <= %u)\n",
+		    ts->sync.ack_seq, seq, ts->sync.ack_seq + ts->sync.window);
+
+	/* That's a forced RST for a lost connection */
+	rst_socket.saddr = skb->nh.iph->daddr;
+	rst_socket.daddr = skb->nh.iph->saddr;
+	rst_socket.sport = th->dest;
+	rst_socket.dport = th->source;
+
+	rst_socket.sync.seq = ntohl(th->ack_seq);
+	rst_socket.sync.ack_seq = rt_tcp_compute_ack_seq(th, data_len);
+
+	if (rt_ip_route_output(&rst_socket.rt, rst_socket.daddr,
+			       rst_socket.saddr) == 0) {
+	    rt_socket_reference(&rst_socket.sock);
+	    rt_tcp_send(&rst_socket, TCP_FLAG_RST|TCP_FLAG_ACK);
+	    rtdev_dereference(rst_socket.rt.rtdev);
+	}
+	goto drop;
+    }
+
+    if (th->rst) {
+	if (ts->tcp_state == TCP_SYN_RECV) {
+	    ts->tcp_state = TCP_LISTEN;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    goto drop;
+	} else {
+	    /* Drop our half-open connection, peer obviously went away. */
+	    signal = rt_tcp_socket_invalidate(ts, TCP_CLOSE);
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	    if (signal)
+		rt_tcp_socket_invalidate_signal(ts);
+
+	    goto drop;
+	}
+    }
+
+    ts->sync.ack_seq = rt_tcp_compute_ack_seq(th, data_len);
+
+    if (th->fin) {
+	if (ts->tcp_state == TCP_ESTABLISHED) {
+	    /* Send ACK */
+	    signal = rt_tcp_socket_invalidate(ts, TCP_CLOSE_WAIT);
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	    if (signal)
+		rt_tcp_socket_invalidate_signal(ts);
+
+	    rt_tcp_send(ts, TCP_FLAG_ACK);
+	    goto feed;
+	} else if ((ts->tcp_state == TCP_FIN_WAIT1 && th->ack) ||
+		   ts->tcp_state == TCP_FIN_WAIT2) {
+	    /* Send ACK */
+	    ts->tcp_state = TCP_TIME_WAIT;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    rt_tcp_send(ts, TCP_FLAG_ACK);
+	    /* data receiving is not possible anymore */
+	    rtdm_sem_destroy(&ts->sock.pending_sem);
+	    goto feed;
+	} else if (ts->tcp_state == TCP_FIN_WAIT1) {
+	    /* Send ACK */
+	    ts->tcp_state = TCP_CLOSING;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    rt_tcp_send(ts, TCP_FLAG_ACK);
+	    /* data receiving is not possible anymore */
+	    rtdm_sem_destroy(&ts->sock.pending_sem);
+	    goto feed;
+	} else {
+	    /* just drop it */
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    goto drop;
+	}
+    }
+
+    if (th->syn) {
+	/* Need to differentiate LISTEN socket from ESTABLISHED one */
+	/* Both of them have the same sport/saddr, but different dport/daddr */
+	/* dport is unknown if it is the first connection of n */
+
+	if (ts->tcp_state == TCP_LISTEN) {
+	    /* Need to store ts->seq while sending SYN earlier */
+	    /* The socket shall be in TCP_LISTEN state */
+
+	    /* safe to update ts->saddr here due to a single task for
+	       rt_tcp_rcv() and rt_tcp_dest_socket() callers */
+	    ts->saddr = skb->nh.iph->daddr;
+
+	    ts->daddr = skb->nh.iph->saddr;
+	    ts->dport = th->source;
+	    ts->sync.seq = rt_tcp_initial_seq();
+	    ts->sync.window = 4096;
+	    ts->tcp_state = TCP_SYN_RECV;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	    /* Send SYN|ACK */
+	    rt_tcp_send(ts, TCP_FLAG_SYN|TCP_FLAG_ACK);
+	    goto drop;
+	}
+
+	/* Send RST|ACK */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	rt_tcp_send(ts, TCP_FLAG_RST|TCP_FLAG_ACK);
+	goto drop;
+    }
+
+    /* ACK received without SYN, FIN or RST flags */
+    if (th->ack) {
+	/* Check ack sequence */
+	if (rt_tcp_before(ts->sync.seq + 1, ntohl(th->ack_seq))) {
+	    printk(KERN_ERR "rttcp: unexpected ACK %u %u %u\n",
+			ts->sync.seq,
+			ts->nacked_first,
+			ntohl(th->ack_seq));
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    goto drop;
+	}
+
+	if (ts->tcp_state == TCP_LAST_ACK) {
+	    /* close connection and free socket data */
+	    ts->tcp_state = TCP_CLOSE;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    /* socket destruction will be done on close() */
+	    goto drop;
+	} else if (ts->tcp_state == TCP_FIN_WAIT1) {
+	    ts->tcp_state = TCP_FIN_WAIT2;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    goto feed;
+	} else if (ts->tcp_state == TCP_SYN_RECV) {
+	    rt_tcp_socket_validate(ts);
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    rtdm_event_signal(&ts->conn_evt);
+	    goto feed;
+	} else if (ts->tcp_state == TCP_CLOSING) {
+	    ts->tcp_state = TCP_TIME_WAIT;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    /* socket destruction will be done on close() */
+	    goto feed;
+	}
+    }
+
+    if (ts->tcp_state != TCP_ESTABLISHED) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	goto drop;
+    }
+
+    if (data_len == 0) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	goto feed;
+    }
+
+    /* Send ACK */
+    ts->sync.window -= data_len;
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+    rt_tcp_send(ts, TCP_FLAG_ACK);
+
+    rtskb_queue_tail(&skb->sk->incoming, skb);
+    up(&ts->sock.pending_sem);
+
+    /* inform retransmission subsystem about arrived ack */
+    if (th->ack) {
+	rt_tcp_retransmit_ack(ts, ntohl(th->ack_seq));
+    }
+
+    rt_tcp_keepalive_feed(ts);
+    rt_tcp_window_update(ts, ntohs(th->window));
+
+    return;
+
+ feed:
+    /* inform retransmission subsystem about arrived ack */
+    if (th->ack) {
+	rt_tcp_retransmit_ack(ts, ntohl(th->ack_seq));
+    }
+
+    rt_tcp_keepalive_feed(ts);
+    rt_tcp_window_update(ts, ntohs(th->window));
+
+ drop:
+    kfree_rtskb(skb);
+    return;
+}
+
+
+/***
+ *  rt_tcp_rcv_err
+ */
+static void rt_tcp_rcv_err(struct rtskb *skb)
+{
+    printk(KERN_ERR "rttcp: rt_tcp_rcv err\n");
+}
+
+static int rt_tcp_window_send(struct tcp_socket *ts, u32 data_len,
+			      u8 *data_ptr)
+{
+    u32 dst_window = ts->sync.dst_window;
+    int ret;
+
+    if (data_len > dst_window)
+	data_len = dst_window;
+
+    if ((ret = rt_tcp_segment(&ts->rt, ts, TCP_FLAG_ACK,
+			      data_len, data_ptr, 0)) < 0) {
+	printk(KERN_ERR "rttcp: cann't send a packet: err %d\n", -ret);
+	return ret;
+    }
+
+    return ret;
+}
+
+
+static int rt_tcp_socket_create(struct tcp_socket* ts)
+{
+    unsigned long  context;
+    int             i;
+    int             index;
+    struct rtsocket *sock = &ts->sock;
+
+    sock->prot.inet.saddr = INADDR_ANY;
+    sock->prot.inet.state = TCP_CLOSE;
+    sock->prot.inet.tos   = 0;
+    /*
+      printk(KERN_ERR "rttcp: rt_tcp_socket_create 0x%p\n", ts);
+    */
+    raw_spin_lock_init(&ts->socket_lock);
+
+    ts->rt.rtdev = NULL;
+
+    ts->tcp_state = TCP_CLOSE;
+
+    ts->is_accepting = 0;
+    ts->is_accepted  = 0;
+    ts->is_binding   = 0;
+    ts->is_bound     = 0;
+    ts->is_valid     = 0;
+    ts->is_closed    = 0;
+
+    ts->sk_sndtimeo = RTDM_TIMEOUT_INFINITE;
+
+    rtdm_event_init(&ts->conn_evt, 0);
+
+    ts->keepalive.enabled = 0;
+
+    ts->timer_state = max_retransmits;
+    timerwheel_init_timer(&ts->timer, rt_tcp_retransmit_handler, ts);
+    rtskb_queue_init(&ts->retransmit_queue);
+
+#ifdef CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION
+    ts->packet_counter = counter_start;
+    ts->error_rate = error_rate;
+    ts->multi_error = multi_error;
+#endif /* CONFIG_RTNET_RTIPV4_TCP_ERROR_INJECTION */
+
+    raw_spin_lock_irqsave(&tcp_socket_base_lock, context);
+
+    /* enforce maximum number of TCP sockets */
+    if (free_ports == 0) {
+	raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+	return -EAGAIN;
+    }
+    free_ports--;
+
+    /* find free auto-port in bitmap */
+    for (i = 0; i < RT_PORT_BITMAP_WORDS; i++)
+	if (port_bitmap[i] != (unsigned long)-1)
+	    break;
+    index = ffz(port_bitmap[i]);
+    set_bit(index, &port_bitmap[i]);
+    index += i*32;
+    sock->prot.inet.reg_index = index;
+    sock->prot.inet.sport     = index + tcp_auto_port_start;
+
+    /* register TCP socket */
+    port_registry[index] = ts;
+    port_hash_insert(ts, INADDR_ANY, sock->prot.inet.sport);
+
+    raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+
+    return 0;
+}
+
+/***
+ *  rt_tcp_socket - create a new TCP-Socket
+ *  @s: socket
+ */
+static int rt_tcp_socket(struct rtsocket **psock)
+{
+    struct tcp_socket *ts;
+    int ret;
+
+    ts = kzalloc(sizeof(struct tcp_socket), GFP_KERNEL);
+    if(!ts)
+	    return -ENOMEM;
+    if ((ret = rt_socket_init(&ts->sock, IPPROTO_TCP)) != 0)
+	return ret;
+
+    if ((ret = rt_tcp_socket_create(ts)) != 0) {
+	rt_socket_cleanup(&ts->sock);
+	kfree(ts);
+	return ret;
+    }
+
+    if(psock)
+    	*psock = &ts->sock;
+
+    return ret;
+}
+
+
+static int rt_tcp_dispatched_packet_send(struct rt_proc_call *call)
+{
+    int ret;
+    struct rt_tcp_dispatched_packet_send_cmd *cmd;
+
+    cmd = rtpc_get_priv(call, struct rt_tcp_dispatched_packet_send_cmd);
+    ret = rt_tcp_send(cmd->ts, cmd->flags);
+
+    return ret;
+}
+
+/***
+ *  rt_tcp_socket_destruct
+ *  this function requires non realtime context
+ */
+static void rt_tcp_socket_destruct(struct tcp_socket* ts)
+{
+    unsigned long  context;
+    struct rtskb    *skb;
+    int             index;
+    int             signal;
+    struct rtsocket *sock = &ts->sock;
+
+    /*
+      printk(KERN_ERR "rttcp: rt_tcp_socket_destruct 0x%p\n", ts);
+    */
+
+    raw_spin_lock_irqsave(&tcp_socket_base_lock, context);
+    if (sock->prot.inet.reg_index >= 0) {
+	index = sock->prot.inet.reg_index;
+
+	clear_bit(index % BITS_PER_LONG, &port_bitmap[index / BITS_PER_LONG]);
+	port_hash_del(port_registry[index]);
+	free_ports++;
+	sock->prot.inet.reg_index = -1;
+    }
+    raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    signal = rt_tcp_socket_invalidate(ts, TCP_CLOSE);
+
+    rt_tcp_keepalive_disable(ts);
+
+    sock->prot.inet.state = TCP_CLOSE;
+
+    /* dereference rtdev */
+    if (ts->rt.rtdev != NULL) {
+	rtdev_dereference(ts->rt.rtdev);
+	ts->rt.rtdev = NULL;
+    }
+
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    if (signal)
+	rt_tcp_socket_invalidate_signal(ts);
+
+    rtdm_event_destroy(&ts->conn_evt);
+
+    /* cleanup already collected fragments */
+    rt_ip_frag_invalidate_socket(sock);
+
+    /* free packets in incoming queue */
+    while ((skb = rtskb_dequeue(&sock->incoming)) != NULL)
+	kfree_rtskb(skb);
+
+    /* ensure that the timer is no longer running */
+    timerwheel_remove_timer_sync(&ts->timer);
+
+    /* free packets in retransmission queue */
+    while ((skb = __rtskb_dequeue(&ts->retransmit_queue)) != NULL)
+	kfree_rtskb(skb);
+}
+
+/***
+ *  rt_tcp_close
+ */
+static void rt_tcp_close(struct rtsocket *sock)
+{
+    struct tcp_socket* ts = container_of(sock, struct tcp_socket, sock);
+    struct rt_tcp_dispatched_packet_send_cmd send_cmd;
+    unsigned long context;
+    int signal = 0;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    ts->is_closed = 1;
+
+    if (ts->tcp_state == TCP_ESTABLISHED ||
+	ts->tcp_state == TCP_SYN_RECV) {
+	/* close() from ESTABLISHED */
+	send_cmd.ts = ts;
+	send_cmd.flags = TCP_FLAG_FIN|TCP_FLAG_ACK;
+	signal = rt_tcp_socket_invalidate(ts, TCP_FIN_WAIT1);
+
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	rtpc_dispatch_call(rt_tcp_dispatched_packet_send, 0, &send_cmd,
+			   sizeof(send_cmd), NULL, NULL);
+	/* result is ignored */
+
+	/* Give the peer some time to reply to our FIN. */
+	msleep(1000);
+    } else if (ts->tcp_state == TCP_CLOSE_WAIT) {
+	/* Send FIN in CLOSE_WAIT */
+	send_cmd.ts = ts;
+	send_cmd.flags = TCP_FLAG_FIN|TCP_FLAG_ACK;
+	signal = rt_tcp_socket_invalidate(ts, TCP_LAST_ACK);
+
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	rtpc_dispatch_call(rt_tcp_dispatched_packet_send, 0, &send_cmd,
+			   sizeof(send_cmd), NULL, NULL);
+	/* result is ignored */
+
+	/* Give the peer some time to reply to our FIN. */
+	msleep(1000);
+    } else {
+	/*
+	  rt_tcp_socket_validate() has not been called at all,
+	  hence socket state is TCP_SYN_SENT or TCP_LISTEN,
+	  or socket is in one of close states,
+	  hence rt_tcp_socket_invalidate() was called,
+	  but close() is called at first time
+	*/
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+    }
+
+    if (signal)
+	rt_tcp_socket_invalidate_signal(ts);
+
+    rt_tcp_socket_destruct(ts);
+
+    rt_socket_cleanup(&ts->sock);
+    
+    kfree(ts);
+}
+
+/***
+ *  rt_tcp_bind - bind socket to local address
+ *  @s:     socket
+ *  @addr:  local address
+ */
+static int rt_tcp_bind(struct tcp_socket *ts, const struct sockaddr *addr,
+		       socklen_t addrlen)
+{
+    struct sockaddr_in  *usin = (struct sockaddr_in *)addr;
+    unsigned long      context;
+    int                 index;
+    int                 bound = 0;
+    int                 ret = 0;
+
+
+    if ((addrlen < (int)sizeof(struct sockaddr_in)) ||
+	((usin->sin_port & tcp_auto_port_mask) == tcp_auto_port_start))
+	return -EINVAL;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+    if (ts->tcp_state != TCP_CLOSE || ts->is_bound || ts->is_binding) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EINVAL;
+    }
+
+    ts->is_binding = 1;
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    raw_spin_lock_irqsave(&tcp_socket_base_lock, context);
+
+    if ((index = ts->sock.prot.inet.reg_index) < 0) {
+	/* socket is destroyed */
+	ret = -EBADF;
+	goto unlock_out;
+    }
+
+    port_hash_del(ts);
+    if (port_hash_insert(ts, usin->sin_addr.s_addr,
+			 usin->sin_port ?: index + tcp_auto_port_start)) {
+	port_hash_insert(ts, ts->saddr, ts->sport);
+
+	ret = -EADDRINUSE;
+	goto unlock_out;
+    }
+
+    bound = 1;
+
+ unlock_out:
+    raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+    ts->is_bound = bound;
+    ts->is_binding = 0;
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    return ret;
+}
+
+
+/***
+ *  rt_tcp_connect
+ */
+static int rt_tcp_connect(struct tcp_socket *ts, const struct sockaddr *serv_addr,
+			  socklen_t addrlen)
+{
+    struct sockaddr_in  *usin = (struct sockaddr_in *) serv_addr;
+    struct dest_route   rt;
+    unsigned long      context;
+    int ret;
+
+    if (addrlen < (int)sizeof(struct sockaddr_in))
+	return -EINVAL;
+
+    if (usin->sin_family != AF_INET)
+	return -EAFNOSUPPORT;
+
+    ret = rt_ip_route_output(&rt, usin->sin_addr.s_addr, ts->saddr);
+    if (ret < 0) {
+	/* no route to host */
+	return -ENETUNREACH;
+    }
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    if (ts->is_closed) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	ret = -EBADF;
+	goto err_deref;
+    }
+
+    if (ts->tcp_state != TCP_CLOSE || ts->is_binding) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	ret = -EINVAL;
+	goto err_deref;
+    }
+
+    if (ts->rt.rtdev == NULL)
+	memcpy(&ts->rt, &rt, sizeof(rt));
+    else
+	rtdev_dereference(rt.rtdev);
+
+    ts->saddr = rt.rtdev->local_ip;
+
+    ts->daddr = usin->sin_addr.s_addr;
+    ts->dport = usin->sin_port;
+
+    ts->sync.seq = rt_tcp_initial_seq();
+    ts->sync.ack_seq = 0;
+    ts->sync.window = 4096;
+    ts->sync.dst_window = 0;
+
+    ts->tcp_state = TCP_SYN_SENT;
+
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    /* Complete three-way handshake */
+    ret = rt_tcp_send(ts, TCP_FLAG_SYN);
+    if (ret < 0) {
+	printk(KERN_ERR "rttcp: cann't send SYN\n");
+	return ret;
+    }
+
+    ret = rtdm_event_timedwait(&ts->conn_evt, rt_tcp_connection_timeout, NULL);
+    if(ret == 0)
+	    return -ETIMEDOUT;
+    else if(ret < 0) {
+	    if(ret == -ERESTARTSYS)
+		    return ret;
+	    else
+		    return -EBADF;
+    }
+#if 0
+    if (unlikely(ret < 0))
+	switch (ret) {
+	    case -EWOULDBLOCK:
+	    case -ETIMEDOUT:
+	    case -EINTR:
+		return ret;
+
+	    default:
+		return -EBADF;
+	}
+#endif
+    if (ts->tcp_state == TCP_SYN_SENT) {
+	/* received conn_evt, but connection is not established */
+	return -ECONNREFUSED;
+    }
+
+    return ret;
+
+ err_deref:
+    rtdev_dereference(rt.rtdev);
+
+    return ret;
+}
+
+
+/***
+ *  rt_tcp_listen
+ */
+static int rt_tcp_listen(struct tcp_socket *ts, unsigned long backlog)
+{
+    int ret;
+    unsigned long      context;
+
+    /* Ignore backlog value, maximum number of queued connections is 1 */
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+    if (ts->is_closed) {
+	ret = -EBADF;
+	goto unlock_out;
+    }
+
+    if (ts->tcp_state != TCP_CLOSE || ts->is_binding) {
+	ret = -EINVAL;
+	goto unlock_out;
+    }
+
+    ts->tcp_state = TCP_LISTEN;
+    ret = 0;
+
+ unlock_out:
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    return ret;
+}
+
+/***
+ *  rt_tcp_accept
+ */
+static int rt_tcp_accept(struct tcp_socket *ts, struct sockaddr *addr,
+			 socklen_t *addrlen)
+{
+    /* Return sockaddr, but bind it with rt_socket_init, so it would be
+       possible to read/write from it in future, return valid file descriptor */
+
+    int ret;
+    struct sockaddr_in  *sin = (struct sockaddr_in*)addr;
+    nanosecs_rel_t      timeout = ts->sock.timeout;
+    unsigned long      context;
+    struct dest_route   rt;
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+    if (ts->is_accepting || ts->is_accepted) {
+	/* socket is already accepted or is accepting a connection right now */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EALREADY;
+    }
+
+    if (ts->tcp_state != TCP_LISTEN || *addrlen < sizeof(struct sockaddr_in)) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EINVAL;
+    }
+
+    ts->is_accepting = 1;
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    ret = rtdm_event_timedwait(&ts->conn_evt, timeout, NULL);
+    if(ret == 0) {
+            ret = -ETIMEDOUT;
+	    goto err;
+    } else if(ret < 0) {
+            if(ret != -ERESTARTSYS)
+            	ret = -EBADF;
+	    goto err;
+    }
+#if 0
+    if (unlikely(ret < 0))
+	switch (ret) {
+	    case -ETIMEDOUT:
+	    case -EINTR:
+		goto err;
+
+	    default:
+		ret = -EBADF;
+		goto err;
+	}
+#endif
+
+    /* accept() reported about connection establishment */
+    ret = rt_ip_route_output(&rt, ts->daddr, ts->saddr);
+    if (ret < 0) {
+	/* strange, no route to host, keep status quo */
+	ret = -EPROTO;
+	goto err;
+    }
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    if (ts->tcp_state != TCP_ESTABLISHED) {
+	/* protocol error */
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	rtdev_dereference(rt.rtdev);
+	ret = -EPROTO;
+	goto err;
+    }
+
+    if (ts->rt.rtdev == NULL)
+	memcpy(&ts->rt, &rt, sizeof(rt));
+    else
+	rtdev_dereference(rt.rtdev);
+
+    sin->sin_family      = AF_INET;
+    sin->sin_port        = ts->dport;
+    sin->sin_addr.s_addr = ts->daddr;
+
+    ts->is_accepted = 1;
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    ret = rt_socket_fd(&ts->sock);
+
+ err:
+    /* it is not critical to leave this unlocked
+       due to single entry nature of accept() */
+    ts->is_accepting = 0;
+
+    return ret;
+}
+
+/***
+ *  rt_tcp_shutdown
+ */
+static int rt_tcp_shutdown(struct tcp_socket *ts, unsigned long how)
+{
+    return -EOPNOTSUPP;
+}
+
+/***
+ *  rt_tcp_setsockopt
+ */
+static int rt_tcp_setsockopt(struct tcp_socket *ts,
+			     int level, int optname, const void *optval,
+			     socklen_t optlen)
+{
+    /* uint64_t val; */
+    struct timeval tv;
+    unsigned long  context;
+
+    switch (optname) {
+	case SO_KEEPALIVE:
+	    if (optlen < sizeof(unsigned int))
+		return -EINVAL;
+
+	    /* commented out, because current implementation transmits
+	       keepalive probes from interrupt context */
+	    /*
+	    val = *(unsigned long*)optval;
+
+	    if (val)
+		rt_tcp_keepalive_enable(ts);
+	    else
+		rt_tcp_keepalive_disable(ts);
+	    */
+	    return 0;
+
+/* avoid compile error */
+#if !defined(__KERNEL__)
+#define SO_SNDTIMEO		SO_SNDTIMEO_OLD
+#else
+#define SO_SNDTIMEO (sizeof(time_t) == sizeof(__kernel_long_t) ? SO_SNDTIMEO_OLD : SO_SNDTIMEO_NEW)
+#endif
+
+	case SO_SNDTIMEO:
+	    if (optlen < sizeof(tv))
+		return -EINVAL;
+	    if (copy_from_user(&tv, optval, sizeof(tv)))
+		return -EFAULT;
+	    if (tv.tv_usec < 0 || tv.tv_usec >= 1000000)
+		return -EDOM;
+
+	    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+	    if (tv.tv_sec < 0) {
+		ts->sk_sndtimeo = RTDM_TIMEOUT_NONE;
+		raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+		return 0;
+	    }
+
+	    ts->sk_sndtimeo = RTDM_TIMEOUT_INFINITE;
+	    if (tv.tv_sec == 0 && tv.tv_usec == 0) {
+		raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+		return 0;
+	    }
+
+	    if (tv.tv_sec < (MAX_SCHEDULE_TIMEOUT/1000000000ull - 1))
+		ts->sk_sndtimeo = (tv.tv_sec * 1000000 + tv.tv_usec) * 1000;
+
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+	    return 0;
+
+	case SO_REUSEADDR:
+	    /* to implement */
+	    return -EOPNOTSUPP;
+    }
+
+    return -ENOPROTOOPT;
+}
+
+/***
+ *  rt_tcp_getsockopt
+ */
+static int rt_tcp_getsockopt(struct tcp_socket *ts,
+			     int level, int optname, void *optval, socklen_t *optlen)
+{
+    int ret = 0;
+
+    if (*optlen < sizeof(unsigned int))
+	return -EINVAL;
+
+    switch (optname) {
+	case SO_ERROR:
+	    ret = 0; /* used in nonblocking connect(), extend later */
+	    break;
+
+	default:
+	    ret = -ENOPROTOOPT;
+	    break;
+    }
+
+    return ret;
+}
+
+/***
+ *  rt_tcp_ioctl
+ */
+static int rt_tcp_ioctl(struct rtsocket *sock,
+			unsigned int request, void __user *arg)
+{
+    struct tcp_socket* ts = container_of(sock, struct tcp_socket, sock);
+    const struct _rtdm_setsockaddr_args *setaddr;
+    struct _rtdm_setsockaddr_args _setaddr;
+    const struct _rtdm_getsockaddr_args *getaddr;
+    struct _rtdm_getsockaddr_args _getaddr;
+    const struct _rtdm_getsockopt_args *getopt;
+    struct _rtdm_getsockopt_args _getopt;
+    const struct _rtdm_setsockopt_args *setopt;
+    struct _rtdm_setsockopt_args _setopt;
+    const long *val;
+    long _val;
+    int in_rt;
+
+    /* fast path for common socket IOCTLs */
+    if (_IOC_TYPE(request) == RTIOC_TYPE_NETWORK)
+	return rt_socket_common_ioctl(sock, request, arg);
+
+    in_rt = rtdm_in_rt_context();
+
+    switch (request) {
+	case _RTIOC_BIND:
+		setaddr = rtnet_get_arg(sock, &_setaddr, arg, sizeof(_setaddr), 1);
+		if (IS_ERR(setaddr))
+			return PTR_ERR(setaddr);
+		return rt_tcp_bind(ts, setaddr->addr, setaddr->addrlen);
+	case _RTIOC_CONNECT:
+		if (!in_rt)
+			return -ENOSYS;
+		setaddr = rtnet_get_arg(sock, &_setaddr, arg, sizeof(_setaddr), 1);
+		if (IS_ERR(setaddr))
+			return PTR_ERR(setaddr);
+		return rt_tcp_connect(ts, setaddr->addr, setaddr->addrlen);
+
+	case _RTIOC_LISTEN:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(long), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+		return rt_tcp_listen(ts, *val);
+
+	case _RTIOC_ACCEPT:
+		if (!in_rt)
+			return -ENOSYS;
+		getaddr = rtnet_get_arg(sock, &_getaddr, arg, sizeof(_getaddr), 1);
+		if (IS_ERR(getaddr))
+			return PTR_ERR(getaddr);
+		return rt_tcp_accept(ts, getaddr->addr, getaddr->addrlen);
+
+	case _RTIOC_SHUTDOWN:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(long), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+		return rt_tcp_shutdown(ts, *val);
+
+	case _RTIOC_SETSOCKOPT:
+		setopt = rtnet_get_arg(sock, &_setopt, arg, sizeof(_setopt), 1);
+		if (IS_ERR(setopt))
+			return PTR_ERR(setopt);
+
+		if (setopt->level != SOL_SOCKET)
+			break;
+
+		return rt_tcp_setsockopt(ts, setopt->level,
+					 setopt->optname, setopt->optval,
+					 setopt->optlen);
+
+	case _RTIOC_GETSOCKOPT:
+		getopt = rtnet_get_arg(sock, &_getopt, arg, sizeof(_getopt), 1);
+		if (IS_ERR(getopt))
+			return PTR_ERR(getopt);
+
+		if (getopt->level != SOL_SOCKET)
+			break;
+
+		return rt_tcp_getsockopt(ts, getopt->level,
+					 getopt->optname, getopt->optval,
+					 getopt->optlen);
+    	default:
+		break;
+    }
+    
+    return rt_ip_ioctl(&ts->sock, request, arg);
+}
+
+
+/***
+ *  rt_tcp_read
+ */
+static ssize_t rt_tcp_read(struct rtsocket *sock, void *buf, size_t nbyte, int msg_in_userspace)
+{
+    struct tcp_socket *ts = container_of(sock, struct tcp_socket, sock);
+
+    struct rtskb      *skb;
+    struct rtskb      *first_skb;
+    size_t            data_len;
+    size_t            th_len;
+    size_t            copied = 0;
+    size_t            block_size;
+    u8                *user_buf = buf;
+    int               ret;
+    unsigned long    context;
+    struct hrtimer_sleeper     timeout, *to=NULL;
+
+#if 0
+    if (!msg_in_userspace) {
+	return -EFAULT;
+    }
+#endif
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    if (ts->is_closed) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EBADF;
+    }
+
+    if (!ts->is_valid) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return 0;
+    }
+
+    if (ts->tcp_state != TCP_ESTABLISHED && ts->tcp_state != TCP_FIN_WAIT2) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EINVAL;
+    }
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    to = NULL;
+    if(sock->timeout) {
+            to = &timeout;
+            /* If the current task is in a real-time scheduling class,
+             * the hrtimer will be marked in the mode for hard interrupt expiry.
+             */
+            hrtimer_init_sleeper_on_stack(to, CLOCK_REALTIME,
+                          HRTIMER_MODE_REL_HARD);
+            hrtimer_set_expires(&to->timer, sock->timeout);
+	    hrtimer_sleeper_start_expires(to, HRTIMER_MODE_REL_HARD);
+    }
+
+    while (copied < nbyte) {
+	ret = down_hrtimeout(&ts->sock.pending_sem, to);
+        if(to) {
+            hrtimer_try_to_cancel(&to->timer);
+	    to = NULL;
+	}
+
+	if (unlikely(ret < 0))
+	    switch (ret) {
+            case -ETIME:
+                ret = -EAGAIN;
+	    case -EWOULDBLOCK:
+	    case -ETIMEDOUT:
+	    case -EINTR:
+		return (copied ? copied : ret);
+
+	    case -EIDRM: /* event is destroyed */
+	    default:
+		if (ts->is_closed) {
+		    return -EBADF;
+		}
+
+		return 0;
+	    }
+
+	skb = rtskb_dequeue_chain(&sock->incoming);
+	RTNET_ASSERT(skb != NULL, return -EFAULT;);
+
+	th_len = (skb->h.th->doff) << 2;
+
+	data_len = skb->len - th_len;
+
+	__rtskb_pull(skb, th_len);
+
+	first_skb = skb;
+
+	/* iterate over all IP fragments */
+    iterate_fragments:
+	block_size = skb->len;
+	copied += block_size;
+	data_len -= block_size;
+
+	if (copied > nbyte) {
+	    block_size -= copied - nbyte;
+	    copied = nbyte;
+	    if(msg_in_userspace) {
+		    if (copy_to_user(user_buf, skb->data, block_size)) {
+			kfree_rtskb(first_skb); /* or store the data? */
+			return -EFAULT;
+		    }
+	    } else
+		    memcpy(user_buf, skb->data, block_size);
+	    raw_spin_lock_irqsave(&ts->socket_lock, context);
+	    if (ts->sync.window) {
+		ts->sync.window += block_size;
+		raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    } else {
+		ts->sync.window = block_size;
+		raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+		rt_tcp_send(ts, TCP_FLAG_ACK); /* window update */
+	    }
+
+	    __rtskb_pull(skb, block_size);
+	    __rtskb_push(first_skb, sizeof(struct tcphdr));
+	    first_skb->h.th->doff = 5;
+	    rtskb_queue_head(&sock->incoming, first_skb);
+	    up(&ts->sock.pending_sem);
+
+	    return copied;
+	}
+	if(msg_in_userspace) {
+		if (copy_to_user(user_buf, skb->data, block_size)) {
+		    kfree_rtskb(first_skb); /* or store the data? */
+		    return -EFAULT;
+		}
+	} else
+		memcpy(user_buf, skb->data, block_size);
+	raw_spin_lock_irqsave(&ts->socket_lock, context);
+	if (ts->sync.window) {
+	    ts->sync.window += block_size;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	} else {
+	    ts->sync.window = block_size;
+	    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	    rt_tcp_send(ts, TCP_FLAG_ACK); /* window update */
+	}
+
+	if ((skb = skb->next) != NULL) {
+	    user_buf += data_len;
+	    goto iterate_fragments;
+	}
+
+	kfree_rtskb(first_skb);
+    }
+
+    return copied;
+}
+
+/***
+ *  rt_tcp_write
+ */
+static ssize_t rt_tcp_write(struct rtsocket *sock, const void *buf, size_t nbyte, int msg_in_userspace)
+{
+    struct tcp_socket *ts = container_of(sock, struct tcp_socket, sock);
+    uint32_t sent_len = 0;
+    unsigned long      context;
+    int ret = 0;
+    nanosecs_rel_t sk_sndtimeo;
+
+#if 0
+    if (!msg_in_userspace) {
+	return -EFAULT;
+    }
+#endif
+
+    raw_spin_lock_irqsave(&ts->socket_lock, context);
+
+    sk_sndtimeo = ts->sk_sndtimeo;
+
+    if (!ts->is_valid) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EPIPE;
+    }
+
+    if ((ts->daddr | ts->dport) == 0 || ts->tcp_state != TCP_ESTABLISHED) {
+	raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+	return -EINVAL;
+    }
+
+    raw_spin_unlock_irqrestore(&ts->socket_lock, context);
+
+    while (sent_len < nbyte) {
+
+	ret = rtdm_event_timedwait(&ts->send_evt, sk_sndtimeo, NULL);
+	if(ret == 0)
+	    return sent_len ? : -ETIMEDOUT;
+	else if(ret < 0) {
+            if(ret == -ERESTARTSYS)
+                return sent_len ? : ret;
+            else {
+	    	if (ts->is_closed)
+                    return -EBADF;
+                return sent_len ? : ret;
+	    }
+    	}
+#if 0
+	if (unlikely(ret < 0))
+	    switch (ret) {
+		case -EWOULDBLOCK:
+		case -ETIMEDOUT:
+		case -EINTR:
+		    return sent_len ? : ret;
+
+		case -EIDRM: /* event is destroyed */
+		default:
+		    if (ts->is_closed)
+			return -EBADF;
+
+		    return sent_len ? : ret;
+	    }
+#endif
+	ret = rt_tcp_window_send(ts, nbyte - sent_len,
+				 ((u8*)buf) + sent_len);
+
+	if (ret < 0) { /* check this branch correctness */
+	    rtdm_event_signal(&ts->send_evt);
+	    break;
+	}
+
+	sent_len += ret;
+	if (ts->sync.dst_window)
+	    rtdm_event_signal(&ts->send_evt);
+    }
+
+    return (ret < 0 ? ret : sent_len);
+}
+
+/***
+ *  rt_tcp_recvmsg
+ */
+static ssize_t rt_tcp_recvmsg(struct rtsocket *sock, struct user_msghdr *msg, int msg_flags, int msg_in_userspace)
+{
+	struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+	struct user_msghdr _msg;
+	ssize_t ret;
+	size_t len;
+	void *buf;
+
+	if (msg_flags)
+		return -EOPNOTSUPP;
+
+	msg = rtnet_get_arg(sock, &_msg, msg, sizeof(*msg), msg_in_userspace);
+	if (IS_ERR(msg))
+		return PTR_ERR(msg);
+
+	/* loop over all vectors to be implemented */
+	if (msg->msg_iovlen != 1)
+		return -EOPNOTSUPP;
+
+	ret = rtdm_get_iovec(&iov, msg, iov_fast, msg_in_userspace);
+	if (ret)
+		return ret;
+
+	len = iov[0].iov_len;
+	if (len > 0) {
+		buf = kmalloc(len, GFP_KERNEL);
+		if (buf == NULL) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		if(msg_in_userspace)
+			ret = copy_from_user(buf, iov[0].iov_base, len);
+		else {
+			memcpy(buf, iov[0].iov_base, len);
+			ret = 0;
+		}
+		if (!ret) {
+			ret = rt_tcp_read(sock, buf, len, 0);
+		        if(msg_in_userspace)
+                        	ret = copy_to_user(iov[0].iov_base, buf, len);
+	                else
+                        	memcpy(iov[0].iov_base, buf, len);
+                }
+		kfree(buf);
+	}
+out:
+	rtdm_drop_iovec(iov, iov_fast);
+
+	return ret;
+}
+
+/***
+ *  rt_tcp_sendmsg
+ */
+static ssize_t rt_tcp_sendmsg(struct rtsocket *sock,
+			      const struct user_msghdr *msg, int msg_flags, int msg_in_userspace)
+{
+	struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+	struct user_msghdr _msg;
+	ssize_t ret;
+	size_t len;
+	void *buf;
+
+	if (msg_flags)
+		return -EOPNOTSUPP;
+
+	msg = rtnet_get_arg(sock, &_msg, msg, sizeof(*msg), msg_in_userspace);
+	if (IS_ERR(msg))
+		return PTR_ERR(msg);
+
+	/* loop over all vectors to be implemented */
+	if (msg->msg_iovlen != 1)
+		return -EOPNOTSUPP;
+
+	ret = rtdm_get_iovec(&iov, msg, iov_fast, msg_in_userspace);
+	if (ret)
+		return ret;
+
+	len = iov[0].iov_len;
+	if (len > 0) {
+		buf = kmalloc(len, GFP_KERNEL);
+		if (buf == NULL) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		if(msg_in_userspace)
+			ret = copy_from_user(buf, iov[0].iov_base, len);
+		else {
+			memcpy(buf, iov[0].iov_base, len);
+			ret = 0;
+		}
+		if (!ret)
+			ret = rt_tcp_write(sock, buf, len, 0);
+		kfree(buf);
+	}
+out:
+	rtdm_drop_iovec(iov, iov_fast);
+
+	return ret;
+}
+
+/***
+ *  rt_tcp_select
+ */
+#if 0
+static int rt_tcp_select(struct rtdm_fd *fd,
+			 rtdm_selector_t *selector,
+			 enum rtdm_selecttype type,
+			 unsigned fd_index)
+{
+    struct tcp_socket *ts = rtdm_fd_to_private(fd);
+
+    switch (type) {
+	case XNSELECT_READ:
+	    return rtdm_sem_select(&ts->sock.pending_sem, selector,
+				XNSELECT_READ, fd_index);
+	case XNSELECT_WRITE:
+	    return rtdm_event_select(&ts->send_evt, selector,
+				    XNSELECT_WRITE, fd_index);
+	default:
+	    return -EBADF;
+    }
+
+    return -EINVAL;
+
+}
+#endif
+
+/***
+ *  TCP-Initialisation
+ */
+static struct rtinet_protocol tcp_protocol = {
+    .protocol =     IPPROTO_TCP,
+    .dest_socket =  &rt_tcp_dest_socket,
+    .rcv_handler =  &rt_tcp_rcv,
+    .err_handler =  &rt_tcp_rcv_err,
+    .init_socket =  &rt_tcp_socket
+};
+
+#if 0
+static struct rtdm_driver tcp_driver = {
+    .profile_info =     RTDM_PROFILE_INFO(tcp,
+					RTDM_CLASS_NETWORK,
+					RTDM_SUBCLASS_RTNET,
+					RTNET_RTDM_VER),
+    .device_flags =     RTDM_PROTOCOL_DEVICE,
+    .device_count =	1,
+    .context_size =     sizeof(struct tcp_socket),
+
+    .protocol_family =  PF_INET,
+    .socket_type =      SOCK_STREAM,
+
+    .ops = {
+	.socket     =   rt_inet_socket,
+	.close      =   rt_tcp_close,
+	.ioctl_rt   =   rt_tcp_ioctl,
+	.ioctl_nrt  =   rt_tcp_ioctl,
+	.read_rt    =   rt_tcp_read,
+	.write_rt   =   rt_tcp_write,
+	.recvmsg_rt =   rt_tcp_recvmsg,
+	.sendmsg_rt =   rt_tcp_sendmsg,
+	.select     =   rt_tcp_select,
+    },
+};
+
+static struct rtdm_device tcp_device = {
+    .driver = &tcp_driver,
+    .label = "tcp",
+};
+#endif
+
+#ifdef CONFIG_XENO_OPT_VFILE
+/***
+ *  rt_tcp_proc_read
+ */
+static inline char* rt_tcp_string_of_state(u8 state)
+{
+    switch (state) {
+	case TCP_ESTABLISHED: return "ESTABLISHED";
+	case TCP_SYN_SENT:    return "SYN_SENT";
+	case TCP_SYN_RECV:    return "SYN_RECV";
+	case TCP_FIN_WAIT1:   return "FIN_WAIT1";
+	case TCP_FIN_WAIT2:   return "FIN_WAIT2";
+	case TCP_TIME_WAIT:   return "TIME_WAIT";
+	case TCP_CLOSE:       return "CLOSE";
+	case TCP_CLOSE_WAIT:  return "CLOSE_WAIT";
+	case TCP_LAST_ACK:    return "LASK_ACK";
+	case TCP_LISTEN:      return "LISTEN";
+	case TCP_CLOSING:     return "CLOSING";
+	default:              return "UNKNOWN";
+    }
+}
+
+static int rtnet_ipv4_tcp_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    unsigned long context;
+    struct tcp_socket *ts;
+    u32 saddr, daddr;
+    u16 sport = 0, dport = 0; /* set to 0 to silence compiler */
+    char sbuffer[24];
+    char dbuffer[24];
+    int state;
+    int index;
+
+    xnvfile_printf(it, "Hash    Local Address           "
+	    "Foreign Address         State\n");
+
+    for (index = 0; index < RT_TCP_SOCKETS; index++) {
+	raw_spin_lock_irqsave(&tcp_socket_base_lock, context);
+
+	ts = port_registry[index];
+	state = ts ? ts->tcp_state : TCP_CLOSE;
+
+	if (ts && ts->tcp_state != TCP_CLOSE) {
+	    saddr = ts->saddr;
+	    sport = ts->sport;
+	    daddr = ts->daddr;
+	    dport = ts->dport;
+	}
+
+	raw_spin_unlock_irqrestore(&tcp_socket_base_lock, context);
+
+	if (state != TCP_CLOSE) {
+	    snprintf(sbuffer, sizeof(sbuffer), "%u.%u.%u.%u:%u",
+		     NIPQUAD(saddr), ntohs(sport));
+	    snprintf(dbuffer, sizeof(dbuffer), "%u.%u.%u.%u:%u",
+		     NIPQUAD(daddr), ntohs(dport));
+
+	    xnvfile_printf(it, "%04X    %-23s %-23s %s\n",
+		    sport & port_hash_mask, sbuffer, dbuffer,
+		    rt_tcp_string_of_state(state));
+	}
+    }
+
+    return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_ipv4_tcp_vfile_ops = {
+	.show = rtnet_ipv4_tcp_show,
+};
+
+static struct xnvfile_regular rtnet_ipv4_tcp_vfile = {
+	.ops = &rtnet_ipv4_tcp_vfile_ops,
+};
+
+/***
+ *  rt_tcp_proc_register
+ */
+static int __init rt_tcp_proc_register(void)
+{
+    return xnvfile_init_regular("tcp", &rtnet_ipv4_tcp_vfile, &ipv4_proc_root);
+}
+
+/***
+ *  rt_tcp_proc_unregister
+ */
+
+static void rt_tcp_proc_unregister(void)
+{
+    xnvfile_destroy_regular(&rtnet_ipv4_tcp_vfile);
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+/***
+ *  rt_tcp_init
+ */
+int __init rt_tcp_init(void)
+{
+    unsigned int skbs;
+    int i;
+    int ret;
+
+    raw_spin_lock_init(&tcp_socket_base_lock);
+
+    if ((tcp_auto_port_start < 0) ||
+	(tcp_auto_port_start >= 0x10000 - RT_TCP_SOCKETS))
+	tcp_auto_port_start = 1024;
+    tcp_auto_port_start = htons(tcp_auto_port_start &
+				(tcp_auto_port_mask & 0xFFFF));
+    tcp_auto_port_mask  = htons(tcp_auto_port_mask | 0xFFFF0000);
+
+    for (i = 0; i < ARRAY_SIZE(port_hash); i++)
+	INIT_HLIST_HEAD(&port_hash[i]);
+
+    /* Perform essential initialization of the RST|ACK socket */
+    skbs = rt_bare_socket_init(&rst_socket.sock, IPPROTO_TCP, RT_TCP_RST_PRIO,
+			       RT_TCP_RST_POOL_SIZE);
+    if (skbs < RT_TCP_RST_POOL_SIZE)
+	printk(KERN_WARNING "rttcp: allocated only %d RST|ACK rtskbs\n", skbs);
+    rst_socket.sock.prot.inet.tos = 0;
+    rst_socket.sock.fd_refs = 1;
+    raw_spin_lock_init(&rst_socket.socket_lock);
+
+    /*
+     * 100 ms forwarding timer with 8.38 ms slots
+     */
+    ret = timerwheel_init(100000000ull, 23);
+    if (ret < 0) {
+	printk(KERN_ERR "rttcp: cann't initialize timerwheel task: %d\n", -ret);
+	goto out_1;
+    }
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    if ((ret = rt_tcp_proc_register()) < 0) {
+	printk(KERN_ERR "rttcp: cann't initialize proc entry: %d\n", -ret);
+	goto out_2;
+    }
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    rt_inet_add_protocol(&tcp_protocol);
+
+    return ret;
+
+    rt_inet_del_protocol(&tcp_protocol);
+#ifdef CONFIG_XENO_OPT_VFILE
+    rt_tcp_proc_unregister();
+out_2:
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    timerwheel_cleanup();
+
+ out_1:
+    rt_bare_socket_cleanup(&rst_socket.sock);
+
+    return ret;
+}
+
+
+/***
+ *  rt_tcp_release
+ */
+void __exit rt_tcp_release(void)
+{
+    rt_inet_del_protocol(&tcp_protocol);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    rt_tcp_proc_unregister();
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    timerwheel_cleanup();
+
+    rt_bare_socket_cleanup(&rst_socket.sock);
+}
+
+module_init(rt_tcp_init);
+module_exit(rt_tcp_release);
diff -Naur a/net/rtnet/stack/ipv4/tcp/timerwheel.c b/net/rtnet/stack/ipv4/tcp/timerwheel.c
--- a/net/rtnet/stack/ipv4/tcp/timerwheel.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/tcp/timerwheel.c	2020-11-23 14:47:05.148825450 +0200
@@ -0,0 +1,229 @@
+/***
+ *
+ *  ipv4/tcp/timerwheel.c - timerwheel implementation for RTnet
+ *
+ *  Copyright (C) 2009 Vladimir Zapolskiy <vladimir.zapolskiy@siemens.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License, version 2, as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/sched/task.h>
+#include <linux/kthread.h>
+#include <uapi/linux/sched/types.h>
+
+#include <rtnet_rtdm.h>
+#include "timerwheel.h"
+
+static int stop_pivot_task = 0;
+
+static struct {
+    /* timer pivot task */
+    struct task_struct *pivot_task;
+
+    /* time length for one period of rotation of timerwheel */
+    nanosecs_rel_t timeout;
+
+    /* timer wheel slots for storing timers up to timerwheel_timeout */
+    unsigned int slots;
+
+    /* timer wheel interval timeout */
+    nanosecs_rel_t interval;
+
+    /* timer wheel interval timeout */
+    unsigned int interval_base;
+
+    /* timerwheel array */
+    struct list_head *ring;
+
+    /* timerwheel slot counter */
+    unsigned int current_slot;
+
+    /* timerwheel current slot lock */
+    raw_spinlock_t slot_lock;
+} wheel;
+
+static struct timerwheel_timer *timerwheel_get_from_current_slot(void)
+{
+    struct timerwheel_timer *timer = NULL;
+    struct list_head *slot_list;
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&wheel.slot_lock, context);
+
+    slot_list = &wheel.ring[wheel.current_slot];
+
+    if (!list_empty(slot_list)) {
+	timer = list_first_entry(slot_list, struct timerwheel_timer, link);
+	list_del(&timer->link);
+	timer->slot = TIMERWHEEL_TIMER_UNUSED;
+	timer->refcount++;
+    }
+
+    raw_spin_unlock_irqrestore(&wheel.slot_lock, context);
+
+    return timer;
+}
+
+int timerwheel_add_timer(struct timerwheel_timer *timer,
+			 nanosecs_rel_t expires)
+{
+    unsigned long context;
+    int slot;
+
+    slot = expires >> wheel.interval_base;
+
+    if (slot >= wheel.slots)
+	return -EINVAL;
+
+    raw_spin_lock_irqsave(&wheel.slot_lock, context);
+
+    /* cancel timer if it's still running */
+    if (timer->slot >= 0)
+	list_del(&timer->link);
+
+    slot = slot + wheel.current_slot;
+    if (slot >= wheel.slots)
+	slot = slot - wheel.slots;
+
+    list_add_tail(&timer->link, &wheel.ring[slot]);
+    timer->slot = slot;
+
+    raw_spin_unlock_irqrestore(&wheel.slot_lock, context);
+
+    return 0;
+}
+
+static int timerwheel_sleep(void)
+{
+    int64_t interval = wheel.interval / NSEC_PER_USEC;
+
+    usleep_range(interval, interval + 100);
+
+    wheel.current_slot++;
+    if (wheel.current_slot == wheel.slots)
+	wheel.current_slot = 0;
+
+    return 0;
+}
+
+static int timerwheel_pivot(void *arg)
+{
+    struct timerwheel_timer *timer;
+    int ret;
+
+    while (!stop_pivot_task) {
+	ret = timerwheel_sleep();
+	if (ret < 0) {
+	    printk(KERN_INFO "timerwheel: timerwheel_pivot interrupted %d\n", -ret);
+	    break;
+	}
+
+	while ((timer = timerwheel_get_from_current_slot())) {
+	    timer->handler(timer->data);
+
+	    smp_mb();
+	    timer->refcount--;
+	}
+    }
+
+    return 0;
+}
+
+int timerwheel_remove_timer(struct timerwheel_timer *timer)
+{
+    unsigned long context;
+    int ret;
+
+    raw_spin_lock_irqsave(&wheel.slot_lock, context);
+
+    if (timer->slot >= 0) {
+	list_del(&timer->link);
+	timer->slot = TIMERWHEEL_TIMER_UNUSED;
+	ret = 0;
+    } else
+	ret = -ENOENT;
+
+    raw_spin_unlock_irqrestore(&wheel.slot_lock, context);
+
+    return ret;
+}
+
+void timerwheel_remove_timer_sync(struct timerwheel_timer *timer)
+{
+    u64 interval_ms = wheel.interval;
+
+    do_div(interval_ms, 1000000);
+
+    timerwheel_remove_timer(timer);
+
+    while (timer->refcount > 0)
+	msleep(interval_ms);
+}
+
+/*
+  timeout     - maximum expiration timeout for timers
+  granularity - is an exponent of 2 representing nanoseconds for
+  one wheel tick
+  heapsize    - is a number of timers to allocate
+*/
+int __init timerwheel_init(nanosecs_rel_t timeout, unsigned int granularity)
+{
+    int i;
+    struct sched_param pivot_task_param = { .sched_priority = (RTDM_TASK_LOWEST_PRIORITY+1) };
+
+    /* the least possible slot timeout is set for 1ms */
+    if (granularity < 10)
+	return -EINVAL;
+
+    wheel.timeout = timeout;
+    wheel.interval_base = granularity;
+    wheel.slots = (timeout >> granularity) + 1;
+    wheel.interval = (1 << granularity);
+    wheel.current_slot = 0;
+
+    wheel.ring = kmalloc(sizeof(struct list_head) * wheel.slots,
+			 GFP_KERNEL);
+    if (!wheel.ring)
+	return -ENOMEM;
+
+    for (i = 0; i < wheel.slots; i++)
+	INIT_LIST_HEAD(&wheel.ring[i]);
+
+    raw_spin_lock_init(&wheel.slot_lock);
+
+    stop_pivot_task = 0;
+    wheel.pivot_task = kthread_create(timerwheel_pivot, NULL, "rttcp timerwheel");
+    if (!wheel.pivot_task) {
+	printk(KERN_ERR "timerwheel: error on pivot task initialization.\n");
+	kfree(wheel.ring);
+	return -1;
+    }
+    sched_setscheduler(wheel.pivot_task, SCHED_FIFO, &pivot_task_param);
+    wake_up_process(wheel.pivot_task);
+
+    return 0;
+}
+
+void timerwheel_cleanup(void)
+{
+    stop_pivot_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(wheel.pivot_task);
+    /* release the task structure */
+    put_task_struct(wheel.pivot_task);
+    kfree(wheel.ring);
+}
diff -Naur a/net/rtnet/stack/ipv4/tcp/timerwheel.h b/net/rtnet/stack/ipv4/tcp/timerwheel.h
--- a/net/rtnet/stack/ipv4/tcp/timerwheel.h	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/tcp/timerwheel.h	2020-11-23 14:47:05.148825450 +0200
@@ -0,0 +1,63 @@
+/***
+ *
+ *  ipv4/tcp/timerwheel.h - timerwheel interface for RTnet
+ *
+ *  Copyright (C) 2009 Vladimir Zapolskiy <vladimir.zapolskiy@siemens.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License, version 2, as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TIMERWHEEL_H_
+#define __TIMERWHEEL_H_
+
+#include <linux/list.h>
+#include <rtdm_net.h>
+
+#define TIMERWHEEL_TIMER_UNUSED    -1
+
+typedef void (*timerwheel_timer_handler)(void *);
+
+struct timerwheel_timer {
+    struct list_head            link;
+    timerwheel_timer_handler    handler;
+    void                        *data;
+    int                         slot;
+    volatile int                refcount; /* only written by wheel task */
+};
+
+static inline void
+timerwheel_init_timer(struct timerwheel_timer *timer,
+                      timerwheel_timer_handler handler, void *data)
+{
+    timer->slot     = TIMERWHEEL_TIMER_UNUSED;
+    timer->handler  = handler;
+    timer->data     = data;
+    timer->refcount = 0;
+}
+
+/* passed data must remain valid till a timer fireup */
+int timerwheel_add_timer(struct timerwheel_timer *timer,
+                         nanosecs_rel_t expires);
+
+int timerwheel_remove_timer(struct timerwheel_timer *timer);
+
+void timerwheel_remove_timer_sync(struct timerwheel_timer *timer);
+
+int timerwheel_init(nanosecs_rel_t timeout,
+                    unsigned int granularity);
+
+void timerwheel_cleanup(void);
+
+#endif
diff -Naur a/net/rtnet/stack/ipv4/udp/Kconfig b/net/rtnet/stack/ipv4/udp/Kconfig
--- a/net/rtnet/stack/ipv4/udp/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/udp/Kconfig	2021-07-14 15:39:13.414124240 +0300
@@ -0,0 +1,6 @@
+config RTNET_RTIPV4_UDP
+    tristate "UDP support"
+    depends on RTNET_RTIPV4
+    default y
+    help
+    Enables UDP support of the RTnet Real-Time IPv4 protocol.
diff -Naur a/net/rtnet/stack/ipv4/udp/Makefile b/net/rtnet/stack/ipv4/udp/Makefile
--- a/net/rtnet/stack/ipv4/udp/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/udp/Makefile	2021-07-14 15:39:13.414124240 +0300
@@ -0,0 +1,5 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_RTIPV4_UDP) += rtudp.o
+
+rtudp-y := udp.o
diff -Naur a/net/rtnet/stack/ipv4/udp/modules.builtin b/net/rtnet/stack/ipv4/udp/modules.builtin
--- a/net/rtnet/stack/ipv4/udp/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/udp/modules.builtin	2021-07-14 15:39:13.414124240 +0300
@@ -0,0 +1 @@
+net/rtnet/stack/ipv4/udp/rtudp.ko
diff -Naur a/net/rtnet/stack/ipv4/udp/udp.c b/net/rtnet/stack/ipv4/udp/udp.c
--- a/net/rtnet/stack/ipv4/udp/udp.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/ipv4/udp/udp.c	2021-07-14 15:39:13.414124240 +0300
@@ -0,0 +1,907 @@
+/***
+ *
+ *  ipv4/udp.c - UDP implementation for RTnet
+ *
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/err.h>
+#include <linux/udp.h>
+#include <linux/tcp.h>
+#include <net/checksum.h>
+#include <linux/list.h>
+#include <linux/hrtimer.h>
+
+#include <rtskb.h>
+#include <rtnet_internal.h>
+#include <rtnet_port.h>
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/ip_sock.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+#include <ipv4/udp.h>
+
+/***
+ *  This structure is used to register a UDP socket for reception. All
+ +  structures are kept in the port_registry array to increase the cache
+ *  locality during the critical port lookup in rt_udp_v4_lookup().
+ */
+struct udp_socket {
+	u16 sport;		/* local port */
+	u32 saddr;		/* local ip-addr */
+	struct rtsocket *sock;
+	struct hlist_node link;
+};
+
+/***
+ *  Automatic port number assignment
+
+ *  The automatic assignment of port numbers to unbound sockets is realised as
+ *  a simple addition of two values:
+ *   - the socket ID (lower 8 bits of file descriptor) which is set during
+ *     initialisation and left unchanged afterwards
+ *   - the start value auto_port_start which is a module parameter
+
+ *  auto_port_mask, also a module parameter, is used to define the range of
+ *  port numbers which are used for automatic assignment. Any number within
+ *  this range will be rejected when passed to bind_rt().
+
+ */
+static unsigned int auto_port_start = 1024;
+static unsigned int auto_port_mask = ~(RT_UDP_SOCKETS - 1);
+static int free_ports = RT_UDP_SOCKETS;
+#define RT_PORT_BITMAP_WORDS \
+    ((RT_UDP_SOCKETS + BITS_PER_LONG - 1) / BITS_PER_LONG)
+static unsigned long port_bitmap[RT_PORT_BITMAP_WORDS];
+static struct udp_socket port_registry[RT_UDP_SOCKETS];
+static raw_spinlock_t udp_socket_base_lock;
+
+static struct hlist_head port_hash[RT_UDP_SOCKETS * 2];
+#define port_hash_mask (RT_UDP_SOCKETS * 2 - 1)
+
+MODULE_LICENSE("GPL");
+
+module_param(auto_port_start, uint, 0444);
+module_param(auto_port_mask, uint, 0444);
+MODULE_PARM_DESC(auto_port_start, "Start of automatically assigned port range");
+MODULE_PARM_DESC(auto_port_mask,
+		 "Mask that defines port range for automatic assignment");
+
+static inline struct udp_socket *port_hash_search(u32 saddr, u16 sport)
+{
+	unsigned bucket = sport & port_hash_mask;
+	struct udp_socket *sock;
+
+	hlist_for_each_entry(sock, &port_hash[bucket], link)
+	    if (sock->sport == sport &&
+		(saddr == INADDR_ANY
+		 || sock->saddr == saddr || sock->saddr == INADDR_ANY))
+		return sock;
+
+	return NULL;
+}
+
+static inline int port_hash_insert(struct udp_socket *sock, u32 saddr,
+				   u16 sport)
+{
+	unsigned bucket;
+
+	if (port_hash_search(saddr, sport))
+		return -EADDRINUSE;
+
+	bucket = sport & port_hash_mask;
+	sock->saddr = saddr;
+	sock->sport = sport;
+	hlist_add_head(&sock->link, &port_hash[bucket]);
+	return 0;
+}
+
+static inline void port_hash_del(struct udp_socket *sock)
+{
+	hlist_del(&sock->link);
+}
+
+/***
+ *  rt_udp_v4_lookup
+ */
+static inline struct rtsocket *rt_udp_v4_lookup(u32 daddr, u16 dport)
+{
+	unsigned long context;
+	struct udp_socket *sock;
+
+	raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+	sock = port_hash_search(daddr, dport);
+	if (sock && rt_socket_reference(sock->sock) == 0) {
+
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+		return sock->sock;
+	}
+
+	raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+	return NULL;
+}
+
+/***
+ *  rt_udp_bind - bind socket to local address
+ *  @s:     socket
+ *  @addr:  local address
+ */
+int rt_udp_bind(struct rtsocket *sock,
+		const struct sockaddr __user * addr, socklen_t addrlen)
+{
+	struct sockaddr_in _sin, *sin;
+	unsigned long context;
+	int index;
+	int err = 0;
+
+	if (addrlen < sizeof(struct sockaddr_in))
+		return -EINVAL;
+
+	sin = rtnet_get_arg(sock, &_sin, addr, sizeof(_sin), 1);
+	if (IS_ERR(sin))
+		return PTR_ERR(sin);
+
+	if ((sin->sin_port & auto_port_mask) == auto_port_start)
+		return -EINVAL;
+
+	raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+	if ((index = sock->prot.inet.reg_index) < 0) {
+		/* socket is being closed */
+		err = -EBADF;
+		goto unlock_out;
+	}
+	if (sock->prot.inet.state != TCP_CLOSE) {
+		err = -EINVAL;
+		goto unlock_out;
+	}
+
+	port_hash_del(&port_registry[index]);
+	if (port_hash_insert(&port_registry[index],
+			     sin->sin_addr.s_addr,
+			     sin->sin_port ? : index + auto_port_start)) {
+		port_hash_insert(&port_registry[index],
+				 port_registry[index].saddr,
+				 port_registry[index].sport);
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+		return -EADDRINUSE;
+	}
+
+	/* set the source-addr */
+	sock->prot.inet.saddr = port_registry[index].saddr;
+
+	/* set source port, if not set by user */
+	sock->prot.inet.sport = port_registry[index].sport;
+
+ unlock_out:
+	raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+	return err;
+}
+
+EXPORT_SYMBOL_GPL(rt_udp_bind);
+
+/***
+ *  rt_udp_connect
+ */
+int rt_udp_connect(struct rtsocket *sock,
+		   const struct sockaddr __user * serv_addr, socklen_t addrlen)
+{
+	struct sockaddr _sa, *sa;
+	struct sockaddr_in _sin, *sin;
+	unsigned long context;
+	int index;
+
+	if (addrlen < sizeof(struct sockaddr))
+		return -EINVAL;
+
+	sa = rtnet_get_arg(sock, &_sa, serv_addr, sizeof(_sa), 1);
+	if (IS_ERR(sa))
+		return PTR_ERR(sa);
+
+	if (sa->sa_family == AF_UNSPEC) {
+		if ((index = sock->prot.inet.reg_index) < 0)
+			/* socket is being closed */
+			return -EBADF;
+
+		raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+		sock->prot.inet.saddr = INADDR_ANY;
+		/* Note: The following line differs from standard
+		   stacks, and we also don't remove the socket from
+		   the port list. Might get fixed in the future... */
+		sock->prot.inet.sport = index + auto_port_start;
+		sock->prot.inet.daddr = INADDR_ANY;
+		sock->prot.inet.dport = 0;
+		sock->prot.inet.state = TCP_CLOSE;
+
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+	} else {
+		if (addrlen < sizeof(struct sockaddr_in))
+			return -EINVAL;
+
+		sin = rtnet_get_arg(sock, &_sin, serv_addr, sizeof(_sin), 1);
+		if (IS_ERR(sin))
+			return PTR_ERR(sin);
+
+		if (sin->sin_family != AF_INET)
+			return -EINVAL;
+
+		raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+		if (sock->prot.inet.state != TCP_CLOSE) {
+			raw_spin_unlock_irqrestore(&udp_socket_base_lock,
+						   context);
+			return -EINVAL;
+		}
+
+		sock->prot.inet.state = TCP_ESTABLISHED;
+		sock->prot.inet.daddr = sin->sin_addr.s_addr;
+		sock->prot.inet.dport = sin->sin_port;
+
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+	}
+
+	return 0;
+}
+
+/***
+ *  rt_udp_socket - create a new UDP-Socket
+ *  @s: socket
+ */
+int rt_udp_socket(struct rtsocket **psock, int family, int type, int protocol)
+{
+	int ret;
+	int i;
+	int index;
+	unsigned long context;
+	struct rtsocket *sock;
+
+	sock = kzalloc(sizeof(struct rtsocket), GFP_KERNEL);
+	if (!sock)
+		return -ENOMEM;
+
+	if ((ret = rt_socket_init(sock, IPPROTO_UDP)) != 0) {
+		kfree(sock);
+		return ret;
+	}
+
+	sock->family = family;
+	sock->type = type;
+	sock->protocol = protocol;
+
+	sock->prot.inet.saddr = INADDR_ANY;
+	sock->prot.inet.state = TCP_CLOSE;
+	sock->prot.inet.tos = 0;
+
+	raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+	/* enforce maximum number of UDP sockets */
+	if (free_ports == 0) {
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+		rt_socket_cleanup(sock);
+		kfree(sock);
+		return -EAGAIN;
+	}
+	free_ports--;
+
+	/* find free auto-port in bitmap */
+	for (i = 0; i < RT_PORT_BITMAP_WORDS; i++)
+		if (port_bitmap[i] != (unsigned long)-1)
+			break;
+	index = ffz(port_bitmap[i]);
+	set_bit(index, &port_bitmap[i]);
+	index += i * 32;
+	sock->prot.inet.reg_index = index;
+	sock->prot.inet.sport = index + auto_port_start;
+
+	/* register UDP socket */
+	port_hash_insert(&port_registry[index], INADDR_ANY,
+			 sock->prot.inet.sport);
+	port_registry[index].sock = sock;
+
+	raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+	*psock = sock;
+
+	return 0;
+}
+
+/***
+ *  rt_udp_close
+ */
+void rt_udp_close(struct rtsocket *sock)
+{
+	struct rtskb *del;
+	int port;
+	unsigned long context;
+
+	raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+	sock->prot.inet.state = TCP_CLOSE;
+
+	if (sock->prot.inet.reg_index >= 0) {
+		port = sock->prot.inet.reg_index;
+		clear_bit(port % BITS_PER_LONG,
+			  &port_bitmap[port / BITS_PER_LONG]);
+		port_hash_del(&port_registry[port]);
+
+		free_ports++;
+
+		sock->prot.inet.reg_index = -1;
+	}
+
+	raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+	/* cleanup already collected fragments */
+	rt_ip_frag_invalidate_socket(sock);
+
+	/* free packets in incoming queue */
+	while ((del = rtskb_dequeue(&sock->incoming)) != NULL)
+		kfree_rtskb(del);
+
+	rt_socket_cleanup(sock);
+
+	kfree(sock);
+}
+
+EXPORT_SYMBOL_GPL(rt_udp_close);
+
+int rt_udp_ioctl(struct rtsocket *sock, unsigned int request, void __user * arg)
+{
+#if 0
+	const struct _rtdm_setsockaddr_args *setaddr;
+	struct _rtdm_setsockaddr_args _setaddr;
+#endif
+	/* fast path for common socket IOCTLs */
+	if (_IOC_TYPE(request) == RTIOC_TYPE_NETWORK)
+		return rt_socket_common_ioctl(sock, request, arg);
+
+	switch (request) {
+#if 0
+	case _RTIOC_CONNECT:
+	case _RTIOC_BIND:
+		setaddr =
+		    rtnet_get_arg(sock, &_setaddr, arg, sizeof(_setaddr), 1);
+		if (IS_ERR(setaddr))
+			return PTR_ERR(setaddr);
+		if (request == _RTIOC_BIND)
+			return rt_udp_bind(sock, setaddr->addr,
+					   setaddr->addrlen);
+		return rt_udp_connect(sock, setaddr->addr, setaddr->addrlen);
+#endif
+
+	default:
+		return rt_ip_ioctl(sock, request, arg);
+	}
+}
+
+EXPORT_SYMBOL_GPL(rt_udp_ioctl);
+
+/*
+ *  rt_udp_recvmsg
+ */
+ssize_t rt_udp_recvmsg(struct rtsocket *sock, struct user_msghdr *u_msg,
+		       int msg_flags, int msg_in_userspace)
+{
+	size_t len;
+	struct rtskb *skb;
+	struct rtskb *first_skb;
+	size_t copied = 0;
+	size_t block_size;
+	size_t data_len;
+	struct udphdr *uh;
+	struct sockaddr_in sin;
+	struct hrtimer_sleeper timeout, *to = NULL;
+	int ret, flags;
+	struct user_msghdr _msg, *msg;
+	socklen_t namelen;
+	struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+	msg = rtnet_get_arg(sock, &_msg, u_msg, sizeof(_msg), msg_in_userspace);
+
+	if (IS_ERR(msg))
+		return PTR_ERR(msg);
+
+	if (msg->msg_iovlen < 0)
+		return -EINVAL;
+
+	if (msg->msg_iovlen == 0)
+		return 0;
+
+	ret = rtdm_get_iovec(&iov, msg, iov_fast, msg_in_userspace);
+	if (ret)
+		return ret;
+
+	/* non-blocking receive? */
+	if (msg_flags & MSG_DONTWAIT) {
+		/* down_trylock returns 0 (success) or 1 (fail). */
+		if ((ret = down_trylock(&sock->pending_sem)))
+			ret = -EWOULDBLOCK;
+	} else {
+		to = NULL;
+		if (sock->timeout) {
+			to = &timeout;
+			/* If the current task is in a real-time scheduling class,
+			 * the hrtimer will be marked in the mode for hard interrupt expiry.
+			 */
+			hrtimer_init_sleeper_on_stack(to, CLOCK_MONOTONIC,
+						      HRTIMER_MODE_REL_HARD);
+			hrtimer_set_expires(&to->timer, sock->timeout);
+			hrtimer_sleeper_start_expires(to, HRTIMER_MODE_REL_HARD);
+		}
+		ret = down_hrtimeout(&sock->pending_sem, to);
+		if (to)
+			hrtimer_try_to_cancel(&to->timer);
+	}
+	if (unlikely(ret < 0)) {
+		switch (ret) {
+		case -ETIME:
+			ret = -EAGAIN;
+		case -EWOULDBLOCK:
+		case -ETIMEDOUT:
+		case -EINTR:
+			rtdm_drop_iovec(iov, iov_fast);
+			break;
+		default:
+			ret = -EBADF;	/* socket has been closed */
+		}
+		return ret;
+	}
+
+	skb = rtskb_dequeue_chain(&sock->incoming);
+	RTNET_ASSERT(skb != NULL, return -EFAULT;
+	    );
+	uh = skb->h.uh;
+	first_skb = skb;
+
+	/* copy the address if required. */
+	if (msg->msg_name) {
+		int size = msg->msg_namelen;
+		memset(&sin, 0, sizeof(sin));
+		sin.sin_family = AF_INET;
+		sin.sin_port = uh->source;
+		sin.sin_addr.s_addr = skb->nh.iph->saddr;
+		if (size > sizeof(sin))
+			size = sizeof(sin);
+		/* we can not access u_msg->msg_name directly if msg_in_userspace */
+		ret =
+		    rtnet_put_arg(sock, msg->msg_name, &sin, size,
+				  msg_in_userspace);
+		if (ret)
+			goto fail;
+		if (!msg_in_userspace) {
+			namelen = sizeof(sin);
+			ret =
+			    rtnet_put_arg(sock, &u_msg->msg_namelen, &namelen,
+					  sizeof(namelen), msg_in_userspace);
+			if (ret)
+				goto fail;
+		}
+	}
+
+	data_len = ntohs(uh->len) - sizeof(struct udphdr);
+
+	/* remove the UDP header */
+	__rtskb_pull(skb, sizeof(struct udphdr));
+
+	flags = msg->msg_flags & ~MSG_TRUNC;
+	len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+
+	/* iterate over all IP fragments */
+	do {
+		rtskb_trim(skb, data_len);
+
+		block_size = skb->len;
+		copied += block_size;
+		data_len -= block_size;
+
+		/* The data must not be longer than the available buffer size */
+		if (copied > len) {
+			block_size -= copied - len;
+			copied = len;
+			flags |= MSG_TRUNC;
+		}
+
+		/* copy the data */
+		ret =
+		    rtnet_write_to_iov(sock, iov, msg->msg_iovlen, skb->data,
+				       block_size, msg_in_userspace);
+		if (ret)
+			goto fail;
+
+		/* next fragment */
+		skb = skb->next;
+	} while (skb && !(flags & MSG_TRUNC));
+
+	/* did we copied all bytes? */
+	if (data_len > 0)
+		flags |= MSG_TRUNC;
+
+	if (flags != msg->msg_flags) {
+		ret =
+		    rtnet_put_arg(sock, &u_msg->msg_flags, &flags,
+				  sizeof(flags), msg_in_userspace);
+		if (ret)
+			goto fail;
+	}
+ out:
+	if ((msg_flags & MSG_PEEK) == 0)
+		kfree_rtskb(first_skb);
+	else {
+		__rtskb_push(first_skb, sizeof(struct udphdr));
+		rtskb_queue_head(&sock->incoming, first_skb);
+		up(&sock->pending_sem);
+	}
+	rtdm_drop_iovec(iov, iov_fast);
+
+	return copied;
+ fail:
+	copied = ret;
+	goto out;
+}
+
+EXPORT_SYMBOL_GPL(rt_udp_recvmsg);
+
+/***
+ *  struct udpfakehdr
+ */
+struct udpfakehdr {
+	struct udphdr uh;
+	u32 daddr;
+	u32 saddr;
+	struct rtsocket *sock;
+	struct iovec *iov;
+	int iovlen;
+	u32 wcheck;
+};
+
+/***
+ *
+ */
+static int rt_udp_getfrag(const void *p, unsigned char *to,
+			  unsigned int offset, unsigned int fraglen,
+			  int msg_in_userspace)
+{
+	struct udpfakehdr *ufh = (struct udpfakehdr *)p;
+	int ret;
+
+	// We should optimize this function a bit (copy+csum...)!
+	if (offset) {
+		ret =
+		    rtnet_read_from_iov(ufh->sock, ufh->iov, ufh->iovlen, to,
+					fraglen, msg_in_userspace);
+		return ret < 0 ? ret : 0;
+	}
+
+	ret = rtnet_read_from_iov(ufh->sock, ufh->iov, ufh->iovlen,
+				  to + sizeof(struct udphdr),
+				  fraglen - sizeof(struct udphdr),
+				  msg_in_userspace);
+	if (ret < 0)
+		return ret;
+
+	/* Checksum of the complete data part of the UDP message: */
+	ufh->wcheck = csum_partial(to + sizeof(struct udphdr),
+				   fraglen - sizeof(struct udphdr),
+				   ufh->wcheck);
+
+	/* Checksum of the udp header: */
+	ufh->wcheck = csum_partial((unsigned char *)ufh,
+				   sizeof(struct udphdr), ufh->wcheck);
+
+	ufh->uh.check =
+	    csum_tcpudp_magic(ufh->saddr, ufh->daddr, ntohs(ufh->uh.len),
+			      IPPROTO_UDP, ufh->wcheck);
+
+	if (ufh->uh.check == 0)
+		ufh->uh.check = -1;
+
+	memcpy(to, ufh, sizeof(struct udphdr));
+
+	return 0;
+}
+
+/***
+ *  rt_udp_sendmsg
+ */
+ssize_t rt_udp_sendmsg(struct rtsocket *sock, const struct user_msghdr *msg,
+		       int msg_flags, int msg_in_userspace)
+{
+	size_t len;
+	int ulen;
+	struct sockaddr_in _sin, *sin;
+	struct udpfakehdr ufh;
+	struct dest_route rt;
+	u32 saddr;
+	u32 daddr;
+	u16 dport;
+	int err;
+	unsigned long context;
+	struct user_msghdr _msg;
+	struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+	if (msg_flags & MSG_OOB)	/* Mirror BSD error message compatibility */
+		return -EOPNOTSUPP;
+
+	if (msg_flags & ~(MSG_DONTROUTE | MSG_DONTWAIT))
+		return -EINVAL;
+
+	msg = rtnet_get_arg(sock, &_msg, msg, sizeof(*msg), msg_in_userspace);
+	if (IS_ERR(msg))
+		return PTR_ERR(msg);
+
+	if (msg->msg_iovlen < 0)
+		return -EINVAL;
+
+	if (msg->msg_iovlen == 0) {
+		printk(KERN_INFO "%s %d\n", __func__, __LINE__);
+		return 0;
+	}
+
+	err = rtdm_get_iovec(&iov, msg, iov_fast, msg_in_userspace);
+	if (err) {
+		return err;
+	}
+
+	len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+	if ((len < 0)
+	    || (len > 0xFFFF - sizeof(struct iphdr) - sizeof(struct udphdr))) {
+		err = -EMSGSIZE;
+		goto out;
+	}
+
+	ulen = len + sizeof(struct udphdr);
+
+	if (msg->msg_name && msg->msg_namelen == sizeof(*sin)) {
+		sin =
+		    rtnet_get_arg(sock, &_sin, msg->msg_name, sizeof(_sin),
+				  msg_in_userspace);
+		if (IS_ERR(sin)) {
+			err = PTR_ERR(sin);
+			goto out;
+		}
+
+		if (sin->sin_family != AF_INET && sin->sin_family != AF_UNSPEC) {
+			err = -EINVAL;
+			goto out;
+		}
+
+		daddr = sin->sin_addr.s_addr;
+		dport = sin->sin_port;
+		raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+	} else {
+		raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+		if (sock->prot.inet.state != TCP_ESTABLISHED) {
+			raw_spin_unlock_irqrestore(&udp_socket_base_lock,
+						   context);
+			err = -ENOTCONN;
+			goto out;
+		}
+
+		daddr = sock->prot.inet.daddr;
+		dport = sock->prot.inet.dport;
+	}
+
+	saddr = sock->prot.inet.saddr;
+	ufh.uh.source = sock->prot.inet.sport;
+
+	raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+	if ((daddr | dport) == 0) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* get output route */
+	err = rt_ip_route_output(&rt, daddr, saddr);
+	if (err)
+		goto out;
+
+	/* we found a route, remember the routing dest-addr could be the netmask */
+	ufh.saddr = saddr != INADDR_ANY ? saddr : rt.rtdev->local_ip;
+	ufh.daddr = daddr;
+	ufh.uh.dest = dport;
+	ufh.uh.len = htons(ulen);
+	ufh.uh.check = 0;
+	ufh.sock = sock;
+	ufh.iov = iov;
+	ufh.iovlen = msg->msg_iovlen;
+	ufh.wcheck = 0;
+
+	err =
+	    rt_ip_build_xmit(sock, rt_udp_getfrag, &ufh, ulen, &rt, msg_flags,
+			     msg_in_userspace);
+
+	/* Drop the reference obtained in rt_ip_route_output() */
+	rtdev_dereference(rt.rtdev);
+ out:
+	rtdm_drop_iovec(iov, iov_fast);
+
+	return err ? : len;
+}
+
+EXPORT_SYMBOL_GPL(rt_udp_sendmsg);
+
+/***
+ *  rt_udp_check
+ */
+static inline unsigned short rt_udp_check(struct udphdr *uh, int len,
+					  unsigned long saddr,
+					  unsigned long daddr,
+					  unsigned long base)
+{
+	return (csum_tcpudp_magic(saddr, daddr, len, IPPROTO_UDP, base));
+}
+
+struct rtsocket *rt_udp_dest_socket(struct rtskb *skb)
+{
+	struct udphdr *uh = skb->h.uh;
+	unsigned short ulen = ntohs(uh->len);
+	u32 saddr = skb->nh.iph->saddr;
+	u32 daddr = skb->nh.iph->daddr;
+	struct rtnet_device *rtdev = skb->rtdev;
+
+	if (uh->check == 0)
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+/* ip_summed (yet) never equals CHECKSUM_PARTIAL
+    else
+        if (skb->ip_summed == CHECKSUM_PARTIAL) {
+            skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+            if ( !rt_udp_check(uh, ulen, saddr, daddr, skb->csum) )
+                return NULL;
+
+            skb->ip_summed = CHECKSUM_NONE;
+        }*/
+
+	if (skb->ip_summed != CHECKSUM_UNNECESSARY)
+		skb->csum =
+		    csum_tcpudp_nofold(saddr, daddr, ulen, IPPROTO_UDP, 0);
+
+	/* patch broadcast daddr */
+	if (daddr == rtdev->broadcast_ip)
+		daddr = rtdev->local_ip;
+
+	/* find the destination socket */
+	skb->sk = rt_udp_v4_lookup(daddr, uh->dest);
+
+	return skb->sk;
+}
+
+/***
+ *  rt_udp_rcv
+ */
+void rt_udp_rcv(struct rtskb *skb)
+{
+	struct rtsocket *sock = skb->sk;
+	void (*callback_func)(void *, void *);
+	void *callback_arg;
+	unsigned long context;
+
+	rtskb_queue_tail(&sock->incoming, skb);
+	up(&sock->pending_sem);
+
+	if (wq_has_sleeper_rtnet(&sock->wq_head_rtnet))
+		wake_up_interruptible_sync_poll_rtnet(&sock->wq_head_rtnet,
+						      EPOLLIN | EPOLLRDNORM |
+						      EPOLLRDBAND);
+	raw_spin_lock_irqsave(&sock->param_lock, context);
+	callback_func = sock->callback_func;
+	callback_arg = sock->callback_arg;
+	raw_spin_unlock_irqrestore(&sock->param_lock, context);
+
+	if (callback_func)
+		callback_func(sock, callback_arg);
+}
+
+/***
+ *  rt_udp_rcv_err
+ */
+void rt_udp_rcv_err(struct rtskb *skb)
+{
+	printk(KERN_ERR "RTnet: rt_udp_rcv err\n");
+}
+
+/***
+ *  UDP-Initialisation
+ */
+static struct rtinet_protocol udp_protocol = {
+	.protocol = IPPROTO_UDP,
+	.dest_socket = &rt_udp_dest_socket,
+	.rcv_handler = &rt_udp_rcv,
+	.err_handler = &rt_udp_rcv_err,
+	.init_socket = &rt_udp_socket
+};
+
+#if 0
+/* kept for doc purposes */
+static struct rtdm_driver udp_driver = {
+	.profile_info = RTDM_PROFILE_INFO(udp,
+					  RTDM_CLASS_NETWORK,
+					  RTDM_SUBCLASS_RTNET,
+					  RTNET_RTDM_VER),
+	.device_flags = RTDM_PROTOCOL_DEVICE,
+	.device_count = 1,
+	.context_size = sizeof(struct rtsocket),
+
+	.protocol_family = PF_INET,
+	.socket_type = SOCK_DGRAM,
+
+	/* default is UDP */
+	.ops = {
+		.socket = rt_inet_socket,
+		.close = rt_udp_close,
+		.ioctl_rt = rt_udp_ioctl,
+		.ioctl_nrt = rt_udp_ioctl,
+		.recvmsg_rt = rt_udp_recvmsg,
+		.sendmsg_rt = rt_udp_sendmsg,
+		.select = rt_socket_select_bind,
+		},
+};
+#endif
+
+/***
+ *  rt_udp_init
+ */
+static int __init rt_udp_init(void)
+{
+	int i;
+
+	raw_spin_lock_init(&udp_socket_base_lock);
+
+	if ((auto_port_start < 0)
+	    || (auto_port_start >= 0x10000 - RT_UDP_SOCKETS))
+		auto_port_start = 1024;
+	auto_port_start = htons(auto_port_start & (auto_port_mask & 0xFFFF));
+	auto_port_mask = htons(auto_port_mask | 0xFFFF0000);
+
+	rt_inet_add_protocol(&udp_protocol);
+
+	for (i = 0; i < ARRAY_SIZE(port_hash); i++)
+		INIT_HLIST_HEAD(&port_hash[i]);
+
+	return 0;
+}
+
+/***
+ *  rt_udp_release
+ */
+static void __exit rt_udp_release(void)
+{
+	rt_inet_del_protocol(&udp_protocol);
+}
+
+module_init(rt_udp_init);
+module_exit(rt_udp_release);
diff -Naur a/net/rtnet/stack/Kconfig b/net/rtnet/stack/Kconfig
--- a/net/rtnet/stack/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/Kconfig	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,41 @@
+menu "Protocol Stack"
+    depends on RTNET
+
+comment "Stack parameters"
+
+config RTNET_RX_FIFO_SIZE
+    int "Size of central RX-FIFO"
+    depends on RTNET
+    default 32
+    help
+    Size of FIFO between NICs and stack manager task. Must be power
+    of two! Effectively, only CONFIG_RTNET_RX_FIFO_SIZE-1 slots will
+    be usable.
+
+config RTNET_ETH_P_ALL
+    depends on RTNET
+    bool "Support for ETH_P_ALL"
+    help
+    Enables core support for registering listeners on all layer 3
+    protocols (ETH_P_ALL). Internally this is currently realised by
+    clone-copying incoming frames for those listners, future versions
+    will implement buffer sharing for efficiency reasons. Use with
+    care, every ETH_P_ALL-listener adds noticable overhead to the
+    reception path.
+
+config RTNET_RTWLAN
+    depends on RTNET
+    bool "Real-Time WLAN"
+    help
+    Enables core support for real-time wireless LAN. RT-WLAN is based
+    on low-level access to 802.11-compliant adapters and is currently
+    in an experimental stage.
+
+comment "Protocols"
+
+source "net/rtnet/stack/ipv4/Kconfig"
+source "net/rtnet/stack/packet/Kconfig"
+source "net/rtnet/stack/rtmac/Kconfig"
+source "net/rtnet/stack/rtcfg/Kconfig"
+
+endmenu
diff -Naur a/net/rtnet/stack/Makefile b/net/rtnet/stack/Makefile
--- a/net/rtnet/stack/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/Makefile	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,26 @@
+ccflags-y += -Inet/rtnet/stack/include -Ikernel/
+
+obj-$(CONFIG_RTNET) += rtnet.o
+
+obj-$(CONFIG_RTNET_RTIPV4) += ipv4/
+
+obj-$(CONFIG_RTNET_RTPACKET) += packet/
+
+obj-$(CONFIG_RTNET_RTMAC) += rtmac/
+
+obj-$(CONFIG_RTNET_RTCFG) += rtcfg/
+
+rtnet-y :=  \
+	rtnet_rtdm.o \
+	iovec.o \
+	rtdev.o \
+	rtdev_mgr.o \
+	rtnet_chrdev.o \
+	rtnet_module.o \
+	rtnet_rtpc.o \
+	rtskb.o \
+	socket.o \
+	stack_mgr.o \
+	eth.o
+
+rtnet-$(CONFIG_RTNET_RTWLAN) += rtwlan.o
diff -Naur a/net/rtnet/stack/modules.builtin b/net/rtnet/stack/modules.builtin
--- a/net/rtnet/stack/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/modules.builtin	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,4 @@
+net/rtnet/stack/rtnet.ko
+net/rtnet/stack/ipv4/rtipv4.ko
+net/rtnet/stack/ipv4/udp/rtudp.ko
+net/rtnet/stack/packet/rtpacket.ko
diff -Naur a/net/rtnet/stack/packet/af_packet.c b/net/rtnet/stack/packet/af_packet.c
--- a/net/rtnet/stack/packet/af_packet.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/packet/af_packet.c	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,719 @@
+/***
+ *
+ *  packet/af_packet.c
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *  Copyright (C) 2006 Jorge Almeida <j-almeida@criticalsoftware.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/err.h>
+#include <linux/poll.h>
+
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+#include <stack_mgr.h>
+
+MODULE_LICENSE("GPL");
+
+/***
+ *  rt_packet_rcv
+ */
+static int rt_packet_rcv(struct rtskb *skb, struct rtpacket_type *pt)
+{
+	struct rtsocket *sock = container_of(pt, struct rtsocket,
+					     prot.packet.packet_type);
+	int ifindex = sock->prot.packet.ifindex;
+	void (*callback_func)(void *, void *);
+	void *callback_arg;
+	unsigned long context;
+
+	if (unlikely((ifindex != 0) && (ifindex != skb->rtdev->ifindex))) {
+		return -EUNATCH;
+	}
+#ifdef CONFIG_RTNET_ETH_P_ALL
+	if (pt->type == htons(ETH_P_ALL)) {
+		struct rtskb *clone_skb = rtskb_clone(skb, &sock->skb_pool);
+		if (clone_skb == NULL)
+			goto out;
+		skb = clone_skb;
+	} else
+#endif				/* CONFIG_RTNET_ETH_P_ALL */
+	if (unlikely(rtskb_acquire(skb, &sock->skb_pool) < 0)) {
+		kfree_rtskb(skb);
+		goto out;
+	}
+
+	rtskb_queue_tail(&sock->incoming, skb);
+	up(&sock->pending_sem);
+
+	if (wq_has_sleeper_rtnet(&sock->wq_head_rtnet))
+		wake_up_interruptible_sync_poll_rtnet(&sock->wq_head_rtnet,
+						      EPOLLIN | EPOLLRDNORM |
+						      EPOLLRDBAND);
+
+	raw_spin_lock_irqsave(&sock->param_lock, context);
+	callback_func = sock->callback_func;
+	callback_arg = sock->callback_arg;
+	raw_spin_unlock_irqrestore(&sock->param_lock, context);
+
+	if (callback_func)
+		callback_func(sock, callback_arg);
+
+ out:
+	return 0;
+}
+
+static bool rt_packet_trylock(struct rtpacket_type *pt)
+{
+	struct rtsocket *sock = container_of(pt, struct rtsocket,
+					     prot.packet.packet_type);
+
+	if (rtdm_fd_lock(sock) < 0)
+		return false;
+
+	return true;
+}
+
+static void rt_packet_unlock(struct rtpacket_type *pt)
+{
+	struct rtsocket *sock = container_of(pt, struct rtsocket,
+					     prot.packet.packet_type);
+
+	rtdm_fd_unlock(sock);
+}
+
+/***
+ *  rt_packet_bind
+ */
+int rt_packet_bind(struct rtsocket *sock,
+		   struct sockaddr __user * addr, socklen_t addrlen)
+{
+	struct sockaddr_ll _sll, *sll;
+	struct rtpacket_type *pt = &sock->prot.packet.packet_type;
+	int new_type;
+	int ret;
+	unsigned long context;
+
+	if (addrlen < sizeof(struct sockaddr_ll))
+		return -EINVAL;
+
+	sll = rtnet_get_arg(sock, &_sll, addr, sizeof(_sll), 1);
+	if (IS_ERR(sll))
+		return PTR_ERR(sll);
+
+	if (sll->sll_family != AF_PACKET)
+		return -EINVAL;
+
+	new_type =
+	    (sll->sll_protocol != 0) ? sll->sll_protocol : sock->protocol;
+
+	raw_spin_lock_irqsave(&sock->param_lock, context);
+
+	/* release existing binding */
+	if (pt->type != 0)
+		rtdev_remove_pack(pt);
+
+	pt->type = new_type;
+	sock->prot.packet.ifindex = sll->sll_ifindex;
+
+	/* if protocol is non-zero, register the packet type */
+	if (new_type != 0) {
+		pt->handler = rt_packet_rcv;
+		pt->err_handler = NULL;
+		pt->trylock = rt_packet_trylock;
+		pt->unlock = rt_packet_unlock;
+
+		ret = rtdev_add_pack(pt);
+	} else
+		ret = 0;
+
+	raw_spin_unlock_irqrestore(&sock->param_lock, context);
+
+	return ret;
+}
+
+EXPORT_SYMBOL_GPL(rt_packet_bind);
+
+/**
+ *  rt_packet_getsockname
+ */
+static int rt_packet_getsockname(struct rtsocket *sock,
+				 struct sockaddr *addr, socklen_t * addrlen)
+{
+	struct sockaddr_ll _sll, *sll;
+	struct rtnet_device *rtdev;
+	unsigned long context;
+	socklen_t _namelen, *namelen;
+	int ret;
+
+	namelen = rtnet_get_arg(sock, &_namelen, addrlen, sizeof(_namelen), 1);
+	if (IS_ERR(namelen))
+		return PTR_ERR(namelen);
+
+	if (*namelen < sizeof(struct sockaddr_ll))
+		return -EINVAL;
+
+	sll = rtnet_get_arg(sock, &_sll, addr, sizeof(_sll), 1);
+	if (IS_ERR(sll))
+		return PTR_ERR(sll);
+
+	raw_spin_lock_irqsave(&sock->param_lock, context);
+
+	sll->sll_family = AF_PACKET;
+	sll->sll_ifindex = sock->prot.packet.ifindex;
+	sll->sll_protocol = sock->protocol;
+
+	raw_spin_unlock_irqrestore(&sock->param_lock, context);
+
+	rtdev = rtdev_get_by_index(sll->sll_ifindex);
+	if (rtdev != NULL) {
+		sll->sll_hatype = rtdev->type;
+		sll->sll_halen = rtdev->addr_len;
+		memcpy(sll->sll_addr, rtdev->dev_addr, rtdev->addr_len);
+		rtdev_dereference(rtdev);
+	} else {
+		sll->sll_hatype = 0;
+		sll->sll_halen = 0;
+	}
+
+	*namelen = sizeof(struct sockaddr_ll);
+
+	ret = rtnet_put_arg(sock, addr, sll, sizeof(*sll), 1);
+	if (ret)
+		return ret;
+
+	return rtnet_put_arg(sock, addrlen, namelen, sizeof(*namelen), 1);
+}
+
+/***
+ * rt_packet_socket - initialize a packet socket
+ */
+int rt_packet_socket(struct rtsocket **psock, int family, int type,
+		     int protocol)
+{
+	int ret;
+	struct rtsocket *sock;
+
+	sock = kzalloc(sizeof(struct rtsocket), GFP_KERNEL);
+	if (!sock)
+		return -ENOMEM;
+
+	if ((ret = rt_socket_init(sock, protocol)) != 0) {
+		kfree(sock);
+		return ret;
+	}
+
+	sock->family = family;
+	sock->type = type;
+	sock->protocol = protocol;
+
+	sock->prot.packet.packet_type.type = protocol;
+	sock->prot.packet.ifindex = 0;
+	sock->prot.packet.packet_type.trylock = rt_packet_trylock;
+	sock->prot.packet.packet_type.unlock = rt_packet_unlock;
+
+	/* if protocol is non-zero, register the packet type */
+	if (protocol != 0) {
+		sock->prot.packet.packet_type.handler = rt_packet_rcv;
+		sock->prot.packet.packet_type.err_handler = NULL;
+
+		if ((ret = rtdev_add_pack(&sock->prot.packet.packet_type)) < 0) {
+			rt_socket_cleanup(sock);
+			kfree(sock);
+			return ret;
+		}
+	}
+
+	*psock = sock;
+
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(rt_packet_socket);
+
+/***
+ *  rt_packet_close
+ */
+void rt_packet_close(struct rtsocket *sock)
+{
+	struct rtpacket_type *pt = &sock->prot.packet.packet_type;
+	struct rtskb *del;
+	unsigned long context;
+
+	raw_spin_lock_irqsave(&sock->param_lock, context);
+
+	if (pt->type != 0) {
+		rtdev_remove_pack(pt);
+		pt->type = 0;
+	}
+
+	raw_spin_unlock_irqrestore(&sock->param_lock, context);
+
+	/* free packets in incoming queue */
+	while ((del = rtskb_dequeue(&sock->incoming)) != NULL) {
+		kfree_rtskb(del);
+	}
+
+	rt_socket_cleanup(sock);
+
+	kfree(sock);
+}
+
+EXPORT_SYMBOL_GPL(rt_packet_close);
+
+/***
+ *  rt_packet_ioctl
+ */
+int rt_packet_ioctl(struct rtsocket *sock, unsigned int request,
+		    void __user * arg)
+{
+#if 0
+	const struct _rtdm_setsockaddr_args *setaddr;
+	struct _rtdm_setsockaddr_args _setaddr;
+#endif
+	const struct _rtdm_getsockaddr_args *getaddr;
+	struct _rtdm_getsockaddr_args _getaddr;
+
+	/* fast path for common socket IOCTLs */
+	if (_IOC_TYPE(request) == RTIOC_TYPE_NETWORK)
+		return rt_socket_common_ioctl(sock, request, arg);
+
+	switch (request) {
+#if 0
+	case _RTIOC_BIND:
+		setaddr =
+		    rtnet_get_arg(sock, &_setaddr, arg, sizeof(_setaddr), 1);
+		if (IS_ERR(setaddr))
+			return PTR_ERR(setaddr);
+		return rt_packet_bind(sock, setaddr->addr, setaddr->addrlen);
+#endif
+	case _RTIOC_GETSOCKNAME:
+		getaddr =
+		    rtnet_get_arg(sock, &_getaddr, arg, sizeof(_getaddr), 1);
+		if (IS_ERR(getaddr))
+			return PTR_ERR(getaddr);
+		return rt_packet_getsockname(sock, getaddr->addr,
+					     getaddr->addrlen);
+
+	default:
+		return rt_socket_if_ioctl(sock, request, arg);
+	}
+}
+
+EXPORT_SYMBOL_GPL(rt_packet_ioctl);
+
+/***
+ *  rt_packet_recvmsg
+ */
+ssize_t
+rt_packet_recvmsg(struct rtsocket *sock, struct user_msghdr *u_msg,
+		  int msg_flags, int msg_in_userspace)
+{
+	ssize_t len;
+	size_t copy_len;
+	struct rtskb *rtskb;
+	struct sockaddr_ll sll;
+	int ret, flags;
+	struct user_msghdr _msg, *msg;
+	socklen_t namelen;
+	struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+	struct hrtimer_sleeper timeout, *to = NULL;
+
+	msg = rtnet_get_arg(sock, &_msg, u_msg, sizeof(_msg), msg_in_userspace);
+	if (IS_ERR(msg))
+		return PTR_ERR(msg);
+
+	if (msg->msg_iovlen < 0)
+		return -EINVAL;
+
+	if (msg->msg_iovlen == 0)
+		return 0;
+
+	ret = rtdm_get_iovec(&iov, msg, iov_fast, msg_in_userspace);
+	if (ret)
+		return ret;
+
+	/* non-blocking receive? */
+	if (msg_flags & MSG_DONTWAIT) {
+		/* down_trylock returns 0 (success) or 1 (fail). */
+		if ((ret = down_trylock(&sock->pending_sem)))
+			ret = -EWOULDBLOCK;
+	} else {
+		to = NULL;
+		if (sock->timeout) {
+			to = &timeout;
+			/* If the current task is in a real-time scheduling class,
+			 * the hrtimer will be marked in the mode for hard interrupt expiry.
+			 */
+			hrtimer_init_sleeper_on_stack(to, CLOCK_MONOTONIC,
+						      HRTIMER_MODE_REL_HARD);
+			hrtimer_set_expires(&to->timer, sock->timeout);
+			hrtimer_sleeper_start_expires(to, HRTIMER_MODE_REL_HARD);
+		}
+		ret = down_hrtimeout(&sock->pending_sem, to);
+		if (to)
+			hrtimer_try_to_cancel(&to->timer);
+	}
+	if (unlikely(ret < 0)) {
+		switch (ret) {
+		case -ETIME:
+			ret = -EAGAIN;
+		case -EWOULDBLOCK:
+		case -ETIMEDOUT:
+		case -EINTR:
+			rtdm_drop_iovec(iov, iov_fast);
+			break;
+		default:
+			ret = -EBADF;	/* socket has been closed */
+		}
+		return ret;
+	}
+
+	rtskb = rtskb_dequeue_chain(&sock->incoming);
+	RTNET_ASSERT(rtskb != NULL, return -EFAULT;
+	    );
+
+	/* copy the address if required. */
+	if (msg->msg_name) {
+		struct rtnet_device *rtdev = rtskb->rtdev;
+		memset(&sll, 0, sizeof(sll));
+		sll.sll_family = AF_PACKET;
+		sll.sll_hatype = rtdev->type;
+		sll.sll_protocol = rtskb->protocol;
+		sll.sll_pkttype = rtskb->pkt_type;
+		sll.sll_ifindex = rtdev->ifindex;
+
+		/* Ethernet specific - we rather need some parse handler here */
+		memcpy(sll.sll_addr, rtskb->mac.ethernet->h_source, ETH_ALEN);
+		sll.sll_halen = ETH_ALEN;
+		ret =
+		    rtnet_put_arg(sock, msg->msg_name, &sll, sizeof(sll),
+				  msg_in_userspace);
+		if (ret)
+			goto fail;
+
+		namelen = sizeof(sll);
+		ret =
+		    rtnet_put_arg(sock, &u_msg->msg_namelen, &namelen,
+				  sizeof(namelen), msg_in_userspace);
+		if (ret)
+			goto fail;
+	}
+
+	/* Include the header in raw delivery */
+	if (sock->type != SOCK_DGRAM)
+		rtskb_push(rtskb, rtskb->data - rtskb->mac.raw);
+
+	/* The data must not be longer than the available buffer size */
+	copy_len = rtskb->len;
+	len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+	if (len < 0) {
+		copy_len = len;
+		goto out;
+	}
+
+	if (copy_len > len) {
+		copy_len = len;
+		flags = msg->msg_flags | MSG_TRUNC;
+		ret =
+		    rtnet_put_arg(sock, &u_msg->msg_flags, &flags,
+				  sizeof(flags), msg_in_userspace);
+		if (ret)
+			goto fail;
+	}
+
+	copy_len =
+	    rtnet_write_to_iov(sock, iov, msg->msg_iovlen, rtskb->data,
+			       copy_len, msg_in_userspace);
+ out:
+	if ((msg_flags & MSG_PEEK) == 0) {
+		kfree_rtskb(rtskb);
+	} else {
+		rtskb_queue_head(&sock->incoming, rtskb);
+		up(&sock->pending_sem);
+	}
+
+	rtdm_drop_iovec(iov, iov_fast);
+
+	return copy_len;
+ fail:
+	copy_len = ret;
+	goto out;
+}
+
+EXPORT_SYMBOL_GPL(rt_packet_recvmsg);
+
+/***
+ *  rt_packet_sendmsg
+ */
+ssize_t
+rt_packet_sendmsg(struct rtsocket *sock, const struct user_msghdr *msg,
+		  int msg_flags, int msg_in_userspace)
+{
+	size_t len;
+	struct sockaddr_ll _sll, *sll;
+	struct rtnet_device *rtdev;
+	struct rtskb *rtskb;
+	unsigned short proto;
+	unsigned char *addr;
+	int ifindex;
+	ssize_t ret;
+	struct user_msghdr _msg;
+	struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+	if (msg_flags & MSG_OOB)	/* Mirror BSD error message compatibility */
+		return -EOPNOTSUPP;
+
+	if (msg_flags & ~MSG_DONTWAIT)
+		return -EINVAL;
+
+	msg = rtnet_get_arg(sock, &_msg, msg, sizeof(*msg), msg_in_userspace);
+	if (IS_ERR(msg))
+		return PTR_ERR(msg);
+
+	if (msg->msg_iovlen < 0)
+		return -EINVAL;
+
+	if (msg->msg_iovlen == 0)
+		return 0;
+
+	ret = rtdm_get_iovec(&iov, msg, iov_fast, msg_in_userspace);
+	if (ret)
+		return ret;
+
+	if (msg->msg_name == NULL) {
+		/* Note: We do not care about races with rt_packet_bind here -
+		   the user has to do so. */
+		ifindex = sock->prot.packet.ifindex;
+		proto = sock->prot.packet.packet_type.type;
+		addr = NULL;
+		sll = NULL;
+	} else {
+		sll =
+		    rtnet_get_arg(sock, &_sll, msg->msg_name, sizeof(_sll),
+				  msg_in_userspace);
+		if (IS_ERR(sll)) {
+			ret = PTR_ERR(sll);
+			goto abort;
+		}
+
+		if ((msg->msg_namelen < sizeof(struct sockaddr_ll)) ||
+		    (msg->msg_namelen <
+		     (sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))
+		    || ((sll->sll_family != AF_PACKET)
+			&& (sll->sll_family != AF_UNSPEC))) {
+			ret = -EINVAL;
+			goto abort;
+		}
+
+		ifindex = sll->sll_ifindex;
+		proto = sll->sll_protocol;
+		addr = sll->sll_addr;
+	}
+
+	if ((rtdev = rtdev_get_by_index(ifindex)) == NULL) {
+		ret = -ENODEV;
+		goto abort;
+	}
+
+	len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+	rtskb = alloc_rtskb(rtdev->hard_header_len + len, &sock->skb_pool);
+	if (rtskb == NULL) {
+		ret = -ENOBUFS;
+		goto out;
+	}
+
+	/* If an RTmac discipline is active, this becomes a pure sanity check to
+	   avoid writing beyond rtskb boundaries. The hard check is then performed
+	   upon rtdev_xmit() by the discipline's xmit handler. */
+	if (len > rtdev->mtu +
+	    (sock->type == SOCK_RAW) ? rtdev->hard_header_len : 0) {
+		ret = -EMSGSIZE;
+		goto err;
+	}
+
+	if ((sll != NULL) && (sll->sll_halen != rtdev->addr_len)) {
+		ret = -EINVAL;
+		goto err;
+	}
+
+	rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+	rtskb->rtdev = rtdev;
+	rtskb->priority = sock->priority;
+
+	if (rtdev->hard_header) {
+		int hdr_len;
+
+		ret = -EINVAL;
+		hdr_len = rtdev->hard_header(rtskb, rtdev, ntohs(proto),
+					     addr, NULL, len);
+		if (sock->type != SOCK_DGRAM) {
+			rtskb->tail = rtskb->data;
+			rtskb->len = 0;
+		} else if (hdr_len < 0)
+			goto err;
+	}
+
+	ret =
+	    rtnet_read_from_iov(sock, iov, msg->msg_iovlen,
+				rtskb_put(rtskb, len), len, msg_in_userspace);
+
+	if ((rtdev->flags & IFF_UP) != 0) {
+		if ((ret = rtdev_xmit(rtskb)) == 0)
+			ret = len;
+	} else {
+		ret = -ENETDOWN;
+		goto err;
+	}
+
+ out:
+	rtdev_dereference(rtdev);
+ abort:
+	rtdm_drop_iovec(iov, iov_fast);
+
+	return ret;
+ err:
+	kfree_rtskb(rtskb);
+	goto out;
+}
+
+EXPORT_SYMBOL_GPL(rt_packet_sendmsg);
+
+#if 0
+static struct rtdm_driver packet_proto_drv = {
+	.profile_info = RTDM_PROFILE_INFO(packet,
+					  RTDM_CLASS_NETWORK,
+					  RTDM_SUBCLASS_RTNET,
+					  RTNET_RTDM_VER),
+	.device_flags = RTDM_PROTOCOL_DEVICE,
+	.device_count = 1,
+	.context_size = sizeof(struct rtsocket),
+
+	.protocol_family = PF_PACKET,
+	.socket_type = SOCK_DGRAM,
+
+	.ops = {
+		.socket = rt_packet_socket,
+		.close = rt_packet_close,
+		.ioctl_rt = rt_packet_ioctl,
+		.ioctl_nrt = rt_packet_ioctl,
+		.recvmsg_rt = rt_packet_recvmsg,
+		.sendmsg_rt = rt_packet_sendmsg,
+		.select = rt_socket_select_bind,
+		},
+};
+
+static struct rtdm_device packet_proto_dev = {
+	.driver = &packet_proto_drv,
+	.label = "packet",
+};
+
+static struct rtdm_driver raw_packet_proto_drv = {
+	.profile_info = RTDM_PROFILE_INFO(raw_packet,
+					  RTDM_CLASS_NETWORK,
+					  RTDM_SUBCLASS_RTNET,
+					  RTNET_RTDM_VER),
+	.device_flags = RTDM_PROTOCOL_DEVICE,
+	.device_count = 1,
+	.context_size = sizeof(struct rtsocket),
+
+	.protocol_family = PF_PACKET,
+	.socket_type = SOCK_RAW,
+
+	.ops = {
+		.socket = rt_packet_socket,
+		.close = rt_packet_close,
+		.ioctl_rt = rt_packet_ioctl,
+		.ioctl_nrt = rt_packet_ioctl,
+		.recvmsg_rt = rt_packet_recvmsg,
+		.sendmsg_rt = rt_packet_sendmsg,
+		.select = rt_socket_select_bind,
+		},
+};
+
+static struct rtdm_device raw_packet_proto_dev = {
+	.driver = &raw_packet_proto_drv,
+	.label = "raw_packet",
+};
+#endif
+
+static int __init rt_packet_proto_init(void)
+{
+	int err = 0;
+
+	return err;
+}
+
+static void rt_packet_proto_release(void)
+{
+}
+
+module_init(rt_packet_proto_init);
+module_exit(rt_packet_proto_release);
+
+/**********************************************************
+ * Utilities                                              *
+ **********************************************************/
+
+static int hex2int(unsigned char hex_char)
+{
+	if ((hex_char >= '0') && (hex_char <= '9'))
+		return hex_char - '0';
+	else if ((hex_char >= 'a') && (hex_char <= 'f'))
+		return hex_char - 'a' + 10;
+	else if ((hex_char >= 'A') && (hex_char <= 'F'))
+		return hex_char - 'A' + 10;
+	else
+		return -EINVAL;
+}
+
+int rt_eth_aton(unsigned char *addr_buf, const char *mac)
+{
+	int i = 0;
+	int nibble;
+
+	while (1) {
+		if (*mac == 0)
+			return -EINVAL;
+
+		if ((nibble = hex2int(*mac++)) < 0)
+			return nibble;
+		*addr_buf = nibble << 4;
+
+		if (*mac == 0)
+			return -EINVAL;
+
+		if ((nibble = hex2int(*mac++)) < 0)
+			return nibble;
+		*addr_buf++ |= nibble;
+
+		if (++i == 6)
+			break;
+
+		if ((*mac == 0) || (*mac++ != ':'))
+			return -EINVAL;
+
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(rt_eth_aton);
diff -Naur a/net/rtnet/stack/packet/Kconfig b/net/rtnet/stack/packet/Kconfig
--- a/net/rtnet/stack/packet/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/packet/Kconfig	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,14 @@
+config RTNET_RTPACKET
+    depends on RTNET
+    tristate "Real-Time Packet Socket Support"
+    default y
+    help
+    Enables real-time packet sockets for RTnet. This support is
+    implemented in a separate module. When loaded, application programs
+    can send and received so-called "cooked" packets directly at OSI layer
+    2 (device layer). This means that RTnet will still maintain the
+    device-dependent packet header but leave the full data segment to the
+    user.
+
+    Examples like raw-ethernet or netshm make use of this support. See
+    also Linux man page packet(7).
diff -Naur a/net/rtnet/stack/packet/Makefile b/net/rtnet/stack/packet/Makefile
--- a/net/rtnet/stack/packet/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/packet/Makefile	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,5 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_RTPACKET) += rtpacket.o
+
+rtpacket-y := af_packet.o
diff -Naur a/net/rtnet/stack/packet/modules.builtin b/net/rtnet/stack/packet/modules.builtin
--- a/net/rtnet/stack/packet/modules.builtin	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/packet/modules.builtin	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1 @@
+net/rtnet/stack/packet/rtpacket.ko
diff -Naur a/net/rtnet/stack/rtcfg/Kconfig b/net/rtnet/stack/rtcfg/Kconfig
--- a/net/rtnet/stack/rtcfg/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/Kconfig	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,23 @@
+config RTNET_RTCFG
+    depends on RTNET
+    tristate "RTcfg Service"
+    default y
+    help
+    The Real-Time Configuration service configures and monitors nodes in
+    a RTnet network. It works both with plain MAC as well as with IPv4
+    addresses (in case CONFIG_RTNET_RTIPV4 has been switched on). RTcfg
+    consists of a configuration server, which can run on the same station
+    as the TDMA master e.g., and one or more clients. Clients can join and
+    leave the network during runtime without interfering with other
+    stations. Besides network configuration, the RTcfg server can also
+    distribute custom data.
+
+    See Documentation/README.rtcfg for further information.
+
+config RTNET_RTCFG_DEBUG
+    bool "RTcfg Debugging"
+    depends on RTNET_RTCFG
+    default n
+    help
+    Enables debug message output of the RTcfg state machines. Switch on if
+    you have to trace some problem related to RTcfg.
diff -Naur a/net/rtnet/stack/rtcfg/Makefile b/net/rtnet/stack/rtcfg/Makefile
--- a/net/rtnet/stack/rtcfg/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/Makefile	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,14 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_RTCFG) += rtcfg.o
+
+rtcfg-y := \
+	rtcfg_module.o \
+	rtcfg_event.o \
+	rtcfg_client_event.o \
+	rtcfg_conn_event.o \
+	rtcfg_ioctl.o \
+	rtcfg_frame.o \
+	rtcfg_timer.o \
+	rtcfg_file.o \
+	rtcfg_proc.o
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_client_event.c b/net/rtnet/stack/rtcfg/rtcfg_client_event.c
--- a/net/rtnet/stack/rtcfg/rtcfg_client_event.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_client_event.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,1183 @@
+/***
+ *
+ *  rtcfg/rtcfg_client_event.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <ipv4/route.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+
+static int rtcfg_client_get_frag(int ifindex, struct rt_proc_call *call);
+static void rtcfg_client_detach(int ifindex, struct rt_proc_call *call);
+static void rtcfg_client_recv_stage_1(int ifindex, struct rtskb *rtskb);
+static int rtcfg_client_recv_announce(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_recv_stage_2_cfg(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_recv_stage_2_frag(int ifindex, struct rtskb *rtskb);
+static int rtcfg_client_recv_ready(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_recv_dead_station(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_update_server(int ifindex, struct rtskb *rtskb);
+
+
+/*** Client States ***/
+
+int rtcfg_main_state_client_0(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            rtcfg_client_recv_stage_1(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_1(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_cmd    *cmd_event;
+    int                 ret;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_CLIENT:
+            /* second trial (buffer was probably too small) */
+            rtcfg_queue_blocking_call(ifindex,
+                (struct rt_proc_call *)event_data);
+
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_0);
+
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            return -CALL_PENDING;
+
+        case RTCFG_CMD_ANNOUNCE:
+            cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+            if (cmd_event->args.announce.burstrate == 0) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                return -EINVAL;
+            }
+
+            rtcfg_queue_blocking_call(ifindex,
+                (struct rt_proc_call *)event_data);
+
+	    if (cmd_event->args.announce.flags & _RTCFG_FLAG_STAGE_2_DATA)
+		set_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags);
+	    if (cmd_event->args.announce.flags & _RTCFG_FLAG_READY)
+		set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags);
+            if (cmd_event->args.announce.burstrate < rtcfg_dev->burstrate)
+                rtcfg_dev->burstrate = cmd_event->args.announce.burstrate;
+
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ANNOUNCED);
+
+            ret = rtcfg_send_announce_new(ifindex);
+            if (ret < 0) {
+                rtcfg_dequeue_blocking_call(ifindex);
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                return ret;
+            }
+
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            return -CALL_PENDING;
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_announced(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_ANNOUNCE:
+            return rtcfg_client_get_frag(ifindex, call);
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_STAGE_2_CFG:
+            rtcfg_client_recv_stage_2_cfg(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_2_CFG_FRAG:
+            rtcfg_client_recv_stage_2_frag(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations)
+                    rtcfg_next_main_state(ifindex,
+                        RTCFG_MAIN_CLIENT_ALL_KNOWN);
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations)
+                    rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_KNOWN);
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_all_known(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_ANNOUNCE:
+            return rtcfg_client_get_frag(ifindex, call);
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_STAGE_2_CFG_FRAG:
+            rtcfg_client_recv_stage_2_frag(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_all_frames(int ifindex, RTCFG_EVENT event_id,
+                                       void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+                    rtcfg_complete_cmd(ifindex, RTCFG_CMD_ANNOUNCE, 0);
+
+                    rtcfg_next_main_state(ifindex,
+					test_bit(RTCFG_FLAG_READY,
+						&rtcfg_dev->flags) ?
+					RTCFG_MAIN_CLIENT_READY
+					: RTCFG_MAIN_CLIENT_2);
+                }
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+                    rtcfg_complete_cmd(ifindex, RTCFG_CMD_ANNOUNCE, 0);
+
+                    rtcfg_next_main_state(ifindex,
+					test_bit(RTCFG_FLAG_READY,
+						&rtcfg_dev->flags) ?
+					RTCFG_MAIN_CLIENT_READY
+					: RTCFG_MAIN_CLIENT_2);
+                }
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+int rtcfg_main_state_client_2(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_READY:
+            rtcfg_dev = &device[ifindex];
+
+            if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+                rtpc_complete_call(call, 0);
+            else
+                rtcfg_queue_blocking_call(ifindex, call);
+
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_READY);
+
+            if (!test_and_set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags))
+                rtcfg_send_ready(ifindex);
+
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            return -CALL_PENDING;
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_ready(int ifindex, RTCFG_EVENT event_id,
+                                  void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0) {
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+                    rtcfg_complete_cmd(ifindex, RTCFG_CMD_READY, 0);
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            rtcfg_client_update_server(ifindex, rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+/*** Client Command Event Handlers ***/
+
+static int rtcfg_client_get_frag(int ifindex, struct rt_proc_call *call)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+    if (test_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags) == 0) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        return -EINVAL;
+    }
+
+    rtcfg_send_ack(ifindex);
+
+    if (rtcfg_dev->spec.clt.cfg_offs >= rtcfg_dev->spec.clt.cfg_len) {
+        if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+            rtpc_complete_call(call, 0);
+
+            rtcfg_next_main_state(ifindex,
+				test_bit(RTCFG_FLAG_READY,
+					&rtcfg_dev->flags) ?
+				RTCFG_MAIN_CLIENT_READY : RTCFG_MAIN_CLIENT_2);
+        } else {
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_FRAMES);
+            rtcfg_queue_blocking_call(ifindex, call);
+        }
+    } else
+        rtcfg_queue_blocking_call(ifindex, call);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return -CALL_PENDING;
+}
+
+
+
+/* releases rtcfg_dev->dev_mutex on return */
+static void rtcfg_client_detach(int ifindex, struct rt_proc_call *call)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+    struct rtcfg_cmd    *cmd_event;
+
+
+    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+    cmd_event->args.detach.station_addr_list =
+        rtcfg_dev->spec.clt.station_addr_list;
+    cmd_event->args.detach.stage2_chain = rtcfg_dev->spec.clt.stage2_chain;
+
+    while (1) {
+        call = rtcfg_dequeue_blocking_call(ifindex);
+        if (call == NULL)
+            break;
+
+        rtpc_complete_call(call, -ENODEV);
+    }
+
+    if (test_and_clear_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags))
+        rtdm_timer_destroy(&rtcfg_dev->timer);
+    rtcfg_reset_device(ifindex);
+
+    rtcfg_next_main_state(cmd_event->internal.data.ifindex, RTCFG_MAIN_OFF);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+}
+
+
+
+/*** Client Frame Event Handlers ***/
+
+static void rtcfg_client_recv_stage_1(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_1_cfg *stage_1_cfg;
+    struct rt_proc_call          *call;
+    struct rtcfg_cmd             *cmd_event;
+    struct rtcfg_device          *rtcfg_dev = &device[ifindex];
+    u8                           addr_type;
+    int                          ret;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_1_cfg));
+
+    addr_type = stage_1_cfg->addr_type;
+
+    switch (stage_1_cfg->addr_type) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+        case RTCFG_ADDR_IP: {
+            struct rtnet_device *rtdev, *tmp;
+            u32                 daddr, saddr, mask, bcast;
+
+            if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg) +
+                    2*RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg "
+                            "frame\n");
+                kfree_rtskb(rtskb);
+                return;
+            }
+
+            rtdev = rtskb->rtdev;
+
+            memcpy(&daddr, stage_1_cfg->client_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            memcpy(&saddr, stage_1_cfg->server_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            __rtskb_pull(rtskb, 2*RTCFG_ADDRSIZE_IP);
+
+            /* Broadcast: IP is used to address client */
+            if (rtskb->pkt_type == PACKET_BROADCAST) {
+                /* directed to us? */
+                if (daddr != rtdev->local_ip) {
+                    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                    kfree_rtskb(rtskb);
+                    return;
+                }
+
+            /* Unicast: IP address is assigned by the server */
+            } else {
+                /* default netmask */
+                if (ntohl(daddr) <= 0x7FFFFFFF)         /* 127.255.255.255  */
+                    mask = 0x000000FF;                  /* 255.0.0.0        */
+                else if (ntohl(daddr) <= 0xBFFFFFFF)    /* 191.255.255.255  */
+                    mask = 0x0000FFFF;                  /* 255.255.0.0      */
+                else
+                    mask = 0x00FFFFFF;                  /* 255.255.255.0    */
+                bcast = daddr | (~mask);
+
+                rt_ip_route_del_all(rtdev); /* cleanup routing table */
+
+                rtdev->local_ip     = daddr;
+                rtdev->broadcast_ip = bcast;
+
+                if ((tmp = rtdev_get_loopback()) != NULL) {
+                    rt_ip_route_add_host(daddr, tmp->dev_addr, tmp);
+                    rtdev_dereference(tmp);
+                }
+
+                if (rtdev->flags & IFF_BROADCAST)
+                    rt_ip_route_add_host(bcast, rtdev->broadcast, rtdev);
+            }
+
+            /* update routing table */
+            rt_ip_route_add_host(saddr, rtskb->mac.ethernet->h_source, rtdev);
+
+            rtcfg_dev->spec.clt.srv_addr.ip_addr = saddr;
+            break;
+        }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        stage_1_cfg->addr_type, __FUNCTION__);
+            kfree_rtskb(rtskb);
+            return;
+    }
+
+    rtcfg_dev->spec.clt.addr_type = addr_type;
+
+    /* Ethernet-specific */
+    memcpy(rtcfg_dev->spec.clt.srv_mac_addr,
+        rtskb->mac.ethernet->h_source, ETH_ALEN);
+
+    rtcfg_dev->burstrate = stage_1_cfg->burstrate;
+
+    rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_1);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    while (1) {
+        call = rtcfg_dequeue_blocking_call(ifindex);
+        if (call == NULL)
+            break;
+
+        cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+        if (cmd_event->internal.data.event_id == RTCFG_CMD_CLIENT) {
+            ret = 0;
+
+            /* note: only the first pending call gets data */
+            if ((rtskb != NULL) &&
+                (cmd_event->args.client.buffer_size > 0)) {
+                ret = ntohs(stage_1_cfg->cfg_len);
+
+                cmd_event->args.client.rtskb = rtskb;
+                rtskb = NULL;
+            }
+        } else
+            ret = -EINVAL;
+
+        rtpc_complete_call(call, ret);
+    }
+
+    if (rtskb)
+        kfree_rtskb(rtskb);
+}
+
+
+
+static int rtcfg_add_to_station_list(struct rtcfg_device *rtcfg_dev,
+                                     u8 *mac_addr, u8 flags)
+{
+   if (rtcfg_dev->stations_found == rtcfg_dev->spec.clt.max_stations) {
+        RTCFG_DEBUG(1, "RTcfg: insufficient memory for storing new station "
+                    "address\n");
+        return -ENOMEM;
+    }
+
+    /* Ethernet-specific! */
+    memcpy(&rtcfg_dev->spec.clt.
+           station_addr_list[rtcfg_dev->stations_found].mac_addr,
+           mac_addr, ETH_ALEN);
+
+    rtcfg_dev->spec.clt.station_addr_list[rtcfg_dev->stations_found].flags =
+        flags;
+
+    rtcfg_dev->stations_found++;
+    if ((flags & _RTCFG_FLAG_READY) != 0)
+        rtcfg_dev->stations_ready++;
+
+    return 0;
+}
+
+
+
+/* Notes:
+ *  o rtcfg_client_recv_announce does not release the passed rtskb.
+ *  o On success, rtcfg_client_recv_announce returns without releasing the
+ *    device lock.
+ */
+static int rtcfg_client_recv_announce(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_announce *announce_frm;
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    u32                       i;
+    u32                       announce_frm_addr;
+    int                       result;
+
+
+    announce_frm = (struct rtcfg_frm_announce *)rtskb->data;
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_announce)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid announce frame (id: %d)\n",
+                    announce_frm->head.id);
+        return -EINVAL;
+    }
+
+    switch (announce_frm->addr_type) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+        case RTCFG_ADDR_IP:
+            if (rtskb->len < sizeof(struct rtcfg_frm_announce) +
+                    RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid announce frame "
+                            "(id: %d)\n", announce_frm->head.id);
+                return -EINVAL;
+            }
+
+            memcpy(&announce_frm_addr, announce_frm->addr, 4);
+
+            /* update routing table */
+            rt_ip_route_add_host(announce_frm_addr,
+                                 rtskb->mac.ethernet->h_source, rtskb->rtdev);
+
+            announce_frm = (struct rtcfg_frm_announce *)
+                (((u8 *)announce_frm) + RTCFG_ADDRSIZE_IP);
+
+            break;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        announce_frm->addr_type, __FUNCTION__);
+            return -EINVAL;
+    }
+
+    for (i = 0; i < rtcfg_dev->stations_found; i++)
+        /* Ethernet-specific! */
+        if (memcmp(rtcfg_dev->spec.clt.station_addr_list[i].mac_addr,
+                   rtskb->mac.ethernet->h_source, ETH_ALEN) == 0)
+            return 0;
+
+    result = rtcfg_add_to_station_list(rtcfg_dev,
+        rtskb->mac.ethernet->h_source, announce_frm->flags);
+    if (result < 0)
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return result;
+}
+
+
+
+static void rtcfg_client_queue_frag(int ifindex, struct rtskb *rtskb,
+                                    size_t data_len)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+    struct rt_proc_call *call;
+    struct rtcfg_cmd    *cmd_event;
+    int                 result;
+
+
+    rtskb_trim(rtskb, data_len);
+
+    if (rtcfg_dev->spec.clt.stage2_chain == NULL)
+        rtcfg_dev->spec.clt.stage2_chain = rtskb;
+    else {
+        rtcfg_dev->spec.clt.stage2_chain->chain_end->next = rtskb;
+        rtcfg_dev->spec.clt.stage2_chain->chain_end = rtskb;
+    }
+
+    rtcfg_dev->spec.clt.cfg_offs  += data_len;
+    rtcfg_dev->spec.clt.chain_len += data_len;
+
+    if ((rtcfg_dev->spec.clt.cfg_offs >= rtcfg_dev->spec.clt.cfg_len) ||
+        (++rtcfg_dev->spec.clt.packet_counter == rtcfg_dev->burstrate)) {
+
+        while (1) {
+            call = rtcfg_dequeue_blocking_call(ifindex);
+            if (call == NULL)
+                break;
+
+            cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+            result = 0;
+
+            /* note: only the first pending call gets data */
+            if (rtcfg_dev->spec.clt.stage2_chain != NULL) {
+                result = rtcfg_dev->spec.clt.chain_len;
+                cmd_event->args.announce.rtskb =
+                    rtcfg_dev->spec.clt.stage2_chain;
+                rtcfg_dev->spec.clt.stage2_chain = NULL;
+            }
+
+            rtpc_complete_call(call,
+                (cmd_event->internal.data.event_id == RTCFG_CMD_ANNOUNCE) ?
+                result : -EINVAL);
+        }
+
+        rtcfg_dev->spec.clt.packet_counter = 0;
+        rtcfg_dev->spec.clt.chain_len      = 0;
+    }
+}
+
+
+
+static void rtcfg_client_recv_stage_2_cfg(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_2_cfg *stage_2_cfg;
+    struct rtcfg_device          *rtcfg_dev = &device[ifindex];
+    size_t                       data_len;
+    int                          ret;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_2_cfg)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_2_cfg frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_2_cfg = (struct rtcfg_frm_stage_2_cfg *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg));
+
+    if (stage_2_cfg->heartbeat_period) {
+	ret = rtdm_timer_init(&rtcfg_dev->timer, rtcfg_timer, "rtcfg-timer");
+	if (ret == 0) {
+	    ret = rtdm_timer_start(&rtcfg_dev->timer,
+				XN_INFINITE,
+				(nanosecs_rel_t)ntohs(stage_2_cfg->heartbeat_period) *
+				1000000,
+				RTDM_TIMERMODE_RELATIVE);
+	    if (ret < 0)
+		rtdm_timer_destroy(&rtcfg_dev->timer);
+	}
+
+        if (ret < 0)
+            /*ERRMSG*/rtdm_printk("RTcfg: unable to create timer task\n");
+        else
+	    set_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags);
+    }
+
+    /* add server to station list */
+    if (rtcfg_add_to_station_list(rtcfg_dev,
+            rtskb->mac.ethernet->h_source, stage_2_cfg->flags) < 0) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: unable to process stage_2_cfg frage\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    rtcfg_dev->other_stations   = ntohl(stage_2_cfg->stations);
+    rtcfg_dev->spec.clt.cfg_len = ntohl(stage_2_cfg->cfg_len);
+    data_len = MIN(rtcfg_dev->spec.clt.cfg_len, rtskb->len);
+
+    if (test_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags) &&
+        (data_len > 0)) {
+        rtcfg_client_queue_frag(ifindex, rtskb, data_len);
+        rtskb = NULL;
+
+        if (rtcfg_dev->stations_found == rtcfg_dev->other_stations)
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_KNOWN);
+    } else {
+        if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+            rtcfg_complete_cmd(ifindex, RTCFG_CMD_ANNOUNCE, 0);
+
+            rtcfg_next_main_state(ifindex,
+				test_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags) ?
+				RTCFG_MAIN_CLIENT_READY : RTCFG_MAIN_CLIENT_2);
+        } else
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_FRAMES);
+
+        rtcfg_send_ack(ifindex);
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+static void rtcfg_client_recv_stage_2_frag(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_2_cfg_frag *stage_2_frag;
+    struct rtcfg_device               *rtcfg_dev = &device[ifindex];
+    size_t                            data_len;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_2_cfg_frag)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_2_cfg_frag frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_2_frag = (struct rtcfg_frm_stage_2_cfg_frag *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg_frag));
+
+    data_len = MIN(rtcfg_dev->spec.clt.cfg_len - rtcfg_dev->spec.clt.cfg_offs,
+                   rtskb->len);
+
+    if (test_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags) == 0) {
+        RTCFG_DEBUG(1, "RTcfg: unexpected stage 2 fragment, we did not "
+                    "request any data!\n");
+
+    } else if (rtcfg_dev->spec.clt.cfg_offs !=
+               ntohl(stage_2_frag->frag_offs)) {
+        RTCFG_DEBUG(1, "RTcfg: unexpected stage 2 fragment (expected: %d, "
+                    "received: %d)\n", rtcfg_dev->spec.clt.cfg_offs,
+                    ntohl(stage_2_frag->frag_offs));
+
+        rtcfg_send_ack(ifindex);
+        rtcfg_dev->spec.clt.packet_counter = 0;
+    } else {
+        rtcfg_client_queue_frag(ifindex, rtskb, data_len);
+        rtskb = NULL;
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+/* Notes:
+ *  o On success, rtcfg_client_recv_ready returns without releasing the
+ *    device lock.
+ */
+static int rtcfg_client_recv_ready(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    u32                     i;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_simple)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid ready frame\n");
+        kfree_rtskb(rtskb);
+        return -EINVAL;
+    }
+
+    for (i = 0; i < rtcfg_dev->stations_found; i++)
+        /* Ethernet-specific! */
+        if (memcmp(rtcfg_dev->spec.clt.station_addr_list[i].mac_addr,
+                   rtskb->mac.ethernet->h_source, ETH_ALEN) == 0) {
+            if ((rtcfg_dev->spec.clt.station_addr_list[i].flags &
+                 _RTCFG_FLAG_READY) == 0) {
+                rtcfg_dev->spec.clt.station_addr_list[i].flags |=
+                    _RTCFG_FLAG_READY;
+                rtcfg_dev->stations_ready++;
+            }
+            break;
+        }
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+static void rtcfg_client_recv_dead_station(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_dead_station *dead_station_frm;
+    struct rtcfg_device           *rtcfg_dev = &device[ifindex];
+    u32                           i;
+
+
+    dead_station_frm = (struct rtcfg_frm_dead_station *)rtskb->data;
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_dead_station)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid dead station frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    switch (dead_station_frm->addr_type) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+        case RTCFG_ADDR_IP: {
+            u32 ip;
+
+            if (rtskb->len < sizeof(struct rtcfg_frm_dead_station) +
+                    RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid dead station frame\n");
+                kfree_rtskb(rtskb);
+                return;
+            }
+
+            memcpy(&ip, dead_station_frm->logical_addr, 4);
+
+            /* only delete remote IPs from routing table */
+            if (rtskb->rtdev->local_ip != ip)
+                rt_ip_route_del_host(ip, rtskb->rtdev);
+
+            dead_station_frm = (struct rtcfg_frm_dead_station *)
+                (((u8 *)dead_station_frm) + RTCFG_ADDRSIZE_IP);
+
+            break;
+        }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        dead_station_frm->addr_type, __FUNCTION__);
+            kfree_rtskb(rtskb);
+            return;
+    }
+
+    for (i = 0; i < rtcfg_dev->stations_found; i++)
+        /* Ethernet-specific! */
+        if (memcmp(rtcfg_dev->spec.clt.station_addr_list[i].mac_addr,
+                   dead_station_frm->physical_addr, ETH_ALEN) == 0) {
+            if ((rtcfg_dev->spec.clt.station_addr_list[i].flags &
+                 _RTCFG_FLAG_READY) != 0)
+                rtcfg_dev->stations_ready--;
+
+            rtcfg_dev->stations_found--;
+            memmove(&rtcfg_dev->spec.clt.station_addr_list[i],
+                    &rtcfg_dev->spec.clt.station_addr_list[i+1],
+                    sizeof(struct rtcfg_station) *
+                    (rtcfg_dev->stations_found - i));
+
+            if (rtcfg_dev->state == RTCFG_MAIN_CLIENT_ALL_KNOWN)
+                rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ANNOUNCED);
+            break;
+        }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+}
+
+
+
+static void rtcfg_client_update_server(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_1_cfg *stage_1_cfg;
+    struct rtcfg_device          *rtcfg_dev = &device[ifindex];
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_1_cfg));
+
+    switch (stage_1_cfg->addr_type) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+        case RTCFG_ADDR_IP: {
+            struct rtnet_device *rtdev;
+            u32                 daddr, saddr;
+
+            if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg) +
+                    2*RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg "
+                            "frame\n");
+                kfree_rtskb(rtskb);
+                break;
+            }
+
+            rtdev = rtskb->rtdev;
+
+            memcpy(&daddr, stage_1_cfg->client_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            memcpy(&saddr, stage_1_cfg->server_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            __rtskb_pull(rtskb, 2*RTCFG_ADDRSIZE_IP);
+
+            /* directed to us? */
+            if ((rtskb->pkt_type == PACKET_BROADCAST) &&
+                (daddr != rtdev->local_ip)) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                kfree_rtskb(rtskb);
+                return;
+            }
+
+            /* update routing table */
+            rt_ip_route_add_host(saddr, rtskb->mac.ethernet->h_source, rtdev);
+
+            rtcfg_dev->spec.clt.srv_addr.ip_addr = saddr;
+            break;
+        }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        stage_1_cfg->addr_type, __FUNCTION__);
+            kfree_rtskb(rtskb);
+            return;
+    }
+
+    /* Ethernet-specific */
+    memcpy(rtcfg_dev->spec.clt.srv_mac_addr,
+        rtskb->mac.ethernet->h_source, ETH_ALEN);
+
+    rtcfg_send_announce_reply(ifindex, rtskb->mac.ethernet->h_source);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+}
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_conn_event.c b/net/rtnet/stack/rtcfg/rtcfg_conn_event.c
--- a/net/rtnet/stack/rtcfg/rtcfg_conn_event.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_conn_event.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,396 @@
+/***
+ *
+ *  rtcfg/rtcfg_conn_event.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+
+#include <ipv4/route.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+
+
+/****************************** states ***************************************/
+static int rtcfg_conn_state_searching(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_stage_1(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_stage_2(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_ready(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_dead(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+
+
+#ifdef CONFIG_RTNET_RTCFG_DEBUG
+const char *rtcfg_conn_state[] = {
+    "RTCFG_CONN_SEARCHING",
+    "RTCFG_CONN_STAGE_1",
+    "RTCFG_CONN_STAGE_2",
+    "RTCFG_CONN_READY",
+    "RTCFG_CONN_DEAD"
+};
+#endif /* CONFIG_RTNET_RTCFG_DEBUG */
+
+
+static void rtcfg_conn_recv_announce_new(struct rtcfg_connection *conn,
+                                         struct rtskb *rtskb);
+static void rtcfg_conn_check_cfg_timeout(struct rtcfg_connection *conn);
+static void rtcfg_conn_check_heartbeat(struct rtcfg_connection *conn);
+
+
+
+static int (*state[])(struct rtcfg_connection *conn, RTCFG_EVENT event_id,
+                      void* event_data) =
+{
+    rtcfg_conn_state_searching,
+    rtcfg_conn_state_stage_1,
+    rtcfg_conn_state_stage_2,
+    rtcfg_conn_state_ready,
+    rtcfg_conn_state_dead
+};
+
+
+
+int rtcfg_do_conn_event(struct rtcfg_connection *conn, RTCFG_EVENT event_id,
+                        void* event_data)
+{
+    int conn_state = conn->state;
+
+
+    RTCFG_DEBUG(3, "RTcfg: %s() conn=%p, event=%s, state=%s\n", __FUNCTION__,
+                conn, rtcfg_event[event_id], rtcfg_conn_state[conn_state]);
+
+    return (*state[conn_state])(conn, event_id, event_data);
+}
+
+
+
+static void rtcfg_next_conn_state(struct rtcfg_connection *conn,
+                                  RTCFG_CONN_STATE state)
+{
+    RTCFG_DEBUG(4, "RTcfg: next connection state=%s \n",
+                rtcfg_conn_state[state]);
+
+    conn->state = state;
+}
+
+
+
+static int rtcfg_conn_state_searching(struct rtcfg_connection *conn,
+                                      RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtcfg_device *rtcfg_dev = &device[conn->ifindex];
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            rtcfg_conn_recv_announce_new(conn, rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            conn->last_frame = rtskb->time_stamp;
+
+            rtcfg_next_conn_state(conn, RTCFG_CONN_READY);
+
+            rtcfg_dev->stations_found++;
+            rtcfg_dev->stations_ready++;
+            rtcfg_dev->spec.srv.clients_configured++;
+            if (rtcfg_dev->spec.srv.clients_configured ==
+                rtcfg_dev->other_stations)
+                rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_WAIT, 0);
+
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_stage_1(struct rtcfg_connection *conn,
+                                    RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtskb             *rtskb     = (struct rtskb *)event_data;
+    struct rtcfg_device      *rtcfg_dev = &device[conn->ifindex];
+    struct rtcfg_frm_ack_cfg *ack_cfg;
+    int                      packets;
+
+
+    switch (event_id) {
+        case RTCFG_FRM_ACK_CFG:
+            conn->last_frame = rtskb->time_stamp;
+
+            ack_cfg = (struct rtcfg_frm_ack_cfg *)rtskb->data;
+            conn->cfg_offs = ntohl(ack_cfg->ack_len);
+
+            if ((conn->flags & _RTCFG_FLAG_STAGE_2_DATA) != 0) {
+                if (conn->cfg_offs >= conn->stage2_file->size) {
+                    rtcfg_dev->spec.srv.clients_configured++;
+                    if (rtcfg_dev->spec.srv.clients_configured ==
+                        rtcfg_dev->other_stations)
+                        rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_WAIT, 0);
+                    rtcfg_next_conn_state(conn,
+                        ((conn->flags & _RTCFG_FLAG_READY) != 0) ?
+                        RTCFG_CONN_READY : RTCFG_CONN_STAGE_2);
+                } else {
+                    packets = conn->burstrate;
+                    while ((conn->cfg_offs < conn->stage2_file->size) &&
+                        (packets > 0)) {
+                        rtcfg_send_stage_2_frag(conn);
+                        packets--;
+                    }
+                }
+            } else {
+                rtcfg_dev->spec.srv.clients_configured++;
+                if (rtcfg_dev->spec.srv.clients_configured ==
+                    rtcfg_dev->other_stations)
+                    rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_WAIT, 0);
+                rtcfg_next_conn_state(conn,
+                    ((conn->flags & _RTCFG_FLAG_READY) != 0) ?
+                    RTCFG_CONN_READY : RTCFG_CONN_STAGE_2);
+            }
+
+            break;
+
+        case RTCFG_TIMER:
+            rtcfg_conn_check_cfg_timeout(conn);
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_stage_2(struct rtcfg_connection *conn,
+                                    RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rtcfg_device *rtcfg_dev = &device[conn->ifindex];
+
+
+    switch (event_id) {
+        case RTCFG_FRM_READY:
+            conn->last_frame = rtskb->time_stamp;
+
+            rtcfg_next_conn_state(conn, RTCFG_CONN_READY);
+
+            conn->flags |= _RTCFG_FLAG_READY;
+            rtcfg_dev->stations_ready++;
+
+            if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+                rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_READY, 0);
+
+            break;
+
+        case RTCFG_TIMER:
+            rtcfg_conn_check_cfg_timeout(conn);
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_ready(struct rtcfg_connection *conn,
+                                  RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtskb *rtskb = (struct rtskb *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_TIMER:
+            rtcfg_conn_check_heartbeat(conn);
+            break;
+
+        case RTCFG_FRM_HEARTBEAT:
+            conn->last_frame = rtskb->time_stamp;
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_dead(struct rtcfg_connection *conn,
+                                 RTCFG_EVENT event_id, void* event_data)
+{
+    switch (event_id) {
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            rtcfg_conn_recv_announce_new(conn, (struct rtskb *)event_data);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            /* Spec to-do: signal station that it is assumed to be dead
+               (=> reboot command?) */
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static void rtcfg_conn_recv_announce_new(struct rtcfg_connection *conn,
+                                         struct rtskb *rtskb)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[conn->ifindex];
+    struct rtcfg_frm_announce *announce_new;
+    int                       packets;
+
+
+    conn->last_frame = rtskb->time_stamp;
+
+    announce_new = (struct rtcfg_frm_announce *)rtskb->data;
+
+    conn->flags = announce_new->flags;
+    if (announce_new->burstrate < conn->burstrate)
+        conn->burstrate = announce_new->burstrate;
+
+    rtcfg_next_conn_state(conn, RTCFG_CONN_STAGE_1);
+
+    rtcfg_dev->stations_found++;
+    if ((conn->flags & _RTCFG_FLAG_READY) != 0)
+        rtcfg_dev->stations_ready++;
+
+    if (((conn->flags & _RTCFG_FLAG_STAGE_2_DATA) != 0) &&
+        (conn->stage2_file != NULL)) {
+        packets = conn->burstrate - 1;
+
+        rtcfg_send_stage_2(conn, 1);
+
+        while ((conn->cfg_offs < conn->stage2_file->size) &&
+            (packets > 0)) {
+            rtcfg_send_stage_2_frag(conn);
+            packets--;
+        }
+    } else {
+        rtcfg_send_stage_2(conn, 0);
+        conn->flags &= ~_RTCFG_FLAG_STAGE_2_DATA;
+    }
+}
+
+
+
+static void rtcfg_conn_check_cfg_timeout(struct rtcfg_connection *conn)
+{
+    struct rtcfg_device *rtcfg_dev;
+
+
+    if (!conn->cfg_timeout)
+        return;
+
+    if (rtdm_clock_read() >= conn->last_frame + conn->cfg_timeout) {
+        rtcfg_dev = &device[conn->ifindex];
+
+        rtcfg_dev->stations_found--;
+        if (conn->state == RTCFG_CONN_STAGE_2)
+            rtcfg_dev->spec.srv.clients_configured--;
+
+        rtcfg_next_conn_state(conn, RTCFG_CONN_SEARCHING);
+        conn->cfg_offs = 0;
+        conn->flags    = 0;
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+        if (conn->addr_type == RTCFG_ADDR_IP) {
+            struct rtnet_device *rtdev;
+
+            /* MAC address yet unknown -> use broadcast address */
+            rtdev = rtdev_get_by_index(conn->ifindex);
+            if (rtdev == NULL)
+                return;
+            memcpy(conn->mac_addr, rtdev->broadcast, MAX_ADDR_LEN);
+            rtdev_dereference(rtdev);
+        }
+#endif /* CONFIG_RTNET_RTIPV4 */
+    }
+}
+
+
+
+static void rtcfg_conn_check_heartbeat(struct rtcfg_connection *conn)
+{
+    u64                 timeout;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    timeout = device[conn->ifindex].spec.srv.heartbeat_timeout;
+    if (!timeout)
+        return;
+
+    if (rtdm_clock_read() >= conn->last_frame + timeout) {
+        rtcfg_dev = &device[conn->ifindex];
+
+        rtcfg_dev->stations_found--;
+        rtcfg_dev->stations_ready--;
+        rtcfg_dev->spec.srv.clients_configured--;
+
+        rtcfg_send_dead_station(conn);
+
+        rtcfg_next_conn_state(conn, RTCFG_CONN_DEAD);
+        conn->cfg_offs = 0;
+        conn->flags    = 0;
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+        if ((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) {
+            struct rtnet_device *rtdev = rtdev_get_by_index(conn->ifindex);
+
+            rt_ip_route_del_host(conn->addr.ip_addr, rtdev);
+
+            if (rtdev == NULL)
+                return;
+
+            if (!(conn->addr_type & FLAG_ASSIGN_ADDR_BY_MAC))
+                /* MAC address yet unknown -> use broadcast address */
+                memcpy(conn->mac_addr, rtdev->broadcast, MAX_ADDR_LEN);
+
+            rtdev_dereference(rtdev);
+        }
+#endif /* CONFIG_RTNET_RTIPV4 */
+    }
+}
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_event.c b/net/rtnet/stack/rtcfg/rtcfg_event.c
--- a/net/rtnet/stack/rtcfg/rtcfg_event.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_event.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,784 @@
+/***
+ *
+ *  rtcfg/rtcfg_event.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/vmalloc.h>
+
+#include <rtdev.h>
+#include <ipv4/route.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_client_event.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_file.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+
+/*** Common and Server States ***/
+static int rtcfg_main_state_off(
+    int ifindex, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_main_state_server_running(
+    int ifindex, RTCFG_EVENT event_id, void* event_data);
+
+
+#ifdef CONFIG_RTNET_RTCFG_DEBUG
+const char *rtcfg_event[] = {
+    "RTCFG_CMD_SERVER",
+    "RTCFG_CMD_ADD",
+    "RTCFG_CMD_DEL",
+    "RTCFG_CMD_WAIT",
+    "RTCFG_CMD_CLIENT",
+    "RTCFG_CMD_ANNOUNCE",
+    "RTCFG_CMD_READY",
+    "RTCFG_CMD_DETACH",
+    "RTCFG_TIMER",
+    "RTCFG_FRM_STAGE_1_CFG",
+    "RTCFG_FRM_ANNOUNCE_NEW",
+    "RTCFG_FRM_ANNOUNCE_REPLY",
+    "RTCFG_FRM_STAGE_2_CFG",
+    "RTCFG_FRM_STAGE_2_CFG_FRAG",
+    "RTCFG_FRM_ACK_CFG",
+    "RTCFG_FRM_READY",
+    "RTCFG_FRM_HEARTBEAT",
+    "RTCFG_FRM_DEAD_STATION"
+};
+
+const char *rtcfg_main_state[] = {
+    "RTCFG_MAIN_OFF",
+    "RTCFG_MAIN_SERVER_RUNNING",
+    "RTCFG_MAIN_CLIENT_0",
+    "RTCFG_MAIN_CLIENT_1",
+    "RTCFG_MAIN_CLIENT_ANNOUNCED",
+    "RTCFG_MAIN_CLIENT_ALL_KNOWN",
+    "RTCFG_MAIN_CLIENT_ALL_FRAMES",
+    "RTCFG_MAIN_CLIENT_2",
+    "RTCFG_MAIN_CLIENT_READY"
+};
+
+int rtcfg_debug = RTCFG_DEFAULT_DEBUG_LEVEL;
+#endif /* CONFIG_RTNET_RTCFG_DEBUG */
+
+
+struct rtcfg_device device[MAX_RT_DEVICES];
+
+static int (*state[])(int ifindex, RTCFG_EVENT event_id, void* event_data) =
+{
+    rtcfg_main_state_off,
+    rtcfg_main_state_server_running,
+    rtcfg_main_state_client_0,
+    rtcfg_main_state_client_1,
+    rtcfg_main_state_client_announced,
+    rtcfg_main_state_client_all_known,
+    rtcfg_main_state_client_all_frames,
+    rtcfg_main_state_client_2,
+    rtcfg_main_state_client_ready
+};
+
+
+static int rtcfg_server_add(struct rtcfg_cmd *cmd_event);
+static int rtcfg_server_del(struct rtcfg_cmd *cmd_event);
+static int rtcfg_server_detach(int ifindex, struct rtcfg_cmd *cmd_event);
+static int rtcfg_server_recv_announce(int ifindex, RTCFG_EVENT event_id,
+				      struct rtskb *rtskb);
+static int rtcfg_server_recv_ack(int ifindex, struct rtskb *rtskb);
+static int rtcfg_server_recv_simple_frame(int ifindex, RTCFG_EVENT event_id,
+					  struct rtskb *rtskb);
+
+
+
+int rtcfg_do_main_event(int ifindex, RTCFG_EVENT event_id, void* event_data)
+{
+    int main_state;
+
+
+    rtdm_mutex_lock(&device[ifindex].dev_mutex);
+
+    main_state = device[ifindex].state;
+
+    RTCFG_DEBUG(3, "RTcfg: %s() rtdev=%d, event=%s, state=%s\n", __FUNCTION__,
+		ifindex, rtcfg_event[event_id], rtcfg_main_state[main_state]);
+
+    return (*state[main_state])(ifindex, event_id, event_data);
+}
+
+
+
+void rtcfg_next_main_state(int ifindex, RTCFG_MAIN_STATE state)
+{
+    RTCFG_DEBUG(4, "RTcfg: next main state=%s \n", rtcfg_main_state[state]);
+
+    device[ifindex].state = state;
+}
+
+
+
+static int rtcfg_main_state_off(int ifindex, RTCFG_EVENT event_id,
+				void* event_data)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct rt_proc_call     *call      = (struct rt_proc_call *)event_data;
+    struct rtcfg_cmd        *cmd_event;
+    int                     ret;
+
+    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+    switch (event_id) {
+	case RTCFG_CMD_SERVER:
+	    INIT_LIST_HEAD(&rtcfg_dev->spec.srv.conn_list);
+
+	    ret = rtdm_timer_init(&rtcfg_dev->timer, rtcfg_timer, "rtcfg-timer");
+	    if (ret == 0) {
+		    ret = rtdm_timer_start(&rtcfg_dev->timer,
+					    XN_INFINITE,
+					    (nanosecs_rel_t)
+					    cmd_event->args.server.period
+					    * 1000000,
+					    RTDM_TIMERMODE_RELATIVE);
+		    if (ret < 0)
+			    rtdm_timer_destroy(&rtcfg_dev->timer);
+	    }
+	    if (ret < 0) {
+		rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+		return ret;
+	    }
+
+	    if (cmd_event->args.server.flags & _RTCFG_FLAG_READY)
+		    set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags);
+	    set_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags);
+
+	    rtcfg_dev->burstrate = cmd_event->args.server.burstrate;
+
+	    rtcfg_dev->spec.srv.heartbeat = cmd_event->args.server.heartbeat;
+
+	    rtcfg_dev->spec.srv.heartbeat_timeout =
+		    ((u64)cmd_event->args.server.heartbeat) * 1000000 *
+		    cmd_event->args.server.threshold;
+
+	    rtcfg_next_main_state(ifindex, RTCFG_MAIN_SERVER_RUNNING);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    break;
+
+	case RTCFG_CMD_CLIENT:
+	    rtcfg_dev->spec.clt.station_addr_list =
+		cmd_event->args.client.station_buf;
+	    cmd_event->args.client.station_buf = NULL;
+
+	    rtcfg_dev->spec.clt.max_stations =
+		cmd_event->args.client.max_stations;
+	    rtcfg_dev->other_stations = -1;
+
+	    rtcfg_queue_blocking_call(ifindex, call);
+
+	    rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_0);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    return -CALL_PENDING;
+
+	default:
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+			rtcfg_event[event_id], ifindex, __FUNCTION__);
+	    return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+/*** Server States ***/
+
+static int rtcfg_main_state_server_running(int ifindex, RTCFG_EVENT event_id,
+					   void* event_data)
+{
+    struct rt_proc_call *call;
+    struct rtcfg_cmd    *cmd_event;
+    struct rtcfg_device *rtcfg_dev;
+    struct rtskb        *rtskb;
+
+
+    switch (event_id) {
+	case RTCFG_CMD_ADD:
+	    call      = (struct rt_proc_call *)event_data;
+	    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	    return rtcfg_server_add(cmd_event);
+
+	case RTCFG_CMD_DEL:
+	    call      = (struct rt_proc_call *)event_data;
+	    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	    return rtcfg_server_del(cmd_event);
+
+	case RTCFG_CMD_WAIT:
+	    call = (struct rt_proc_call *)event_data;
+
+	    rtcfg_dev = &device[ifindex];
+
+	    if (rtcfg_dev->spec.srv.clients_configured ==
+		rtcfg_dev->other_stations)
+		rtpc_complete_call(call, 0);
+	    else
+		rtcfg_queue_blocking_call(ifindex, call);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    return -CALL_PENDING;
+
+	case RTCFG_CMD_READY:
+	    call = (struct rt_proc_call *)event_data;
+
+	    rtcfg_dev = &device[ifindex];
+
+	    if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+		rtpc_complete_call(call, 0);
+	    else
+		rtcfg_queue_blocking_call(ifindex, call);
+
+	    if (!test_and_set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags))
+		rtcfg_send_ready(ifindex);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    return -CALL_PENDING;
+
+	case RTCFG_CMD_DETACH:
+	    call      = (struct rt_proc_call *)event_data;
+	    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	    return rtcfg_server_detach(ifindex, cmd_event);
+
+	case RTCFG_FRM_ANNOUNCE_NEW:
+	case RTCFG_FRM_ANNOUNCE_REPLY:
+	    rtskb = (struct rtskb *)event_data;
+	    return rtcfg_server_recv_announce(ifindex, event_id, rtskb);
+
+	case RTCFG_FRM_ACK_CFG:
+	    rtskb = (struct rtskb *)event_data;
+	    return rtcfg_server_recv_ack(ifindex, rtskb);
+
+	case RTCFG_FRM_READY:
+	case RTCFG_FRM_HEARTBEAT:
+	    rtskb = (struct rtskb *)event_data;
+	    return rtcfg_server_recv_simple_frame(ifindex, event_id, rtskb);
+
+	default:
+	    rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+
+	    RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+			rtcfg_event[event_id], ifindex, __FUNCTION__);
+	    return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+/*** Server Command Event Handlers ***/
+
+static int rtcfg_server_add(struct rtcfg_cmd *cmd_event)
+{
+    struct rtcfg_device     *rtcfg_dev;
+    struct rtcfg_connection *conn;
+    struct rtcfg_connection *new_conn;
+    struct list_head        *entry;
+    unsigned int            addr_type;
+
+    rtcfg_dev = &device[cmd_event->internal.data.ifindex];
+    addr_type = cmd_event->args.add.addr_type & RTCFG_ADDR_MASK;
+
+    new_conn = cmd_event->args.add.conn_buf;
+    memset(new_conn, 0, sizeof(struct rtcfg_connection));
+
+    new_conn->ifindex      = cmd_event->internal.data.ifindex;
+    new_conn->state        = RTCFG_CONN_SEARCHING;
+    new_conn->addr_type    = cmd_event->args.add.addr_type;
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+    new_conn->addr.ip_addr = cmd_event->args.add.ip_addr;
+#endif
+    new_conn->stage1_data  = cmd_event->args.add.stage1_data;
+    new_conn->stage1_size  = cmd_event->args.add.stage1_size;
+    new_conn->burstrate    = rtcfg_dev->burstrate;
+    new_conn->cfg_timeout  = ((u64)cmd_event->args.add.timeout) * 1000000;
+
+    if (cmd_event->args.add.addr_type == RTCFG_ADDR_IP) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	struct rtnet_device *rtdev;
+
+	/* MAC address yet unknown -> use broadcast address */
+	rtdev = rtdev_get_by_index(cmd_event->internal.data.ifindex);
+	if (rtdev == NULL) {
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+	    return -ENODEV;
+	}
+	memcpy(new_conn->mac_addr, rtdev->broadcast, MAX_ADDR_LEN);
+	rtdev_dereference(rtdev);
+#else /* !CONFIG_RTNET_RTIPV4 */
+	return -EPROTONOSUPPORT;
+#endif /* CONFIG_RTNET_RTIPV4 */
+    } else
+	memcpy(new_conn->mac_addr, cmd_event->args.add.mac_addr, MAX_ADDR_LEN);
+
+    /* get stage 2 file */
+    if (cmd_event->args.add.stage2_file != NULL) {
+	if (cmd_event->args.add.stage2_file->buffer != NULL) {
+	    new_conn->stage2_file = cmd_event->args.add.stage2_file;
+	    rtcfg_add_file(new_conn->stage2_file);
+
+	    cmd_event->args.add.stage2_file = NULL;
+	} else {
+	    new_conn->stage2_file =
+		rtcfg_get_file(cmd_event->args.add.stage2_file->name);
+	    if (new_conn->stage2_file == NULL) {
+		rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+		return 1;
+	    }
+	}
+    }
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	if (
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	    ((addr_type == RTCFG_ADDR_IP) &&
+	     (conn->addr.ip_addr == cmd_event->args.add.ip_addr)) ||
+#endif /* CONFIG_RTNET_RTIPV4 */
+	    ((addr_type == RTCFG_ADDR_MAC) &&
+	     (memcmp(conn->mac_addr, new_conn->mac_addr,
+		     MAX_ADDR_LEN) == 0))
+	   ) {
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    if ((new_conn->stage2_file) &&
+		(rtcfg_release_file(new_conn->stage2_file) == 0)) {
+		/* Note: This assignment cannot overwrite a valid file pointer.
+		 * Effectively, it will only be executed when
+		 * new_conn->stage2_file is the pointer originally passed by
+		 * rtcfg_ioctl. But checking this assumptions does not cause
+		 * any harm :o)
+		 */
+		RTNET_ASSERT(cmd_event->args.add.stage2_file == NULL, ;);
+
+		cmd_event->args.add.stage2_file = new_conn->stage2_file;
+	    }
+
+	    return -EEXIST;
+	}
+    }
+
+    list_add_tail(&new_conn->entry, &rtcfg_dev->spec.srv.conn_list);
+    rtcfg_dev->other_stations++;
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    cmd_event->args.add.conn_buf    = NULL;
+    cmd_event->args.add.stage1_data = NULL;
+
+    return 0;
+}
+
+
+
+static int rtcfg_server_del(struct rtcfg_cmd *cmd_event)
+{
+    struct rtcfg_connection *conn;
+    struct list_head        *entry;
+    unsigned int            addr_type;
+    struct rtcfg_device     *rtcfg_dev;
+
+
+    rtcfg_dev = &device[cmd_event->internal.data.ifindex];
+    addr_type = cmd_event->args.add.addr_type & RTCFG_ADDR_MASK;
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	if ((addr_type == conn->addr_type) && (
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	     ((addr_type == RTCFG_ADDR_IP) &&
+	      (conn->addr.ip_addr == cmd_event->args.add.ip_addr)) ||
+#endif /* CONFIG_RTNET_RTIPV4 */
+	     ((addr_type == RTCFG_ADDR_MAC) &&
+	      (memcmp(conn->mac_addr, cmd_event->args.add.mac_addr,
+		      MAX_ADDR_LEN) == 0)))) {
+	    list_del(&conn->entry);
+	    rtcfg_dev->other_stations--;
+
+	    if (conn->state > RTCFG_CONN_SEARCHING) {
+		rtcfg_dev->stations_found--;
+		if (conn->state >= RTCFG_CONN_STAGE_2)
+		    rtcfg_dev->spec.srv.clients_configured--;
+		if (conn->flags & _RTCFG_FLAG_READY)
+		    rtcfg_dev->stations_ready--;
+	    }
+
+	    if ((conn->stage2_file) &&
+		(rtcfg_release_file(conn->stage2_file) == 0))
+		cmd_event->args.del.stage2_file = conn->stage2_file;
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    cmd_event->args.del.conn_buf = conn;
+
+	    return 0;
+	}
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return -ENOENT;
+}
+
+
+
+static int rtcfg_server_detach(int ifindex, struct rtcfg_cmd *cmd_event)
+{
+    struct rtcfg_connection *conn;
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+
+
+    if (!list_empty(&rtcfg_dev->spec.srv.conn_list)) {
+	conn = list_entry(rtcfg_dev->spec.srv.conn_list.next,
+			  struct rtcfg_connection, entry);
+
+	list_del(&conn->entry);
+	rtcfg_dev->other_stations--;
+
+	if (conn->state > RTCFG_CONN_SEARCHING) {
+	    rtcfg_dev->stations_found--;
+	    if (conn->state >= RTCFG_CONN_STAGE_2)
+		rtcfg_dev->spec.srv.clients_configured--;
+	    if (conn->flags & _RTCFG_FLAG_READY)
+		rtcfg_dev->stations_ready--;
+	}
+
+	if ((conn->stage2_file) &&
+	    (rtcfg_release_file(conn->stage2_file) == 0))
+	    cmd_event->args.detach.stage2_file = conn->stage2_file;
+
+	rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	cmd_event->args.detach.conn_buf = conn;
+
+	return -EAGAIN;
+    }
+
+    if (test_and_clear_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags))
+	rtdm_timer_destroy(&rtcfg_dev->timer);
+    rtcfg_reset_device(ifindex);
+
+    rtcfg_next_main_state(ifindex, RTCFG_MAIN_OFF);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return 0;
+}
+
+
+
+/*** Server Frame Event Handlers ***/
+
+static int rtcfg_server_recv_announce(int ifindex, RTCFG_EVENT event_id,
+				      struct rtskb *rtskb)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    struct list_head          *entry;
+    struct rtcfg_frm_announce *announce;
+    struct rtcfg_connection   *conn;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_announce)) {
+	rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+	RTCFG_DEBUG(1, "RTcfg: received invalid announce frame\n");
+	return -EINVAL;
+    }
+
+    announce = (struct rtcfg_frm_announce *)rtskb->data;
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	switch (announce->addr_type) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	    u32 announce_addr;
+	    case RTCFG_ADDR_IP:
+		memcpy(&announce_addr, announce->addr, 4);
+
+		if (((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) &&
+		    (announce_addr == conn->addr.ip_addr)) {
+		    /* save MAC address - Ethernet-specific! */
+		    memcpy(conn->mac_addr, rtskb->mac.ethernet->h_source,
+			   ETH_ALEN);
+
+		    /* update routing table */
+		    rt_ip_route_add_host(conn->addr.ip_addr, conn->mac_addr,
+					 rtskb->rtdev);
+
+		    /* remove IP address */
+		    __rtskb_pull(rtskb, RTCFG_ADDRSIZE_IP);
+
+		    rtcfg_do_conn_event(conn, event_id, rtskb);
+
+		    goto out;
+		}
+		break;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+	    case RTCFG_ADDR_MAC:
+		/* Ethernet-specific! */
+		if (memcmp(conn->mac_addr, rtskb->mac.ethernet->h_source,
+			   ETH_ALEN) == 0) {
+		    rtcfg_do_conn_event(conn, event_id, rtskb);
+
+		    goto out;
+		}
+		break;
+	}
+    }
+
+out:
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+static int rtcfg_server_recv_ack(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct list_head        *entry;
+    struct rtcfg_connection *conn;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_ack_cfg)) {
+	rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+	RTCFG_DEBUG(1, "RTcfg: received invalid ack_cfg frame\n");
+	return -EINVAL;
+    }
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	/* find the corresponding connection - Ethernet-specific! */
+	if (memcmp(conn->mac_addr,
+		   rtskb->mac.ethernet->h_source, ETH_ALEN) != 0)
+	    continue;
+
+	rtcfg_do_conn_event(conn, RTCFG_FRM_ACK_CFG, rtskb);
+
+	break;
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+static int rtcfg_server_recv_simple_frame(int ifindex, RTCFG_EVENT event_id,
+					  struct rtskb *rtskb)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct list_head        *entry;
+    struct rtcfg_connection *conn;
+
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	/* find the corresponding connection - Ethernet-specific! */
+	if (memcmp(conn->mac_addr,
+		   rtskb->mac.ethernet->h_source, ETH_ALEN) != 0)
+	    continue;
+
+	rtcfg_do_conn_event(conn, event_id, rtskb);
+
+	break;
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+/*** Utility Functions ***/
+
+void rtcfg_queue_blocking_call(int ifindex, struct rt_proc_call *call)
+{
+    rtdm_lockctx_t      context;
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+
+    rtdm_lock_get_irqsave(&rtcfg_dev->event_calls_lock, context);
+    list_add_tail(&call->list_entry, &rtcfg_dev->event_calls);
+    rtdm_lock_put_irqrestore(&rtcfg_dev->event_calls_lock, context);
+}
+
+
+
+struct rt_proc_call *rtcfg_dequeue_blocking_call(int ifindex)
+{
+    rtdm_lockctx_t      context;
+    struct rt_proc_call *call;
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+
+    rtdm_lock_get_irqsave(&rtcfg_dev->event_calls_lock, context);
+    if (!list_empty(&rtcfg_dev->event_calls)) {
+	call = (struct rt_proc_call *)rtcfg_dev->event_calls.next;
+	list_del(&call->list_entry);
+    } else
+	call = NULL;
+    rtdm_lock_put_irqrestore(&rtcfg_dev->event_calls_lock, context);
+
+    return call;
+}
+
+
+
+void rtcfg_complete_cmd(int ifindex, RTCFG_EVENT event_id, int result)
+{
+    struct rt_proc_call *call;
+    struct rtcfg_cmd    *cmd_event;
+
+
+    while (1) {
+	call = rtcfg_dequeue_blocking_call(ifindex);
+	if (call == NULL)
+	    break;
+
+	cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	rtpc_complete_call(call,
+	    (cmd_event->internal.data.event_id == event_id) ? result
+							    : -EINVAL);
+    }
+}
+
+
+
+void rtcfg_reset_device(int ifindex)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+
+    rtcfg_dev->other_stations = 0;
+    rtcfg_dev->stations_found = 0;
+    rtcfg_dev->stations_ready = 0;
+    rtcfg_dev->flags          = 0;
+    rtcfg_dev->burstrate      = 0;
+
+    memset(&rtcfg_dev->spec, 0, sizeof(rtcfg_dev->spec));
+    INIT_LIST_HEAD(&rtcfg_dev->spec.srv.conn_list);
+}
+
+
+
+void rtcfg_init_state_machines(void)
+{
+    int                 i;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    memset(device, 0, sizeof(device));
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtcfg_dev = &device[i];
+	rtcfg_dev->state = RTCFG_MAIN_OFF;
+
+	rtdm_mutex_init(&rtcfg_dev->dev_mutex);
+
+	INIT_LIST_HEAD(&rtcfg_dev->event_calls);
+	rtdm_lock_init(&rtcfg_dev->event_calls_lock);
+    }
+}
+
+
+
+void rtcfg_cleanup_state_machines(void)
+{
+    int                     i;
+    struct rtcfg_device     *rtcfg_dev;
+    struct rtcfg_connection *conn;
+    struct list_head        *entry;
+    struct list_head        *tmp;
+    struct rt_proc_call     *call;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtcfg_dev = &device[i];
+
+	if (test_and_clear_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags))
+		rtdm_timer_destroy(&rtcfg_dev->timer);
+
+	/*
+	 * No need to synchronize with rtcfg_timer here: the task running
+	 * rtcfg_timer is already dead.
+	 */
+
+	rtdm_mutex_destroy(&rtcfg_dev->dev_mutex);
+
+	if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+	    list_for_each_safe(entry, tmp, &rtcfg_dev->spec.srv.conn_list) {
+		conn = list_entry(entry, struct rtcfg_connection, entry);
+
+		if (conn->stage1_data != NULL)
+		    kfree(conn->stage1_data);
+
+		if ((conn->stage2_file != NULL) &&
+		    (rtcfg_release_file(conn->stage2_file) == 0)){
+		    vfree(conn->stage2_file->buffer);
+		    kfree(conn->stage2_file);
+		}
+
+		kfree(entry);
+	    }
+	} else if (rtcfg_dev->state != RTCFG_MAIN_OFF) {
+	    if (rtcfg_dev->spec.clt.station_addr_list != NULL)
+		kfree(rtcfg_dev->spec.clt.station_addr_list);
+
+	    if (rtcfg_dev->spec.clt.stage2_chain != NULL)
+		kfree_rtskb(rtcfg_dev->spec.clt.stage2_chain);
+	}
+
+	while (1) {
+	    call = rtcfg_dequeue_blocking_call(i);
+	    if (call == NULL)
+		break;
+
+	    rtpc_complete_call_nrt(call, -ENODEV);
+	}
+    }
+}
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_file.c b/net/rtnet/stack/rtcfg/rtcfg_file.c
--- a/net/rtnet/stack/rtcfg/rtcfg_file.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_file.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,86 @@
+/***
+ *
+ *  rtcfg/rtcfg_file.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+
+#include <rtdm/driver.h>
+#include <rtcfg_chrdev.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_file.h>
+
+
+/* Note:
+ * We don't need any special lock protection while manipulating the
+ * rtcfg_files list. The list is only accessed through valid connections, and
+ * connections are already lock-protected.
+ */
+LIST_HEAD(rtcfg_files);
+
+
+struct rtcfg_file *rtcfg_get_file(const char *filename)
+{
+    struct list_head  *entry;
+    struct rtcfg_file *file;
+
+
+    RTCFG_DEBUG(4, "RTcfg: looking for file %s\n", filename);
+
+    list_for_each(entry, &rtcfg_files) {
+        file = list_entry(entry, struct rtcfg_file, entry);
+
+        if (strcmp(file->name, filename) == 0) {
+            file->ref_count++;
+
+            RTCFG_DEBUG(4, "RTcfg: reusing file entry, now %d users\n",
+                        file->ref_count);
+
+            return file;
+        }
+    }
+
+    return NULL;
+}
+
+
+
+void rtcfg_add_file(struct rtcfg_file *file)
+{
+    RTCFG_DEBUG(4, "RTcfg: adding file %s to list\n", file->name);
+
+    file->ref_count = 1;
+    list_add_tail(&file->entry, &rtcfg_files);
+}
+
+
+
+int rtcfg_release_file(struct rtcfg_file *file)
+{
+    if (--file->ref_count == 0) {
+        RTCFG_DEBUG(4, "RTcfg: removing file %s from list\n", file->name);
+
+        list_del(&file->entry);
+    }
+
+    return file->ref_count;
+}
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_frame.c b/net/rtnet/stack/rtcfg/rtcfg_frame.c
--- a/net/rtnet/stack/rtcfg/rtcfg_frame.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_frame.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,596 @@
+/***
+ *
+ *  rtcfg/rtcfg_frame.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/if_ether.h>
+
+#include <stack_mgr.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+
+static unsigned int num_rtskbs = 32;
+module_param(num_rtskbs, uint, 0444);
+MODULE_PARM_DESC(num_rtskbs, "Number of realtime socket buffers used by RTcfg");
+
+static struct rtskb_pool    rtcfg_pool;
+static rtdm_task_t          rx_task;
+static rtdm_event_t         rx_event;
+static struct rtskb_queue   rx_queue;
+
+
+void rtcfg_thread_signal(void)
+{
+    rtdm_event_signal(&rx_event);
+}
+
+static int rtcfg_rx_handler(struct rtskb *rtskb, struct rtpacket_type *pt)
+{
+    if (rtskb_acquire(rtskb, &rtcfg_pool) == 0) {
+	rtskb_queue_tail(&rx_queue, rtskb);
+	rtcfg_thread_signal();
+   } else
+	kfree_rtskb(rtskb);
+
+    return 0;
+}
+
+
+
+static void rtcfg_rx_task(void *arg)
+{
+    struct rtskb          *rtskb;
+    struct rtcfg_frm_head *frm_head;
+    struct rtnet_device   *rtdev;
+
+
+    while (!rtdm_task_should_stop()) {
+	if (rtdm_event_wait(&rx_event) < 0)
+	    break;
+
+	while ((rtskb = rtskb_dequeue(&rx_queue))) {
+	    rtdev = rtskb->rtdev;
+
+	    if (rtskb->pkt_type == PACKET_OTHERHOST) {
+		kfree_rtskb(rtskb);
+		continue;
+	    }
+
+	    if (rtskb->len < sizeof(struct rtcfg_frm_head)) {
+		RTCFG_DEBUG(1, "RTcfg: %s() received an invalid frame\n",
+			    __FUNCTION__);
+		kfree_rtskb(rtskb);
+		continue;
+	    }
+
+	    frm_head = (struct rtcfg_frm_head *)rtskb->data;
+
+	    if (rtcfg_do_main_event(rtskb->rtdev->ifindex,
+				    frm_head->id + RTCFG_FRM_STAGE_1_CFG,
+				    rtskb) < 0)
+		kfree_rtskb(rtskb);
+	}
+
+	rtcfg_timer_run();
+    }
+}
+
+
+
+int rtcfg_send_frame(struct rtskb *rtskb, struct rtnet_device *rtdev,
+		     u8 *dest_addr)
+{
+    int ret;
+
+
+    rtskb->rtdev    = rtdev;
+    rtskb->priority = RTCFG_SKB_PRIO;
+
+    if (rtdev->hard_header) {
+	ret = rtdev->hard_header(rtskb, rtdev, ETH_RTCFG, dest_addr,
+				 rtdev->dev_addr, rtskb->len);
+	if (ret < 0)
+	    goto err;
+    }
+
+    if ((rtdev->flags & IFF_UP) != 0) {
+	ret = 0;
+	if (rtdev_xmit(rtskb) != 0)
+	    ret = -EAGAIN;
+    } else {
+	ret = -ENETDOWN;
+	goto err;
+    }
+
+    rtdev_dereference(rtdev);
+    return ret;
+
+  err:
+    kfree_rtskb(rtskb);
+    rtdev_dereference(rtdev);
+    return ret;
+}
+
+
+
+int rtcfg_send_stage_1(struct rtcfg_connection *conn)
+{
+    struct rtnet_device          *rtdev;
+    struct rtskb                 *rtskb;
+    unsigned int                 rtskb_size;
+    struct rtcfg_frm_stage_1_cfg *stage_1_frm;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_stage_1_cfg) + conn->stage1_size +
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	(((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) ?
+	2*RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_RTNET_RTIPV4 */
+	0;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    stage_1_frm = (struct rtcfg_frm_stage_1_cfg *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_stage_1_cfg));
+
+    stage_1_frm->head.id      = RTCFG_ID_STAGE_1_CFG;
+    stage_1_frm->head.version = 0;
+    stage_1_frm->addr_type    = conn->addr_type & RTCFG_ADDR_MASK;
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+    if (stage_1_frm->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, 2*RTCFG_ADDRSIZE_IP);
+
+	memcpy(stage_1_frm->client_addr, &(conn->addr.ip_addr), 4);
+
+	stage_1_frm = (struct rtcfg_frm_stage_1_cfg *)
+	    (((u8 *)stage_1_frm) + RTCFG_ADDRSIZE_IP);
+
+	memcpy(stage_1_frm->server_addr, &(rtdev->local_ip), 4);
+
+	stage_1_frm = (struct rtcfg_frm_stage_1_cfg *)
+	    (((u8 *)stage_1_frm) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    stage_1_frm->burstrate = device[conn->ifindex].burstrate;
+    stage_1_frm->cfg_len   = htons(conn->stage1_size);
+
+    memcpy(rtskb_put(rtskb, conn->stage1_size), conn->stage1_data,
+	   conn->stage1_size);
+
+    return rtcfg_send_frame(rtskb, rtdev, conn->mac_addr);
+}
+
+
+
+int rtcfg_send_stage_2(struct rtcfg_connection *conn, int send_data)
+{
+    struct rtnet_device          *rtdev;
+    struct rtcfg_device          *rtcfg_dev = &device[conn->ifindex];
+    struct rtskb                 *rtskb;
+    unsigned int                 rtskb_size;
+    struct rtcfg_frm_stage_2_cfg *stage_2_frm;
+    size_t                       total_size;
+    size_t                       frag_size;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    if (send_data) {
+	total_size = conn->stage2_file->size;
+	frag_size  = MIN(rtdev->get_mtu(rtdev, RTCFG_SKB_PRIO) -
+			 sizeof(struct rtcfg_frm_stage_2_cfg), total_size);
+    } else {
+	total_size = 0;
+	frag_size  = 0;
+    }
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_stage_2_cfg) + frag_size;
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    stage_2_frm = (struct rtcfg_frm_stage_2_cfg *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg));
+
+    stage_2_frm->head.id          = RTCFG_ID_STAGE_2_CFG;
+    stage_2_frm->head.version     = 0;
+    stage_2_frm->flags            = rtcfg_dev->flags;
+    stage_2_frm->stations         = htonl(rtcfg_dev->other_stations);
+    stage_2_frm->heartbeat_period = htons(rtcfg_dev->spec.srv.heartbeat);
+    stage_2_frm->cfg_len          = htonl(total_size);
+
+    if (send_data)
+	memcpy(rtskb_put(rtskb, frag_size), conn->stage2_file->buffer,
+	       frag_size);
+    conn->cfg_offs = frag_size;
+
+    return rtcfg_send_frame(rtskb, rtdev, conn->mac_addr);
+}
+
+
+
+int rtcfg_send_stage_2_frag(struct rtcfg_connection *conn)
+{
+    struct rtnet_device               *rtdev;
+    struct rtskb                      *rtskb;
+    unsigned int                      rtskb_size;
+    struct rtcfg_frm_stage_2_cfg_frag *stage_2_frm;
+    size_t                            frag_size;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    frag_size = MIN(rtdev->get_mtu(rtdev, RTCFG_SKB_PRIO) -
+		    sizeof(struct rtcfg_frm_stage_2_cfg_frag),
+		    conn->stage2_file->size - conn->cfg_offs);
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_stage_2_cfg_frag) + frag_size;
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    stage_2_frm = (struct rtcfg_frm_stage_2_cfg_frag *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg_frag));
+
+    stage_2_frm->head.id      = RTCFG_ID_STAGE_2_CFG_FRAG;
+    stage_2_frm->head.version = 0;
+    stage_2_frm->frag_offs    = htonl(conn->cfg_offs);
+
+    memcpy(rtskb_put(rtskb, frag_size),
+	   conn->stage2_file->buffer + conn->cfg_offs, frag_size);
+    conn->cfg_offs += frag_size;
+
+    return rtcfg_send_frame(rtskb, rtdev, conn->mac_addr);
+}
+
+
+
+int rtcfg_send_announce_new(int ifindex)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    struct rtnet_device       *rtdev;
+    struct rtskb              *rtskb;
+    unsigned int              rtskb_size;
+    struct rtcfg_frm_announce *announce_new;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len + sizeof(struct rtcfg_frm_announce) +
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	(((rtcfg_dev->spec.clt.addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) ?
+	RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_RTNET_RTIPV4 */
+	0;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    announce_new = (struct rtcfg_frm_announce *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_announce));
+
+    announce_new->head.id      = RTCFG_ID_ANNOUNCE_NEW;
+    announce_new->head.version = 0;
+    announce_new->addr_type    = rtcfg_dev->spec.clt.addr_type;
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+    if (announce_new->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, RTCFG_ADDRSIZE_IP);
+
+	memcpy(announce_new->addr, &(rtdev->local_ip), 4);
+
+	announce_new = (struct rtcfg_frm_announce *)
+	    (((u8 *)announce_new) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    announce_new->flags     = rtcfg_dev->flags;
+    announce_new->burstrate = rtcfg_dev->burstrate;
+
+    return rtcfg_send_frame(rtskb, rtdev, rtdev->broadcast);
+}
+
+
+
+int rtcfg_send_announce_reply(int ifindex, u8 *dest_mac_addr)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    struct rtnet_device       *rtdev;
+    struct rtskb              *rtskb;
+    unsigned int              rtskb_size;
+    struct rtcfg_frm_announce *announce_rpl;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_announce) +
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	((rtcfg_dev->spec.clt.addr_type == RTCFG_ADDR_IP) ?
+	RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_RTNET_RTIPV4 */
+	0;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    announce_rpl = (struct rtcfg_frm_announce *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_announce));
+
+    announce_rpl->head.id      = RTCFG_ID_ANNOUNCE_REPLY;
+    announce_rpl->head.version = 0;
+    announce_rpl->addr_type    = rtcfg_dev->spec.clt.addr_type;
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+    if (announce_rpl->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, RTCFG_ADDRSIZE_IP);
+
+	memcpy(announce_rpl->addr, &(rtdev->local_ip), 4);
+
+	announce_rpl = (struct rtcfg_frm_announce *)
+	    (((u8 *)announce_rpl) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    announce_rpl->flags     = rtcfg_dev->flags & _RTCFG_FLAG_READY;
+    announce_rpl->burstrate = 0; /* padding field */
+
+    return rtcfg_send_frame(rtskb, rtdev, dest_mac_addr);
+}
+
+
+
+int rtcfg_send_ack(int ifindex)
+{
+    struct rtnet_device      *rtdev;
+    struct rtskb             *rtskb;
+    unsigned int             rtskb_size;
+    struct rtcfg_frm_ack_cfg *ack_frm;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len + sizeof(struct rtcfg_frm_ack_cfg);
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    ack_frm = (struct rtcfg_frm_ack_cfg *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_ack_cfg));
+
+    ack_frm->head.id      = RTCFG_ID_ACK_CFG;
+    ack_frm->head.version = 0;
+    ack_frm->ack_len      = htonl(device[ifindex].spec.clt.cfg_offs);
+
+    return rtcfg_send_frame(rtskb, rtdev,
+			    device[ifindex].spec.clt.srv_mac_addr);
+}
+
+
+
+int rtcfg_send_simple_frame(int ifindex, int frame_id, u8 *dest_addr)
+{
+    struct rtnet_device     *rtdev;
+    struct rtskb            *rtskb;
+    unsigned int            rtskb_size;
+    struct rtcfg_frm_simple *simple_frm;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len + sizeof(struct rtcfg_frm_simple);
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    simple_frm = (struct rtcfg_frm_simple *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_simple));
+
+    simple_frm->head.id      = frame_id;
+    simple_frm->head.version = 0;
+
+    return rtcfg_send_frame(rtskb, rtdev,
+			    (dest_addr) ? dest_addr : rtdev->broadcast);
+}
+
+
+
+int rtcfg_send_dead_station(struct rtcfg_connection *conn)
+{
+    struct rtnet_device           *rtdev;
+    struct rtskb                  *rtskb;
+    unsigned int                  rtskb_size;
+    struct rtcfg_frm_dead_station *dead_station_frm;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_dead_station) +
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	(((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) ?
+	RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_RTNET_RTIPV4 */
+	0;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    dead_station_frm = (struct rtcfg_frm_dead_station *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_dead_station));
+
+    dead_station_frm->head.id      = RTCFG_ID_DEAD_STATION;
+    dead_station_frm->head.version = 0;
+    dead_station_frm->addr_type    = conn->addr_type & RTCFG_ADDR_MASK;
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+    if (dead_station_frm->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, RTCFG_ADDRSIZE_IP);
+
+	memcpy(dead_station_frm->logical_addr, &(conn->addr.ip_addr), 4);
+
+	dead_station_frm = (struct rtcfg_frm_dead_station *)
+	    (((u8 *)dead_station_frm) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+    /* Ethernet-specific! */
+    memcpy(dead_station_frm->physical_addr, conn->mac_addr, ETH_ALEN);
+    memset(&dead_station_frm->physical_addr[ETH_ALEN], 0,
+	sizeof(dead_station_frm->physical_addr) - ETH_ALEN);
+
+    return rtcfg_send_frame(rtskb, rtdev, rtdev->broadcast);
+}
+
+
+
+static struct rtpacket_type rtcfg_packet_type = {
+    .type =     __constant_htons(ETH_RTCFG),
+    .handler =  rtcfg_rx_handler
+};
+
+
+
+int __init rtcfg_init_frames(void)
+{
+    int ret;
+
+
+    if (rtskb_module_pool_init(&rtcfg_pool, num_rtskbs) < num_rtskbs)
+	return -ENOMEM;
+
+    rtskb_queue_init(&rx_queue);
+    rtdm_event_init(&rx_event, 0);
+
+    ret = rtdm_task_init(&rx_task, "rtcfg-rx", rtcfg_rx_task, 0,
+			 RTDM_TASK_LOWEST_PRIORITY, 0);
+    if (ret < 0) {
+	rtdm_event_destroy(&rx_event);
+	goto error1;
+    }
+
+    ret = rtdev_add_pack(&rtcfg_packet_type);
+    if (ret < 0)
+	goto error2;
+
+    return 0;
+
+  error2:
+    rtdm_event_destroy(&rx_event);
+    rtdm_task_destroy(&rx_task);
+
+  error1:
+    rtskb_pool_release(&rtcfg_pool);
+
+    return ret;
+}
+
+
+
+void rtcfg_cleanup_frames(void)
+{
+    struct rtskb *rtskb;
+
+
+    rtdev_remove_pack(&rtcfg_packet_type);
+
+    rtdm_event_destroy(&rx_event);
+    rtdm_task_destroy(&rx_task);
+
+    while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+	kfree_rtskb(rtskb);
+    }
+
+    rtskb_pool_release(&rtcfg_pool);
+}
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_ioctl.c b/net/rtnet/stack/rtcfg/rtcfg_ioctl.c
--- a/net/rtnet/stack/rtcfg/rtcfg_ioctl.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_ioctl.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,455 @@
+/***
+ *
+ *  rtcfg/rtcfg_ioctl.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/file.h>
+#include <linux/vmalloc.h>
+
+#include <rtcfg_chrdev.h>
+#include <rtnet_rtpc.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_proc.h>
+
+
+int rtcfg_event_handler(struct rt_proc_call *call)
+{
+    struct rtcfg_cmd *cmd_event;
+
+
+    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+    return rtcfg_do_main_event(cmd_event->internal.data.ifindex,
+                               cmd_event->internal.data.event_id, call);
+}
+
+
+
+void keep_cmd_add(struct rt_proc_call *call, void *priv_data)
+{
+    /* do nothing on error (<0), or if file already present (=0) */
+    if (rtpc_get_result(call) <= 0)
+        return;
+
+    /* Don't cleanup any buffers, we are going to recycle them! */
+    rtpc_set_cleanup_handler(call, NULL);
+}
+
+
+
+void cleanup_cmd_add(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *buf;
+
+
+    /* unlock proc and update directory structure */
+    rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+
+    buf = cmd->args.add.conn_buf;
+    if (buf != NULL)
+        kfree(buf);
+
+    buf = cmd->args.add.stage1_data;
+    if (buf != NULL)
+        kfree(buf);
+
+    if (cmd->args.add.stage2_file != NULL) {
+        buf = cmd->args.add.stage2_file->buffer;
+        if (buf != NULL)
+            vfree(buf);
+        kfree(cmd->args.add.stage2_file);
+    }
+}
+
+
+
+void cleanup_cmd_del(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *buf;
+
+
+    /* unlock proc and update directory structure */
+    rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+
+    if (cmd->args.del.conn_buf != NULL) {
+        buf = cmd->args.del.conn_buf->stage1_data;
+        if (buf != NULL)
+            kfree(buf);
+        kfree(cmd->args.del.conn_buf);
+    }
+
+    if (cmd->args.del.stage2_file != NULL) {
+        buf = cmd->args.del.stage2_file->buffer;
+        if (buf != NULL)
+            vfree(buf);
+        kfree(cmd->args.del.stage2_file);
+    }
+}
+
+
+
+void copy_stage_1_data(struct rt_proc_call *call, void *priv_data)
+{
+    struct rtcfg_cmd *cmd;
+    int              result = rtpc_get_result(call);
+
+
+    if (result <= 0)
+        return;
+
+    cmd = rtpc_get_priv(call, struct rtcfg_cmd);
+
+    if (cmd->args.client.buffer_size < (size_t)result)
+        rtpc_set_result(call, -ENOSPC);
+    else if (copy_to_user(cmd->args.client.buffer,
+                          cmd->args.client.rtskb->data, result) != 0)
+        rtpc_set_result(call, -EFAULT);
+}
+
+
+
+void cleanup_cmd_client(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *station_buf;
+    struct rtskb     *rtskb;
+
+
+    station_buf = cmd->args.client.station_buf;
+    if (station_buf != NULL)
+        kfree(station_buf);
+
+    rtskb = cmd->args.client.rtskb;
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+void copy_stage_2_data(struct rt_proc_call *call, void *priv_data)
+{
+    struct rtcfg_cmd *cmd;
+    int              result = rtpc_get_result(call);
+    struct rtskb     *rtskb;
+
+
+    if (result <= 0)
+        return;
+
+    cmd = rtpc_get_priv(call, struct rtcfg_cmd);
+
+    if (cmd->args.announce.buffer_size < (size_t)result)
+        rtpc_set_result(call, -ENOSPC);
+    else {
+        rtskb = cmd->args.announce.rtskb;
+        do {
+            if (copy_to_user(cmd->args.announce.buffer,
+                             rtskb->data, rtskb->len) != 0) {
+                rtpc_set_result(call, -EFAULT);
+                break;
+            }
+            cmd->args.announce.buffer += rtskb->len;
+            rtskb = rtskb->next;
+        } while (rtskb != NULL);
+    }
+}
+
+
+
+void cleanup_cmd_announce(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    struct rtskb     *rtskb;
+
+
+    rtskb = cmd->args.announce.rtskb;
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+void cleanup_cmd_detach(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *buf;
+
+
+    /* unlock proc and update directory structure */
+    rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+
+    if (cmd->args.detach.conn_buf) {
+        buf = cmd->args.detach.conn_buf->stage1_data;
+        if (buf != NULL)
+            kfree(buf);
+        kfree(cmd->args.detach.conn_buf);
+    }
+
+    if (cmd->args.detach.stage2_file != NULL) {
+        buf = cmd->args.detach.stage2_file->buffer;
+        if (buf)
+            vfree(buf);
+        kfree(cmd->args.detach.stage2_file);
+    }
+
+    if (cmd->args.detach.station_addr_list)
+        kfree(cmd->args.detach.station_addr_list);
+
+    if (cmd->args.detach.stage2_chain)
+        kfree_rtskb(cmd->args.detach.stage2_chain);
+}
+
+
+
+int rtcfg_ioctl_add(struct rtnet_device *rtdev, struct rtcfg_cmd *cmd)
+{
+    struct rtcfg_connection *conn_buf;
+    struct rtcfg_file       *file = NULL;
+    void                    *data_buf;
+    size_t                  size;
+    int                     ret;
+
+
+    conn_buf = kmalloc(sizeof(struct rtcfg_connection), GFP_KERNEL);
+    if (conn_buf == NULL)
+        return -ENOMEM;
+    cmd->args.add.conn_buf = conn_buf;
+
+    data_buf = NULL;
+    size = cmd->args.add.stage1_size;
+    if (size > 0) {
+        /* check stage 1 data size */
+        if (sizeof(struct rtcfg_frm_stage_1_cfg) + 2*RTCFG_ADDRSIZE_IP + size >
+                rtdev->get_mtu(rtdev, RTCFG_SKB_PRIO)) {
+            ret = -ESTAGE1SIZE;
+            goto err;
+        }
+
+        data_buf = kmalloc(size, GFP_KERNEL);
+        if (data_buf == NULL) {
+            ret = -ENOMEM;
+            goto err;
+        }
+
+        ret = copy_from_user(data_buf, cmd->args.add.stage1_data, size);
+        if (ret != 0) {
+            ret = -EFAULT;
+            goto err;
+        }
+    }
+    cmd->args.add.stage1_data = data_buf;
+
+    if (cmd->args.add.stage2_filename != NULL) {
+        size = strnlen_user(cmd->args.add.stage2_filename, PATH_MAX);
+
+        file = kmalloc(sizeof(struct rtcfg_file) + size, GFP_KERNEL);
+        if (file == NULL) {
+            ret = -ENOMEM;
+            goto err;
+        }
+
+        file->name   = (char *)file + sizeof(struct rtcfg_file);
+        file->buffer = NULL;
+
+        ret = copy_from_user((void *)file + sizeof(struct rtcfg_file),
+                             (const void *)cmd->args.add.stage2_filename,
+                             size);
+        if (ret != 0) {
+            ret = -EFAULT;
+            goto err;
+        }
+    }
+    cmd->args.add.stage2_file = file;
+
+    /* lock proc structure for modification */
+    rtcfg_lockwr_proc(cmd->internal.data.ifindex);
+
+    ret = rtpc_dispatch_call(rtcfg_event_handler, 0, cmd,
+                             sizeof(*cmd), keep_cmd_add,
+                             cleanup_cmd_add);
+
+    /* load file if missing */
+    if (ret > 0) {
+        struct file  *filp;
+        mm_segment_t oldfs;
+
+
+        filp = filp_open(file->name, O_RDONLY, 0);
+        if (IS_ERR(filp)) {
+            rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+            ret = PTR_ERR(filp);
+            goto err;
+        }
+
+        file->size = filp->f_path.dentry->d_inode->i_size;
+
+        /* allocate buffer even for empty files */
+        file->buffer = vmalloc((file->size)? file->size : 1);
+        if (file->buffer == NULL) {
+            rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+            fput(filp);
+            ret = -ENOMEM;
+            goto err;
+        }
+
+        oldfs = get_fs();
+        set_fs(KERNEL_DS);
+        filp->f_pos = 0;
+
+        ret = filp->f_op->read(filp, file->buffer, file->size,
+                               &filp->f_pos);
+
+        set_fs(oldfs);
+        fput(filp);
+
+        if (ret != (int)file->size) {
+            rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+            ret = -EIO;
+            goto err;
+        }
+
+        /* dispatch again, this time with new file attached */
+        ret = rtpc_dispatch_call(rtcfg_event_handler, 0, cmd,
+                                 sizeof(*cmd), NULL, cleanup_cmd_add);
+    }
+
+    return ret;
+
+  err:
+    kfree(conn_buf);
+    if (data_buf != NULL)
+        kfree(data_buf);
+    if (file != NULL) {
+        if (file->buffer != NULL)
+            vfree(file->buffer);
+        kfree(file);
+    }
+    return ret;
+}
+
+
+
+int rtcfg_ioctl(struct rtnet_device *rtdev, unsigned int request, unsigned long arg)
+{
+    struct rtcfg_cmd        cmd;
+    struct rtcfg_station    *station_buf;
+    int                     ret;
+
+
+    ret = copy_from_user(&cmd, (void *)arg, sizeof(cmd));
+    if (ret != 0)
+        return -EFAULT;
+
+    cmd.internal.data.ifindex  = rtdev->ifindex;
+    cmd.internal.data.event_id = _IOC_NR(request);
+
+    switch (request) {
+        case RTCFG_IOC_SERVER:
+            ret = rtpc_dispatch_call(rtcfg_event_handler, 0, &cmd,
+                                     sizeof(cmd), NULL, NULL);
+            break;
+
+        case RTCFG_IOC_ADD:
+            ret = rtcfg_ioctl_add(rtdev, &cmd);
+            break;
+
+        case RTCFG_IOC_DEL:
+            cmd.args.del.conn_buf    = NULL;
+            cmd.args.del.stage2_file = NULL;
+
+            /* lock proc structure for modification
+               (unlock in cleanup_cmd_del) */
+            rtcfg_lockwr_proc(cmd.internal.data.ifindex);
+
+            ret = rtpc_dispatch_call(rtcfg_event_handler, 0, &cmd,
+                                     sizeof(cmd), NULL, cleanup_cmd_del);
+            break;
+
+        case RTCFG_IOC_WAIT:
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.wait.timeout, &cmd,
+                                     sizeof(cmd), NULL, NULL);
+            break;
+
+        case RTCFG_IOC_CLIENT:
+            station_buf = kmalloc(sizeof(struct rtcfg_station) *
+                                  cmd.args.client.max_stations, GFP_KERNEL);
+            if (station_buf == NULL)
+                return -ENOMEM;
+            cmd.args.client.station_buf = station_buf;
+            cmd.args.client.rtskb       = NULL;
+
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.client.timeout, &cmd,
+                                     sizeof(cmd), copy_stage_1_data,
+                                     cleanup_cmd_client);
+            break;
+
+        case RTCFG_IOC_ANNOUNCE:
+            cmd.args.announce.rtskb = NULL;
+
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.announce.timeout, &cmd,
+                                     sizeof(cmd), copy_stage_2_data,
+                                     cleanup_cmd_announce);
+            break;
+
+        case RTCFG_IOC_READY:
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.ready.timeout, &cmd,
+                                     sizeof(cmd), NULL, NULL);
+            break;
+
+        case RTCFG_IOC_DETACH:
+            do {
+                cmd.args.detach.conn_buf          = NULL;
+                cmd.args.detach.stage2_file       = NULL;
+                cmd.args.detach.station_addr_list = NULL;
+                cmd.args.detach.stage2_chain      = NULL;
+
+                /* lock proc structure for modification
+                   (unlock in cleanup_cmd_detach) */
+                rtcfg_lockwr_proc(cmd.internal.data.ifindex);
+
+                ret = rtpc_dispatch_call(rtcfg_event_handler, 0, &cmd,
+                                         sizeof(cmd), NULL,
+                                         cleanup_cmd_detach);
+            } while (ret == -EAGAIN);
+            break;
+
+        default:
+            ret = -ENOTTY;
+    }
+
+    return ret;
+}
+
+
+
+struct rtnet_ioctls rtcfg_ioctls = {
+    .service_name = "RTcfg",
+    .ioctl_type =   RTNET_IOC_TYPE_RTCFG,
+    .handler =      rtcfg_ioctl
+};
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_module.c b/net/rtnet/stack/rtcfg/rtcfg_module.c
--- a/net/rtnet/stack/rtcfg/rtcfg_module.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_module.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,89 @@
+/***
+ *
+ *  rtcfg/rtcfg_module.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_ioctl.h>
+#include <rtcfg/rtcfg_proc.h>
+
+
+MODULE_LICENSE("GPL");
+
+int __init rtcfg_init(void)
+{
+    int ret;
+
+
+    printk(KERN_INFO "RTcfg: init real-time configuration distribution protocol\n");
+
+    ret = rtcfg_init_ioctls();
+    if (ret != 0)
+	goto error1;
+
+    rtcfg_init_state_machines();
+
+    ret = rtcfg_init_frames();
+    if (ret != 0)
+	goto error2;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    ret = rtcfg_init_proc();
+    if (ret != 0) {
+	rtcfg_cleanup_frames();
+	goto error2;
+    }
+#endif
+
+    return 0;
+
+  error2:
+    rtcfg_cleanup_state_machines();
+    rtcfg_cleanup_ioctls();
+
+  error1:
+    return ret;
+}
+
+
+
+void rtcfg_cleanup(void)
+{
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtcfg_cleanup_proc();
+#endif
+    rtcfg_cleanup_frames();
+    rtcfg_cleanup_state_machines();
+    rtcfg_cleanup_ioctls();
+
+    printk(KERN_INFO "RTcfg: unloaded\n");
+}
+
+
+
+module_init(rtcfg_init);
+module_exit(rtcfg_cleanup);
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_proc.c b/net/rtnet/stack/rtcfg/rtcfg_proc.c
--- a/net/rtnet/stack/rtcfg/rtcfg_proc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_proc.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,350 @@
+/***
+ *
+ *	rtcfg/rtcfg_proc.c
+ *
+ *	Real-Time Configuration Distribution Protocol
+ *
+ *	Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License as published by
+ *	the Free Software Foundation; either version 2 of the License, or
+ *	(at your option) any later version.
+ *
+ *	This program is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	You should have received a copy of the GNU General Public License
+ *	along with this program; if not, write to the Free Software
+ *	Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_port.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+DEFINE_MUTEX(nrt_proc_lock);
+static struct xnvfile_directory rtcfg_proc_root;
+
+static int rtnet_rtcfg_proc_lock_get(struct xnvfile *vfile)
+{
+	return mutex_lock_interruptible(&nrt_proc_lock);
+}
+
+static void rtnet_rtcfg_proc_lock_put(struct xnvfile *vfile)
+{
+	return mutex_unlock(&nrt_proc_lock);
+}
+
+static struct xnvfile_lock_ops rtnet_rtcfg_proc_lock_ops = {
+	.get = rtnet_rtcfg_proc_lock_get,
+	.put = rtnet_rtcfg_proc_lock_put,
+};
+
+int rtnet_rtcfg_dev_state_show(struct xnvfile_regular_iterator *it, void *data)
+{
+	struct rtcfg_device *rtcfg_dev = xnvfile_priv(it->vfile);
+	const char *state_name[] = {
+		"OFF", "SERVER_RUNNING", "CLIENT_0", "CLIENT_1",
+		"CLIENT_ANNOUNCED", "CLIENT_ALL_KNOWN",
+		"CLIENT_ALL_FRAMES", "CLIENT_2", "CLIENT_READY"
+	};
+
+	xnvfile_printf(it, "state:\t\t\t%d (%s)\n"
+				"flags:\t\t\t%08lX\n"
+				"other stations:\t\t%d\n"
+				"stations found:\t\t%d\n"
+				"stations ready:\t\t%d\n",
+				rtcfg_dev->state, state_name[rtcfg_dev->state],
+				rtcfg_dev->flags, rtcfg_dev->other_stations,
+				rtcfg_dev->stations_found,
+				rtcfg_dev->stations_ready);
+
+	if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+		xnvfile_printf(it, "configured clients:\t%d\n"
+					"burstrate:\t\t%d\n"
+					"heartbeat period:\t%d ms\n",
+					rtcfg_dev->spec.srv.clients_configured,
+					rtcfg_dev->burstrate, rtcfg_dev->spec.srv.heartbeat);
+	} else if (rtcfg_dev->state != RTCFG_MAIN_OFF) {
+		xnvfile_printf(it, "address type:\t\t%d\n"
+					"server address:\t\t%02X:%02X:%02X:%02X:%02X:%02X\n"
+					"stage 2 config:\t\t%d/%d\n",
+					rtcfg_dev->spec.clt.addr_type,
+					rtcfg_dev->spec.clt.srv_mac_addr[0],
+					rtcfg_dev->spec.clt.srv_mac_addr[1],
+					rtcfg_dev->spec.clt.srv_mac_addr[2],
+					rtcfg_dev->spec.clt.srv_mac_addr[3],
+					rtcfg_dev->spec.clt.srv_mac_addr[4],
+					rtcfg_dev->spec.clt.srv_mac_addr[5],
+					rtcfg_dev->spec.clt.cfg_offs,
+					rtcfg_dev->spec.clt.cfg_len);
+	}
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtcfg_dev_state_vfile_ops = {
+	.show = rtnet_rtcfg_dev_state_show,
+};
+
+int rtnet_rtcfg_dev_stations_show(struct xnvfile_regular_iterator *it, void *d)
+{
+	struct rtcfg_device *rtcfg_dev = xnvfile_priv(it->vfile);
+	struct rtcfg_connection *conn;
+	struct rtcfg_station *station;
+	int i;
+
+	if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+		list_for_each_entry(conn, &rtcfg_dev->spec.srv.conn_list, entry) {
+			if ((conn->state != RTCFG_CONN_SEARCHING) &&
+				(conn->state != RTCFG_CONN_DEAD))
+				xnvfile_printf(it, "%02X:%02X:%02X:%02X:%02X:%02X\t%02X\n",
+							conn->mac_addr[0], conn->mac_addr[1],
+							conn->mac_addr[2], conn->mac_addr[3],
+							conn->mac_addr[4], conn->mac_addr[5],
+							conn->flags);
+		}
+	} else if (rtcfg_dev->spec.clt.station_addr_list) {
+		for (i = 0; i < rtcfg_dev->stations_found; i++) {
+			station = &rtcfg_dev->spec.clt.station_addr_list[i];
+
+			xnvfile_printf(it, "%02X:%02X:%02X:%02X:%02X:%02X\t%02X\n",
+						station->mac_addr[0], station->mac_addr[1],
+						station->mac_addr[2], station->mac_addr[3],
+						station->mac_addr[4], station->mac_addr[5],
+						station->flags);
+		}
+	}
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtcfg_dev_stations_vfile_ops = {
+	.show = rtnet_rtcfg_dev_stations_show,
+};
+
+int
+rtnet_rtcfg_dev_conn_state_show(struct xnvfile_regular_iterator *it, void *d)
+{
+	struct rtcfg_connection *conn = xnvfile_priv(it->vfile);
+	char *state_name[] =
+		{ "SEARCHING", "STAGE_1", "STAGE_2", "READY", "DEAD" };
+
+	xnvfile_printf(it, "state:\t\t\t%d (%s)\n"
+				"flags:\t\t\t%02X\n"
+				"stage 1 size:\t\t%zd\n"
+				"stage 2 filename:\t%s\n"
+				"stage 2 size:\t\t%zd\n"
+				"stage 2 offset:\t\t%d\n"
+				"burstrate:\t\t%d\n"
+				"mac address:\t\t%02X:%02X:%02X:%02X:%02X:%02X\n",
+				conn->state, state_name[conn->state], conn->flags,
+				conn->stage1_size,
+				(conn->stage2_file)? conn->stage2_file->name: "-",
+				(conn->stage2_file)? conn->stage2_file->size: 0,
+				conn->cfg_offs, conn->burstrate,
+				conn->mac_addr[0], conn->mac_addr[1],
+				conn->mac_addr[2], conn->mac_addr[3],
+				conn->mac_addr[4], conn->mac_addr[5]);
+
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+	if ((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP)
+		xnvfile_printf(it, "ip:\t\t\t%u.%u.%u.%u\n",
+					NIPQUAD(conn->addr.ip_addr));
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtcfg_dev_conn_state_vfile_ops = {
+	.show = rtnet_rtcfg_dev_conn_state_show,
+};
+
+void rtcfg_update_conn_proc_entries(int ifindex)
+{
+	struct rtcfg_device		*dev = &device[ifindex];
+	struct rtcfg_connection *conn;
+	char					name_buf[64];
+
+	if (dev->state != RTCFG_MAIN_SERVER_RUNNING)
+		return;
+
+	list_for_each_entry(conn, &dev->spec.srv.conn_list, entry) {
+		switch (conn->addr_type & RTCFG_ADDR_MASK) {
+#if IS_ENABLED(CONFIG_RTNET_RTIPV4)
+		case RTCFG_ADDR_IP:
+			snprintf(name_buf, 64, "CLIENT_%u.%u.%u.%u",
+					NIPQUAD(conn->addr.ip_addr));
+			break;
+#endif /* CONFIG_RTNET_RTIPV4 */
+
+		default: /* RTCFG_ADDR_MAC */
+			snprintf(name_buf, 64,
+					"CLIENT_%02X%02X%02X%02X%02X%02X",
+					conn->mac_addr[0], conn->mac_addr[1],
+					conn->mac_addr[2], conn->mac_addr[3],
+					conn->mac_addr[4], conn->mac_addr[5]);
+			break;
+		}
+		memset(&conn->proc_entry, '\0', sizeof(conn->proc_entry));
+		conn->proc_entry.entry.lockops = &rtnet_rtcfg_proc_lock_ops;
+		conn->proc_entry.ops = &rtnet_rtcfg_dev_conn_state_vfile_ops;
+		xnvfile_priv(&conn->proc_entry) = conn;
+
+		xnvfile_init_regular(name_buf, &conn->proc_entry, &dev->proc_entry);
+	}
+}
+
+
+
+void rtcfg_remove_conn_proc_entries(int ifindex)
+{
+	struct rtcfg_device		*dev = &device[ifindex];
+	struct rtcfg_connection *conn;
+
+
+	if (dev->state != RTCFG_MAIN_SERVER_RUNNING)
+		return;
+
+	list_for_each_entry(conn, &dev->spec.srv.conn_list, entry)
+		xnvfile_destroy_regular(&conn->proc_entry);
+}
+
+
+
+void rtcfg_new_rtdev(struct rtnet_device *rtdev)
+{
+	struct rtcfg_device *dev = &device[rtdev->ifindex];
+	int err;
+
+
+	mutex_lock(&nrt_proc_lock);
+
+	memset(&dev->proc_entry, '\0', sizeof(dev->proc_entry));
+	err = xnvfile_init_dir(rtdev->name, &dev->proc_entry, &rtcfg_proc_root);
+	if (err < 0)
+		goto error1;
+
+	memset(&dev->proc_state_vfile, '\0', sizeof(dev->proc_state_vfile));
+	dev->proc_state_vfile.entry.lockops = &rtnet_rtcfg_proc_lock_ops;
+	dev->proc_state_vfile.ops = &rtnet_rtcfg_dev_state_vfile_ops;
+	xnvfile_priv(&dev->proc_state_vfile) = dev;
+
+	err = xnvfile_init_regular("state",
+							&dev->proc_state_vfile, &dev->proc_entry);
+	if (err < 0)
+		goto error2;
+
+	memset(&dev->proc_stations_vfile, '\0', sizeof(dev->proc_stations_vfile));
+	dev->proc_stations_vfile.entry.lockops = &rtnet_rtcfg_proc_lock_ops;
+	dev->proc_stations_vfile.ops = &rtnet_rtcfg_dev_stations_vfile_ops;
+	xnvfile_priv(&dev->proc_stations_vfile) = dev;
+
+	err = xnvfile_init_regular("stations_list",
+							&dev->proc_stations_vfile, &dev->proc_entry);
+	if (err < 0)
+		goto error3;
+
+	mutex_unlock(&nrt_proc_lock);
+
+	return;
+
+  error3:
+	xnvfile_destroy_regular(&dev->proc_state_vfile);
+  error2:
+	xnvfile_destroy_dir(&dev->proc_entry);
+  error1:
+	dev->proc_entry.entry.pde = NULL;
+	mutex_unlock(&nrt_proc_lock);
+}
+
+
+
+void rtcfg_remove_rtdev(struct rtnet_device *rtdev)
+{
+	struct rtcfg_device *dev = &device[rtdev->ifindex];
+
+
+	// To-Do: issue down command
+
+	mutex_lock(&nrt_proc_lock);
+
+	if (dev->proc_entry.entry.pde) {
+		rtcfg_remove_conn_proc_entries(rtdev->ifindex);
+
+		xnvfile_destroy_regular(&dev->proc_stations_vfile);
+		xnvfile_destroy_regular(&dev->proc_state_vfile);
+		xnvfile_destroy_dir(&dev->proc_entry);
+		dev->proc_entry.entry.pde = NULL;
+	}
+
+	mutex_unlock(&nrt_proc_lock);
+}
+
+
+
+static struct rtdev_event_hook rtdev_hook = {
+	.register_device =	rtcfg_new_rtdev,
+	.unregister_device =rtcfg_remove_rtdev,
+	.ifup =				NULL,
+	.ifdown =			NULL
+};
+
+
+
+int rtcfg_init_proc(void)
+{
+	struct rtnet_device *rtdev;
+	int					i, err;
+
+	err = xnvfile_init_dir("rtcfg", &rtcfg_proc_root, &rtnet_proc_root);
+	if (err < 0)
+		goto err1;
+
+	for (i = 0; i < MAX_RT_DEVICES; i++) {
+		rtdev = rtdev_get_by_index(i);
+		if (rtdev) {
+			rtcfg_new_rtdev(rtdev);
+			rtdev_dereference(rtdev);
+		}
+	}
+
+	rtdev_add_event_hook(&rtdev_hook);
+	return 0;
+
+  err1:
+	printk(KERN_ERR "RTcfg: unable to initialise /proc entries\n");
+	return err;
+}
+
+
+
+void rtcfg_cleanup_proc(void)
+{
+	struct rtnet_device *rtdev;
+	int					i;
+
+
+	rtdev_del_event_hook(&rtdev_hook);
+
+	for (i = 0; i < MAX_RT_DEVICES; i++) {
+		rtdev = rtdev_get_by_index(i);
+		if (rtdev) {
+			rtcfg_remove_rtdev(rtdev);
+			rtdev_dereference(rtdev);
+		}
+	}
+
+	xnvfile_destroy_dir(&rtcfg_proc_root);
+}
+
+#endif /* CONFIG_XENO_OPT_VFILE */
diff -Naur a/net/rtnet/stack/rtcfg/rtcfg_timer.c b/net/rtnet/stack/rtcfg/rtcfg_timer.c
--- a/net/rtnet/stack/rtcfg/rtcfg_timer.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtcfg/rtcfg_timer.c	2021-07-14 15:39:13.350124689 +0300
@@ -0,0 +1,104 @@
+/***
+ *
+ *  rtcfg/rtcfg_timer.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <rtdev.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+void rtcfg_timer(rtdm_timer_t *t)
+{
+    struct rtcfg_device *rtcfg_dev =
+	container_of(t, struct rtcfg_device, timer);
+
+    set_bit(FLAG_TIMER_PENDING, &rtcfg_dev->flags);
+    rtcfg_thread_signal();
+}
+
+void rtcfg_timer_run_one(int ifindex)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct list_head        *entry;
+    struct rtcfg_connection *conn;
+    int                     last_stage_1 = -1;
+    int                     burst_credit;
+    int                     index;
+    int                     ret, shutdown;
+
+    shutdown = test_and_clear_bit(FLAG_TIMER_SHUTDOWN, &rtcfg_dev->flags);
+
+    if (!test_and_clear_bit(FLAG_TIMER_PENDING, &rtcfg_dev->flags)
+	|| shutdown)
+	return;
+
+    rtdm_mutex_lock(&rtcfg_dev->dev_mutex);
+
+    if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+	index = 0;
+	burst_credit = rtcfg_dev->burstrate;
+
+	list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	    conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	    if ((conn->state == RTCFG_CONN_SEARCHING) ||
+		(conn->state == RTCFG_CONN_DEAD)){
+		if ((burst_credit > 0) && (index > last_stage_1)) {
+		    if ((ret = rtcfg_send_stage_1(conn)) < 0) {
+			RTCFG_DEBUG(2, "RTcfg: error %d while sending "
+                                        "stage 1 frame\n", ret);
+		    }
+		    burst_credit--;
+		    last_stage_1 = index;
+		}
+	    } else {
+		/* skip connection in history */
+		if (last_stage_1 == (index-1))
+		    last_stage_1 = index;
+
+		rtcfg_do_conn_event(conn, RTCFG_TIMER, NULL);
+	    }
+	    index++;
+	}
+
+	/* handle pointer overrun of the last stage 1 transmission */
+	if (last_stage_1 == (index-1))
+	    last_stage_1 = -1;
+    } else if (rtcfg_dev->state == RTCFG_MAIN_CLIENT_READY)
+	rtcfg_send_heartbeat(ifindex);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+}
+
+void rtcfg_timer_run(void)
+{
+    int ifindex;
+
+    for (ifindex = 0; ifindex < MAX_RT_DEVICES; ifindex++)
+	rtcfg_timer_run_one(ifindex);
+}
diff -Naur a/net/rtnet/stack/rtdev.c b/net/rtnet/stack/rtdev.c
--- a/net/rtnet/stack/rtdev.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtdev.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,843 @@
+/***
+ *
+ *  stack/rtdev.c - NIC device driver layer
+ *
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/spinlock.h>
+#include <linux/if.h>
+#include <linux/if_arp.h> /* ARPHRD_ETHER */
+#include <linux/netdevice.h>
+#include <linux/moduleparam.h>
+
+#include <rtnet_internal.h>
+#include <rtskb.h>
+#include <ethernet/eth.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtnet_port.h>
+
+
+static unsigned int device_rtskbs = DEFAULT_DEVICE_RTSKBS;
+module_param(device_rtskbs, uint, 0444);
+MODULE_PARM_DESC(device_rtskbs, "Number of additional global realtime socket "
+		 "buffers per network adapter");
+
+struct rtnet_device         *rtnet_devices[MAX_RT_DEVICES];
+static struct rtnet_device  *loopback_device;
+static raw_spinlock_t rtnet_devices_rt_lock;
+static int initialized_rtnet_devices_rt_lock = 0;
+static LIST_HEAD(rtskb_mapped_list);
+static LIST_HEAD(rtskb_mapwait_list);
+
+LIST_HEAD(event_hook_list);
+DEFINE_MUTEX(rtnet_devices_nrt_lock);
+
+static int rtdev_locked_xmit(struct rtskb *skb, struct rtnet_device *rtdev);
+
+int rtdev_reference(struct rtnet_device *rtdev)
+{
+    smp_mb__before_atomic();
+    if (rtdev->rt_owner && atomic_add_unless(&rtdev->refcount, 1, 0) == 0) {
+	if (!try_module_get(rtdev->rt_owner))
+	    return 0;
+	if (atomic_inc_return(&rtdev->refcount) != 1)
+	    module_put(rtdev->rt_owner);
+    }
+    return 1;
+}
+EXPORT_SYMBOL_GPL(rtdev_reference);
+
+struct rtskb *rtnetdev_alloc_rtskb(struct rtnet_device *rtdev, unsigned int size)
+{
+    struct rtskb *rtskb = alloc_rtskb(size, &rtdev->dev_pool);
+    if (rtskb)
+	rtskb->rtdev = rtdev;
+    return rtskb;
+}
+EXPORT_SYMBOL_GPL(rtnetdev_alloc_rtskb);
+
+/***
+ *  __rtdev_get_by_name - find a rtnet_device by its name
+ *  @name: name to find
+ *  @note: caller must hold rtnet_devices_nrt_lock
+ */
+static struct rtnet_device *__rtdev_get_by_name(const char *name)
+{
+    int                 i;
+    struct rtnet_device *rtdev;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtdev = rtnet_devices[i];
+	if ((rtdev != NULL) && (strncmp(rtdev->name, name, IFNAMSIZ) == 0))
+	    return rtdev;
+    }
+    return NULL;
+}
+
+
+/***
+ *  rtdev_get_by_name - find and lock a rtnet_device by its name
+ *  @name: name to find
+ */
+struct rtnet_device *rtdev_get_by_name(const char *name)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = __rtdev_get_by_name(name);
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_get_by_index - find and lock a rtnet_device by its ifindex
+ *  @ifindex: index of device
+ */
+struct rtnet_device *rtdev_get_by_index(int ifindex)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    if ((ifindex <= 0) || (ifindex > MAX_RT_DEVICES))
+	return NULL;
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = __rtdev_get_by_index(ifindex);
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  __rtdev_get_by_hwaddr - find a rtnetdevice by its mac-address
+ *  @type:          Type of the net_device (may be ARPHRD_ETHER)
+ *  @hw_addr:       MAC-Address
+ */
+static inline struct rtnet_device *__rtdev_get_by_hwaddr(unsigned short type, char *hw_addr)
+{
+    int                 i;
+    struct rtnet_device *rtdev;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtdev = rtnet_devices[i];
+	if ((rtdev != NULL) && (rtdev->type == type) &&
+	    (!memcmp(rtdev->dev_addr, hw_addr, rtdev->addr_len))) {
+	    return rtdev;
+	}
+    }
+    return NULL;
+}
+
+
+
+/***
+ *  rtdev_get_by_hwaddr - find and lock a rtnetdevice by its mac-address
+ *  @type:          Type of the net_device (may be ARPHRD_ETHER)
+ *  @hw_addr:       MAC-Address
+ */
+struct rtnet_device *rtdev_get_by_hwaddr(unsigned short type, char *hw_addr)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = __rtdev_get_by_hwaddr(type, hw_addr);
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_get_by_hwaddr - find and lock the loopback device if available
+ */
+struct rtnet_device *rtdev_get_loopback(void)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = loopback_device;
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_alloc_name - allocate a name for the rtnet_device
+ *  @rtdev:         the rtnet_device
+ *  @name_mask:     a name mask (e.g. "rteth%d" for ethernet)
+ *
+ *  This function have to be called from the driver probe function.
+ */
+void rtdev_alloc_name(struct rtnet_device *rtdev, const char *mask)
+{
+    char                buf[IFNAMSIZ];
+    int                 i;
+    struct rtnet_device *tmp;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	snprintf(buf, IFNAMSIZ, mask, i);
+	if ((tmp = rtdev_get_by_name(buf)) == NULL) {
+	    strncpy(rtdev->name, buf, IFNAMSIZ);
+	    break;
+	}
+	else
+	    rtdev_dereference(tmp);
+    }
+}
+
+static int rtdev_pool_trylock(void *cookie)
+{
+    return rtdev_reference(cookie);
+}
+
+static void rtdev_pool_unlock(void *cookie)
+{
+    rtdev_dereference(cookie);
+}
+
+static const struct rtskb_pool_lock_ops rtdev_ops = {
+    .trylock = rtdev_pool_trylock,
+    .unlock = rtdev_pool_unlock,
+};
+
+/***
+ *  rtdev_alloc
+ *  @int sizeof_priv:
+ *
+ *  allocate memory for a new rt-network-adapter
+ */
+struct rtnet_device *rtdev_alloc(unsigned sizeof_priv, unsigned dev_pool_size)
+{
+    struct rtnet_device *rtdev;
+    unsigned            alloc_size;
+    int                 ret;
+
+
+    /* ensure 32-byte alignment of the private area */
+    alloc_size = sizeof (*rtdev) + sizeof_priv + 31;
+
+    rtdev = (struct rtnet_device *)kmalloc(alloc_size, GFP_KERNEL);
+    if (rtdev == NULL) {
+	printk(KERN_ERR "RTnet: cannot allocate rtnet device\n");
+	return NULL;
+    }
+
+    memset(rtdev, 0, alloc_size);
+
+    ret = rtskb_pool_init(&rtdev->dev_pool, dev_pool_size, &rtdev_ops, rtdev);
+    if (ret < dev_pool_size) {
+	printk(KERN_ERR "RTnet: cannot allocate rtnet device pool\n");
+	rtskb_pool_release(&rtdev->dev_pool);
+	kfree(rtdev);
+	return NULL;
+    }
+
+    rt_mutex_init(&rtdev->xmit_mutex);
+    raw_spin_lock_init(&rtdev->rtdev_lock);
+    mutex_init(&rtdev->nrt_lock);
+
+    atomic_set(&rtdev->refcount, 0);
+
+    /* scale global rtskb pool */
+    rtdev->add_rtskbs = rtskb_pool_extend(&global_pool, device_rtskbs);
+
+    if (sizeof_priv)
+	rtdev->priv = (void *)(((long)(rtdev + 1) + 31) & ~31);
+
+    if(!initialized_rtnet_devices_rt_lock) {
+    	raw_spin_lock_init(&rtnet_devices_rt_lock);
+	initialized_rtnet_devices_rt_lock = 1;
+    }
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_free
+ */
+void rtdev_free (struct rtnet_device *rtdev)
+{
+    if (rtdev != NULL) {
+	rtskb_pool_release(&rtdev->dev_pool);
+	rtskb_pool_shrink(&global_pool, rtdev->add_rtskbs);
+	rtdev->stack_event = NULL;
+	rt_mutex_destroy(&rtdev->xmit_mutex);
+	kfree(rtdev);
+    }
+}
+
+
+
+/**
+ * rtalloc_etherdev - Allocates and sets up an ethernet device
+ * @sizeof_priv: size of additional driver-private structure to
+ *               be allocated for this ethernet device
+ * @dev_pool_size: size of the rx pool
+ * @module: module creating the deivce
+ *
+ * Fill in the fields of the device structure with ethernet-generic
+ * values. Basically does everything except registering the device.
+ *
+ * A 32-byte alignment is enforced for the private data area.
+ */
+struct rtnet_device *__rt_alloc_etherdev(unsigned sizeof_priv,
+					unsigned dev_pool_size,
+					struct module *module)
+{
+    struct rtnet_device *rtdev;
+
+    rtdev = rtdev_alloc(sizeof_priv, dev_pool_size);
+    if (!rtdev)
+	return NULL;
+
+    rtdev->hard_header     = rt_eth_header;
+    rtdev->type            = ARPHRD_ETHER;
+    rtdev->hard_header_len = ETH_HLEN;
+    rtdev->mtu             = 1500; /* eth_mtu */
+    rtdev->addr_len        = ETH_ALEN;
+    rtdev->flags           = IFF_BROADCAST; /* TODO: IFF_MULTICAST; */
+    rtdev->get_mtu         = rt_hard_mtu;
+    rtdev->rt_owner	   = module;
+
+    memset(rtdev->broadcast, 0xFF, ETH_ALEN);
+    strcpy(rtdev->name, "rteth%d");
+
+    return rtdev;
+}
+
+/*
+ * @txqs: The number of TX queues this device has.
+ * @rxqs: The number of RX queues this device has.
+*/
+struct rtnet_device *__rt_alloc_etherdev_mqs(unsigned sizeof_priv,
+                                        unsigned dev_pool_size,
+					unsigned int txqs, unsigned int rxqs,
+                                        struct module *module)
+{
+	/* TODO: allow txqs and rxqs bigger than 1. */
+	if((txqs != 1) || (rxqs != 1))
+		printk(KERN_WARNING "%s defaults to one txq and one rxq. "
+				"Requested txqs=%d rxqs=%d\n", __func__, txqs, rxqs);
+	
+	return __rt_alloc_etherdev(sizeof_priv, dev_pool_size, module);
+}
+
+static inline int __rtdev_new_index(void)
+{
+    int i;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++)
+	if (rtnet_devices[i] == NULL)
+	     return i+1;
+
+    return -ENOMEM;
+}
+
+
+
+static int rtskb_map(struct rtnet_device *rtdev, struct rtskb *skb)
+{
+    dma_addr_t addr;
+
+    addr = rtdev->map_rtskb(rtdev, skb);
+
+    if (WARN_ON(addr == RTSKB_UNMAPPED))
+	return -ENOMEM;
+
+    if (skb->buf_dma_addr != RTSKB_UNMAPPED &&
+	addr != skb->buf_dma_addr) {
+	printk(KERN_ERR "RTnet: device %s maps skb differently than others. "
+	       "Different IOMMU domain?\nThis is not supported.\n",
+	       rtdev->name);
+	return -EACCES;
+    }
+
+    skb->buf_dma_addr = addr;
+
+    return 0;
+}
+
+
+
+int rtdev_map_rtskb(struct rtskb *skb)
+{
+    struct rtnet_device *rtdev;
+    int err = 0;
+    int i;
+
+    skb->buf_dma_addr = RTSKB_UNMAPPED;
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtdev = rtnet_devices[i];
+	if (rtdev && rtdev->map_rtskb) {
+	    err = rtskb_map(rtdev, skb);
+	    if (err)
+		break;
+	}
+    }
+
+    if (!err) {
+        if (skb->buf_dma_addr != RTSKB_UNMAPPED)
+	    list_add(&skb->entry, &rtskb_mapped_list);
+        else
+	    list_add(&skb->entry, &rtskb_mapwait_list);
+    }
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+
+    return err;
+}
+
+
+
+static int rtdev_map_all_rtskbs(struct rtnet_device *rtdev)
+{
+    struct rtskb *skb, *n;
+    int err = 0;
+
+    if (!rtdev->map_rtskb)
+	return 0;
+
+    list_for_each_entry(skb, &rtskb_mapped_list, entry) {
+	err = rtskb_map(rtdev, skb);
+	if (err)
+	   break;
+    }
+
+    list_for_each_entry_safe(skb, n, &rtskb_mapwait_list, entry) {
+	err = rtskb_map(rtdev, skb);
+	if (err)
+	   break;
+	list_del(&skb->entry);
+	list_add(&skb->entry, &rtskb_mapped_list);
+    }
+
+    return err;
+}
+
+
+
+void rtdev_unmap_rtskb(struct rtskb *skb)
+{
+    struct rtnet_device *rtdev;
+    int i;
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+
+    list_del(&skb->entry);
+
+    if (skb->buf_dma_addr != RTSKB_UNMAPPED) {
+	for (i = 0; i < MAX_RT_DEVICES; i++) {
+	    rtdev = rtnet_devices[i];
+	    if (rtdev && rtdev->unmap_rtskb) {
+		rtdev->unmap_rtskb(rtdev, skb);
+	    }
+	}
+    }
+
+    skb->buf_dma_addr = RTSKB_UNMAPPED;
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+
+
+static void rtdev_unmap_all_rtskbs(struct rtnet_device *rtdev)
+{
+    struct rtskb *skb;
+
+    if (!rtdev->unmap_rtskb)
+	return;
+
+    list_for_each_entry(skb, &rtskb_mapped_list, entry) {
+	rtdev->unmap_rtskb(rtdev, skb);
+    }
+}
+
+
+
+/***
+ * rt_register_rtnetdev: register a new rtnet_device (linux-like)
+ * @rtdev:               the device
+ */
+int rt_register_rtnetdev(struct rtnet_device *rtdev)
+{
+    struct list_head        *entry;
+    struct rtdev_event_hook *hook;
+    unsigned long          context;
+    int                     ifindex;
+    int                     err;
+
+#if 0
+    /* requires at least driver layer version 2.0 */
+    if (rtdev->vers < RTDEV_VERS_2_0)
+	return -EINVAL;
+#endif
+
+    if (rtdev->features & NETIF_F_LLTX)
+	rtdev->start_xmit = rtdev->hard_start_xmit;
+    else
+	rtdev->start_xmit = rtdev_locked_xmit;
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+
+    ifindex = __rtdev_new_index();
+    if (ifindex < 0) {
+	mutex_unlock(&rtnet_devices_nrt_lock);
+	return ifindex;
+    }
+    rtdev->ifindex = ifindex;
+
+    if (strchr(rtdev->name,'%') != NULL)
+	rtdev_alloc_name(rtdev, rtdev->name);
+
+    if (__rtdev_get_by_name(rtdev->name) != NULL) {
+	mutex_unlock(&rtnet_devices_nrt_lock);
+	return -EEXIST;
+    }
+
+    err = rtdev_map_all_rtskbs(rtdev);
+    if (err) {
+	mutex_unlock(&rtnet_devices_nrt_lock);
+	return err;
+    }
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    if (rtdev->flags & IFF_LOOPBACK) {
+	/* allow only one loopback device */
+	if (loopback_device) {
+	    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+	    mutex_unlock(&rtnet_devices_nrt_lock);
+	    return -EEXIST;
+	}
+	loopback_device = rtdev;
+    }
+    rtnet_devices[rtdev->ifindex-1] = rtdev;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    list_for_each(entry, &event_hook_list) {
+	hook = list_entry(entry, struct rtdev_event_hook, entry);
+	if (hook->register_device)
+	    hook->register_device(rtdev);
+    }
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+
+    /* Default state at registration is that the device is present. */
+    set_bit(__RTNET_LINK_STATE_PRESENT, &rtdev->link_state);
+
+    printk(KERN_INFO "RTnet: registered %s\n", rtdev->name);
+
+    return 0;
+}
+
+
+
+/***
+ * rt_unregister_rtnetdev: unregister a rtnet_device
+ * @rtdev:                 the device
+ */
+int rt_unregister_rtnetdev(struct rtnet_device *rtdev)
+{
+    struct list_head        *entry;
+    struct rtdev_event_hook *hook;
+    unsigned long          context;
+
+
+    if(rtdev->ifindex == 0) {
+	printk(KERN_ERR "RTnet: device %s/%p was not registered\n", rtdev->name, rtdev);
+	return -ENODEV;
+    }
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    RTNET_ASSERT(atomic_read(&rtdev->refcount) == 0, BUG(););
+    rtnet_devices[rtdev->ifindex-1] = NULL;
+    if (rtdev->flags & IFF_LOOPBACK)
+	loopback_device = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    list_for_each(entry, &event_hook_list) {
+	hook = list_entry(entry, struct rtdev_event_hook, entry);
+	if (hook->unregister_device)
+	    hook->unregister_device(rtdev);
+    }
+
+    rtdev_unmap_all_rtskbs(rtdev);
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+
+    clear_bit(__RTNET_LINK_STATE_PRESENT, &rtdev->link_state);
+
+    RTNET_ASSERT(atomic_read(&rtdev->refcount) == 0,
+	   printk(KERN_WARNING "RTnet: rtdev reference counter < 0!\n"););
+
+    printk(KERN_INFO "RTnet: unregistered %s\n", rtdev->name);
+
+    return 0;
+}
+
+
+
+void rtdev_add_event_hook(struct rtdev_event_hook *hook)
+{
+    mutex_lock(&rtnet_devices_nrt_lock);
+    list_add(&hook->entry, &event_hook_list);
+    mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+
+
+void rtdev_del_event_hook(struct rtdev_event_hook *hook)
+{
+    mutex_lock(&rtnet_devices_nrt_lock);
+    list_del(&hook->entry);
+    mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+
+
+/***
+ *  rtdev_open
+ *
+ *  Prepare an interface for use.
+ */
+int rtdev_open(struct rtnet_device *rtdev)
+{
+    int ret = 0;
+
+
+    if (rtdev->flags & IFF_UP)              /* Is it already up?                */
+	return 0;
+
+    if (!rtdev_reference(rtdev))
+	return -EIDRM;
+
+    if (rtdev->open)                        /* Call device private open method  */
+	ret = rtdev->open(rtdev);
+
+    if ( !ret )  {
+	rtdev->flags |= IFF_UP;
+	set_bit(__RTNET_LINK_STATE_START, &rtdev->link_state);
+    } else
+	rtdev_dereference(rtdev);
+
+    return ret;
+}
+
+
+
+/***
+ *  rtdev_close
+ */
+int rtdev_close(struct rtnet_device *rtdev)
+{
+    int ret = 0;
+
+
+    if ( !(rtdev->flags & IFF_UP) )
+	return 0;
+
+    if (rtdev->stop)
+	ret = rtdev->stop(rtdev);
+
+    rtdev->flags &= ~(IFF_UP|IFF_RUNNING);
+    clear_bit(__RTNET_LINK_STATE_START, &rtdev->link_state);
+
+    if (ret == 0)
+	rtdev_dereference(rtdev);
+
+    return ret;
+}
+
+
+
+static int rtdev_locked_xmit(struct rtskb *skb, struct rtnet_device *rtdev)
+{
+    int ret;
+    
+    rt_mutex_lock(&rtdev->xmit_mutex);
+    if(rtnetif_queue_stopped(rtdev))
+        ret = -EBUSY;
+    else 
+        ret = rtdev->hard_start_xmit(skb, rtdev);
+    rt_mutex_unlock(&rtdev->xmit_mutex);
+
+    return ret;
+}
+
+/***
+ *  rtdev_xmit - send real-time packet
+ */
+int rtdev_xmit(struct rtskb *rtskb)
+{
+    struct rtnet_device *rtdev;
+    int                 err;
+
+    RTNET_ASSERT(rtskb != NULL, return -EINVAL;);
+
+    rtdev = rtskb->rtdev;
+
+    if (!rtnetif_carrier_ok(rtdev)) {
+	err = -EAGAIN;
+	kfree_rtskb(rtskb);
+	return err;
+    }
+
+    if (rtskb_acquire(rtskb, &rtdev->dev_pool) != 0) {
+	err = -ENOBUFS;
+	kfree_rtskb(rtskb);
+	return err;
+    }
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+
+    err = rtdev->start_xmit(rtskb, rtdev);
+    if(err == NETDEV_TX_BUSY)
+        err = -EBUSY;
+
+    if (err) {
+	/* on error we must free the rtskb here */
+	kfree_rtskb(rtskb);
+	trace_printk("err=%d\n", err);
+    }
+    return err;
+}
+
+
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+/***
+ 
+ *      rtdev_xmit_proxy - send rtproxy packet
+ */
+int rtdev_xmit_proxy(struct rtskb *rtskb)
+{
+    struct rtnet_device *rtdev;
+    int                 err;
+
+
+    RTNET_ASSERT(rtskb != NULL, return -EINVAL;);
+
+    rtdev = rtskb->rtdev;
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+
+    /* TODO: make these lines race-condition-safe */
+    if (rtdev->mac_disc) {
+	RTNET_ASSERT(rtdev->mac_disc->nrt_packet_tx != NULL, return -EINVAL;);
+
+	err = rtdev->mac_disc->nrt_packet_tx(rtskb);
+    } else {
+	err = rtdev->start_xmit(rtskb, rtdev);
+        if(err == NETDEV_TX_BUSY)
+            err = -EBUSY;
+    }
+
+    if (err) {
+        /* on error we must free the rtskb here */
+        kfree_rtskb(rtskb);
+        trace_printk("err=%d\n", err);
+    }
+
+    return err;
+}
+#endif /* CONFIG_RTNET_ADDON_PROXY */
+
+
+
+unsigned int rt_hard_mtu(struct rtnet_device *rtdev, unsigned int priority)
+{
+    return rtdev->mtu;
+}
+
+
+EXPORT_SYMBOL_GPL(__rt_alloc_etherdev);
+EXPORT_SYMBOL_GPL(rtdev_free);
+
+EXPORT_SYMBOL_GPL(rtdev_alloc_name);
+
+EXPORT_SYMBOL_GPL(rt_register_rtnetdev);
+EXPORT_SYMBOL_GPL(rt_unregister_rtnetdev);
+
+EXPORT_SYMBOL_GPL(rtdev_add_event_hook);
+EXPORT_SYMBOL_GPL(rtdev_del_event_hook);
+
+EXPORT_SYMBOL_GPL(rtdev_get_by_name);
+EXPORT_SYMBOL_GPL(rtdev_get_by_index);
+EXPORT_SYMBOL_GPL(rtdev_get_by_hwaddr);
+EXPORT_SYMBOL_GPL(rtdev_get_loopback);
+
+EXPORT_SYMBOL_GPL(rtdev_xmit);
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+EXPORT_SYMBOL_GPL(rtdev_xmit_proxy);
+#endif
+
+EXPORT_SYMBOL_GPL(rt_hard_mtu);
diff -Naur a/net/rtnet/stack/rtdev_mgr.c b/net/rtnet/stack/rtdev_mgr.c
--- a/net/rtnet/stack/rtdev_mgr.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtdev_mgr.c	2021-07-14 15:39:13.334124801 +0300
@@ -0,0 +1,132 @@
+/***
+ *
+ *  stack/rtdev_mgr.c - device error manager
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/netdevice.h>
+
+#include <rtdev.h>
+#include <rtdm_net.h>
+#include <rtnet_internal.h>
+
+/***
+ *  rtnetif_err_rx: will be called from the  driver
+ *
+ *
+ *  @rtdev - the network-device
+ */
+void rtnetif_err_rx(struct rtnet_device *rtdev)
+{
+}
+
+/***
+ *  rtnetif_err_tx: will be called from the  driver
+ *
+ *
+ *  @rtdev - the network-device
+ */
+void rtnetif_err_tx(struct rtnet_device *rtdev)
+{
+}
+
+/***
+ *  do_rtdev_task
+ */
+/*static void do_rtdev_task(int mgr_id)
+{
+    struct rtnet_msg msg;
+    struct rtnet_mgr *mgr = (struct rtnet_mgr *)mgr_id;
+
+    while (1) {
+        rt_mbx_receive(&(mgr->mbx), &msg, sizeof(struct rtnet_msg));
+        if (msg.rtdev) {
+            trace_printk("RTnet: error on rtdev %s\n", msg.rtdev->name);
+        }
+    }
+}*/
+
+/***
+ *  rt_rtdev_connect
+ */
+void rt_rtdev_connect (struct rtnet_device *rtdev, struct rtnet_mgr *mgr)
+{
+/*    rtdev->rtdev_mbx=&(mgr->mbx);*/
+}
+
+/***
+ *  rt_rtdev_disconnect
+ */
+void rt_rtdev_disconnect (struct rtnet_device *rtdev)
+{
+/*    rtdev->rtdev_mbx=NULL;*/
+}
+
+/***
+ *  rt_rtdev_mgr_start
+ */
+int rt_rtdev_mgr_start (struct rtnet_mgr *mgr)
+{
+    return /*(rt_task_resume(&(mgr->task)))*/ 0;
+}
+
+/***
+ *  rt_rtdev_mgr_stop
+ */
+int rt_rtdev_mgr_stop (struct rtnet_mgr *mgr)
+{
+    return /*(rt_task_suspend(&(mgr->task)))*/ 0;
+}
+
+/***
+ *  rt_rtdev_mgr_init
+ */
+int rt_rtdev_mgr_init (struct rtnet_mgr *mgr)
+{
+    int ret = 0;
+
+/*    if ( (ret=rt_mbx_init (&(mgr->mbx), sizeof(struct rtnet_msg))) )
+        return ret;
+    if ( (ret=rt_task_init(&(mgr->task), &do_rtdev_task, (int)mgr, 4096, RTNET_RTDEV_PRIORITY, 0, 0)) )
+        return ret;
+    if ( (ret=rt_task_resume(&(mgr->task))) )
+        return ret;*/
+
+    return (ret);
+}
+
+/***
+ *  rt_rtdev_mgr_delete
+ */
+void rt_rtdev_mgr_delete (struct rtnet_mgr *mgr)
+{
+/*    rt_task_delete(&(mgr->task));
+    rt_mbx_delete(&(mgr->mbx));*/
+}
+
+
+EXPORT_SYMBOL_GPL(rtnetif_err_rx);
+EXPORT_SYMBOL_GPL(rtnetif_err_tx);
+
+EXPORT_SYMBOL_GPL(rt_rtdev_connect);
+EXPORT_SYMBOL_GPL(rt_rtdev_disconnect);
diff -Naur a/net/rtnet/stack/rtmac/Kconfig b/net/rtnet/stack/rtmac/Kconfig
--- a/net/rtnet/stack/rtmac/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/Kconfig	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,16 @@
+menuconfig RTNET_RTMAC
+    depends on RTNET
+    tristate "RTmac Layer"
+    default y
+    help
+    The Real-Time Media Access Control layer allows to extend the RTnet
+    stack with software-based access control mechanisms (also called
+    disciplines) for nondeterministic transport media. Disciplines can be
+    attached and detached per real-time device. RTmac also provides a
+    framework for tunnelling non-time-critical packets through real-time
+    networks by installing virtual NICs (VNIC) in the Linux domain.
+
+    See Documentation/README.rtmac for further information.
+
+source "net/rtnet/stack/rtmac/tdma/Kconfig"
+source "net/rtnet/stack/rtmac/nomac/Kconfig"
diff -Naur a/net/rtnet/stack/rtmac/Makefile b/net/rtnet/stack/rtmac/Makefile
--- a/net/rtnet/stack/rtmac/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/Makefile	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,15 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_NOMAC) += nomac/
+
+obj-$(CONFIG_RTNET_TDMA) += tdma/
+
+obj-$(CONFIG_RTNET_RTMAC) += rtmac.o
+
+rtmac-y := \
+	rtmac_disc.o \
+	rtmac_module.o \
+	rtmac_proc.o \
+	rtmac_proto.o \
+	rtmac_syms.o \
+	rtmac_vnic.o
diff -Naur a/net/rtnet/stack/rtmac/nomac/Kconfig b/net/rtnet/stack/rtmac/nomac/Kconfig
--- a/net/rtnet/stack/rtmac/nomac/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/nomac/Kconfig	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,9 @@
+config RTNET_NOMAC
+    tristate "NoMAC discipline for RTmac"
+    depends on RTNET_RTMAC
+    default n
+    help
+    This no-operation RTmac discipline is intended to act as a template
+    for new implementations. However, it can be compiled and used (see
+    nomaccfg management tool), but don't expect any improved determinism
+    of your network. ;)
diff -Naur a/net/rtnet/stack/rtmac/nomac/Makefile b/net/rtnet/stack/rtmac/nomac/Makefile
--- a/net/rtnet/stack/rtmac/nomac/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/nomac/Makefile	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,9 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_NOMAC) += nomac.o
+
+nomac-y := \
+	nomac_dev.o \
+	nomac_ioctl.o \
+	nomac_module.o \
+	nomac_proto.o
diff -Naur a/net/rtnet/stack/rtmac/nomac/nomac_dev.c b/net/rtnet/stack/rtmac/nomac/nomac_dev.c
--- a/net/rtnet/stack/rtmac/nomac/nomac_dev.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/nomac/nomac_dev.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,87 @@
+/***
+ *
+ *  rtmac/nomac/nomac_dev.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/list.h>
+
+#include <rtdev.h>
+#include <rtmac.h>
+#include <rtmac/nomac/nomac.h>
+
+
+static int nomac_dev_openclose(void)
+{
+    return 0;
+}
+
+
+
+static int nomac_dev_ioctl(struct rtdm_fd *fd, unsigned int request, void *arg)
+{
+    struct nomac_priv   *nomac;
+
+
+    nomac = container_of(rtdm_fd_to_context(fd)->device,
+			 struct nomac_priv, api_device);
+
+    switch (request) {
+	case RTMAC_RTIOC_TIMEOFFSET:
+
+	case RTMAC_RTIOC_WAITONCYCLE:
+
+	default:
+	    return -ENOTTY;
+    }
+}
+
+static struct rtdm_driver nomac_driver = {
+    .profile_info = RTDM_PROFILE_INFO(nomac,
+				    RTDM_CLASS_RTMAC,
+				    RTDM_SUBCLASS_UNMANAGED,
+				    RTNET_RTDM_VER),
+    .device_flags = RTDM_NAMED_DEVICE,
+    .device_count = 1,
+    .context_size = 0,
+    .ops = {
+	.open =         (typeof(nomac_driver.ops.open))nomac_dev_openclose,
+	.ioctl_rt =     nomac_dev_ioctl,
+	.ioctl_nrt =    nomac_dev_ioctl,
+	.close =        (typeof(nomac_driver.ops.close))nomac_dev_openclose,
+    }
+};
+
+int nomac_dev_init(struct rtnet_device *rtdev, struct nomac_priv *nomac)
+{
+    char    *pos;
+
+    strcpy(nomac->device_name, "NOMAC");
+    for (pos = rtdev->name + strlen(rtdev->name) - 1;
+	(pos >= rtdev->name) && ((*pos) >= '0') && (*pos <= '9'); pos--);
+    strncat(nomac->device_name+5, pos+1, IFNAMSIZ-5);
+
+    nomac->api_driver           = nomac_driver;
+    nomac->api_device.driver    = &nomac->api_driver;
+    nomac->api_device.label     = nomac->device_name;
+
+    return rtdm_dev_register(&nomac->api_device);
+}
diff -Naur a/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c b/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c
--- a/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,107 @@
+/***
+ *
+ *  rtmac/nomac/nomac_ioctl.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/uaccess.h>
+
+#include <nomac_chrdev.h>
+#include <rtmac/nomac/nomac.h>
+
+
+static int nomac_ioctl_attach(struct rtnet_device *rtdev)
+{
+    struct nomac_priv   *nomac;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL) {
+        ret = rtmac_disc_attach(rtdev, &nomac_disc);
+        if (ret < 0)
+            return ret;
+    }
+
+    nomac = (struct nomac_priv *)rtdev->mac_priv->disc_priv;
+    if (nomac->magic != NOMAC_MAGIC)
+        return -ENOTTY;
+
+    /* ... */
+
+    return 0;
+}
+
+
+
+static int nomac_ioctl_detach(struct rtnet_device *rtdev)
+{
+    struct nomac_priv   *nomac;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    nomac = (struct nomac_priv *)rtdev->mac_priv->disc_priv;
+    if (nomac->magic != NOMAC_MAGIC)
+        return -ENOTTY;
+
+    ret = rtmac_disc_detach(rtdev);
+
+    /* ... */
+
+    return ret;
+}
+
+
+
+int nomac_ioctl(struct rtnet_device *rtdev, unsigned int request,
+                unsigned long arg)
+{
+    struct nomac_config cfg;
+    int                 ret;
+
+
+    ret = copy_from_user(&cfg, (void *)arg, sizeof(cfg));
+    if (ret != 0)
+        return -EFAULT;
+
+    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+        return -ERESTARTSYS;
+
+    switch (request) {
+        case NOMAC_IOC_ATTACH:
+            ret = nomac_ioctl_attach(rtdev);
+            break;
+
+        case NOMAC_IOC_DETACH:
+            ret = nomac_ioctl_detach(rtdev);
+            break;
+
+        default:
+            ret = -ENOTTY;
+    }
+
+    mutex_unlock(&rtdev->nrt_lock);
+
+    return ret;
+}
diff -Naur a/net/rtnet/stack/rtmac/nomac/nomac_module.c b/net/rtnet/stack/rtmac/nomac/nomac_module.c
--- a/net/rtnet/stack/rtmac/nomac/nomac_module.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/nomac/nomac_module.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,182 @@
+/***
+ *
+ *  rtmac/nomac/nomac_module.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+
+#include <rtdm/driver.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/nomac/nomac.h>
+#include <rtmac/nomac/nomac_dev.h>
+#include <rtmac/nomac/nomac_ioctl.h>
+#include <rtmac/nomac/nomac_proto.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+LIST_HEAD(nomac_devices);
+DEFINE_RT_MUTEX(nomac_nrt_lock);
+
+
+int nomac_proc_read(struct xnvfile_regular_iterator *it, void *data)
+{
+    struct nomac_priv *entry;
+
+    mutex_lock(&nomac_nrt_lock);
+
+    xnvfile_printf(it, "Interface       API Device      State\n");
+
+    list_for_each_entry(entry, &nomac_devices, list_entry)
+	xnvfile_printf(it, "%-15s %-15s Attached\n", entry->rtdev->name,
+		    entry->api_device.name);
+
+    mutex_unlock(&nomac_nrt_lock);
+
+    return 0;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+int nomac_attach(struct rtnet_device *rtdev, void *priv)
+{
+    struct nomac_priv   *nomac = (struct nomac_priv *)priv;
+    int                 ret;
+
+
+    nomac->magic = NOMAC_MAGIC;
+    nomac->rtdev = rtdev;
+
+    /* ... */
+
+    ret = nomac_dev_init(rtdev, nomac);
+    if (ret < 0)
+	return ret;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    mutex_lock(&nomac_nrt_lock);
+    list_add(&nomac->list_entry, &nomac_devices);
+    mutex_unlock(&nomac_nrt_lock);
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    return 0;
+}
+
+
+
+int nomac_detach(struct rtnet_device *rtdev, void *priv)
+{
+    struct nomac_priv   *nomac = (struct nomac_priv *)priv;
+
+
+    nomac_dev_release(nomac);
+
+    /* ... */
+#ifdef CONFIG_XENO_OPT_VFILE
+    mutex_lock(&nomac_nrt_lock);
+    list_del(&nomac->list_entry);
+    mutex_unlock(&nomac_nrt_lock);
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    return 0;
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct rtmac_proc_entry nomac_proc_entries[] = {
+    { name: "nomac", handler: nomac_proc_read },
+};
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+struct rtmac_disc nomac_disc = {
+    name:           "NoMAC",
+    priv_size:      sizeof(struct nomac_priv),
+    disc_type:      __constant_htons(RTMAC_TYPE_NOMAC),
+
+    packet_rx:      nomac_packet_rx,
+    rt_packet_tx:   nomac_rt_packet_tx,
+    nrt_packet_tx:  nomac_nrt_packet_tx,
+
+    get_mtu:        NULL,
+
+    vnic_xmit:      RTMAC_DEFAULT_VNIC,
+
+    attach:         nomac_attach,
+    detach:         nomac_detach,
+
+    ioctls:         {
+	service_name:   "RTmac/NoMAC",
+	ioctl_type:     RTNET_IOC_TYPE_RTMAC_NOMAC,
+	handler:        nomac_ioctl
+    },
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    proc_entries:   nomac_proc_entries,
+    nr_proc_entries: ARRAY_SIZE(nomac_proc_entries),
+#endif /* CONFIG_XENO_OPT_VFILE */
+};
+
+
+
+int __init nomac_init(void)
+{
+    int ret;
+
+
+    printk(KERN_INFO "RTmac/NoMAC: init void media access control mechanism\n");
+
+    ret = nomac_proto_init();
+    if (ret < 0)
+	return ret;
+
+    ret = rtmac_disc_register(&nomac_disc);
+    if (ret < 0) {
+	nomac_proto_cleanup();
+	return ret;
+    }
+
+    return 0;
+}
+
+
+
+void nomac_release(void)
+{
+    rtmac_disc_deregister(&nomac_disc);
+    nomac_proto_cleanup();
+
+    printk(KERN_INFO "RTmac/NoMAC: unloaded\n");
+}
+
+
+
+module_init(nomac_init);
+module_exit(nomac_release);
+
+MODULE_AUTHOR("Jan Kiszka");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/stack/rtmac/nomac/nomac_proto.c b/net/rtnet/stack/rtmac/nomac/nomac_proto.c
--- a/net/rtnet/stack/rtmac/nomac/nomac_proto.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/nomac/nomac_proto.c	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,143 @@
+/***
+ *
+ *  rtmac/nomac/nomac_proto.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+
+#include <rtdev.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/nomac/nomac.h>
+
+
+static struct rtskb_queue   nrt_rtskb_queue;
+static rtdm_task_t          wrapper_task;
+static rtdm_event_t         wakeup_sem;
+
+
+int nomac_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    /* unused here, just to demonstrate access to the discipline state
+    struct nomac_priv   *nomac =
+        (struct nomac_priv *)rtdev->mac_priv->disc_priv; */
+    int                 ret;
+
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    /* no MAC: we simply transmit the packet under xmit_lock */
+    rtdm_mutex_lock(&rtdev->xmit_mutex);
+    ret = rtmac_xmit(rtskb);
+    rtdm_mutex_unlock(&rtdev->xmit_mutex);
+
+    return ret;
+}
+
+
+
+int nomac_nrt_packet_tx(struct rtskb *rtskb)
+{
+    struct rtnet_device *rtdev = rtskb->rtdev;
+    /* unused here, just to demonstrate access to the discipline state
+    struct nomac_priv   *nomac =
+        (struct nomac_priv *)rtdev->mac_priv->disc_priv; */
+    int                 ret;
+
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    /* note: this routine may be called both in rt and non-rt context
+     *       => detect and wrap the context if necessary */
+    if (!rtdm_in_rt_context()) {
+        rtskb_queue_tail(&nrt_rtskb_queue, rtskb);
+        rtdm_event_signal(&wakeup_sem);
+        return 0;
+    } else {
+        /* no MAC: we simply transmit the packet under xmit_lock */
+        rtdm_mutex_lock(&rtdev->xmit_mutex);
+        ret = rtmac_xmit(rtskb);
+        rtdm_mutex_unlock(&rtdev->xmit_mutex);
+
+        return ret;
+    }
+}
+
+
+
+void nrt_xmit_task(void *arg)
+{
+    struct rtskb        *rtskb;
+    struct rtnet_device *rtdev;
+
+
+    while (!rtdm_task_should_stop()) {
+	if (rtdm_event_wait(&wakeup_sem) < 0)
+	    break;
+
+        while ((rtskb = rtskb_dequeue(&nrt_rtskb_queue))) {
+            rtdev = rtskb->rtdev;
+
+            /* no MAC: we simply transmit the packet under xmit_lock */
+            rtdm_mutex_lock(&rtdev->xmit_mutex);
+            rtmac_xmit(rtskb);
+            rtdm_mutex_unlock(&rtdev->xmit_mutex);
+        }
+    }
+}
+
+
+
+int nomac_packet_rx(struct rtskb *rtskb)
+{
+    /* actually, NoMAC doesn't expect any control packet */
+    kfree_rtskb(rtskb);
+
+    return 0;
+}
+
+
+
+int __init nomac_proto_init(void)
+{
+    int ret;
+
+
+    rtskb_queue_init(&nrt_rtskb_queue);
+    rtdm_event_init(&wakeup_sem, 0);
+
+    ret = rtdm_task_init(&wrapper_task, "rtnet-nomac", nrt_xmit_task, 0,
+                         RTDM_TASK_LOWEST_PRIORITY, 0);
+    if (ret < 0) {
+        rtdm_event_destroy(&wakeup_sem);
+        return ret;
+    }
+
+    return 0;
+}
+
+
+
+void nomac_proto_cleanup(void)
+{
+    rtdm_event_destroy(&wakeup_sem);
+    rtdm_task_destroy(&wrapper_task);
+}
diff -Naur a/net/rtnet/stack/rtmac/rtmac_disc.c b/net/rtnet/stack/rtmac/rtmac_disc.c
--- a/net/rtnet/stack/rtmac/rtmac_disc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/rtmac_disc.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,290 @@
+/***
+ *
+ *  rtmac_disc.c
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/netdevice.h>
+#include <linux/mutex.h>
+
+#include <rtnet_internal.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proc.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+
+static DEFINE_RT_MUTEX(disc_list_lock);
+static LIST_HEAD(disc_list);
+
+
+
+/***
+ *  rtmac_disc_attach
+ *
+ *  @rtdev       attaches a discipline to a device
+ *  @disc        discipline to attach
+ *
+ *  0            success
+ *  -EBUSY       other discipline active
+ *  -ENOMEM      could not allocate memory
+ *
+ *  Note: must be called with rtdev->nrt_lock acquired
+ */
+int rtmac_disc_attach(struct rtnet_device *rtdev, struct rtmac_disc *disc)
+{
+    int                 ret;
+    struct rtmac_priv   *priv;
+
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->attach != NULL, return -EINVAL;);
+
+    if (rtdev->mac_disc) {
+	printk(KERN_ERR "RTmac: another discipline for rtdev '%s' active.\n", rtdev->name);
+	return -EBUSY;
+    }
+
+    if (rtdev->flags & IFF_LOOPBACK)
+	return -EINVAL;
+
+    if (!try_module_get(disc->owner))
+	return -EIDRM;
+
+    if (!rtdev_reference(rtdev)) {
+	ret = -EIDRM;
+	goto err_module_put;
+    }
+
+    /* alloc memory */
+    priv = kmalloc(sizeof(struct rtmac_priv) + disc->priv_size, GFP_KERNEL);
+    if (!priv) {
+	printk(KERN_ERR "RTmac: kmalloc returned NULL for rtmac!\n");
+	return -ENOMEM;
+    }
+    priv->orig_start_xmit = rtdev->start_xmit;
+
+    /* call attach function of discipline */
+    ret = disc->attach(rtdev, priv->disc_priv);
+    if (ret < 0)
+	goto err_kfree_priv;
+
+    /* now attach RTmac to device */
+    rtdev->mac_disc = disc;
+    rtdev->mac_priv = priv;
+    rtdev->start_xmit = disc->rt_packet_tx;
+    if (disc->get_mtu)
+	rtdev->get_mtu = disc->get_mtu;
+    rtdev->mac_detach = rtmac_disc_detach;
+
+    /* create the VNIC */
+    ret = rtmac_vnic_add(rtdev, disc->vnic_xmit);
+    if (ret < 0) {
+	printk(KERN_WARNING "RTmac: Warning, VNIC creation failed for rtdev %s.\n", rtdev->name);
+	goto err_disc_detach;
+    }
+
+    return 0;
+
+  err_disc_detach:
+    disc->detach(rtdev, priv->disc_priv);
+  err_kfree_priv:
+    kfree(priv);
+    rtdev_dereference(rtdev);
+  err_module_put:
+    module_put(disc->owner);
+    return ret;
+}
+
+
+
+/***
+ *  rtmac_disc_detach
+ *
+ *  @rtdev       detaches a discipline from a device
+ *
+ *  0            success
+ *  -1           discipline has no detach function
+ *  -EINVAL      called with rtdev=NULL
+ *  -ENODEV      no discipline active on dev
+ *
+ *  Note: must be called with rtdev->nrt_lock acquired
+ */
+int rtmac_disc_detach(struct rtnet_device *rtdev)
+{
+    int                 ret;
+    struct rtmac_disc   *disc;
+    struct rtmac_priv   *priv;
+
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+
+    disc = rtdev->mac_disc;
+    if (!disc)
+	return -ENODEV;
+
+    RTNET_ASSERT(disc->detach != NULL, return -EINVAL;);
+
+    priv = rtdev->mac_priv;
+    RTNET_ASSERT(priv != NULL, return -EINVAL;);
+
+    ret = rtmac_vnic_unregister(rtdev);
+    if (ret < 0)
+	return ret;
+
+    /* call release function of discipline */
+    ret = disc->detach(rtdev, priv->disc_priv);
+    if (ret < 0)
+	return ret;
+
+    rtmac_vnic_cleanup(rtdev);
+
+    /* restore start_xmit and get_mtu */
+    rtdev->start_xmit = priv->orig_start_xmit;
+    rtdev->get_mtu    = rt_hard_mtu;
+
+    /* remove pointers from rtdev */
+    rtdev->mac_disc   = NULL;
+    rtdev->mac_priv   = NULL;
+    rtdev->mac_detach = NULL;
+
+    rtdev_dereference(rtdev);
+
+    kfree(priv);
+
+    module_put(disc->owner);
+
+    return 0;
+}
+
+
+
+static struct rtmac_disc *rtmac_get_disc_by_name(const char *name)
+{
+    struct list_head    *disc;
+
+
+    mutex_lock(&disc_list_lock);
+
+    list_for_each(disc, &disc_list) {
+	if (strcmp(((struct rtmac_disc *)disc)->name, name) == 0) {
+	    mutex_unlock(&disc_list_lock);
+	    return (struct rtmac_disc *)disc;
+	}
+    }
+
+    mutex_unlock(&disc_list_lock);
+
+    return NULL;
+}
+
+
+
+int __rtmac_disc_register(struct rtmac_disc *disc, struct module *module)
+{
+    int ret;
+
+
+    RTNET_ASSERT(disc != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->name != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->rt_packet_tx != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->nrt_packet_tx != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->attach != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->detach != NULL, return -EINVAL;);
+
+    disc->owner = module;
+
+    if (rtmac_get_disc_by_name(disc->name) != NULL)
+    {
+	printk(KERN_ERR "RTmac: discipline '%s' already registered!\n", disc->name);
+	return -EBUSY;
+    }
+
+    ret = rtnet_register_ioctls(&disc->ioctls);
+    if (ret < 0)
+	return ret;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    ret = rtmac_disc_proc_register(disc);
+    if (ret < 0) {
+	rtnet_unregister_ioctls(&disc->ioctls);
+	return ret;
+    }
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    mutex_lock(&disc_list_lock);
+
+    list_add(&disc->list, &disc_list);
+
+    mutex_unlock(&disc_list_lock);
+
+    return 0;
+}
+
+
+
+void rtmac_disc_deregister(struct rtmac_disc *disc)
+{
+    RTNET_ASSERT(disc != NULL, return;);
+
+    mutex_lock(&disc_list_lock);
+
+    list_del(&disc->list);
+
+    mutex_unlock(&disc_list_lock);
+
+    rtnet_unregister_ioctls(&disc->ioctls);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtmac_disc_proc_unregister(disc);
+#endif /* CONFIG_XENO_OPT_VFILE */
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_disciplines_show(struct xnvfile_regular_iterator *it, void *d)
+{
+    struct rtmac_disc    *disc;
+    int err;
+
+    err = mutex_lock_interruptible(&disc_list_lock);
+    if (err < 0)
+	return err;
+
+    xnvfile_printf(it, "Name\t\tID\n");
+
+    list_for_each_entry(disc, &disc_list, list)
+	xnvfile_printf(it, "%-15s %04X\n",disc->name, ntohs(disc->disc_type));
+
+    mutex_unlock(&disc_list_lock);
+
+    return 0;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
diff -Naur a/net/rtnet/stack/rtmac/rtmac_module.c b/net/rtnet/stack/rtmac/rtmac_module.c
--- a/net/rtnet/stack/rtmac/rtmac_module.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/rtmac_module.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,86 @@
+/* rtmac_module.c
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *               2003 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+
+#include <rtdm/driver.h>
+
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proc.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+int __init rtmac_init(void)
+{
+    int ret = 0;
+
+
+    printk(KERN_INFO "RTmac: init realtime media access control\n");
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    ret = rtmac_proc_register();
+    if (ret < 0)
+        return ret;
+#endif
+
+    ret = rtmac_vnic_module_init();
+    if (ret < 0)
+        goto error1;
+
+    ret = rtmac_proto_init();
+    if (ret < 0)
+        goto error2;
+
+    return 0;
+
+error2:
+    rtmac_vnic_module_cleanup();
+
+error1:
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtmac_proc_release();
+#endif
+    return ret;
+}
+
+
+
+void rtmac_release(void)
+{
+    rtmac_proto_release();
+    rtmac_vnic_module_cleanup();
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtmac_proc_release();
+#endif
+
+    printk(KERN_INFO "RTmac: unloaded\n");
+}
+
+
+
+module_init(rtmac_init);
+module_exit(rtmac_release);
+
+MODULE_AUTHOR("Marc Kleine-Budde, Jan Kiszka");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/stack/rtmac/rtmac_proc.c b/net/rtnet/stack/rtmac/rtmac_proc.c
--- a/net/rtnet/stack/rtmac/rtmac_proc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/rtmac_proc.c	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,137 @@
+/***
+ *
+ *  rtmac_proc.c
+ *
+ *  rtmac - real-time networking medium access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+
+#include <rtnet_internal.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/rtmac_proc.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct xnvfile_directory rtmac_proc_root;
+
+static struct xnvfile_regular_ops rtnet_rtmac_disciplines_vfile_ops = {
+    .show = rtnet_rtmac_disciplines_show,
+};
+
+static struct xnvfile_regular rtnet_rtmac_disciplines_vfile = {
+    .ops = &rtnet_rtmac_disciplines_vfile_ops,
+};
+
+static struct xnvfile_regular_ops rtnet_rtmac_vnics_vfile_ops = {
+    .show = rtnet_rtmac_vnics_show,
+};
+
+static struct xnvfile_regular rtnet_rtmac_vnics_vfile = {
+    .ops = &rtnet_rtmac_vnics_vfile_ops,
+};
+
+static int
+rtnet_rtmac_disc_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    struct rtmac_proc_entry *entry;
+    entry = container_of(it->vfile, struct rtmac_proc_entry, vfile);
+    return entry->handler(it, data);
+}
+
+static struct xnvfile_regular_ops rtnet_rtmac_disc_vfile_ops = {
+    .show = rtnet_rtmac_disc_show,
+};
+
+int rtmac_disc_proc_register(struct rtmac_disc *disc)
+{
+    int                     i, err;
+    struct rtmac_proc_entry *entry;
+
+
+    for (i = 0; i < disc->nr_proc_entries; i++) {
+	entry = &disc->proc_entries[i];
+
+	entry->vfile.ops = &rtnet_rtmac_disc_vfile_ops;
+	err = xnvfile_init_regular(entry->name, &entry->vfile, &rtmac_proc_root);
+	if (err < 0) {
+	    while (--i >= 0)
+		xnvfile_destroy_regular(&disc->proc_entries[i].vfile);
+	    return err;
+	}
+    }
+
+    return 0;
+}
+
+
+
+void rtmac_disc_proc_unregister(struct rtmac_disc *disc)
+{
+    int i;
+
+    for (i = 0; i < disc->nr_proc_entries; i++)
+	xnvfile_destroy_regular(&disc->proc_entries[i].vfile);
+}
+
+
+
+int rtmac_proc_register(void)
+{
+    int err;
+
+    err = xnvfile_init_dir("rtmac", &rtmac_proc_root, &rtnet_proc_root);
+    if (err < 0)
+	goto err1;
+
+    err = xnvfile_init_regular("disciplines", &rtnet_rtmac_disciplines_vfile,
+			    &rtmac_proc_root);
+    if (err < 0)
+	goto err2;
+
+    err = xnvfile_init_regular("vnics", &rtnet_rtmac_vnics_vfile,
+			    &rtmac_proc_root);
+    if (err < 0)
+	goto err3;
+
+    return 0;
+
+  err3:
+    xnvfile_destroy_regular(&rtnet_rtmac_disciplines_vfile);
+
+  err2:
+    xnvfile_destroy_dir(&rtmac_proc_root);
+
+  err1:
+    /*ERRMSG*/printk(KERN_ERR "RTmac: unable to initialize /proc entries\n");
+    return err;
+}
+
+
+
+void rtmac_proc_release(void)
+{
+    xnvfile_destroy_regular(&rtnet_rtmac_vnics_vfile);
+    xnvfile_destroy_regular(&rtnet_rtmac_disciplines_vfile);
+    xnvfile_destroy_dir(&rtmac_proc_root);
+}
+
+#endif /* CONFIG_XENO_OPT_VFILE */
diff -Naur a/net/rtnet/stack/rtmac/rtmac_proto.c b/net/rtnet/stack/rtmac/rtmac_proto.c
--- a/net/rtnet/stack/rtmac/rtmac_proto.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/rtmac_proto.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,77 @@
+/***
+ *
+ *  rtmac/rtmac_proto.c
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+
+#include <rtdm/driver.h>
+#include <stack_mgr.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+
+int rtmac_proto_rx(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct rtmac_disc *disc = skb->rtdev->mac_disc;
+    struct rtmac_hdr  *hdr;
+
+
+    if (disc == NULL) {
+        goto error;
+    }
+
+    hdr = (struct rtmac_hdr *)skb->data;
+    rtskb_pull(skb, sizeof(struct rtmac_hdr));
+
+    if (hdr->ver != RTMAC_VERSION) {
+        rtdm_printk("RTmac: received unsupported RTmac protocol version on "
+                    "device %s.  Got 0x%x but expected 0x%x\n",
+                    skb->rtdev->name, hdr->ver, RTMAC_VERSION);
+        goto error;
+    }
+
+    if (hdr->flags & RTMAC_FLAG_TUNNEL)
+        rtmac_vnic_rx(skb, hdr->type);
+    else if (disc->disc_type == hdr->type)
+        disc->packet_rx(skb);
+    return 0;
+
+  error:
+    kfree_rtskb(skb);
+    return 0;
+}
+
+
+
+struct rtpacket_type rtmac_packet_type = {
+    .type =     __constant_htons(ETH_RTMAC),
+    .handler =  rtmac_proto_rx
+};
+
+
+
+void rtmac_proto_release(void)
+{
+    rtdev_remove_pack(&rtmac_packet_type);
+}
diff -Naur a/net/rtnet/stack/rtmac/rtmac_syms.c b/net/rtnet/stack/rtmac/rtmac_syms.c
--- a/net/rtnet/stack/rtmac/rtmac_syms.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/rtmac_syms.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,37 @@
+/* rtmac_syms.c
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>
+ *               2003 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+EXPORT_SYMBOL_GPL(__rtmac_disc_register);
+EXPORT_SYMBOL_GPL(rtmac_disc_deregister);
+
+EXPORT_SYMBOL_GPL(rtmac_disc_attach);
+EXPORT_SYMBOL_GPL(rtmac_disc_detach);
+
+EXPORT_SYMBOL_GPL(rtmac_vnic_set_max_mtu);
+
+EXPORT_SYMBOL_GPL(rtmac_vnic_xmit);
diff -Naur a/net/rtnet/stack/rtmac/rtmac_vnic.c b/net/rtnet/stack/rtmac/rtmac_vnic.c
--- a/net/rtnet/stack/rtmac/rtmac_vnic.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/rtmac_vnic.c	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,360 @@
+/* rtmac_vnic.c
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *               2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#include <linux/moduleparam.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/rtnetlink.h>
+
+#include <rtnet_internal.h>
+#include <rtdev.h>
+#include <rtnet_port.h> /* for netdev_priv() */
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+static unsigned int vnic_rtskbs = DEFAULT_VNIC_RTSKBS;
+module_param(vnic_rtskbs, uint, 0444);
+MODULE_PARM_DESC(vnic_rtskbs, "Number of realtime socket buffers per virtual NIC");
+
+static rtdm_nrtsig_t        vnic_signal;
+static struct rtskb_queue   rx_queue;
+
+
+
+int rtmac_vnic_rx(struct rtskb *rtskb, u16 type)
+{
+    struct rtmac_priv *mac_priv = rtskb->rtdev->mac_priv;
+    struct rtskb_pool *pool = &mac_priv->vnic_skb_pool;
+
+
+    if (rtskb_acquire(rtskb, pool) != 0) {
+	mac_priv->vnic_stats.rx_dropped++;
+	kfree_rtskb(rtskb);
+	return -1;
+    }
+
+    rtskb->protocol = type;
+
+    rtskb_queue_tail(&rx_queue, rtskb);
+    rtdm_nrtsig_pend(&vnic_signal);
+
+    return 0;
+}
+
+
+
+static void rtmac_vnic_signal_handler(rtdm_nrtsig_t *nrtsig, void *arg)
+{
+    struct rtskb            *rtskb;
+    struct sk_buff          *skb;
+    unsigned                hdrlen;
+    struct net_device_stats *stats;
+    struct rtnet_device     *rtdev;
+
+
+    while (1)
+    {
+	rtskb = rtskb_dequeue(&rx_queue);
+	if (!rtskb)
+	    break;
+
+	rtdev  = rtskb->rtdev;
+	hdrlen = rtdev->hard_header_len;
+
+	skb = dev_alloc_skb(hdrlen + rtskb->len + 2);
+	if (skb) {
+	    /* the rtskb stamp is useless (different clock), get new one */
+	    __net_timestamp(skb);
+
+	    skb_reserve(skb, 2); /* Align IP on 16 byte boundaries */
+
+	    /* copy Ethernet header */
+	    memcpy(skb_put(skb, hdrlen),
+		   rtskb->data - hdrlen - sizeof(struct rtmac_hdr), hdrlen);
+
+	    /* patch the protocol field in the original Ethernet header */
+	    ((struct ethhdr*)skb->data)->h_proto = rtskb->protocol;
+
+	    /* copy data */
+	    memcpy(skb_put(skb, rtskb->len), rtskb->data, rtskb->len);
+
+	    skb->dev      = rtskb->rtdev->mac_priv->vnic;
+	    skb->protocol = eth_type_trans(skb, skb->dev);
+
+	    stats = &rtskb->rtdev->mac_priv->vnic_stats;
+
+	    kfree_rtskb(rtskb);
+
+	    stats->rx_packets++;
+	    stats->rx_bytes += skb->len;
+
+	    netif_rx(skb);
+	}
+	else {
+	    printk(KERN_ERR "RTmac: VNIC fails to allocate linux skb\n");
+	    kfree_rtskb(rtskb);
+	}
+    }
+}
+
+
+
+static int rtmac_vnic_copy_mac(struct net_device *dev)
+{
+    memcpy(dev->dev_addr,
+	   (*(struct rtnet_device **)netdev_priv(dev))->dev_addr,
+	   MAX_ADDR_LEN);
+
+    return 0;
+}
+
+
+
+int rtmac_vnic_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+    struct rtnet_device     *rtdev = *(struct rtnet_device **)netdev_priv(dev);
+    struct net_device_stats *stats = &rtdev->mac_priv->vnic_stats;
+    struct rtskb_pool       *pool = &rtdev->mac_priv->vnic_skb_pool;
+    struct ethhdr           *ethernet = (struct ethhdr*)skb->data;
+    struct rtskb            *rtskb;
+    int                     res;
+    int                     data_len;
+
+
+    rtskb =
+	alloc_rtskb((skb->len + sizeof(struct rtmac_hdr) + 15) & ~15, pool);
+    if (!rtskb)
+	return NETDEV_TX_BUSY;
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len + sizeof(struct rtmac_hdr));
+
+    data_len = skb->len - dev->hard_header_len;
+    memcpy(rtskb_put(rtskb, data_len), skb->data + dev->hard_header_len,
+	   data_len);
+
+    res = rtmac_add_header(rtdev, ethernet->h_dest, rtskb,
+			   ntohs(ethernet->h_proto), RTMAC_FLAG_TUNNEL);
+    if (res < 0) {
+	stats->tx_dropped++;
+	kfree_rtskb(rtskb);
+	goto done;
+    }
+
+    RTNET_ASSERT(rtdev->mac_disc->nrt_packet_tx != NULL, kfree_rtskb(rtskb);
+		 goto done;);
+
+    res = rtdev->mac_disc->nrt_packet_tx(rtskb);
+    if (res < 0) {
+	stats->tx_dropped++;
+	kfree_rtskb(rtskb);
+    } else {
+	stats->tx_packets++;
+	stats->tx_bytes += skb->len;
+    }
+
+done:
+    dev_kfree_skb(skb);
+    return NETDEV_TX_OK;
+}
+
+
+
+static struct net_device_stats *rtmac_vnic_get_stats(struct net_device *dev)
+{
+    return &(*(struct rtnet_device **)netdev_priv(dev))->mac_priv->vnic_stats;
+}
+
+
+
+static int rtmac_vnic_change_mtu(struct net_device *dev, int new_mtu)
+{
+    if ((new_mtu < 68) ||
+	((unsigned)new_mtu > 1500 - sizeof(struct rtmac_hdr)))
+	return -EINVAL;
+    dev->mtu = new_mtu;
+    return 0;
+}
+
+
+
+void rtmac_vnic_set_max_mtu(struct rtnet_device *rtdev, unsigned int max_mtu)
+{
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+    struct net_device   *vnic = mac_priv->vnic;
+    unsigned int        prev_mtu  = mac_priv->vnic_max_mtu;
+
+
+    mac_priv->vnic_max_mtu = max_mtu - sizeof(struct rtmac_hdr);
+
+    /* set vnic mtu in case max_mtu is smaller than the current mtu or
+       the current mtu was set to previous max_mtu */
+    rtnl_lock();
+    if ((vnic->mtu > mac_priv->vnic_max_mtu) || (prev_mtu == mac_priv->vnic_max_mtu)) {
+	dev_set_mtu(vnic, mac_priv->vnic_max_mtu);
+    }
+    rtnl_unlock();
+}
+
+
+static struct net_device_ops vnic_netdev_ops = {
+    .ndo_open       = rtmac_vnic_copy_mac,
+    .ndo_get_stats  = rtmac_vnic_get_stats,
+    .ndo_change_mtu = rtmac_vnic_change_mtu,
+};
+
+static void rtmac_vnic_setup(struct net_device *dev)
+{
+    ether_setup(dev);
+
+    dev->netdev_ops      = &vnic_netdev_ops;
+    dev->flags           &= ~IFF_MULTICAST;
+}
+
+int rtmac_vnic_add(struct rtnet_device *rtdev, vnic_xmit_handler vnic_xmit)
+{
+    int                 res;
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+    struct net_device   *vnic;
+    char                buf[IFNAMSIZ];
+
+
+    /* does the discipline request vnic support? */
+    if (!vnic_xmit)
+	return 0;
+
+    mac_priv->vnic = NULL;
+    mac_priv->vnic_max_mtu = rtdev->mtu - sizeof(struct rtmac_hdr);
+    memset(&mac_priv->vnic_stats, 0, sizeof(mac_priv->vnic_stats));
+
+    /* create the rtskb pool */
+    if (rtskb_pool_init(&mac_priv->vnic_skb_pool,
+			    vnic_rtskbs, NULL, NULL) < vnic_rtskbs) {
+	res = -ENOMEM;
+	goto error;
+    }
+
+    snprintf(buf, sizeof(buf), "vnic%d", rtdev->ifindex-1);
+
+    vnic = alloc_netdev(sizeof(struct rtnet_device *), buf,
+		    NET_NAME_UNKNOWN, rtmac_vnic_setup);
+    if (!vnic) {
+	res = -ENOMEM;
+	goto error;
+    }
+
+    vnic_netdev_ops.ndo_start_xmit = vnic_xmit;
+    vnic->mtu = mac_priv->vnic_max_mtu;
+    *(struct rtnet_device **)netdev_priv(vnic) = rtdev;
+    rtmac_vnic_copy_mac(vnic);
+
+    res = register_netdev(vnic);
+    if (res < 0)
+	goto error;
+
+    mac_priv->vnic = vnic;
+
+    return 0;
+
+ error:
+    rtskb_pool_release(&mac_priv->vnic_skb_pool);
+    return res;
+}
+
+
+
+int rtmac_vnic_unregister(struct rtnet_device *rtdev)
+{
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+
+    if (mac_priv->vnic) {
+	rtskb_pool_release(&mac_priv->vnic_skb_pool);
+	unregister_netdev(mac_priv->vnic);
+	free_netdev(mac_priv->vnic);
+	mac_priv->vnic = NULL;
+    }
+
+    return 0;
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_vnics_show(struct xnvfile_regular_iterator *it, void *d)
+{
+    struct rtnet_device *rtdev;
+    int                 i;
+    int                 err;
+
+    xnvfile_printf(it, "RT-NIC name\tVNIC name\n");
+
+    for (i = 1; i <= MAX_RT_DEVICES; i++) {
+	rtdev = rtdev_get_by_index(i);
+	if (rtdev == NULL)
+	    continue;
+
+	err = mutex_lock_interruptible(&rtdev->nrt_lock);
+	if (err < 0) {
+	    rtdev_dereference(rtdev);
+	    return err;
+	}
+
+	if (rtdev->mac_priv != NULL) {
+	    struct rtmac_priv *rtmac;
+
+	    rtmac = (struct rtmac_priv *)rtdev->mac_priv;
+	    xnvfile_printf(it, "%-15s %s\n", rtdev->name, rtmac->vnic->name);
+	}
+
+	mutex_unlock(&rtdev->nrt_lock);
+	rtdev_dereference(rtdev);
+    }
+
+    return 0;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+int __init rtmac_vnic_module_init(void)
+{
+    rtskb_queue_init(&rx_queue);
+
+    rtdm_nrtsig_init(&vnic_signal, rtmac_vnic_signal_handler, NULL);
+
+    return 0;
+}
+
+
+
+void rtmac_vnic_module_cleanup(void)
+{
+    struct rtskb *rtskb;
+
+
+    rtdm_nrtsig_destroy(&vnic_signal);
+
+    while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+	kfree_rtskb(rtskb);
+    }
+}
diff -Naur a/net/rtnet/stack/rtmac/tdma/Kconfig b/net/rtnet/stack/rtmac/tdma/Kconfig
--- a/net/rtnet/stack/rtmac/tdma/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/Kconfig	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,21 @@
+config RTNET_TDMA
+    tristate "TDMA discipline for RTmac"
+    depends on RTNET_RTMAC
+    default y
+    help
+    The Time Division Multiple Access discipline is the default RTmac
+    protocol for Ethernet networks. It consists of a master synchronising
+    the access of the slaves to the media by periodically issuing frames.
+    Backup masters can be set up to take over if the primary master fails.
+    TDMA also provides a global clock across all participants. The tdmacfg
+    tool can be used to configure a real-time NIC to use TDMA.
+
+    See Documenatation/README.rtmac for further details.
+
+config RTNET_TDMA_MASTER
+    bool "TDMA master support"
+    depends on RTNET_TDMA
+    default y
+    help
+    Enables TDMA master and backup master support for the node. This can
+    be switched of to reduce the memory footprint of pure slave nodes.
diff -Naur a/net/rtnet/stack/rtmac/tdma/Makefile b/net/rtnet/stack/rtmac/tdma/Makefile
--- a/net/rtnet/stack/rtmac/tdma/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/Makefile	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,10 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_RTNET_TDMA) += tdma.o
+
+tdma-y := \
+	tdma_dev.o \
+	tdma_ioctl.o \
+	tdma_module.o \
+	tdma_proto.o \
+	tdma_worker.o
diff -Naur a/net/rtnet/stack/rtmac/tdma/tdma_dev.c b/net/rtnet/stack/rtmac/tdma/tdma_dev.c
--- a/net/rtnet/stack/rtmac/tdma/tdma_dev.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/tdma_dev.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,195 @@
+/***
+ *
+ *  rtmac/tdma/tdma_dev.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2003-2006 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/list.h>
+
+#include <rtdev.h>
+#include <rtmac.h>
+#include <rtmac/tdma/tdma.h>
+
+
+struct tdma_dev_ctx {
+    rtdm_task_t *cycle_waiter;
+};
+
+
+static int tdma_dev_open(struct rtdm_fd *fd, int oflags)
+{
+    struct tdma_dev_ctx *ctx = rtdm_fd_to_private(fd);
+
+    ctx->cycle_waiter = NULL;
+
+    return 0;
+}
+
+
+static void tdma_dev_close(struct rtdm_fd *fd)
+{
+    struct tdma_dev_ctx *ctx = rtdm_fd_to_private(fd);
+    rtdm_lockctx_t lock_ctx;
+
+
+    cobalt_atomic_enter(lock_ctx);
+    if (ctx->cycle_waiter)
+	rtdm_task_unblock(ctx->cycle_waiter);
+    cobalt_atomic_leave(lock_ctx);
+}
+
+
+static int wait_on_sync(struct tdma_dev_ctx *tdma_ctx,
+			rtdm_event_t *sync_event)
+{
+    rtdm_lockctx_t lock_ctx;
+    int ret;
+
+
+    cobalt_atomic_enter(lock_ctx);
+    /* keep it simple: only one waiter per device instance allowed */
+    if (!tdma_ctx->cycle_waiter) {
+	tdma_ctx->cycle_waiter = rtdm_task_current();
+	ret = rtdm_event_wait(sync_event);
+	tdma_ctx->cycle_waiter = NULL;
+    } else
+	ret = -EBUSY;
+    cobalt_atomic_leave(lock_ctx);
+
+    return ret;
+}
+
+
+static int tdma_dev_ioctl(struct rtdm_fd *fd, unsigned int request, void *arg)
+{
+    struct tdma_dev_ctx *ctx = rtdm_fd_to_private(fd);
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      lock_ctx;
+    int                 ret;
+
+
+    tdma = container_of(rtdm_fd_to_context(fd)->device,
+			struct tdma_priv, api_device);
+
+    switch (request) {
+    case RTMAC_RTIOC_TIMEOFFSET: {
+	nanosecs_rel_t offset;
+
+	rtdm_lock_get_irqsave(&tdma->lock, lock_ctx);
+	offset = tdma->clock_offset;
+	rtdm_lock_put_irqrestore(&tdma->lock, lock_ctx);
+
+	if (rtdm_fd_is_user(fd)) {
+	    if (!rtdm_rw_user_ok(fd, arg, sizeof(__s64)) ||
+		rtdm_copy_to_user(fd, arg, &offset, sizeof(__s64)))
+		return -EFAULT;
+	} else
+	    *(__s64 *)arg = offset;
+
+	return 0;
+    }
+    case RTMAC_RTIOC_WAITONCYCLE:
+	if (!rtdm_in_rt_context())
+	    return -ENOSYS;
+
+	if ((long)arg !=TDMA_WAIT_ON_SYNC)
+	    return -EINVAL;
+
+	return wait_on_sync(ctx, &tdma->sync_event);
+
+    case RTMAC_RTIOC_WAITONCYCLE_EX: {
+	struct rtmac_waitinfo   *waitinfo = (struct rtmac_waitinfo *)arg;
+	struct rtmac_waitinfo   waitinfo_buf;
+
+#define WAITINFO_HEAD_SIZE                                              \
+	((char *)&waitinfo_buf.cycle_no - (char *)&waitinfo_buf)
+
+	if (!rtdm_in_rt_context())
+	    return -ENOSYS;
+
+	if (rtdm_fd_is_user(fd)) {
+	    if (!rtdm_rw_user_ok(fd, waitinfo,
+				    sizeof(struct rtmac_waitinfo)) ||
+		rtdm_copy_from_user(fd, &waitinfo_buf, arg,
+				    WAITINFO_HEAD_SIZE))
+		return -EFAULT;
+
+	    waitinfo = &waitinfo_buf;
+	}
+
+	if ((waitinfo->type != TDMA_WAIT_ON_SYNC) ||
+	    (waitinfo->size < sizeof(struct rtmac_waitinfo)))
+	    return -EINVAL;
+
+	ret = wait_on_sync(ctx, &tdma->sync_event);
+	if (ret)
+	    return ret;
+
+	rtdm_lock_get_irqsave(&tdma->lock, lock_ctx);
+	waitinfo->cycle_no     = tdma->current_cycle;
+	waitinfo->cycle_start  = tdma->current_cycle_start;
+	waitinfo->clock_offset = tdma->clock_offset;
+	rtdm_lock_put_irqrestore(&tdma->lock, lock_ctx);
+
+	if (rtdm_fd_is_user(fd)) {
+	    if (rtdm_copy_to_user(fd, arg, &waitinfo_buf,
+				    sizeof(struct rtmac_waitinfo)))
+		return -EFAULT;
+	}
+
+	return 0;
+    }
+    default:
+	return -ENOTTY;
+    }
+}
+
+static struct rtdm_driver tdma_driver = {
+    .profile_info = RTDM_PROFILE_INFO(tdma,
+				    RTDM_CLASS_RTMAC,
+				    RTDM_SUBCLASS_TDMA,
+				    RTNET_RTDM_VER),
+    .device_flags = RTDM_NAMED_DEVICE,
+    .device_count = 1,
+    .context_size = sizeof(struct tdma_dev_ctx),
+    .ops = {
+	.open =         tdma_dev_open,
+	.ioctl_rt =     tdma_dev_ioctl,
+	.ioctl_nrt =    tdma_dev_ioctl,
+	.close =        tdma_dev_close,
+    }
+};
+
+int tdma_dev_init(struct rtnet_device *rtdev, struct tdma_priv *tdma)
+{
+    char    *pos;
+
+    strcpy(tdma->device_name, "TDMA");
+    for (pos = rtdev->name + strlen(rtdev->name) - 1;
+	(pos >= rtdev->name) && ((*pos) >= '0') && (*pos <= '9'); pos--);
+    strncat(tdma->device_name+4, pos+1, IFNAMSIZ-4);
+
+    tdma->api_driver            = tdma_driver;
+    tdma->api_device.driver     = &tdma->api_driver;
+    tdma->api_device.label      = tdma->device_name;
+
+    return rtdm_dev_register(&tdma->api_device);
+}
diff -Naur a/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c b/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c
--- a/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,683 @@
+/***
+ *
+ *  rtmac/tdma/tdma_ioctl.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+#include <asm/div64.h>
+
+#include <tdma_chrdev.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/tdma/tdma.h>
+
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+static int tdma_ioctl_master(struct rtnet_device *rtdev,
+                             struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+    u64                 cycle_ms;
+    unsigned int        table_size;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL) {
+        ret = rtmac_disc_attach(rtdev, &tdma_disc);
+        if (ret < 0)
+            return ret;
+    }
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC) {
+        /* note: we don't clean up an unknown discipline */
+        return -ENOTTY;
+    }
+
+    if (test_bit(TDMA_FLAG_ATTACHED, &tdma->flags)) {
+        /* already attached */
+        return -EBUSY;
+    }
+
+    set_bit(TDMA_FLAG_MASTER, &tdma->flags);
+
+    tdma->cal_rounds = cfg->args.master.cal_rounds;
+
+    /* search at least 3 cycle periods for other masters */
+    cycle_ms = cfg->args.master.cycle_period;
+    do_div(cycle_ms, 1000000);
+    if (cycle_ms == 0)
+        cycle_ms = 1;
+    msleep(3*cycle_ms);
+
+    if (rtskb_module_pool_init(&tdma->cal_rtskb_pool,
+                                cfg->args.master.max_cal_requests) !=
+        cfg->args.master.max_cal_requests) {
+        ret = -ENOMEM;
+        goto err_out;
+    }
+
+    table_size = sizeof(struct tdma_slot *) *
+        ((cfg->args.master.max_slot_id >= 1) ?
+            cfg->args.master.max_slot_id + 1 : 2);
+
+    tdma->slot_table = (struct tdma_slot **)kmalloc(table_size, GFP_KERNEL);
+    if (!tdma->slot_table) {
+        ret = -ENOMEM;
+        goto err_out;
+    }
+    tdma->max_slot_id = cfg->args.master.max_slot_id;
+    memset(tdma->slot_table, 0, table_size);
+
+    tdma->cycle_period = cfg->args.master.cycle_period;
+    tdma->sync_job.ref_count = 0;
+    INIT_LIST_HEAD(&tdma->sync_job.entry);
+
+    if (cfg->args.master.backup_sync_offset == 0)
+        tdma->sync_job.id = XMIT_SYNC;
+    else {
+        set_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags);
+        tdma->sync_job.id     = BACKUP_SYNC;
+        tdma->backup_sync_inc =
+                cfg->args.master.backup_sync_offset + tdma->cycle_period;
+    }
+
+    /* did we detect another active master? */
+    if (test_bit(TDMA_FLAG_RECEIVED_SYNC, &tdma->flags)) {
+        /* become a slave, we need to calibrate first */
+        tdma->sync_job.id = WAIT_ON_SYNC;
+    } else {
+        if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags))
+            printk(KERN_WARNING "TDMA: warning, no primary master detected!\n");
+        set_bit(TDMA_FLAG_CALIBRATED, &tdma->flags);
+        tdma->current_cycle_start = rtdm_clock_read();
+    }
+
+    tdma->first_job = tdma->current_job = &tdma->sync_job;
+
+    rtdm_event_signal(&tdma->worker_wakeup);
+
+    set_bit(TDMA_FLAG_ATTACHED, &tdma->flags);
+
+    return 0;
+
+  err_out:
+    rtmac_disc_detach(rtdev);
+    return ret;
+}
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+
+
+
+static int tdma_ioctl_slave(struct rtnet_device *rtdev,
+                            struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+    unsigned int        table_size;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL) {
+        ret = rtmac_disc_attach(rtdev, &tdma_disc);
+        if (ret < 0)
+            return ret;
+    }
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC) {
+        /* note: we don't clean up an unknown discipline */
+        return -ENOTTY;
+    }
+
+    if (test_bit(TDMA_FLAG_ATTACHED, &tdma->flags)) {
+        /* already attached */
+        return -EBUSY;
+    }
+
+    tdma->cal_rounds = cfg->args.slave.cal_rounds;
+    if (tdma->cal_rounds == 0)
+        set_bit(TDMA_FLAG_CALIBRATED, &tdma->flags);
+
+    table_size = sizeof(struct tdma_slot *) *
+        ((cfg->args.slave.max_slot_id >= 1) ?
+            cfg->args.slave.max_slot_id + 1 : 2);
+
+    tdma->slot_table = (struct tdma_slot **)kmalloc(table_size, GFP_KERNEL);
+    if (!tdma->slot_table) {
+        ret = -ENOMEM;
+        goto err_out;
+    }
+    tdma->max_slot_id = cfg->args.slave.max_slot_id;
+    memset(tdma->slot_table, 0, table_size);
+
+    tdma->sync_job.id        = WAIT_ON_SYNC;
+    tdma->sync_job.ref_count = 0;
+    INIT_LIST_HEAD(&tdma->sync_job.entry);
+
+    tdma->first_job = tdma->current_job = &tdma->sync_job;
+
+    rtdm_event_signal(&tdma->worker_wakeup);
+
+    set_bit(TDMA_FLAG_ATTACHED, &tdma->flags);
+
+    return 0;
+
+  err_out:
+    rtmac_disc_detach(rtdev);
+    return ret;
+}
+
+
+
+static int tdma_ioctl_cal_result_size(struct rtnet_device *rtdev,
+                                      struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    if (!test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags))
+        return tdma->cal_rounds;
+    else
+        return 0;
+}
+
+
+
+int start_calibration(struct rt_proc_call *call)
+{
+    struct tdma_request_cal *req_cal;
+    struct tdma_priv        *tdma;
+    rtdm_lockctx_t          context;
+
+
+    req_cal = rtpc_get_priv(call, struct tdma_request_cal);
+    tdma    = req_cal->tdma;
+
+    /* there are no slots yet, simply add this job after first_job */
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+    tdma->calibration_call = call;
+    tdma->job_list_revision++;
+    list_add(&req_cal->head.entry, &tdma->first_job->entry);
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return -CALL_PENDING;
+}
+
+
+
+void copyback_calibration(struct rt_proc_call *call, void *priv_data)
+{
+    struct tdma_request_cal *req_cal;
+    struct tdma_priv        *tdma;
+    int                     i;
+    u64                     value;
+    u64                     average = 0;
+    u64                     min = 0x7FFFFFFFFFFFFFFFLL;
+    u64                     max = 0;
+
+
+    req_cal = rtpc_get_priv(call, struct tdma_request_cal);
+    tdma    = req_cal->tdma;
+
+    for (i = 0; i < tdma->cal_rounds; i++) {
+        value = req_cal->result_buffer[i];
+        average += value;
+        if (value < min)
+            min = value;
+        if (value > max)
+            max = value;
+        if ((req_cal->cal_results) &&
+            (copy_to_user(&req_cal->cal_results[i], &value,
+                          sizeof(value)) != 0))
+            rtpc_set_result(call, -EFAULT);
+    }
+    do_div(average, tdma->cal_rounds);
+    tdma->master_packet_delay_ns = average;
+
+    average += 500;
+    do_div(average, 1000);
+    min += 500;
+    do_div(min, 1000);
+    max += 500;
+    do_div(max, 1000);
+    printk(KERN_INFO "TDMA: calibrated master-to-slave packet delay: "
+           "%ld us (min/max: %ld/%ld us)\n",
+           (unsigned long)average, (unsigned long)min,
+           (unsigned long)max);
+}
+
+
+
+void cleanup_calibration(void *priv_data)
+{
+    struct tdma_request_cal *req_cal;
+
+
+    req_cal = (struct tdma_request_cal *)priv_data;
+    kfree(req_cal->result_buffer);
+}
+
+
+
+static int tdma_ioctl_set_slot(struct rtnet_device *rtdev,
+                               struct tdma_config *cfg)
+{
+    struct tdma_priv        *tdma;
+    int                     id;
+    int                     jnt_id;
+    struct tdma_slot        *slot, *old_slot;
+    struct tdma_job         *job, *prev_job;
+    struct tdma_request_cal req_cal;
+    struct rtskb            *rtskb;
+    unsigned int            job_list_revision;
+    rtdm_lockctx_t          context;
+    int                     ret;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    id = cfg->args.set_slot.id;
+    if (id > tdma->max_slot_id)
+        return -EINVAL;
+
+    if (cfg->args.set_slot.size == 0)
+        cfg->args.set_slot.size = rtdev->mtu;
+    else if (cfg->args.set_slot.size > rtdev->mtu)
+        return -EINVAL;
+
+    jnt_id = cfg->args.set_slot.joint_slot;
+    if ((jnt_id >= 0) &&
+        ((jnt_id >= tdma->max_slot_id) ||
+         (tdma->slot_table[jnt_id] == 0) ||
+         (tdma->slot_table[jnt_id]->mtu != cfg->args.set_slot.size)))
+        return -EINVAL;
+
+    slot = (struct tdma_slot *)kmalloc(sizeof(struct tdma_slot), GFP_KERNEL);
+    if (!slot)
+        return -ENOMEM;
+
+    if (!test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags)) {
+        req_cal.head.id        = XMIT_REQ_CAL;
+        req_cal.head.ref_count = 0;
+        req_cal.tdma           = tdma;
+        req_cal.offset         = cfg->args.set_slot.offset;
+        req_cal.period         = cfg->args.set_slot.period;
+        req_cal.phasing        = cfg->args.set_slot.phasing;
+        req_cal.cal_rounds     = tdma->cal_rounds;
+        req_cal.cal_results    = cfg->args.set_slot.cal_results;
+
+        req_cal.result_buffer =
+            kmalloc(req_cal.cal_rounds * sizeof(u64), GFP_KERNEL);
+        if (!req_cal.result_buffer) {
+            kfree(slot);
+            return -ENOMEM;
+        }
+
+        ret = rtpc_dispatch_call(start_calibration, 0, &req_cal,
+                                 sizeof(req_cal), copyback_calibration,
+                                 cleanup_calibration);
+        if (ret < 0) {
+            /* kick out any pending calibration job before returning */
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            job = list_entry(tdma->first_job->entry.next, struct tdma_job,
+                             entry);
+            if (job != tdma->first_job) {
+                __list_del(job->entry.prev, job->entry.next);
+
+                while (job->ref_count > 0) {
+                    rtdm_lock_put_irqrestore(&tdma->lock, context);
+                    msleep(100);
+                    rtdm_lock_get_irqsave(&tdma->lock, context);
+                }
+            }
+
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            kfree(slot);
+            return ret;
+        }
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+        if (test_bit(TDMA_FLAG_MASTER, &tdma->flags)) {
+            u32 cycle_no = (volatile u32)tdma->current_cycle;
+            u64 cycle_ms;
+
+
+            /* switch back to [backup] master mode */
+            if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags))
+                tdma->sync_job.id = BACKUP_SYNC;
+            else
+                tdma->sync_job.id = XMIT_SYNC;
+
+            /* wait 2 cycle periods for the mode switch */
+            cycle_ms = tdma->cycle_period;
+            do_div(cycle_ms, 1000000);
+            if (cycle_ms == 0)
+                cycle_ms = 1;
+            msleep(2*cycle_ms);
+
+            /* catch the very unlikely case that the current master died
+               while we just switched the mode */
+            if (cycle_no == (volatile u32)tdma->current_cycle) {
+                kfree(slot);
+                return -ETIME;
+            }
+        }
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+
+        set_bit(TDMA_FLAG_CALIBRATED, &tdma->flags);
+    }
+
+    slot->head.id        = id;
+    slot->head.ref_count = 0;
+    slot->period         = cfg->args.set_slot.period;
+    slot->phasing        = cfg->args.set_slot.phasing;
+    slot->mtu            = cfg->args.set_slot.size;
+    slot->size           = cfg->args.set_slot.size + rtdev->hard_header_len;
+    slot->offset         = cfg->args.set_slot.offset;
+    slot->queue          = &slot->local_queue;
+    rtskb_prio_queue_init(&slot->local_queue);
+
+    if (jnt_id >= 0)    /* all other validation tests performed above */
+        slot->queue = tdma->slot_table[jnt_id]->queue;
+
+    old_slot = tdma->slot_table[id];
+    if ((id == DEFAULT_NRT_SLOT) &&
+        (old_slot == tdma->slot_table[DEFAULT_SLOT]))
+        old_slot = NULL;
+
+  restart:
+    job_list_revision = tdma->job_list_revision;
+
+    if (!old_slot) {
+        job = tdma->first_job;
+        while (1) {
+            prev_job = job;
+            job = list_entry(job->entry.next, struct tdma_job, entry);
+            if (((job->id >= 0) &&
+                 ((slot->offset < SLOT_JOB(job)->offset) ||
+                  ((slot->offset == SLOT_JOB(job)->offset) &&
+                   (slot->head.id <= SLOT_JOB(job)->head.id)))) ||
+#ifdef CONFIG_RTNET_TDMA_MASTER
+                ((job->id == XMIT_RPL_CAL) &&
+                  (slot->offset < REPLY_CAL_JOB(job)->reply_offset)) ||
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+                (job == tdma->first_job))
+                break;
+        }
+
+    } else
+        prev_job = list_entry(old_slot->head.entry.prev,
+                              struct tdma_job, entry);
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    if (job_list_revision != tdma->job_list_revision) {
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+        msleep(100);
+        goto restart;
+    }
+
+    if (old_slot)
+        __list_del(old_slot->head.entry.prev, old_slot->head.entry.next);
+
+    list_add(&slot->head.entry, &prev_job->entry);
+    tdma->slot_table[id] = slot;
+    if ((id == DEFAULT_SLOT) &&
+        (tdma->slot_table[DEFAULT_NRT_SLOT] == old_slot))
+        tdma->slot_table[DEFAULT_NRT_SLOT] = slot;
+
+    if (old_slot) {
+        while (old_slot->head.ref_count > 0) {
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+            msleep(100);
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+        }
+
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+        /* search for other slots linked to the old one */
+        for (jnt_id = 0; jnt_id < tdma->max_slot_id; jnt_id++)
+            if ((tdma->slot_table[jnt_id] != 0) &&
+                (tdma->slot_table[jnt_id]->queue == &old_slot->local_queue)) {
+                /* found a joint slot, move or detach it now */
+                rtdm_lock_get_irqsave(&tdma->lock, context);
+
+                while (tdma->slot_table[jnt_id]->head.ref_count > 0) {
+                    rtdm_lock_put_irqrestore(&tdma->lock, context);
+                    msleep(100);
+                    rtdm_lock_get_irqsave(&tdma->lock, context);
+                }
+
+                /* If the new slot size is larger, detach the other slot,
+                 * update it otherwise. */
+                if (slot->mtu > tdma->slot_table[jnt_id]->mtu)
+                    tdma->slot_table[jnt_id]->queue =
+                        &tdma->slot_table[jnt_id]->local_queue;
+                else {
+                    tdma->slot_table[jnt_id]->mtu   = slot->mtu;
+                    tdma->slot_table[jnt_id]->queue = slot->queue;
+                }
+
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+            }
+    } else
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    rtmac_vnic_set_max_mtu(rtdev, cfg->args.set_slot.size);
+
+    if (old_slot) {
+        /* avoid that the formerly joint queue gets purged */
+        old_slot->queue = &old_slot->local_queue;
+
+        /* Without any reference to the old job and no joint slots we can
+         * safely purge its queue without lock protection.
+         * NOTE: Reconfiguring a slot during runtime may lead to packet
+         *       drops! */
+        while ((rtskb = __rtskb_prio_dequeue(old_slot->queue)))
+            kfree_rtskb(rtskb);
+
+        kfree(old_slot);
+    }
+
+    return 0;
+}
+
+
+
+int tdma_cleanup_slot(struct tdma_priv *tdma, struct tdma_slot *slot)
+{
+    struct rtskb        *rtskb;
+    unsigned int        id, jnt_id;
+    rtdm_lockctx_t      context;
+
+
+    if (!slot)
+        return -EINVAL;
+
+    id = slot->head.id;
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    __list_del(slot->head.entry.prev, slot->head.entry.next);
+
+    if (id == DEFAULT_NRT_SLOT)
+        tdma->slot_table[DEFAULT_NRT_SLOT] = tdma->slot_table[DEFAULT_SLOT];
+    else {
+        if ((id == DEFAULT_SLOT) &&
+            (tdma->slot_table[DEFAULT_NRT_SLOT] == slot))
+            tdma->slot_table[DEFAULT_NRT_SLOT] = NULL;
+        tdma->slot_table[id] = NULL;
+    }
+
+    while (slot->head.ref_count > 0) {
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+        msleep(100);
+        rtdm_lock_get_irqsave(&tdma->lock, context);
+    }
+
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    /* search for other slots linked to this one */
+    for (jnt_id = 0; jnt_id < tdma->max_slot_id; jnt_id++)
+        if ((tdma->slot_table[jnt_id] != 0) &&
+            (tdma->slot_table[jnt_id]->queue == &slot->local_queue)) {
+            /* found a joint slot, detach it now under lock protection */
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            while (tdma->slot_table[jnt_id]->head.ref_count > 0) {
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+                msleep(100);
+                rtdm_lock_get_irqsave(&tdma->lock, context);
+            }
+            tdma->slot_table[jnt_id]->queue =
+                &tdma->slot_table[jnt_id]->local_queue;
+
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+        }
+
+    /* avoid that the formerly joint queue gets purged */
+    slot->queue = &slot->local_queue;
+
+    /* No need to protect the queue access here -
+     * no one is referring to this job anymore
+     * (ref_count == 0, all joint slots detached). */
+    while ((rtskb = __rtskb_prio_dequeue(slot->queue)))
+        kfree_rtskb(rtskb);
+
+    kfree(slot);
+
+    return 0;
+}
+
+
+
+static int tdma_ioctl_remove_slot(struct rtnet_device *rtdev,
+                                  struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+    int                 id;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    id = cfg->args.remove_slot.id;
+    if (id > tdma->max_slot_id)
+        return -EINVAL;
+
+    if ((id == DEFAULT_NRT_SLOT) &&
+        (tdma->slot_table[DEFAULT_NRT_SLOT] == tdma->slot_table[DEFAULT_SLOT]))
+        return -EINVAL;
+
+    return tdma_cleanup_slot(tdma, tdma->slot_table[id]);
+}
+
+
+
+static int tdma_ioctl_detach(struct rtnet_device *rtdev)
+{
+    struct tdma_priv    *tdma;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    ret = rtmac_disc_detach(rtdev);
+
+    return ret;
+}
+
+
+
+int tdma_ioctl(struct rtnet_device *rtdev, unsigned int request,
+               unsigned long arg)
+{
+    struct tdma_config  cfg;
+    int                 ret;
+
+
+    ret = copy_from_user(&cfg, (void *)arg, sizeof(cfg));
+    if (ret != 0)
+        return -EFAULT;
+
+    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+        return -ERESTARTSYS;
+
+    switch (request) {
+#ifdef CONFIG_RTNET_TDMA_MASTER
+        case TDMA_IOC_MASTER:
+            ret = tdma_ioctl_master(rtdev, &cfg);
+            break;
+#endif
+        case TDMA_IOC_SLAVE:
+            ret = tdma_ioctl_slave(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_CAL_RESULT_SIZE:
+            ret = tdma_ioctl_cal_result_size(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_SET_SLOT:
+            ret = tdma_ioctl_set_slot(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_REMOVE_SLOT:
+            ret = tdma_ioctl_remove_slot(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_DETACH:
+            ret = tdma_ioctl_detach(rtdev);
+            break;
+
+        default:
+            ret = -ENOTTY;
+    }
+
+    mutex_unlock(&rtdev->nrt_lock);
+
+    return ret;
+}
diff -Naur a/net/rtnet/stack/rtmac/tdma/tdma_module.c b/net/rtnet/stack/rtmac/tdma/tdma_module.c
--- a/net/rtnet/stack/rtmac/tdma/tdma_module.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/tdma_module.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,327 @@
+/***
+ *
+ *  rtmac/tdma/tdma_module.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <asm/div64.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+
+#include <rtdm/driver.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/tdma/tdma.h>
+#include <rtmac/tdma/tdma_dev.h>
+#include <rtmac/tdma/tdma_ioctl.h>
+#include <rtmac/tdma/tdma_proto.h>
+#include <rtmac/tdma/tdma_worker.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int tdma_proc_read(struct xnvfile_regular_iterator *it, void *data)
+{
+    int                 d, err = 0;
+    struct rtnet_device *rtdev;
+    struct tdma_priv    *tdma;
+    const char          *state;
+#ifdef CONFIG_RTNET_TDMA_MASTER
+    u64                 cycle;
+#endif
+
+    xnvfile_printf(it, "Interface       API Device      Operation Mode  "
+	    "Cycle   State\n");
+
+    for (d = 1; d <= MAX_RT_DEVICES; d++) {
+	rtdev = rtdev_get_by_index(d);
+	if (!rtdev)
+	    continue;
+
+	err = mutex_lock_interruptible(&rtdev->nrt_lock);
+	if (err < 0) {
+	    rtdev_dereference(rtdev);
+	    break;
+	}
+
+	if (!rtdev->mac_priv)
+	    goto unlock_dev;
+	tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+	xnvfile_printf(it, "%-15s %-15s ",
+		    rtdev->name, tdma->api_device.name);
+
+	if (test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags)) {
+#ifdef CONFIG_RTNET_TDMA_MASTER
+	    if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags) &&
+		!test_bit(TDMA_FLAG_BACKUP_ACTIVE, &tdma->flags))
+		state = "stand-by";
+	    else
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+		state = "active";
+	} else
+	    state = "init";
+#ifdef CONFIG_RTNET_TDMA_MASTER
+	if (test_bit(TDMA_FLAG_MASTER, &tdma->flags)) {
+	    cycle = tdma->cycle_period + 500;
+	    do_div(cycle, 1000);
+	    if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags))
+		xnvfile_printf(it, "Backup Master   %-7ld %s\n",
+			    (unsigned long)cycle, state);
+	    else
+		xnvfile_printf(it, "Master          %-7ld %s\n",
+			    (unsigned long)cycle, state);
+	} else
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+	      xnvfile_printf(it, "Slave           -       %s\n", state);
+
+unlock_dev:
+	mutex_unlock(&rtdev->nrt_lock);
+	rtdev_dereference(rtdev);
+    }
+
+    return err;
+}
+
+
+
+int tdma_slots_proc_read(struct xnvfile_regular_iterator *it, void *data)
+{
+    int                 d, i, err = 0;
+    struct rtnet_device *rtdev;
+    struct tdma_priv    *tdma;
+    struct tdma_slot    *slot;
+    int                 jnt_id;
+    u64                 slot_offset;
+
+
+    xnvfile_printf(it, "Interface       "
+		"Slots (id[->joint]:offset:phasing/period:size)\n");
+
+    for (d = 1; d <= MAX_RT_DEVICES; d++) {
+	rtdev = rtdev_get_by_index(d);
+	if (!rtdev)
+	    continue;
+
+	err = mutex_lock_interruptible(&rtdev->nrt_lock);
+	if (err < 0) {
+	    rtdev_dereference(rtdev);
+	    break;
+	}
+
+	if (!rtdev->mac_priv)
+	    goto unlock_dev;
+	tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+	xnvfile_printf(it, "%-15s ", rtdev->name);
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+	if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags)) {
+	    slot_offset = tdma->backup_sync_inc - tdma->cycle_period + 500;
+	    do_div(slot_offset, 1000);
+	    xnvfile_printf(it, "bak:%ld  ", (unsigned long)slot_offset);
+	}
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+
+	if (tdma->slot_table)
+	    for (i = 0; i <= tdma->max_slot_id; i++) {
+		slot = tdma->slot_table[i];
+		if (!slot ||
+		    ((i == DEFAULT_NRT_SLOT) &&
+		     (tdma->slot_table[DEFAULT_SLOT] == slot)))
+		    continue;
+
+		if (slot->queue == &slot->local_queue) {
+		    xnvfile_printf(it, "%d", i);
+		} else
+		    for (jnt_id = 0; jnt_id <= tdma->max_slot_id; jnt_id++)
+			if (&tdma->slot_table[jnt_id]->local_queue ==
+			    slot->queue) {
+			    xnvfile_printf(it, "%d->%d", i, jnt_id);
+			    break;
+			}
+
+		slot_offset = slot->offset + 500;
+		do_div(slot_offset, 1000);
+		xnvfile_printf(it, ":%ld:%d/%d:%d  ",
+			    (unsigned long)slot_offset, slot->phasing + 1,
+			    slot->period, slot->mtu);
+	    }
+
+	xnvfile_printf(it, "\n");
+
+unlock_dev:
+	mutex_unlock(&rtdev->nrt_lock);
+	rtdev_dereference(rtdev);
+    }
+
+    return err;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+int tdma_attach(struct rtnet_device *rtdev, void *priv)
+{
+    struct tdma_priv   *tdma = (struct tdma_priv *)priv;
+    int                 ret;
+
+
+    memset(tdma, 0, sizeof(struct tdma_priv));
+
+    tdma->magic        = TDMA_MAGIC;
+    tdma->rtdev        = rtdev;
+
+    rtdm_lock_init(&tdma->lock);
+
+    rtdm_event_init(&tdma->worker_wakeup, 0);
+    rtdm_event_init(&tdma->xmit_event, 0);
+    rtdm_event_init(&tdma->sync_event, 0);
+
+    ret = tdma_dev_init(rtdev, tdma);
+    if (ret < 0)
+	goto err_out1;
+
+    ret = rtdm_task_init(&tdma->worker_task, "rtnet-tdma", tdma_worker, tdma,
+			 DEF_WORKER_PRIO, 0);
+    if (ret != 0)
+	goto err_out2;
+
+    return 0;
+
+
+  err_out2:
+    tdma_dev_release(tdma);
+
+  err_out1:
+    rtdm_event_destroy(&tdma->sync_event);
+    rtdm_event_destroy(&tdma->xmit_event);
+    rtdm_event_destroy(&tdma->worker_wakeup);
+
+    return ret;
+}
+
+
+
+int tdma_detach(struct rtnet_device *rtdev, void *priv)
+{
+    struct tdma_priv    *tdma = (struct tdma_priv *)priv;
+    struct tdma_job     *job, *tmp;
+
+
+    rtdm_event_destroy(&tdma->sync_event);
+    rtdm_event_destroy(&tdma->xmit_event);
+    rtdm_event_destroy(&tdma->worker_wakeup);
+
+    tdma_dev_release(tdma);
+
+    rtdm_task_destroy(&tdma->worker_task);
+
+    list_for_each_entry_safe(job, tmp, &tdma->first_job->entry, entry) {
+	if (job->id >= 0)
+	    tdma_cleanup_slot(tdma, SLOT_JOB(job));
+	else if (job->id == XMIT_RPL_CAL) {
+	    __list_del(job->entry.prev, job->entry.next);
+	    kfree_rtskb(REPLY_CAL_JOB(job)->reply_rtskb);
+	}
+    }
+
+    if (tdma->slot_table)
+	kfree(tdma->slot_table);
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+    if (test_bit(TDMA_FLAG_MASTER, &tdma->flags))
+	rtskb_pool_release(&tdma->cal_rtskb_pool);
+#endif
+
+    return 0;
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct rtmac_proc_entry tdma_proc_entries[] = {
+    { name: "tdma", handler: tdma_proc_read },
+    { name: "tdma_slots", handler: tdma_slots_proc_read },
+};
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+struct rtmac_disc tdma_disc = {
+    name:           "TDMA",
+    priv_size:      sizeof(struct tdma_priv),
+    disc_type:      __constant_htons(RTMAC_TYPE_TDMA),
+
+    packet_rx:      tdma_packet_rx,
+    rt_packet_tx:   tdma_rt_packet_tx,
+    nrt_packet_tx:  tdma_nrt_packet_tx,
+
+    get_mtu:        tdma_get_mtu,
+
+    vnic_xmit:      RTMAC_DEFAULT_VNIC,
+
+    attach:         tdma_attach,
+    detach:         tdma_detach,
+
+    ioctls:         {
+	service_name:   "RTmac/TDMA",
+	ioctl_type:     RTNET_IOC_TYPE_RTMAC_TDMA,
+	handler:        tdma_ioctl
+    },
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    proc_entries:   tdma_proc_entries,
+    nr_proc_entries: ARRAY_SIZE(tdma_proc_entries),
+#endif /* CONFIG_XENO_OPT_VFILE */
+};
+
+
+
+int __init tdma_init(void)
+{
+    int ret;
+
+
+    printk(KERN_INFO "RTmac/TDMA: init time division multiple access control "
+	   "mechanism\n");
+
+    ret = rtmac_disc_register(&tdma_disc);
+    if (ret < 0)
+	return ret;
+
+    return 0;
+}
+
+
+
+void tdma_release(void)
+{
+    rtmac_disc_deregister(&tdma_disc);
+
+    printk(KERN_INFO "RTmac/TDMA: unloaded\n");
+}
+
+
+
+module_init(tdma_init);
+module_exit(tdma_release);
+
+MODULE_AUTHOR("Jan Kiszka");
+MODULE_LICENSE("GPL");
diff -Naur a/net/rtnet/stack/rtmac/tdma/tdma_proto.c b/net/rtnet/stack/rtmac/tdma/tdma_proto.c
--- a/net/rtnet/stack/rtmac/tdma/tdma_proto.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/tdma_proto.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,412 @@
+/***
+ *
+ *  rtmac/tdma/tdma_proto.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+#include "asm/div64.h"
+
+#include <rtdev.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/tdma/tdma_proto.h>
+
+
+void tdma_xmit_sync_frame(struct tdma_priv *tdma)
+{
+    struct rtnet_device     *rtdev = tdma->rtdev;
+    struct rtskb            *rtskb;
+    struct tdma_frm_sync    *sync;
+
+
+    rtskb = alloc_rtskb(rtdev->hard_header_len + sizeof(struct rtmac_hdr) +
+                        sizeof(struct tdma_frm_sync) + 15, &global_pool);
+    if (!rtskb)
+        goto err_out;
+
+    rtskb_reserve(rtskb,
+        (rtdev->hard_header_len + sizeof(struct rtmac_hdr) + 15) & ~15);
+
+    sync = (struct tdma_frm_sync *)rtskb_put(rtskb,
+                                             sizeof(struct tdma_frm_sync));
+
+    if (rtmac_add_header(rtdev, rtdev->broadcast,
+                         rtskb, RTMAC_TYPE_TDMA, 0) < 0) {
+        kfree_rtskb(rtskb);
+        goto err_out;
+    }
+
+    sync->head.version = __constant_htons(TDMA_FRM_VERSION);
+    sync->head.id      = __constant_htons(TDMA_FRM_SYNC);
+
+    sync->cycle_no         = htonl(tdma->current_cycle);
+    sync->xmit_stamp       = tdma->clock_offset;
+    sync->sched_xmit_stamp =
+            cpu_to_be64(tdma->clock_offset + tdma->current_cycle_start);
+
+    rtskb->xmit_stamp = &sync->xmit_stamp;
+
+    rtmac_xmit(rtskb);
+
+    /* signal local waiters */
+    rtdm_event_pulse(&tdma->sync_event);
+
+    return;
+
+  err_out:
+    /*ERROR*/rtdm_printk("TDMA: Failed to transmit sync frame!\n");
+    return;
+}
+
+
+
+int tdma_xmit_request_cal_frame(struct tdma_priv *tdma, u32 reply_cycle,
+                                u64 reply_slot_offset)
+{
+    struct rtnet_device     *rtdev = tdma->rtdev;
+    struct rtskb            *rtskb;
+    struct tdma_frm_req_cal *req_cal;
+    int                     ret;
+
+
+    rtskb = alloc_rtskb(rtdev->hard_header_len + sizeof(struct rtmac_hdr) +
+                        sizeof(struct tdma_frm_req_cal) + 15, &global_pool);
+    ret = -ENOMEM;
+    if (!rtskb)
+        goto err_out;
+
+    rtskb_reserve(rtskb,
+        (rtdev->hard_header_len + sizeof(struct rtmac_hdr) + 15) & ~15);
+
+    req_cal = (struct tdma_frm_req_cal *)
+        rtskb_put(rtskb, sizeof(struct tdma_frm_req_cal));
+
+    if ((ret = rtmac_add_header(rtdev, tdma->master_hw_addr,
+                                rtskb, RTMAC_TYPE_TDMA, 0)) < 0) {
+        kfree_rtskb(rtskb);
+        goto err_out;
+    }
+
+    req_cal->head.version = __constant_htons(TDMA_FRM_VERSION);
+    req_cal->head.id      = __constant_htons(TDMA_FRM_REQ_CAL);
+
+    req_cal->xmit_stamp        = 0;
+    req_cal->reply_cycle       = htonl(reply_cycle);
+    req_cal->reply_slot_offset = cpu_to_be64(reply_slot_offset);
+
+    rtskb->xmit_stamp = &req_cal->xmit_stamp;
+
+    ret = rtmac_xmit(rtskb);
+    if (ret < 0)
+        goto err_out;
+
+    return 0;
+
+  err_out:
+    /*ERROR*/rtdm_printk("TDMA: Failed to transmit request calibration "
+                         "frame!\n");
+    return ret;
+}
+
+
+
+int tdma_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      context;
+    struct tdma_slot    *slot;
+    int                 ret = 0;
+
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    slot = tdma->slot_table[(rtskb->priority & RTSKB_CHANNEL_MASK) >>
+                            RTSKB_CHANNEL_SHIFT];
+
+    if (unlikely(!slot)) {
+        ret = -EAGAIN;
+        goto err_out;
+    }
+
+    if (unlikely(rtskb->len > slot->size)) {
+        ret = -EMSGSIZE;
+        goto err_out;
+    }
+
+    __rtskb_prio_queue_tail(slot->queue, rtskb);
+
+  err_out:
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return ret;
+}
+
+
+
+int tdma_nrt_packet_tx(struct rtskb *rtskb)
+{
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      context;
+    struct tdma_slot    *slot;
+    int                 ret = 0;
+
+
+    tdma = (struct tdma_priv *)rtskb->rtdev->mac_priv->disc_priv;
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    rtskb->priority = RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO, DEFAULT_NRT_SLOT);
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    slot = tdma->slot_table[DEFAULT_NRT_SLOT];
+
+    if (unlikely(!slot)) {
+        ret = -EAGAIN;
+        goto err_out;
+    }
+
+    if (unlikely(rtskb->len > slot->size)) {
+        ret = -EMSGSIZE;
+        goto err_out;
+    }
+
+    __rtskb_prio_queue_tail(slot->queue, rtskb);
+
+  err_out:
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return ret;
+}
+
+
+
+int tdma_packet_rx(struct rtskb *rtskb)
+{
+    struct tdma_priv        *tdma;
+    struct tdma_frm_head    *head;
+    u64                     delay;
+    u64                     cycle_start;
+    nanosecs_rel_t          clock_offset;
+    struct rt_proc_call     *call;
+    struct tdma_request_cal *req_cal_job;
+    rtdm_lockctx_t          context;
+#ifdef CONFIG_RTNET_TDMA_MASTER
+    struct rtskb            *reply_rtskb;
+    struct rtnet_device     *rtdev;
+    struct tdma_frm_rpl_cal *rpl_cal_frm;
+    struct tdma_reply_cal   *rpl_cal_job;
+    struct tdma_job         *job;
+#endif
+
+
+    tdma = (struct tdma_priv *)rtskb->rtdev->mac_priv->disc_priv;
+
+    head = (struct tdma_frm_head *)rtskb->data;
+
+    if (head->version != __constant_htons(TDMA_FRM_VERSION))
+        goto kfree_out;
+
+    switch (head->id) {
+        case __constant_htons(TDMA_FRM_SYNC):
+            rtskb_pull(rtskb, sizeof(struct tdma_frm_sync));
+
+            /* see "Time Arithmetics" in the TDMA specification */
+            clock_offset = be64_to_cpu(SYNC_FRM(head)->xmit_stamp) +
+                    tdma->master_packet_delay_ns;
+            clock_offset -= rtskb->time_stamp;
+
+            cycle_start = be64_to_cpu(SYNC_FRM(head)->sched_xmit_stamp) -
+                    clock_offset;
+
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+            tdma->current_cycle       = ntohl(SYNC_FRM(head)->cycle_no);
+            tdma->current_cycle_start = cycle_start;
+            tdma->clock_offset        = clock_offset;
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            /* note: Ethernet-specific! */
+            memcpy(tdma->master_hw_addr, rtskb->mac.ethernet->h_source,
+                   ETH_ALEN);
+
+            set_bit(TDMA_FLAG_RECEIVED_SYNC, &tdma->flags);
+
+            rtdm_event_pulse(&tdma->sync_event);
+            break;
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+        case __constant_htons(TDMA_FRM_REQ_CAL):
+            RTNET_ASSERT(test_bit(TDMA_FLAG_MASTER, &tdma->flags) &&
+                         test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags),
+                         break;);
+
+            rtskb_pull(rtskb, sizeof(struct tdma_frm_req_cal));
+
+            rtdev = rtskb->rtdev;
+
+            reply_rtskb = alloc_rtskb(rtdev->hard_header_len +
+                                      sizeof(struct rtmac_hdr) +
+                                      sizeof(struct tdma_frm_rpl_cal) + 15,
+                                      &tdma->cal_rtskb_pool);
+            if (unlikely(!reply_rtskb)) {
+                /*ERROR*/rtdm_printk("TDMA: Too many calibration requests "
+                                     "pending!\n");
+                break;
+            }
+
+            rtskb_reserve(reply_rtskb, (rtdev->hard_header_len +
+                          sizeof(struct rtmac_hdr) + 15) & ~15);
+
+            rpl_cal_frm = (struct tdma_frm_rpl_cal *)
+                rtskb_put(reply_rtskb, sizeof(struct tdma_frm_rpl_cal));
+
+            /* note: Ethernet-specific! */
+            if (unlikely(rtmac_add_header(rtdev, rtskb->mac.ethernet->h_source,
+                                          reply_rtskb, RTMAC_TYPE_TDMA,
+                                          0) < 0)) {
+                kfree_rtskb(reply_rtskb);
+                break;
+            }
+
+            rpl_cal_frm->head.version = __constant_htons(TDMA_FRM_VERSION);
+            rpl_cal_frm->head.id      = __constant_htons(TDMA_FRM_RPL_CAL);
+
+            rpl_cal_frm->request_xmit_stamp = REQ_CAL_FRM(head)->xmit_stamp;
+            rpl_cal_frm->reception_stamp    = cpu_to_be64(rtskb->time_stamp);
+            rpl_cal_frm->xmit_stamp         = 0;
+
+            reply_rtskb->xmit_stamp = &rpl_cal_frm->xmit_stamp;
+
+            /* use reply_rtskb memory behind the frame as job buffer */
+            rpl_cal_job = (struct tdma_reply_cal *)reply_rtskb->tail;
+            RTNET_ASSERT(reply_rtskb->tail +
+                sizeof(struct tdma_reply_cal) <= reply_rtskb->buf_end,
+                rtskb_over_panic(reply_rtskb, sizeof(struct tdma_reply_cal),
+                                 current_text_addr()););
+
+            rpl_cal_job->head.id        = XMIT_RPL_CAL;
+            rpl_cal_job->head.ref_count = 0;
+            rpl_cal_job->reply_cycle    =
+                    ntohl(REQ_CAL_FRM(head)->reply_cycle);
+            rpl_cal_job->reply_rtskb    = reply_rtskb;
+            rpl_cal_job->reply_offset   =
+                    be64_to_cpu(REQ_CAL_FRM(head)->reply_slot_offset);
+
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            job = tdma->current_job;
+            while (1) {
+                job = list_entry(job->entry.prev, struct tdma_job, entry);
+                if ((job == tdma->first_job) ||
+                    ((job->id >= 0) &&
+                     (SLOT_JOB(job)->offset < rpl_cal_job->reply_offset)) ||
+                    ((job->id == XMIT_RPL_CAL) &&
+                     (REPLY_CAL_JOB(job)->reply_offset <
+                            rpl_cal_job->reply_offset)))
+                    break;
+            }
+            list_add(&rpl_cal_job->head.entry, &job->entry);
+            tdma->job_list_revision++;
+
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            break;
+#endif
+
+        case __constant_htons(TDMA_FRM_RPL_CAL):
+            rtskb_pull(rtskb, sizeof(struct tdma_frm_rpl_cal));
+
+            /* see "Time Arithmetics" in the TDMA specification */
+            delay = (rtskb->time_stamp -
+                     be64_to_cpu(RPL_CAL_FRM(head)->request_xmit_stamp)) -
+                    (be64_to_cpu(RPL_CAL_FRM(head)->xmit_stamp) -
+                     be64_to_cpu(RPL_CAL_FRM(head)->reception_stamp));
+            delay = (delay + 1) >> 1;
+
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            call = tdma->calibration_call;
+            if (call == NULL) {
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+                break;
+            }
+            req_cal_job = rtpc_get_priv(call, struct tdma_request_cal);
+
+            req_cal_job->result_buffer[--req_cal_job->cal_rounds] = delay;
+
+            if (req_cal_job->cal_rounds > 0) {
+                tdma->job_list_revision++;
+                list_add(&req_cal_job->head.entry, &tdma->first_job->entry);
+
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            } else {
+                tdma->calibration_call = NULL;
+
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+                rtpc_complete_call(call, 0);
+            }
+
+            break;
+
+        default:
+            /*ERROR*/rtdm_printk("TDMA: Unknown frame %d!\n", ntohs(head->id));
+    }
+
+  kfree_out:
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+unsigned int tdma_get_mtu(struct rtnet_device *rtdev, unsigned int priority)
+{
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      context;
+    struct tdma_slot    *slot;
+    unsigned int        mtu;
+
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    slot = tdma->slot_table[(priority & RTSKB_CHANNEL_MASK) >>
+                            RTSKB_CHANNEL_SHIFT];
+
+    if (unlikely(!slot)) {
+        mtu = rtdev->mtu;
+        goto out;
+    }
+
+    mtu = slot->mtu;
+
+  out:
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return mtu;
+}
diff -Naur a/net/rtnet/stack/rtmac/tdma/tdma_worker.c b/net/rtnet/stack/rtmac/tdma/tdma_worker.c
--- a/net/rtnet/stack/rtmac/tdma/tdma_worker.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtmac/tdma/tdma_worker.c	2021-07-14 15:39:13.338124773 +0300
@@ -0,0 +1,229 @@
+/***
+ *
+ *  rtmac/tdma/tdma_worker.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/tdma/tdma_proto.h>
+
+
+static void do_slot_job(struct tdma_priv *tdma, struct tdma_slot *job,
+                        rtdm_lockctx_t lockctx)
+{
+    struct rtskb *rtskb;
+
+    if ((job->period != 1) &&
+        (tdma->current_cycle % job->period != job->phasing))
+        return;
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    /* wait for slot begin, then send one pending packet */
+    rtdm_task_sleep_abs(tdma->current_cycle_start + SLOT_JOB(job)->offset,
+                        RTDM_TIMERMODE_REALTIME);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+    rtskb = __rtskb_prio_dequeue(SLOT_JOB(job)->queue);
+    if (!rtskb)
+        return;
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    rtmac_xmit(rtskb);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+}
+
+static void do_xmit_sync_job(struct tdma_priv *tdma, rtdm_lockctx_t lockctx)
+{
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    /* wait for beginning of next cycle, then send sync */
+    rtdm_task_sleep_abs(tdma->current_cycle_start + tdma->cycle_period,
+                        RTDM_TIMERMODE_REALTIME);
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+    tdma->current_cycle++;
+    tdma->current_cycle_start += tdma->cycle_period;
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    tdma_xmit_sync_frame(tdma);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+}
+
+static void do_backup_sync_job(struct tdma_priv *tdma, rtdm_lockctx_t lockctx)
+{
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    /* wait for backup slot */
+    rtdm_task_sleep_abs(tdma->current_cycle_start + tdma->backup_sync_inc,
+                        RTDM_TIMERMODE_REALTIME);
+
+    /* take over sync transmission if all earlier masters failed */
+    if (!test_and_clear_bit(TDMA_FLAG_RECEIVED_SYNC, &tdma->flags)) {
+        rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+        tdma->current_cycle++;
+        tdma->current_cycle_start += tdma->cycle_period;
+        rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+        tdma_xmit_sync_frame(tdma);
+
+        set_bit(TDMA_FLAG_BACKUP_ACTIVE, &tdma->flags);
+    } else
+        clear_bit(TDMA_FLAG_BACKUP_ACTIVE, &tdma->flags);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+}
+
+static struct tdma_job *do_request_cal_job(struct tdma_priv *tdma,
+                                           struct tdma_request_cal *job,
+                                           rtdm_lockctx_t lockctx)
+{
+    struct rt_proc_call *call;
+    struct tdma_job     *prev_job;
+    int                 err;
+
+    if ((job->period != 1) &&
+        (tdma->current_cycle % job->period != job->phasing))
+        return &job->head;
+
+    /* remove job until we get a reply */
+    __list_del(job->head.entry.prev, job->head.entry.next);
+    job->head.ref_count--;
+    prev_job = tdma->current_job =
+        list_entry(job->head.entry.prev, struct tdma_job, entry);
+    prev_job->ref_count++;
+    tdma->job_list_revision++;
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    rtdm_task_sleep_abs(tdma->current_cycle_start + job->offset,
+                        RTDM_TIMERMODE_REALTIME);
+    err = tdma_xmit_request_cal_frame(tdma,
+            tdma->current_cycle + job->period, job->offset);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+
+    /* terminate call on error */
+    if (err < 0) {
+        call = tdma->calibration_call;
+        tdma->calibration_call = NULL;
+
+        if (call) {
+            rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+            rtpc_complete_call(call, err);
+            rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+        }
+    }
+
+    return prev_job;
+}
+
+static struct tdma_job *do_reply_cal_job(struct tdma_priv *tdma,
+                                         struct tdma_reply_cal *job,
+                                         rtdm_lockctx_t lockctx)
+{
+    struct tdma_job *prev_job;
+
+    if (job->reply_cycle > tdma->current_cycle)
+        return &job->head;
+
+    /* remove the job */
+    __list_del(job->head.entry.prev, job->head.entry.next);
+    job->head.ref_count--;
+    prev_job = tdma->current_job =
+        list_entry(job->head.entry.prev, struct tdma_job, entry);
+    prev_job->ref_count++;
+    tdma->job_list_revision++;
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    if (job->reply_cycle == tdma->current_cycle) {
+        /* send reply in the assigned slot */
+        rtdm_task_sleep_abs(tdma->current_cycle_start + job->reply_offset,
+                            RTDM_TIMERMODE_REALTIME);
+        rtmac_xmit(job->reply_rtskb);
+    } else {
+        /* cleanup if cycle already passed */
+        kfree_rtskb(job->reply_rtskb);
+    }
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+
+    return prev_job;
+}
+
+void tdma_worker(void *arg)
+{
+    struct tdma_priv    *tdma = arg;
+    struct tdma_job     *job;
+    rtdm_lockctx_t      lockctx;
+    int ret;
+
+    ret = rtdm_event_wait(&tdma->worker_wakeup);
+    if (ret)
+	    return;
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+
+    job = tdma->first_job;
+
+    while (!rtdm_task_should_stop()) {
+        job->ref_count++;
+        switch (job->id) {
+            case WAIT_ON_SYNC:
+                rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+                ret = rtdm_event_wait(&tdma->sync_event);
+		if (ret)
+			return;
+                rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+                break;
+
+            case XMIT_REQ_CAL:
+                job = do_request_cal_job(tdma, REQUEST_CAL_JOB(job), lockctx);
+                break;
+
+#ifdef CONFIG_RTNET_TDMA_MASTER
+            case XMIT_SYNC:
+                do_xmit_sync_job(tdma, lockctx);
+                break;
+
+            case BACKUP_SYNC:
+                do_backup_sync_job(tdma, lockctx);
+                break;
+
+            case XMIT_RPL_CAL:
+                job = do_reply_cal_job(tdma, REPLY_CAL_JOB(job), lockctx);
+                break;
+#endif /* CONFIG_RTNET_TDMA_MASTER */
+
+            default:
+                do_slot_job(tdma, SLOT_JOB(job), lockctx);
+                break;
+        }
+        job->ref_count--;
+
+        job = tdma->current_job =
+            list_entry(job->entry.next, struct tdma_job, entry);
+    }
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+}
diff -Naur a/net/rtnet/stack/rtnet_chrdev.c b/net/rtnet/stack/rtnet_chrdev.c
--- a/net/rtnet/stack/rtnet_chrdev.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtnet_chrdev.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,341 @@
+/***
+ *
+ *  stack/rtnet_chrdev.c - implements char device for management interface
+ *
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@fet.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/if_arp.h>
+#include <linux/kmod.h>
+#include <linux/miscdevice.h>
+#include <linux/netdevice.h>
+#include <linux/spinlock.h>
+
+#include <rtnet_chrdev.h>
+#include <rtnet_internal.h>
+#include <ipv4/route.h>
+
+
+static DEFINE_SPINLOCK(ioctl_handler_lock);
+static LIST_HEAD(ioctl_handlers);
+
+static long rtnet_ioctl(struct file *file,
+			unsigned int request, unsigned long arg)
+{
+    struct rtnet_ioctl_head head;
+    struct rtnet_device     *rtdev = NULL;
+    struct rtnet_ioctls     *ioctls;
+    struct list_head        *entry;
+    int                     ret;
+
+
+    if (!capable(CAP_SYS_ADMIN))
+	return -EPERM;
+
+    ret = copy_from_user(&head, (void *)arg, sizeof(head));
+    if (ret != 0)
+	return -EFAULT;
+
+    spin_lock(&ioctl_handler_lock);
+
+    list_for_each(entry, &ioctl_handlers) {
+	ioctls = list_entry(entry, struct rtnet_ioctls, entry);
+
+	if (ioctls->ioctl_type == _IOC_TYPE(request)) {
+	    atomic_inc(&ioctls->ref_count);
+
+	    spin_unlock(&ioctl_handler_lock);
+
+	    if ((_IOC_NR(request) & RTNET_IOC_NODEV_PARAM) == 0) {
+		rtdev = rtdev_get_by_name(head.if_name);
+		if (!rtdev) {
+		    atomic_dec(&ioctls->ref_count);
+		    return -ENODEV;
+		}
+	    }
+
+	    ret = ioctls->handler(rtdev, request, arg);
+
+	    if (rtdev)
+		rtdev_dereference(rtdev);
+	    atomic_dec(&ioctls->ref_count);
+
+	    return ret;
+	}
+    }
+
+    spin_unlock(&ioctl_handler_lock);
+
+    return -ENOTTY;
+}
+
+
+
+static int rtnet_core_ioctl(struct rtnet_device *rtdev, unsigned int request,
+			    unsigned long arg)
+{
+    struct rtnet_core_cmd   cmd;
+    struct list_head        *entry;
+    struct rtdev_event_hook *hook;
+    int                     ret;
+    unsigned long          context;
+
+
+    ret = copy_from_user(&cmd, (void *)arg, sizeof(cmd));
+    if (ret != 0)
+	return -EFAULT;
+
+    switch (request) {
+	case IOC_RT_IFUP:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    /* We cannot change the promisc flag or the hardware address if
+	       the device is already up. */
+	    if ((rtdev->flags & IFF_UP) &&
+		(((cmd.args.up.set_dev_flags | cmd.args.up.clear_dev_flags) &
+		  IFF_PROMISC) ||
+		 (cmd.args.up.dev_addr_type != ARPHRD_VOID))) {
+		ret = -EBUSY;
+		goto up_out;
+	    }
+
+	    rtdev->flags |= cmd.args.up.set_dev_flags;
+	    rtdev->flags &= ~cmd.args.up.clear_dev_flags;
+
+	    if (cmd.args.up.dev_addr_type != ARPHRD_VOID) {
+		if (cmd.args.up.dev_addr_type != rtdev->type) {
+		    ret = -EINVAL;
+		    goto up_out;
+		}
+		memcpy(rtdev->dev_addr, cmd.args.up.dev_addr, MAX_ADDR_LEN);
+	    }
+
+	    set_bit(PRIV_FLAG_UP, &rtdev->priv_flags);
+
+	    ret = rtdev_open(rtdev);    /* also == 0 if rtdev is already up */
+
+	    if (ret == 0) {
+		mutex_lock(&rtnet_devices_nrt_lock);
+
+		list_for_each(entry, &event_hook_list) {
+		    hook = list_entry(entry, struct rtdev_event_hook, entry);
+		    if (hook->ifup)
+			hook->ifup(rtdev, &cmd);
+		}
+
+		mutex_unlock(&rtnet_devices_nrt_lock);
+	    } else
+		clear_bit(PRIV_FLAG_UP, &rtdev->priv_flags);
+
+	  up_out:
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_IFDOWN:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    /* spin lock required for sync with routing code */
+	    raw_spin_lock_irqsave(&rtdev->rtdev_lock, context);
+
+	    if (test_bit(PRIV_FLAG_ADDING_ROUTE, &rtdev->priv_flags)) {
+		raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+
+		mutex_unlock(&rtdev->nrt_lock);
+		return -EBUSY;
+	    }
+	    clear_bit(PRIV_FLAG_UP, &rtdev->priv_flags);
+
+	    raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+
+	    ret = 0;
+	    if (rtdev->mac_detach != NULL)
+		ret = rtdev->mac_detach(rtdev);
+
+	    if (ret == 0) {
+		mutex_lock(&rtnet_devices_nrt_lock);
+
+		list_for_each(entry, &event_hook_list) {
+		    hook = list_entry(entry, struct rtdev_event_hook, entry);
+		    if (hook->ifdown)
+			hook->ifdown(rtdev);
+		}
+
+		mutex_unlock(&rtnet_devices_nrt_lock);
+
+		ret = rtdev_close(rtdev);
+	    }
+
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_IFINFO:
+	    if (cmd.args.info.ifindex > 0)
+		rtdev = rtdev_get_by_index(cmd.args.info.ifindex);
+	    else
+		rtdev = rtdev_get_by_name(cmd.head.if_name);
+	    if (rtdev == NULL)
+		return -ENODEV;
+
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock)) {
+		rtdev_dereference(rtdev);
+		return -ERESTARTSYS;
+	    }
+
+	    memcpy(cmd.head.if_name, rtdev->name, IFNAMSIZ);
+	    cmd.args.info.ifindex      = rtdev->ifindex;
+	    cmd.args.info.type         = rtdev->type;
+	    cmd.args.info.ip_addr      = rtdev->local_ip;
+	    cmd.args.info.broadcast_ip = rtdev->broadcast_ip;
+	    cmd.args.info.mtu          = rtdev->mtu;
+	    cmd.args.info.flags        = rtdev->flags;
+            if ((cmd.args.info.flags & IFF_UP)
+		    && (rtdev->link_state
+			    & (RTNET_LINK_STATE_PRESENT
+				    | RTNET_LINK_STATE_NOCARRIER))
+                    == RTNET_LINK_STATE_PRESENT)
+                    cmd.args.info.flags |= IFF_RUNNING;
+
+	    memcpy(cmd.args.info.dev_addr, rtdev->dev_addr, MAX_ADDR_LEN);
+
+	    mutex_unlock(&rtdev->nrt_lock);
+
+	    rtdev_dereference(rtdev);
+
+	    if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+		return -EFAULT;
+	    break;
+
+	default:
+	    ret = -ENOTTY;
+    }
+
+    return ret;
+}
+
+
+
+int rtnet_register_ioctls(struct rtnet_ioctls *ioctls)
+{
+    struct list_head    *entry;
+    struct rtnet_ioctls *registered_ioctls;
+
+
+    RTNET_ASSERT(ioctls->handler != NULL, return -EINVAL;);
+
+    spin_lock(&ioctl_handler_lock);
+
+    list_for_each(entry, &ioctl_handlers) {
+	registered_ioctls = list_entry(entry, struct rtnet_ioctls, entry);
+	if (registered_ioctls->ioctl_type == ioctls->ioctl_type) {
+	    spin_unlock(&ioctl_handler_lock);
+	    return -EEXIST;
+	}
+    }
+
+    list_add_tail(&ioctls->entry, &ioctl_handlers);
+    atomic_set(&ioctls->ref_count, 0);
+
+    spin_unlock(&ioctl_handler_lock);
+
+    return 0;
+}
+
+
+
+void rtnet_unregister_ioctls(struct rtnet_ioctls *ioctls)
+{
+    spin_lock(&ioctl_handler_lock);
+
+    while (atomic_read(&ioctls->ref_count) != 0) {
+	spin_unlock(&ioctl_handler_lock);
+
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	schedule_timeout(1*HZ); /* wait a second */
+
+	spin_lock(&ioctl_handler_lock);
+    }
+
+    list_del(&ioctls->entry);
+
+    spin_unlock(&ioctl_handler_lock);
+}
+
+
+
+static struct file_operations rtnet_fops = {
+    .owner = THIS_MODULE,
+    .unlocked_ioctl = rtnet_ioctl,
+};
+
+static struct miscdevice rtnet_chr_misc_dev = {
+    .minor= RTNET_MINOR,
+    .name = "rtnet",
+    .fops = &rtnet_fops,
+};
+
+static struct rtnet_ioctls core_ioctls = {
+    .service_name = "RTnet Core",
+    .ioctl_type =   RTNET_IOC_TYPE_CORE,
+    .handler =      rtnet_core_ioctl
+};
+
+
+
+/**
+ * rtnet_chrdev_init -
+ *
+ */
+int __init rtnet_chrdev_init(void)
+{
+    int err;
+
+    err = misc_register(&rtnet_chr_misc_dev);
+    if (err) {
+	printk(KERN_ERR "RTnet: unable to register rtnet management device/class "
+	       "(error %d)\n", err);
+	return err;
+    }
+
+    rtnet_register_ioctls(&core_ioctls);
+    return 0;
+}
+
+
+
+/**
+ * rtnet_chrdev_release -
+ *
+ */
+void rtnet_chrdev_release(void)
+{
+    misc_deregister(&rtnet_chr_misc_dev);
+}
+
+
+EXPORT_SYMBOL_GPL(rtnet_register_ioctls);
+EXPORT_SYMBOL_GPL(rtnet_unregister_ioctls);
diff -Naur a/net/rtnet/stack/rtnet_module.c b/net/rtnet/stack/rtnet_module.c
--- a/net/rtnet/stack/rtnet_module.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtnet_module.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,402 @@
+/***
+ *
+ *  stack/rtnet_module.c - module framework, proc file system
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include <rtdev_mgr.h>
+#include <rtnet_chrdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_socket.h>
+#include <rtnet_rtpc.h>
+#include <stack_mgr.h>
+#include <rtwlan.h>
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("RTnet stack core");
+
+
+struct rtnet_mgr STACK_manager;
+struct rtnet_mgr RTDEV_manager;
+
+EXPORT_SYMBOL_GPL(STACK_manager);
+EXPORT_SYMBOL_GPL(RTDEV_manager);
+
+const char rtnet_rtdm_provider_name[] =
+    "(C) 1999-2008 RTnet Development Team, http://www.rtnet.org";
+
+EXPORT_SYMBOL_GPL(rtnet_rtdm_provider_name);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+/***
+ *      proc filesystem section
+ */
+struct xnvfile_directory rtnet_proc_root;
+EXPORT_SYMBOL_GPL(rtnet_proc_root);
+
+
+static int rtnet_devices_nrt_lock_get(struct xnvfile *vfile)
+{
+	return mutex_lock_interruptible(&rtnet_devices_nrt_lock);
+}
+
+static void rtnet_devices_nrt_lock_put(struct xnvfile *vfile)
+{
+	mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+static struct xnvfile_lock_ops rtnet_devices_nrt_lock_ops = {
+	.get = rtnet_devices_nrt_lock_get,
+	.put = rtnet_devices_nrt_lock_put,
+};
+
+static void *rtnet_devices_begin(struct xnvfile_regular_iterator *it)
+{
+	if (it->pos == 0)
+		return VFILE_SEQ_START;
+
+	return (void *)2UL;
+}
+
+static void *rtnet_devices_next(struct xnvfile_regular_iterator *it)
+{
+	if (it->pos >= MAX_RT_DEVICES)
+		return NULL;
+
+	return (void *)2UL;
+}
+
+static int rtnet_devices_show(struct xnvfile_regular_iterator *it, void *data)
+{
+	struct rtnet_device *rtdev;
+
+	if (data == NULL) {
+	    xnvfile_printf(it, "Index\tName\t\tFlags\n");
+		return 0;
+	}
+
+	rtdev = __rtdev_get_by_index(it->pos);
+	if (rtdev == NULL)
+		return VFILE_SEQ_SKIP;
+
+	xnvfile_printf(it, "%d\t%-15s %s%s%s%s\n",
+				rtdev->ifindex, rtdev->name,
+				(rtdev->flags & IFF_UP) ? "UP" : "DOWN",
+				(rtdev->flags & IFF_BROADCAST) ? " BROADCAST" : "",
+				(rtdev->flags & IFF_LOOPBACK) ? " LOOPBACK" : "",
+				(rtdev->flags & IFF_PROMISC) ? " PROMISC" : "");
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_devices_vfile_ops = {
+	.begin = rtnet_devices_begin,
+	.next = rtnet_devices_next,
+	.show = rtnet_devices_show,
+};
+
+static struct xnvfile_regular rtnet_devices_vfile = {
+	.entry = { .lockops = &rtnet_devices_nrt_lock_ops, },
+	.ops = &rtnet_devices_vfile_ops,
+};
+
+static int rtnet_rtskb_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    unsigned int rtskb_len;
+
+    rtskb_len = ALIGN_RTSKB_STRUCT_LEN + SKB_DATA_ALIGN(RTSKB_SIZE);
+
+    xnvfile_printf(it, "Statistics\t\tCurrent\tMaximum\n"
+		     "rtskb pools\t\t%d\t%d\n"
+		     "rtskbs\t\t\t%d\t%d\n"
+		     "rtskb memory need\t%d\t%d\n",
+		     rtskb_pools, rtskb_pools_max,
+		     rtskb_amount, rtskb_amount_max,
+		     rtskb_amount * rtskb_len, rtskb_amount_max * rtskb_len);
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtskb_vfile_ops = {
+	.show = rtnet_rtskb_show,
+};
+
+static struct xnvfile_regular rtnet_rtskb_vfile = {
+	.ops = &rtnet_rtskb_vfile_ops,
+};
+
+static int rtnet_version_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    const char verstr[] =
+	    "RTnet for Xenomai v" XENO_VERSION_STRING "\n"
+		"RTcap:      "
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+	    "yes\n"
+#else
+	    "no\n"
+#endif
+		"rtnetproxy: "
+#if IS_ENABLED(CONFIG_RTNET_ADDON_PROXY)
+	    "yes\n"
+#else
+	    "no\n"
+#endif
+		"bug checks: "
+#ifdef CONFIG_RTNET_CHECKED
+	    "yes\n"
+#else
+	    "no\n"
+#endif
+		;
+
+	xnvfile_printf(it, "%s", verstr);
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_version_vfile_ops = {
+	.show = rtnet_version_show,
+};
+
+static struct xnvfile_regular rtnet_version_vfile = {
+	.ops = &rtnet_version_vfile_ops,
+};
+
+static void *rtnet_stats_begin(struct xnvfile_regular_iterator *it)
+{
+	return (void *)1UL;
+}
+
+static void *rtnet_stats_next(struct xnvfile_regular_iterator *it)
+{
+	if (it->pos >= MAX_RT_DEVICES)
+		return NULL;
+
+	return (void *)1UL;
+}
+
+static int rtnet_stats_show(struct xnvfile_regular_iterator *it, void *data)
+{
+	struct net_device_stats *stats;
+	struct rtnet_device *rtdev;
+
+	if (it->pos == 0) {
+		xnvfile_printf(it, "Inter-|   Receive                            "
+					"                    |  Transmit\n");
+		xnvfile_printf(it, " face |bytes    packets errs drop fifo frame "
+					"compressed multicast|bytes    packets errs "
+					"drop fifo colls carrier compressed\n");
+		return 0;
+	}
+
+	rtdev = __rtdev_get_by_index(it->pos);
+	if (rtdev == NULL)
+		return VFILE_SEQ_SKIP;
+
+	if (rtdev->get_stats == NULL) {
+		xnvfile_printf(it, "%6s: No statistics available.\n", rtdev->name);
+		return 0;
+	}
+
+	stats = rtdev->get_stats(rtdev);
+	xnvfile_printf(it,
+				"%6s:%8lu %7lu %4lu %4lu %4lu %5lu %10lu %9lu "
+				"%8lu %7lu %4lu %4lu %4lu %5lu %7lu %10lu\n",
+				rtdev->name, stats->rx_bytes, stats->rx_packets,
+				stats->rx_errors,
+				stats->rx_dropped + stats->rx_missed_errors,
+				stats->rx_fifo_errors,
+				stats->rx_length_errors + stats->rx_over_errors +
+				stats->rx_crc_errors + stats->rx_frame_errors,
+				stats->rx_compressed, stats->multicast,
+				stats->tx_bytes, stats->tx_packets,
+				stats->tx_errors, stats->tx_dropped,
+				stats->tx_fifo_errors, stats->collisions,
+				stats->tx_carrier_errors +
+				stats->tx_aborted_errors +
+				stats->tx_window_errors +
+				stats->tx_heartbeat_errors,
+				stats->tx_compressed);
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_stats_vfile_ops = {
+	.begin = rtnet_stats_begin,
+	.next = rtnet_stats_next,
+	.show = rtnet_stats_show,
+};
+
+static struct xnvfile_regular rtnet_stats_vfile = {
+	.entry = { .lockops = &rtnet_devices_nrt_lock_ops, },
+	.ops = &rtnet_stats_vfile_ops,
+};
+
+static int rtnet_proc_register(void)
+{
+	int err;
+
+	err = xnvfile_init_dir("rtnet", &rtnet_proc_root, NULL);
+	if (err < 0)
+		goto error1;
+
+	err = xnvfile_init_regular("devices", &rtnet_devices_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error2;
+
+	err = xnvfile_init_regular("rtskb", &rtnet_rtskb_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error3;
+
+	err = xnvfile_init_regular("version", &rtnet_version_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error4;
+
+	err = xnvfile_init_regular("stats", &rtnet_stats_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error5;
+
+    return 0;
+
+  error5:
+	xnvfile_destroy_regular(&rtnet_version_vfile);
+
+  error4:
+	xnvfile_destroy_regular(&rtnet_rtskb_vfile);
+
+  error3:
+	xnvfile_destroy_regular(&rtnet_devices_vfile);
+
+  error2:
+	xnvfile_destroy_dir(&rtnet_proc_root);
+
+  error1:
+    printk(KERN_ERR "RTnet: unable to initialize /proc entries\n");
+    return err;
+}
+
+
+
+static void rtnet_proc_unregister(void)
+{
+	xnvfile_destroy_regular(&rtnet_stats_vfile);
+	xnvfile_destroy_regular(&rtnet_version_vfile);
+	xnvfile_destroy_regular(&rtnet_rtskb_vfile);
+	xnvfile_destroy_regular(&rtnet_devices_vfile);
+	xnvfile_destroy_dir(&rtnet_proc_root);
+}
+#endif  /* CONFIG_XENO_OPT_VFILE */
+
+
+
+/**
+ *  rtnet_init()
+ */
+int __init rtnet_init(void)
+{
+    int err = 0;
+
+
+    printk(KERN_INFO "\n*** RTnet for PREEMPT_RT ***\n\n");
+    printk(KERN_INFO "RTnet: initialising real-time networking\n");
+
+    if ((err = rtskb_pools_init()) != 0)
+	goto err_out1;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    if ((err = rtnet_proc_register()) != 0)
+	goto err_out2;
+#endif
+
+    /* initialize the Stack-Manager */
+    if ((err = rt_stack_mgr_init(&STACK_manager)) != 0)
+	goto err_out3;
+
+    /* initialize the RTDEV-Manager */
+    if ((err = rt_rtdev_mgr_init(&RTDEV_manager)) != 0)
+	goto err_out4;
+
+    rtnet_chrdev_init();
+
+    if ((err = rtwlan_init()) != 0)
+	goto err_out5;
+
+    if ((err = rtpc_init()) != 0)
+	goto err_out6;
+
+    return 0;
+
+
+err_out6:
+    rtwlan_exit();
+
+err_out5:
+    rtnet_chrdev_release();
+    rt_rtdev_mgr_delete(&RTDEV_manager);
+
+err_out4:
+    rt_stack_mgr_delete(&STACK_manager);
+
+err_out3:
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtnet_proc_unregister();
+
+err_out2:
+#endif
+    rtskb_pools_release();
+
+err_out1:
+    return err;
+}
+
+
+/**
+ *  rtnet_release()
+ */
+void __exit rtnet_release(void)
+{
+    rtpc_cleanup();
+
+    rtwlan_exit();
+
+    rtnet_chrdev_release();
+
+    rt_stack_mgr_delete(&STACK_manager);
+    rt_rtdev_mgr_delete(&RTDEV_manager);
+
+    rtskb_pools_release();
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtnet_proc_unregister();
+#endif
+
+    printk(KERN_INFO "RTnet: unloaded\n");
+}
+
+
+module_init(rtnet_init);
+module_exit(rtnet_release);
diff -Naur a/net/rtnet/stack/rtnet_rtdm.c b/net/rtnet/stack/rtnet_rtdm.c
--- a/net/rtnet/stack/rtnet_rtdm.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtnet_rtdm.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,140 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * RTNet RTDM functions
+ *
+ * Copyright (C) 2020, Laurentiu-Cristian Duca
+ *		laurentiu [dot] duca [at] gmail [dot] com
+ */
+
+#include <linux/sched.h>
+#include <linux/swait.h>
+#include <linux/jiffies.h>
+#include <linux/semaphore.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <rtnet_rtdm.h>
+
+void __rtdm_event_init(rtdm_event_t *event)
+{
+	event->condition = 0;
+	init_swait_queue_head(&event->head_swait);
+}
+EXPORT_SYMBOL_GPL(__rtdm_event_init);
+
+void rtdm_event_signal_one(rtdm_event_t *event)
+{
+	/* use swait.h model to signal condition true */
+	event->condition = 1;
+	smp_mb();
+ 	if (swait_active(&event->head_swait))
+		swake_up_one(&event->head_swait);
+}
+EXPORT_SYMBOL_GPL(rtdm_event_signal_one);
+
+int rtdm_event_wait_one(rtdm_event_t *event)
+{
+	DECLARE_SWAITQUEUE(swait);
+
+	/* wait for condition using swait.h model */
+	for (;;) {
+		prepare_to_swait_exclusive(&event->head_swait, &swait, TASK_INTERRUPTIBLE);
+ 		/* smp_mb() from set_current_state() */
+ 		if (event->condition)
+ 			break;
+ 		schedule();
+ 	}
+	finish_swait(&event->head_swait, &swait);
+	event->condition = 0;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rtdm_event_wait_one);
+
+void rtdm_event_signal(rtdm_event_t *event)
+{
+	event->condition = 1;
+	smp_mb();
+	if (swait_active(&event->head_swait))
+		swake_up_all(&event->head_swait);
+}
+EXPORT_SYMBOL_GPL(rtdm_event_signal);
+
+int rtdm_event_wait(rtdm_event_t *event)
+{
+	int ret;
+	ret = __swait_event_interruptible(event->head_swait, event->condition);
+	if(ret < 0)
+		return ret;
+	event->condition = 0;
+	smp_mb();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rtdm_event_wait);
+
+int rtdm_event_timedwait(rtdm_event_t *event, ktime_t timeout, void *not_used)
+{
+        return swait_event_interruptible_timeout_exclusive(event->head_swait,
+			event->condition, nsecs_to_jiffies(timeout));
+}
+EXPORT_SYMBOL_GPL(rtdm_event_timedwait);
+
+void rtdm_sem_destroy(struct semaphore *sem)
+{
+	int success=1;
+	/* awake all current waiters */
+	while(down_trylock(sem)) {
+		success = 0;
+		/* there are waiters on the sem */
+		up(sem);
+	}
+	if(success)
+		up(sem);
+}
+EXPORT_SYMBOL_GPL(rtdm_sem_destroy);
+
+int rtdm_get_iovec(struct iovec **iovp,
+                   const struct user_msghdr *msg,
+                   struct iovec *iov_fast, int msg_in_userspace)
+{
+        size_t len = sizeof(struct iovec) * msg->msg_iovlen;
+        struct iovec *iov = iov_fast;
+
+        /*
+         * If the I/O vector doesn't fit in the fast memory, allocate
+         * a chunk from the system heap which is large enough to hold
+         * it.
+         */
+        if (msg->msg_iovlen > RTDM_IOV_FASTMAX) {
+                iov = kzalloc(len, GFP_ATOMIC);
+                if (iov == NULL)
+                        return -ENOMEM;
+        }
+
+        *iovp = iov;
+
+        if (!msg_in_userspace) {
+                memcpy(iov, msg->msg_iov, len);
+                return 0;
+        }
+
+	return copy_from_user(iov, msg->msg_iov, len);
+}
+EXPORT_SYMBOL_GPL(rtdm_get_iovec);
+
+ssize_t rtdm_get_iov_flatlen(struct iovec *iov, int iovlen)
+{
+        ssize_t len;
+        int nvec;
+
+        /* Return the flattened vector length. */
+        for (len = 0, nvec = 0; nvec < iovlen; nvec++) {
+                ssize_t l = iov[nvec].iov_len;
+                if (l < 0 || len + l < len) /* SuS wants this. */
+                        return -EINVAL;
+                len += l;
+        }
+
+        return len;
+}
+EXPORT_SYMBOL_GPL(rtdm_get_iov_flatlen);
+
diff -Naur a/net/rtnet/stack/rtnet_rtpc.c b/net/rtnet/stack/rtnet_rtpc.c
--- a/net/rtnet/stack/rtnet_rtpc.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtnet_rtpc.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,323 @@
+/***
+ *
+ *  stack/rtnet_rtpc.c
+ *
+ *  RTnet - real-time networking subsystem
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/sched/task.h>
+#include <uapi/linux/sched/types.h>
+
+#include <rtnet_rtpc.h>
+
+
+static raw_spinlock_t pending_calls_lock;
+static raw_spinlock_t processed_calls_lock;
+static rtdm_event_t     dispatch_event;
+static rtdm_event_t     rtpc_nrt_signal_event;
+static struct task_struct *dispatch_task;
+static struct task_struct *rtpc_nrt_signal_task;
+static u8 stop_dispatch_task = 0;
+static u8 stop_rtpc_nrt_signal_task = 0;
+
+LIST_HEAD(pending_calls);
+LIST_HEAD(processed_calls);
+
+
+#ifndef __wait_event_interruptible_timeout
+#define __wait_event_interruptible_timeout(wq, condition, ret)              \
+do {                                                                        \
+    wait_queue_t __wait;                                                    \
+    init_waitqueue_entry(&__wait, current);                                 \
+									    \
+    add_wait_queue(&wq, &__wait);                                           \
+    for (;;) {                                                              \
+	set_current_state(TASK_INTERRUPTIBLE);                              \
+	if (condition)                                                      \
+	    break;                                                          \
+	if (!signal_pending(current)) {                                     \
+	    ret = schedule_timeout(ret);                                    \
+	    if (!ret)                                                       \
+		break;                                                      \
+	    continue;                                                       \
+	}                                                                   \
+	ret = -ERESTARTSYS;                                                 \
+	break;                                                              \
+    }                                                                       \
+    current->state = TASK_RUNNING;                                          \
+    remove_wait_queue(&wq, &__wait);                                        \
+} while (0)
+#endif
+
+#ifndef wait_event_interruptible_timeout
+#define wait_event_interruptible_timeout(wq, condition, timeout)            \
+({                                                                          \
+    long __ret = timeout;                                                   \
+    if (!(condition))                                                       \
+	__wait_event_interruptible_timeout(wq, condition, __ret);           \
+    __ret;                                                                  \
+})
+#endif
+
+
+
+int rtnet_rtpc_dispatch_call(rtpc_proc proc, unsigned int timeout,
+			     void* priv_data, size_t priv_data_size,
+			     rtpc_copy_back_proc copy_back_handler,
+			     rtpc_cleanup_proc cleanup_handler)
+{
+    struct rt_proc_call *call;
+    unsigned long      context;
+    int                 ret;
+
+
+    call = kmalloc(sizeof(struct rt_proc_call) + priv_data_size, GFP_KERNEL);
+    if (call == NULL)
+	return -ENOMEM;
+
+    memcpy(call->priv_data, priv_data, priv_data_size);
+
+    call->processed       = 0;
+    call->proc            = proc;
+    call->result          = 0;
+    call->cleanup_handler = cleanup_handler;
+    atomic_set(&call->ref_count, 2);    /* dispatcher + rt-procedure */
+    init_waitqueue_head(&call->call_wq);
+
+    raw_spin_lock_irqsave(&pending_calls_lock, context);
+    list_add_tail(&call->list_entry, &pending_calls);
+    raw_spin_unlock_irqrestore(&pending_calls_lock, context);
+
+    rtdm_event_signal(&dispatch_event);
+
+    if (timeout > 0) {
+	ret = wait_event_interruptible_timeout(call->call_wq,
+	    call->processed, (timeout * HZ) / 1000);
+	if (ret == 0)
+	    ret = -ETIME;
+    } else
+	ret = wait_event_interruptible(call->call_wq, call->processed);
+
+    if (ret >= 0) {
+	if (copy_back_handler != NULL)
+	    copy_back_handler(call, priv_data);
+	ret = call->result;
+    }
+
+    if (atomic_dec_and_test(&call->ref_count)) {
+	if (call->cleanup_handler != NULL)
+	    call->cleanup_handler(&call->priv_data);
+	kfree(call);
+    }
+
+    return ret;
+}
+
+
+
+static inline struct rt_proc_call *rtpc_dequeue_pending_call(void)
+{
+    unsigned long      context;
+    struct rt_proc_call *call = NULL;
+
+
+    raw_spin_lock_irqsave(&pending_calls_lock, context);
+    if (!list_empty(&pending_calls)) {
+	call = (struct rt_proc_call *)pending_calls.next;
+	list_del(&call->list_entry);
+    }
+    raw_spin_unlock_irqrestore(&pending_calls_lock, context);
+
+    return call;
+}
+
+
+
+static inline void rtpc_queue_processed_call(struct rt_proc_call *call)
+{
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&processed_calls_lock, context);
+    list_add_tail(&call->list_entry, &processed_calls);
+    raw_spin_unlock_irqrestore(&processed_calls_lock, context);
+
+    rtdm_event_signal(&rtpc_nrt_signal_event);
+}
+
+
+
+static inline struct rt_proc_call *rtpc_dequeue_processed_call(void)
+{
+    unsigned long      context;
+    struct rt_proc_call *call = NULL;
+
+
+    raw_spin_lock_irqsave(&processed_calls_lock, context);
+    if (!list_empty(&processed_calls)) {
+	call = (struct rt_proc_call *)processed_calls.next;
+	list_del(&call->list_entry);
+    }
+    raw_spin_unlock_irqrestore(&processed_calls_lock, context);
+
+    return call;
+}
+
+
+
+static int rtpc_dispatch_handler(void *arg)
+{
+    struct rt_proc_call *call;
+    int                 ret;
+
+
+    while (!stop_dispatch_task) {
+	if (rtdm_event_wait(&dispatch_event) < 0)
+	    break;
+
+	while ((call = rtpc_dequeue_pending_call())) {
+	    ret = call->proc(call);
+	    if (ret != -CALL_PENDING)
+		rtpc_complete_call(call, ret);
+	}
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+
+
+static int rtpc_signal_handler(void *arg)
+{
+    struct rt_proc_call *call;
+
+    while(!stop_rtpc_nrt_signal_task) {
+	if (rtdm_event_wait(&rtpc_nrt_signal_event) < 0)
+        	break;
+    	while ((call = rtpc_dequeue_processed_call()) != NULL) {
+		call->processed = 1;
+		wake_up(&call->call_wq);
+
+		if (atomic_dec_and_test(&call->ref_count)) {
+	    	if (call->cleanup_handler != NULL)
+			call->cleanup_handler(&call->priv_data);
+	    	kfree(call);
+		}
+      	}
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+
+
+void rtnet_rtpc_complete_call(struct rt_proc_call *call, int result)
+{
+    call->result = result;
+    rtpc_queue_processed_call(call);
+}
+
+
+
+void rtnet_rtpc_complete_call_nrt(struct rt_proc_call *call, int result)
+{
+    RTNET_ASSERT(!rtdm_in_rt_context(),
+		 rtnet_rtpc_complete_call(call, result); return;);
+
+    call->processed = 1;
+    wake_up(&call->call_wq);
+
+    if (atomic_dec_and_test(&call->ref_count)) {
+	if (call->cleanup_handler != NULL)
+	    call->cleanup_handler(&call->priv_data);
+	kfree(call);
+    }
+}
+
+
+
+int __init rtpc_init(void)
+{
+    int ret = 0;
+    struct sched_param dispatch_task_param = { .sched_priority = (RTDM_TASK_LOWEST_PRIORITY+1) };
+
+    raw_spin_lock_init(&pending_calls_lock);
+    raw_spin_lock_init(&processed_calls_lock);
+
+    rtdm_event_init(&dispatch_event, 0);
+    rtdm_event_init(&rtpc_nrt_signal_event, 0);
+
+    stop_dispatch_task = 0;
+    dispatch_task = kthread_create(rtpc_dispatch_handler, NULL, "rtnet-rtpc");
+    if (!dispatch_task)
+	    goto dispatch_task_failed; 
+    sched_setscheduler(dispatch_task, SCHED_FIFO, &dispatch_task_param);
+    wake_up_process(dispatch_task);
+
+    stop_rtpc_nrt_signal_task = 0;
+    rtpc_nrt_signal_task = kthread_run(rtpc_signal_handler, NULL, "rtnet-rtpc_nrt_signal");
+    if(rtpc_nrt_signal_task)
+	    return 0;
+
+    stop_dispatch_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(dispatch_task);
+    /* release the task structure */
+    put_task_struct(dispatch_task);
+dispatch_task_failed:
+    rtdm_event_destroy(&dispatch_event);
+    rtdm_event_destroy(&rtpc_nrt_signal_event);
+
+    return ret;
+}
+
+
+
+void rtpc_cleanup(void)
+{
+    rtdm_event_destroy(&dispatch_event);
+    rtdm_event_destroy(&rtpc_nrt_signal_event);
+
+    stop_dispatch_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(dispatch_task);
+    /* release the task structure */
+    put_task_struct(dispatch_task);
+
+    stop_rtpc_nrt_signal_task = 1;
+    kthread_stop(rtpc_nrt_signal_task);
+    put_task_struct(rtpc_nrt_signal_task);
+}
+
+
+EXPORT_SYMBOL_GPL(rtnet_rtpc_dispatch_call);
+EXPORT_SYMBOL_GPL(rtnet_rtpc_complete_call);
+EXPORT_SYMBOL_GPL(rtnet_rtpc_complete_call_nrt);
diff -Naur a/net/rtnet/stack/rtskb.c b/net/rtnet/stack/rtskb.c
--- a/net/rtnet/stack/rtskb.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtskb.c	2021-07-14 15:39:13.342124745 +0300
@@ -0,0 +1,626 @@
+/***
+ *
+ *  stack/rtskb.c - rtskb implementation for rtnet
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@fet.uni-hannover.de>,
+ *  Copyright (C) 2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *  Copyright (C) 2006 Jorge Almeida <j-almeida@criticalsoftware.com>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/slab.h>
+#include <net/checksum.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtskb.h>
+#include <rtnet_port.h>
+
+static unsigned int global_rtskbs    = DEFAULT_GLOBAL_RTSKBS;
+module_param(global_rtskbs, uint, 0444);
+MODULE_PARM_DESC(global_rtskbs, "Number of realtime socket buffers in global pool");
+
+
+/* Linux slab pool for rtskbs */
+static struct kmem_cache *rtskb_slab_pool;
+
+/* pool of rtskbs for global use */
+struct rtskb_pool global_pool;
+EXPORT_SYMBOL_GPL(global_pool);
+
+/* pool statistics */
+unsigned int rtskb_pools=0;
+unsigned int rtskb_pools_max=0;
+unsigned int rtskb_amount=0;
+unsigned int rtskb_amount_max=0;
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+/* RTcap interface */
+rtdm_lock_t rtcap_lock;
+EXPORT_SYMBOL_GPL(rtcap_lock);
+
+void (*rtcap_handler)(struct rtskb *skb) = NULL;
+EXPORT_SYMBOL_GPL(rtcap_handler);
+#endif
+
+
+/***
+ *  rtskb_copy_and_csum_bits
+ */
+unsigned int rtskb_copy_and_csum_bits(const struct rtskb *skb, int offset,
+				      u8 *to, int len, unsigned int csum)
+{
+    int copy;
+
+    /* Copy header. */
+    if ((copy = skb->len-offset) > 0) {
+	if (copy > len)
+	    copy = len;
+	csum = csum_partial_copy_nocheck(skb->data+offset, to, copy, csum);
+	if ((len -= copy) == 0)
+	    return csum;
+	offset += copy;
+	to += copy;
+    }
+
+    RTNET_ASSERT(len == 0, );
+    return csum;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_copy_and_csum_bits);
+
+
+/***
+ *  rtskb_copy_and_csum_dev
+ */
+void rtskb_copy_and_csum_dev(const struct rtskb *skb, u8 *to)
+{
+    unsigned int csum;
+    unsigned int csstart;
+
+    if (skb->ip_summed == CHECKSUM_PARTIAL) {
+	csstart = skb->h.raw - skb->data;
+
+	if (csstart > skb->len)
+	    BUG();
+    } else
+	csstart = skb->len;
+
+    memcpy(to, skb->data, csstart);
+
+    csum = 0;
+    if (csstart != skb->len)
+	csum = rtskb_copy_and_csum_bits(skb, csstart, to+csstart, skb->len-csstart, 0);
+
+    if (skb->ip_summed == CHECKSUM_PARTIAL) {
+	unsigned int csstuff = csstart + skb->csum;
+
+	*((unsigned short *)(to + csstuff)) = csum_fold(csum);
+    }
+}
+
+EXPORT_SYMBOL_GPL(rtskb_copy_and_csum_dev);
+
+
+#ifdef CONFIG_RTNET_CHECKED
+/**
+ *  skb_over_panic - private function
+ *  @skb: buffer
+ *  @sz: size
+ *  @here: address
+ *
+ *  Out of line support code for rtskb_put(). Not user callable.
+ */
+void rtskb_over_panic(struct rtskb *skb, int sz, void *here)
+{
+    printk(KERN_ERR "RTnet: rtskb_put :over: %p:%d put:%d dev:%s\n", here,
+		skb->len, sz, (skb->rtdev) ? skb->rtdev->name : "<NULL>");
+}
+
+EXPORT_SYMBOL_GPL(rtskb_over_panic);
+
+
+/**
+ *  skb_under_panic - private function
+ *  @skb: buffer
+ *  @sz: size
+ *  @here: address
+ *
+ *  Out of line support code for rtskb_push(). Not user callable.
+ */
+void rtskb_under_panic(struct rtskb *skb, int sz, void *here)
+{
+    printk(KERN_ERR "RTnet: rtskb_push :under: %p:%d put:%d dev:%s\n", here,
+		skb->len, sz, (skb->rtdev) ? skb->rtdev->name : "<NULL>");
+}
+
+EXPORT_SYMBOL_GPL(rtskb_under_panic);
+#endif /* CONFIG_RTNET_CHECKED */
+
+static struct rtskb *__rtskb_pool_dequeue(struct rtskb_pool *pool)
+{
+    struct rtskb_queue *queue = &pool->queue;
+    struct rtskb *skb;
+
+    if (!pool->lock_ops->trylock(pool->lock_cookie))
+	    return NULL;
+    skb = __rtskb_dequeue(queue);
+    if (skb == NULL)
+	    pool->lock_ops->unlock(pool->lock_cookie);
+
+    return skb;
+}
+
+struct rtskb *rtskb_pool_dequeue(struct rtskb_pool *pool)
+{
+    struct rtskb_queue *queue = &pool->queue;
+    unsigned long context;
+    struct rtskb *skb;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    skb = __rtskb_pool_dequeue(pool);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+
+    return skb;
+}
+EXPORT_SYMBOL_GPL(rtskb_pool_dequeue);
+
+static void __rtskb_pool_queue_tail(struct rtskb_pool *pool, struct rtskb *skb)
+{
+    struct rtskb_queue *queue = &pool->queue;
+
+    if(queue == NULL) {
+	    trace_printk("queue is NULL\n");
+	    return;
+    }
+    __rtskb_queue_tail(queue,skb);
+    pool->lock_ops->unlock(pool->lock_cookie);
+}
+
+void rtskb_pool_queue_tail(struct rtskb_pool *pool, struct rtskb *skb)
+{
+    struct rtskb_queue *queue = &pool->queue;
+    unsigned long context;
+    
+    if(queue == NULL) {
+            trace_printk("queue is NULL\n");
+            return;
+    }
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    __rtskb_pool_queue_tail(pool, skb);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+}
+EXPORT_SYMBOL_GPL(rtskb_pool_queue_tail);
+
+/***
+ *  alloc_rtskb - allocate an rtskb from a pool
+ *  @size: required buffer size (to check against maximum boundary)
+ *  @pool: pool to take the rtskb from
+ */
+struct rtskb *alloc_rtskb(unsigned int size, struct rtskb_pool *pool)
+{
+    struct rtskb *skb;
+
+    RTNET_ASSERT(size <= SKB_DATA_ALIGN(RTSKB_SIZE), return NULL;);
+
+    skb = rtskb_pool_dequeue(pool);
+    if (!skb) {
+	trace_printk("rtskb_pool_dequeue returns null\n");
+	return NULL;
+    }
+
+    /* Load the data pointers. */
+    skb->head = skb->buf_start;
+    skb->data = skb->buf_start;
+    skb->tail = skb->buf_start;
+    skb->end  = skb->buf_start + size;
+    skb->mac.raw = skb->buf_start;
+
+    /* Set up other states */
+    skb->chain_end = skb;
+    skb->data_len = 0;
+    skb->len = 0;
+    skb->pkt_type = PACKET_HOST;
+    skb->xmit_stamp = NULL;
+    skb->ip_summed = CHECKSUM_NONE;
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+    skb->cap_flags = 0;
+#endif
+
+    return skb;
+}
+
+EXPORT_SYMBOL_GPL(alloc_rtskb);
+
+
+/***
+ *  kfree_rtskb
+ *  @skb    rtskb
+ */
+void kfree_rtskb(struct rtskb *skb)
+{
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+    unsigned long  context;
+    struct rtskb    *comp_skb;
+    struct rtskb    *next_skb;
+    struct rtskb    *chain_end;
+#endif
+
+
+    RTNET_ASSERT(skb != NULL, return;);
+    RTNET_ASSERT(skb->pool != NULL, return;);
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+    next_skb  = skb;
+    chain_end = skb->chain_end;
+
+    do {
+	skb      = next_skb;
+	next_skb = skb->next;
+
+	raw_spin_lock_irqsave(&rtcap_lock, context);
+
+	if (skb->cap_flags & RTSKB_CAP_SHARED) {
+	    skb->cap_flags &= ~RTSKB_CAP_SHARED;
+
+	    comp_skb  = skb->cap_comp_skb;
+	    skb->pool = xchg(&comp_skb->pool, skb->pool);
+
+	    raw_spin_unlock_irqrestore(&rtcap_lock, context);
+
+	    rtskb_pool_queue_tail(comp_skb->pool, comp_skb);
+	}
+	else {
+	    raw_spin_unlock_irqrestore(&rtcap_lock, context);
+
+	    skb->chain_end = skb;
+	    rtskb_pool_queue_tail(skb->pool, skb);
+	}
+
+    } while (chain_end != skb);
+
+#else  /* CONFIG_RTNET_ADDON_RTCAP */
+
+    rtskb_pool_queue_tail(skb->pool, skb);
+
+
+#endif /* CONFIG_RTNET_ADDON_RTCAP */
+}
+
+EXPORT_SYMBOL_GPL(kfree_rtskb);
+
+
+static int rtskb_nop_pool_trylock(void *cookie)
+{
+    return 1;
+}
+
+static void rtskb_nop_pool_unlock(void *cookie)
+{
+}
+
+static const struct rtskb_pool_lock_ops rtskb_nop_pool_lock_ops = {
+    .trylock = rtskb_nop_pool_trylock,
+    .unlock = rtskb_nop_pool_unlock,
+};
+
+
+/***
+ *  rtskb_pool_init
+ *  @pool: pool to be initialized
+ *  @initial_size: number of rtskbs to allocate
+ *  return: number of actually allocated rtskbs
+ */
+unsigned int rtskb_pool_init(struct rtskb_pool *pool,
+			    unsigned int initial_size,
+			    const struct rtskb_pool_lock_ops *lock_ops,
+			    void *lock_cookie)
+{
+    unsigned int i;
+
+    rtskb_queue_init(&pool->queue);
+
+    if ((i = rtskb_pool_extend(pool, initial_size)) != initial_size)
+	    printk(KERN_WARNING "%s rtskb_pool_extend returns %d != initial_size(%d)\n",
+			    __func__, i, initial_size);
+
+    rtskb_pools++;
+    if (rtskb_pools > rtskb_pools_max)
+	rtskb_pools_max = rtskb_pools;
+
+    pool->lock_ops = lock_ops ?: &rtskb_nop_pool_lock_ops;
+    pool->lock_cookie = lock_cookie;
+
+    return i;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_pool_init);
+
+static int rtskb_module_pool_trylock(void *cookie)
+{
+    int err = 1;
+    if (cookie)
+	err = try_module_get(cookie);
+    return err;
+}
+
+static void rtskb_module_pool_unlock(void *cookie)
+{
+    if (cookie)
+	module_put(cookie);
+}
+
+static const struct rtskb_pool_lock_ops rtskb_module_lock_ops = {
+    .trylock = rtskb_module_pool_trylock,
+    .unlock = rtskb_module_pool_unlock,
+};
+
+unsigned int __rtskb_module_pool_init(struct rtskb_pool *pool,
+				    unsigned int initial_size,
+				    struct module *module)
+{
+    return rtskb_pool_init(pool, initial_size, &rtskb_module_lock_ops, module);
+}
+EXPORT_SYMBOL_GPL(__rtskb_module_pool_init);
+
+
+/***
+ *  __rtskb_pool_release
+ *  @pool: pool to release
+ */
+void rtskb_pool_release(struct rtskb_pool *pool)
+{
+    struct rtskb *skb;
+
+    while ((skb = rtskb_dequeue(&pool->queue)) != NULL) {
+	rtdev_unmap_rtskb(skb);
+	kmem_cache_free(rtskb_slab_pool, skb);
+	rtskb_amount--;
+    }
+
+    rtskb_pools--;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_pool_release);
+
+
+unsigned int rtskb_pool_extend(struct rtskb_pool *pool,
+			       unsigned int add_rtskbs)
+{
+    unsigned int i;
+    struct rtskb *skb;
+
+
+    RTNET_ASSERT(pool != NULL, return -EINVAL;);
+
+    for (i = 0; i < add_rtskbs; i++) {
+	/* get rtskb from slab pool */
+	if (!(skb = kmem_cache_alloc(rtskb_slab_pool, GFP_KERNEL))) {
+	    printk(KERN_ERR "RTnet: rtskb allocation from slab pool failed\n");
+	    break;
+	}
+
+	/* fill the header with zero */
+	memset(skb, 0, sizeof(struct rtskb));
+
+	skb->chain_end = skb;
+	skb->pool = pool;
+	skb->buf_start = ((unsigned char *)skb) + ALIGN_RTSKB_STRUCT_LEN;
+#ifdef CONFIG_RTNET_CHECKED
+	skb->buf_end = skb->buf_start + SKB_DATA_ALIGN(RTSKB_SIZE) - 1;
+#endif
+
+	if (rtdev_map_rtskb(skb) < 0) {
+	    kmem_cache_free(rtskb_slab_pool, skb);
+	    break;
+	}
+
+	rtskb_queue_tail(&pool->queue, skb);
+
+	rtskb_amount++;
+	if (rtskb_amount > rtskb_amount_max)
+	    rtskb_amount_max = rtskb_amount;
+    }
+
+    return i;
+}
+
+
+unsigned int rtskb_pool_shrink(struct rtskb_pool *pool,
+			       unsigned int rem_rtskbs)
+{
+    unsigned int    i;
+    struct rtskb    *skb;
+
+
+    for (i = 0; i < rem_rtskbs; i++) {
+	if ((skb = rtskb_dequeue(&pool->queue)) == NULL)
+	    break;
+
+	rtdev_unmap_rtskb(skb);
+	kmem_cache_free(rtskb_slab_pool, skb);
+	rtskb_amount--;
+    }
+
+    return i;
+}
+
+
+/* Note: acquires only the first skb of a chain! */
+int rtskb_acquire(struct rtskb *rtskb, struct rtskb_pool *comp_pool)
+{
+    struct rtskb *comp_rtskb;
+    struct rtskb_pool *release_pool;
+    unsigned long context;
+
+
+    raw_spin_lock_irqsave(&comp_pool->queue.lock, context);
+
+    comp_rtskb = __rtskb_pool_dequeue(comp_pool);
+    if (!comp_rtskb) {
+	raw_spin_unlock_irqrestore(&comp_pool->queue.lock, context);
+	return -ENOMEM;
+    }
+
+    raw_spin_unlock(&comp_pool->queue.lock);
+
+    comp_rtskb->chain_end = comp_rtskb;
+    comp_rtskb->pool = release_pool = rtskb->pool;
+
+    raw_spin_lock(&release_pool->queue.lock);
+
+    __rtskb_pool_queue_tail(release_pool, comp_rtskb);
+
+    raw_spin_unlock_irqrestore(&release_pool->queue.lock, context);
+
+    rtskb->pool = comp_pool;
+
+    return 0;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_acquire);
+
+
+/* clone rtskb to another, allocating the new rtskb from pool */
+struct rtskb* rtskb_clone(struct rtskb *rtskb, struct rtskb_pool *pool)
+{
+    struct rtskb    *clone_rtskb;
+    unsigned int    total_len;
+
+    clone_rtskb = alloc_rtskb(rtskb->end - rtskb->buf_start, pool);
+    if (clone_rtskb == NULL) {
+	trace_printk("clone_rtskb is NULL\n");
+	return NULL;
+    }
+
+    if(!rtskb->mac.raw) {
+	printk(KERN_ERR "%s rtskb->mac.raw is NULL\n", __func__);
+	return NULL;
+    }
+
+    /* Note: We don't clone
+	- rtskb.sk
+	- rtskb.xmit_stamp
+       until real use cases show up. */
+
+    clone_rtskb->priority   = rtskb->priority;
+    clone_rtskb->rtdev      = rtskb->rtdev;
+    clone_rtskb->time_stamp = rtskb->time_stamp;
+
+    clone_rtskb->mac.raw    = clone_rtskb->buf_start;
+    clone_rtskb->nh.raw     = clone_rtskb->buf_start;
+    clone_rtskb->h.raw      = clone_rtskb->buf_start;
+
+    clone_rtskb->data       += rtskb->data - rtskb->buf_start;
+    clone_rtskb->tail       += rtskb->tail - rtskb->buf_start;
+    clone_rtskb->mac.raw    += rtskb->mac.raw - rtskb->buf_start;
+    clone_rtskb->nh.raw     += rtskb->nh.raw - rtskb->buf_start;
+    clone_rtskb->h.raw      += rtskb->h.raw - rtskb->buf_start;
+
+    clone_rtskb->protocol   = rtskb->protocol;
+    clone_rtskb->pkt_type   = rtskb->pkt_type;
+
+    clone_rtskb->ip_summed  = rtskb->ip_summed;
+    clone_rtskb->csum       = rtskb->csum;
+
+    total_len = rtskb->len + rtskb->data - rtskb->mac.raw;
+    memcpy(clone_rtskb->mac.raw, rtskb->mac.raw, total_len);
+    clone_rtskb->len = rtskb->len;
+
+    return clone_rtskb;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_clone);
+
+
+struct rtskb* rtskb_realloc_headroom(struct rtnet_device *rtdev, struct rtskb *skb, unsigned int headroom)
+{
+        struct rtskb *skb2 = NULL;
+        int delta = headroom - rtskb_headroom(skb);
+        int aligned_delta = SKB_DATA_ALIGN(delta);
+        int max_size = (SKB_DATA_ALIGN(RTSKB_SIZE) + ALIGN_RTSKB_CONTROL_BLOCK_SIZE);
+        int size = skb->end - skb->head + aligned_delta;
+
+        RTNET_ASSERT(skb != NULL, return NULL;);
+        RTNET_ASSERT(size <= max_size, return NULL;);
+
+        if(size > max_size) {
+                trace_printk("size=%d <= max_size=%d\n", size, max_size);
+                return NULL;
+        }
+
+        if (delta <= 0) {
+                trace_printk("delta=%d <= 0\n", delta);
+                return NULL;
+        } else {
+                skb2 = rtskb_clone(skb, &rtdev->dev_pool);
+                if (skb2) {
+                        skb2->data += aligned_delta;
+                        memset(skb2->head, 0, aligned_delta);
+                        memcpy(skb2->head + aligned_delta, skb->head, skb->tail - skb->head);
+                        skb2->tail += aligned_delta;
+                        skb2->end += aligned_delta;
+                }
+        }
+
+        return skb2;
+}
+
+
+int rtskb_pools_init(void)
+{
+    rtskb_slab_pool = kmem_cache_create("rtskb_slab_pool",
+	ALIGN_RTSKB_STRUCT_LEN + SKB_DATA_ALIGN(RTSKB_SIZE) + ALIGN_RTSKB_CONTROL_BLOCK_SIZE,
+	0, SLAB_HWCACHE_ALIGN, NULL);
+    if (rtskb_slab_pool == NULL)
+	return -ENOMEM;
+
+    /* reset the statistics (cache is accounted separately) */
+    rtskb_pools      = 0;
+    rtskb_pools_max  = 0;
+    rtskb_amount     = 0;
+    rtskb_amount_max = 0;
+
+    /* create the global rtskb pool */
+    if (rtskb_module_pool_init(&global_pool, global_rtskbs) < global_rtskbs)
+	goto err_out;
+
+#if IS_ENABLED(CONFIG_RTNET_ADDON_RTCAP)
+    rtdm_lock_init(&rtcap_lock);
+#endif
+
+    return 0;
+
+err_out:
+    rtskb_pool_release(&global_pool);
+    kmem_cache_destroy(rtskb_slab_pool);
+
+    return -ENOMEM;
+}
+
+
+void rtskb_pools_release(void)
+{
+    rtskb_pool_release(&global_pool);
+    kmem_cache_destroy(rtskb_slab_pool);
+}
diff -Naur a/net/rtnet/stack/rtwlan.c b/net/rtnet/stack/rtwlan.c
--- a/net/rtnet/stack/rtwlan.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/rtwlan.c	2021-07-14 15:39:13.334124801 +0300
@@ -0,0 +1,222 @@
+/* rtwlan.c
+ *
+ * rtwlan protocol stack
+ * Copyright (c) 2006, Daniel Gregorek <dxg@gmx.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+
+#include <rtnet_port.h>
+
+#include <rtwlan.h>
+
+int rtwlan_rx(struct rtskb * rtskb, struct rtnet_device * rtnet_dev)
+{
+    struct ieee80211_hdr  * hdr = (struct ieee80211_hdr *)rtskb->data;
+    u16 fc = le16_to_cpu(hdr->frame_ctl);
+
+    /* strip rtwlan header */
+    rtskb_pull(rtskb, ieee80211_get_hdrlen(fc));
+    rtskb->protocol = rt_eth_type_trans (rtskb, rtnet_dev);
+
+    /* forward rtskb to rtnet */
+    rtnetif_rx(rtskb);
+
+    return 0;
+}
+
+EXPORT_SYMBOL_GPL(rtwlan_rx);
+
+
+int rtwlan_tx(struct rtskb *rtskb, struct rtnet_device *rtnet_dev)
+{
+    struct rtwlan_device * rtwlan_dev = rtnetdev_priv(rtnet_dev);
+    struct ieee80211_hdr_3addr header = {	/* Ensure zero initialized */
+	.duration_id = 0,
+	.seq_ctl = 0
+    };
+    int ret;
+    u8 dest[ETH_ALEN], src[ETH_ALEN];
+
+    /* Get source and destination addresses */
+
+    memcpy(src, rtskb->data + ETH_ALEN, ETH_ALEN);
+
+    if(rtwlan_dev->mode == RTWLAN_TXMODE_MCAST) {
+	memcpy(dest, rtnet_dev->dev_addr, ETH_ALEN);
+	dest[0] |= 0x01;
+    } else {
+	memcpy(dest, rtskb->data, ETH_ALEN);
+    }
+
+    /*
+     * Generate ieee80211 compatible header
+     */
+    memcpy(header.addr3, src, ETH_ALEN);	/* BSSID */
+    memcpy(header.addr2, src, ETH_ALEN);	/* SA */
+    memcpy(header.addr1, dest, ETH_ALEN);	/* DA */
+
+    /* Write frame control field */
+    header.frame_ctl = cpu_to_le16(IEEE80211_FTYPE_DATA | IEEE80211_STYPE_DATA);
+
+    memcpy(rtskb_push(rtskb, IEEE80211_3ADDR_LEN), &header, IEEE80211_3ADDR_LEN);
+
+    ret = (*rtwlan_dev->hard_start_xmit)(rtskb,rtnet_dev);
+
+    return ret;
+}
+
+EXPORT_SYMBOL_GPL(rtwlan_tx);
+
+
+/**
+ * rtalloc_wlandev - Allocates and sets up a wlan device
+ * @sizeof_priv: size of additional driver-private structure to
+ *               be allocated for this wlan device
+ *
+ * Fill in the fields of the device structure with wlan-generic
+ * values. Basically does everything except registering the device.
+ *
+ * A 32-byte alignment is enforced for the private data area.
+ */
+
+struct rtnet_device *rtwlan_alloc_dev(unsigned sizeof_priv, unsigned dev_pool_size)
+{
+    struct rtnet_device *rtnet_dev;
+
+    RTWLAN_DEBUG("Start.\n");
+
+    rtnet_dev = rt_alloc_etherdev(sizeof(struct rtwlan_device) + sizeof_priv,
+			    dev_pool_size);
+    if (!rtnet_dev)
+	return NULL;
+
+    rtnet_dev->hard_start_xmit = rtwlan_tx;
+
+    rtdev_alloc_name(rtnet_dev, "rtwlan%d");
+
+    return rtnet_dev;
+}
+
+EXPORT_SYMBOL_GPL(rtwlan_alloc_dev);
+
+
+int rtwlan_ioctl(struct rtnet_device * rtdev,
+		 unsigned int request,
+		 unsigned long arg)
+{
+    struct rtwlan_cmd cmd;
+    struct ifreq ifr;
+    int ret=0;
+
+    if (copy_from_user(&cmd, (void *)arg, sizeof(cmd)) != 0)
+	return -EFAULT;
+
+    /*
+     * FIXME: proper do_ioctl() should expect a __user pointer
+     * arg. This only works with the existing WLAN support because the
+     * only driver currently providing this feature is broken, not
+     * doing the copy_to/from_user dance.
+     */
+    memset(&ifr, 0, sizeof(ifr));
+    ifr.ifr_data = &cmd;
+   
+    switch(request) {
+    case IOC_RTWLAN_IFINFO:
+	if (cmd.args.info.ifindex > 0)
+	    rtdev = rtdev_get_by_index(cmd.args.info.ifindex);
+	else
+	    rtdev = rtdev_get_by_name(cmd.head.if_name);
+	if (rtdev == NULL)
+	    return -ENODEV;
+
+	if (mutex_lock_interruptible(&rtdev->nrt_lock)) {
+	    rtdev_dereference(rtdev);
+	    return -ERESTARTSYS;
+	}
+
+	if (rtdev->do_ioctl)
+	    ret = rtdev->do_ioctl(rtdev, &ifr, request);
+	else
+	    ret = -ENORTWLANDEV;
+
+	memcpy(cmd.head.if_name, rtdev->name, IFNAMSIZ);
+	cmd.args.info.ifindex      = rtdev->ifindex;
+	cmd.args.info.flags        = rtdev->flags;
+
+	mutex_unlock(&rtdev->nrt_lock);
+
+	rtdev_dereference(rtdev);
+
+	break;
+
+    case IOC_RTWLAN_TXMODE:
+    case IOC_RTWLAN_BITRATE:
+    case IOC_RTWLAN_CHANNEL:
+    case IOC_RTWLAN_RETRY:
+    case IOC_RTWLAN_TXPOWER:
+    case IOC_RTWLAN_AUTORESP:
+    case IOC_RTWLAN_DROPBCAST:
+    case IOC_RTWLAN_DROPMCAST:
+    case IOC_RTWLAN_REGREAD:
+    case IOC_RTWLAN_REGWRITE:
+    case IOC_RTWLAN_BBPWRITE:
+    case IOC_RTWLAN_BBPREAD:
+    case IOC_RTWLAN_BBPSENS:
+            if (mutex_lock_interruptible(&rtdev->nrt_lock))
+	        return -ERESTARTSYS;
+
+	    if (rtdev->do_ioctl)
+	        ret = rtdev->do_ioctl(rtdev, &ifr, request);
+	    else
+		ret = -ENORTWLANDEV;
+
+	    mutex_unlock(&rtdev->nrt_lock);
+
+	    break;
+
+    default:
+	ret = -ENOTTY;
+    }
+
+    if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+	return -EFAULT;
+
+    return ret;
+}
+
+
+struct rtnet_ioctls rtnet_wlan_ioctls = {
+    service_name: "rtwlan ioctl",
+    ioctl_type: RTNET_IOC_TYPE_RTWLAN,
+    handler: rtwlan_ioctl
+};
+
+int __init rtwlan_init(void)
+{
+    if (rtnet_register_ioctls(&rtnet_wlan_ioctls))
+	rtdm_printk(KERN_ERR "Failed to register rtnet_wlan_ioctl!\n");
+
+    return 0;
+}
+
+
+void rtwlan_exit(void)
+{
+    rtnet_unregister_ioctls(&rtnet_wlan_ioctls);
+}
diff -Naur a/net/rtnet/stack/socket.c b/net/rtnet/stack/socket.c
--- a/net/rtnet/stack/socket.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/socket.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,494 @@
+/***
+ *
+ *  stack/socket.c - sockets implementation for rtnet
+ *
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/spinlock.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/err.h>
+#include <linux/ip.h>
+#include <linux/tcp.h>
+#include <linux/wait.h>
+#include <linux/semaphore.h>
+#include <asm/bitops.h>
+
+#include <rtdm_net.h>
+#include <rtnet_internal.h>
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+#include <ipv4/protocol.h>
+
+
+#define SKB_POOL_CLOSED     0
+
+static raw_spinlock_t fdtree_lock;
+static unsigned char fdtree_lock_initialized;
+
+static unsigned int socket_rtskbs = DEFAULT_SOCKET_RTSKBS;
+module_param(socket_rtskbs, uint, 0444);
+MODULE_PARM_DESC(socket_rtskbs, "Default number of realtime socket buffers in socket pools");
+
+
+/************************************************************************
+ *  internal socket functions                                           *
+ ************************************************************************/
+
+int __rt_bare_socket_init_icmp(struct rtsocket *sock, unsigned short protocol,
+                        unsigned int priority, unsigned int pool_size,
+                        struct module *module)
+{
+    int err;
+
+    err = try_module_get(module);
+    if (!err)
+        return -EAFNOSUPPORT;
+
+    err = rtskb_pool_init(&sock->skb_pool, pool_size, NULL, sock);
+    if (err < 0) {
+        module_put(module);
+        return err;
+    }
+
+    sock->protocol = protocol;
+    sock->priority = priority;
+    sock->owner = module;
+
+    return err;
+}
+EXPORT_SYMBOL_GPL(__rt_bare_socket_init_icmp);
+
+int __rt_bare_socket_init(struct rtsocket *sock, unsigned short protocol,
+			unsigned int priority, unsigned int pool_size,
+			struct module *module)
+{
+    int err;
+
+    err = try_module_get(module);
+    if (!err)
+	return -EAFNOSUPPORT;
+
+    err = rtskb_pool_init(&sock->skb_pool, pool_size, NULL, sock);
+    if (err < 0) {
+	module_put(module);
+	return err;
+    }
+
+    sock->protocol = protocol;
+    sock->priority = priority;
+    sock->owner = module;
+
+    return err;
+}
+EXPORT_SYMBOL_GPL(__rt_bare_socket_init);
+
+/***
+ *  rt_socket_init - initialises a new socket structure
+ */
+int __rt_socket_init(struct rtsocket *sock, unsigned short protocol,
+		struct module *module)
+{
+    unsigned int    pool_size;
+
+    sock->fd_refs = 1;
+    if(!fdtree_lock_initialized) {
+	raw_spin_lock_init(&fdtree_lock);
+	fdtree_lock_initialized = 1;
+    }
+
+    init_waitqueue_head_rtnet(&sock->wq_head_rtnet);
+
+    sock->flags = 0;
+    sock->callback_func = NULL;
+
+    rtskb_queue_init(&sock->incoming);
+
+    sock->timeout = 0;
+
+    raw_spin_lock_init(&sock->param_lock);
+    sema_init(&sock->pending_sem, 0);
+
+    pool_size = __rt_bare_socket_init(sock, protocol,
+				    RTSKB_PRIO_VALUE(SOCK_DEF_PRIO,
+						    RTSKB_DEF_RT_CHANNEL),
+				    socket_rtskbs, module);
+    sock->pool_size = pool_size;
+    mutex_init(&sock->pool_nrt_lock);
+
+    if (pool_size < socket_rtskbs) {
+	/* fix statistics */
+	if (pool_size == 0)
+	    rtskb_pools--;
+
+	rt_socket_cleanup(sock);
+	return -ENOMEM;
+    }
+
+    return 0;
+}
+EXPORT_SYMBOL_GPL(__rt_socket_init);
+
+
+/***
+ *  rt_socket_cleanup - releases resources allocated for the socket
+ */
+void rt_socket_cleanup(struct rtsocket *sock)
+{
+    rtdm_sem_destroy(&sock->pending_sem);
+
+    mutex_lock(&sock->pool_nrt_lock);
+
+    set_bit(SKB_POOL_CLOSED, &sock->flags);
+
+    if (sock->pool_size > 0)
+	rtskb_pool_release(&sock->skb_pool);
+
+    mutex_unlock(&sock->pool_nrt_lock);
+
+    module_put(sock->owner);
+}
+EXPORT_SYMBOL_GPL(rt_socket_cleanup);
+
+
+
+/***
+ *  rt_socket_common_ioctl
+ */
+int rt_socket_common_ioctl(struct rtsocket *sock, int request, void __user *arg)
+{
+    int                     ret = 0;
+    const unsigned int *val;
+    unsigned int _val;
+    const nanosecs_rel_t *timeout;
+    nanosecs_rel_t _timeout;
+#if 0
+    struct rtnet_callback   *callback;
+    unsigned long          context;
+#endif
+
+    switch (request) {
+	case RTNET_RTIOC_XMITPARAMS:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(_val), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+		sock->priority = *val;
+		break;
+
+	case RTNET_RTIOC_TIMEOUT:
+		timeout = rtnet_get_arg(sock, &_timeout, arg, sizeof(_timeout), 1);
+		if (IS_ERR(timeout))
+			return PTR_ERR(timeout);
+		sock->timeout = *timeout;
+		break;
+
+	case RTNET_RTIOC_CALLBACK:
+#if 0
+	    if (rtdm_fd_is_user(fd))
+		return -EACCES;
+#endif
+	    return -EACCES;
+#if 0
+	    raw_spin_lock_irqsave(&sock->param_lock, context);
+
+	    callback = arg;
+	    sock->callback_func = callback->func;
+	    sock->callback_arg  = callback->arg;
+
+	    raw_spin_unlock_irqrestore(&sock->param_lock, context);
+	    break;
+#endif
+
+	case RTNET_RTIOC_EXTPOOL:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(_val), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+
+		if (rtdm_in_rt_context())
+			return -ENOSYS;
+
+		mutex_lock(&sock->pool_nrt_lock);
+
+		if (test_bit(SKB_POOL_CLOSED, &sock->flags)) {
+			mutex_unlock(&sock->pool_nrt_lock);
+			return -EBADF;
+		}
+		ret = rtskb_pool_extend(&sock->skb_pool, *val);
+		sock->pool_size += ret;
+
+		mutex_unlock(&sock->pool_nrt_lock);
+
+		if (ret == 0 && *val > 0)
+			ret = -ENOMEM;
+
+		break;
+
+	case RTNET_RTIOC_SHRPOOL:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(_val), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+
+		if (rtdm_in_rt_context())
+			return -ENOSYS;
+
+		mutex_lock(&sock->pool_nrt_lock);
+
+		ret = rtskb_pool_shrink(&sock->skb_pool, *val);
+		sock->pool_size -= ret;
+
+		mutex_unlock(&sock->pool_nrt_lock);
+
+		if (ret == 0 && *val > 0)
+			ret = -EBUSY;
+
+		break;
+
+	default:
+	    ret = -EOPNOTSUPP;
+	    break;
+    }
+
+    return ret;
+}
+EXPORT_SYMBOL_GPL(rt_socket_common_ioctl);
+
+
+
+/***
+ *  rt_socket_if_ioctl
+ */
+int rt_socket_if_ioctl(struct rtsocket *sock, int request, void __user *arg)
+{
+    struct rtnet_device *rtdev;
+    struct ifreq _ifr, *ifr, *u_ifr;
+    struct sockaddr_in  _sin;
+    struct ifconf _ifc, *ifc, *u_ifc;
+    int ret = 0, size = 0, i;
+    short flags;
+
+
+    if (request == SIOCGIFCONF) {
+	u_ifc = arg;
+	ifc = rtnet_get_arg(sock, &_ifc, u_ifc, sizeof(_ifc), 1);
+	if (IS_ERR(ifc))
+		return PTR_ERR(ifc);
+
+	for (u_ifr = ifc->ifc_req, i = 1; i <= MAX_RT_DEVICES; i++, u_ifr++) {
+		rtdev = rtdev_get_by_index(i);
+		if (rtdev == NULL)
+			continue;
+
+		if ((rtdev->flags & IFF_UP) == 0) {
+			rtdev_dereference(rtdev);
+			continue;
+		}
+
+		size += sizeof(struct ifreq);
+		if (size > ifc->ifc_len) {
+			rtdev_dereference(rtdev);
+			size = ifc->ifc_len;
+			break;
+		}
+
+		ret = rtnet_put_arg(sock, u_ifr->ifr_name, rtdev->name, IFNAMSIZ, 1);
+		if (ret == 0) {
+			memset(&_sin, 0, sizeof(_sin));
+			_sin.sin_family      = AF_INET;
+			_sin.sin_addr.s_addr = rtdev->local_ip;
+			ret = rtnet_put_arg(sock, &u_ifr->ifr_addr, &_sin, sizeof(_sin), 1);
+		}
+		
+		rtdev_dereference(rtdev);
+		if (ret)
+			return ret;
+	}
+
+	return rtnet_put_arg(sock, &u_ifc->ifc_len, &size, sizeof(size), 1);
+    }
+
+    u_ifr = arg;
+    ifr = rtnet_get_arg(sock, &_ifr, u_ifr, sizeof(_ifr), 1);
+    if (IS_ERR(ifr))
+	    return PTR_ERR(ifr);
+
+    if (request == SIOCGIFNAME) {
+        rtdev = rtdev_get_by_index(ifr->ifr_ifindex);
+        if (rtdev == NULL)
+            return -ENODEV;
+	ret = rtnet_put_arg(sock, u_ifr->ifr_name, rtdev->name, IFNAMSIZ, 1);
+	goto out;
+    }
+
+    rtdev = rtdev_get_by_name(ifr->ifr_name);
+    if (rtdev == NULL)
+	    return -ENODEV;
+
+    switch (request) {
+	case SIOCGIFINDEX:
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_ifindex, &rtdev->ifindex,
+				    sizeof(u_ifr->ifr_ifindex), 1);
+		break;
+
+	case SIOCGIFFLAGS:
+		flags = rtdev->flags;
+		if ((ifr->ifr_flags & IFF_UP)
+		    && (rtdev->link_state
+			& (RTNET_LINK_STATE_PRESENT
+			   | RTNET_LINK_STATE_NOCARRIER))
+                    == RTNET_LINK_STATE_PRESENT)
+			flags |= IFF_RUNNING;
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_flags, &flags,
+				    sizeof(u_ifr->ifr_flags), 1);
+		break;
+
+	case SIOCGIFHWADDR:
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_hwaddr.sa_data,
+				    rtdev->dev_addr, rtdev->addr_len, 1);
+		if (!ret)
+			ret = rtnet_put_arg(sock, &u_ifr->ifr_hwaddr.sa_family,
+					    &rtdev->type, sizeof(u_ifr->ifr_hwaddr.sa_family), 1);
+		break;
+
+	case SIOCGIFADDR:
+		memset(&_sin, 0, sizeof(_sin));
+		_sin.sin_family      = AF_INET;
+		_sin.sin_addr.s_addr = rtdev->local_ip;
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_addr, &_sin, sizeof(_sin), 1);
+		break;
+
+	case SIOCETHTOOL:
+		if (rtdev->do_ioctl != NULL) {
+			if (rtdm_in_rt_context())
+				return -ENOSYS;
+			ret = rtdev->do_ioctl(rtdev, ifr, request);
+		} else
+			ret = -EOPNOTSUPP;
+		break;
+
+	case SIOCDEVPRIVATE ... SIOCDEVPRIVATE + 15:
+		if (rtdev->do_ioctl != NULL)
+			ret = rtdev->do_ioctl(rtdev, ifr, request);
+		else
+			ret = -EOPNOTSUPP;
+		break;
+
+	default:
+		ret = -EOPNOTSUPP;
+		break;
+    }
+
+  out:
+    rtdev_dereference(rtdev);
+    return ret;
+}
+EXPORT_SYMBOL_GPL(rt_socket_if_ioctl);
+
+#if 0
+/* no select on the socket for the moment */
+int rt_socket_select_bind(struct rtdm_fd *fd,
+			  rtdm_selector_t *selector,
+			  enum rtdm_selecttype type,
+			  unsigned fd_index)
+{
+    struct rtsocket *sock = rtdm_fd_to_private(fd);
+
+    switch (type) {
+	case XNSELECT_READ:
+	    return rtdm_sem_select(&sock->pending_sem, selector,
+				XNSELECT_READ, fd_index);
+	default:
+	    return -EBADF;
+    }
+
+    return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(rt_socket_select_bind);
+#endif
+
+void *rtnet_get_arg(struct rtsocket *sock, void *tmp, const void *src, size_t len, int msg_in_userspace)
+{
+	int ret;
+	
+	if (!msg_in_userspace)
+		return (void *)src;
+
+	ret = copy_from_user(tmp, src, len);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return tmp;
+}
+EXPORT_SYMBOL_GPL(rtnet_get_arg);
+
+int rtnet_put_arg(struct rtsocket *sock, void *dst, const void *src, size_t len, int msg_in_userspace)
+{
+	if (!msg_in_userspace) {
+		if (dst != src)
+			memcpy(dst, src, len);
+		return 0;
+	}
+
+	return copy_to_user(dst, src, len);
+}
+EXPORT_SYMBOL_GPL(rtnet_put_arg);
+
+int rtdm_fd_lock(struct rtsocket *sock)
+{
+	unsigned long context;
+
+	/* xenomai-3/kernel/cobalt/rtdm/fd.c */
+
+        raw_spin_lock_irqsave(&fdtree_lock, context);
+        if (sock->fd_refs == 0) {
+                raw_spin_unlock_irqrestore(&fdtree_lock, context);
+                return -EIDRM;
+        }
+        ++sock->fd_refs;
+        raw_spin_unlock_irqrestore(&fdtree_lock, context);
+
+        return 0;
+
+}
+EXPORT_SYMBOL_GPL(rtdm_fd_lock);
+
+int rtdm_fd_unlock(struct rtsocket *sock)
+{
+        unsigned long context;
+
+	/* xenomai-3/kernel/cobalt/rtdm/fd.c */
+        raw_spin_lock_irqsave(&fdtree_lock, context);
+
+	if(sock->fd_refs <= 0)
+		printk(KERN_WARNING "%s sock->fd=%d sock->fd_refs=%d\n", __func__, sock->fd, sock->fd_refs);
+	
+	--sock->fd_refs;
+	
+	raw_spin_unlock_irqrestore(&fdtree_lock, context);
+	
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rtdm_fd_unlock);
diff -Naur a/net/rtnet/stack/stack_mgr.c b/net/rtnet/stack/stack_mgr.c
--- a/net/rtnet/stack/stack_mgr.c	1970-01-01 02:00:00.000000000 +0200
+++ b/net/rtnet/stack/stack_mgr.c	2021-07-14 15:39:13.346124717 +0300
@@ -0,0 +1,295 @@
+/***
+ *
+ *  stack/stack_mgr.c - Stack-Manager
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *  Copyright (C) 2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *  Copyright (C) 2006 Jorge Almeida <j-almeida@criticalsoftware.com>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <uapi/linux/sched/types.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtskb_fifo.h>
+#include <stack_mgr.h>
+
+static unsigned int stack_mgr_prio = RTNET_DEF_STACK_PRIORITY;
+static u8 stop_rt_stack_mgr_task = 0;
+module_param(stack_mgr_prio, uint, 0444);
+MODULE_PARM_DESC(stack_mgr_prio, "Priority of the stack manager task");
+
+
+#if (CONFIG_RTNET_RX_FIFO_SIZE & (CONFIG_RTNET_RX_FIFO_SIZE-1)) != 0
+#error CONFIG_RTNET_RX_FIFO_SIZE must be power of 2!
+#endif
+static DECLARE_RTSKB_FIFO(rx, CONFIG_RTNET_RX_FIFO_SIZE);
+
+struct list_head    rt_packets[RTPACKET_HASH_TBL_SIZE];
+#ifdef CONFIG_RTNET_ETH_P_ALL
+struct list_head    rt_packets_all;
+#endif /* CONFIG_RTNET_ETH_P_ALL */
+raw_spinlock_t rt_packets_lock;
+
+/***
+ *  rtdev_add_pack:         add protocol (Layer 3)
+ *  @pt:                    the new protocol
+ */
+int __rtdev_add_pack(struct rtpacket_type *pt, struct module *module)
+{
+    int                     ret = 0;
+    unsigned long          context;
+
+    INIT_LIST_HEAD(&pt->list_entry);
+    pt->refcount = 0;
+    if (pt->trylock == NULL)
+	pt->trylock = rtdev_lock_pack;
+    if (pt->unlock == NULL)
+	pt->unlock = rtdev_unlock_pack;
+    pt->owner = module;
+
+    raw_spin_lock_irqsave(&rt_packets_lock, context);
+
+    if (pt->type == htons(ETH_P_ALL))
+#ifdef CONFIG_RTNET_ETH_P_ALL
+	list_add_tail(&pt->list_entry, &rt_packets_all);
+#else /* !CONFIG_RTNET_ETH_P_ALL */
+	ret = -EINVAL;
+#endif /* CONFIG_RTNET_ETH_P_ALL */
+    else
+	list_add_tail(&pt->list_entry,
+		      &rt_packets[ntohs(pt->type) & RTPACKET_HASH_KEY_MASK]);
+
+    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+    return ret;
+}
+
+EXPORT_SYMBOL_GPL(__rtdev_add_pack);
+
+
+/***
+ *  rtdev_remove_pack:  remove protocol (Layer 3)
+ *  @pt:                protocol
+ */
+void rtdev_remove_pack(struct rtpacket_type *pt)
+{
+    unsigned long  context;
+
+
+    RTNET_ASSERT(pt != NULL, return;);
+
+    raw_spin_lock_irqsave(&rt_packets_lock, context);
+    list_del(&pt->list_entry);
+    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+}
+
+EXPORT_SYMBOL_GPL(rtdev_remove_pack);
+
+
+/***
+ *  rtnetif_rx: will be called from the driver interrupt handler
+ *  (IRQs disabled!) and send a message to rtdev-owned stack-manager
+ *
+ *  @skb - the packet
+ */
+void rtnetif_rx(struct rtskb *skb)
+{
+    RTNET_ASSERT(skb != NULL, return;);
+    RTNET_ASSERT(skb->rtdev != NULL, return;);
+
+    if (unlikely(rtskb_fifo_insert_inirq(&rx.fifo, skb) < 0)) {
+	printk(KERN_WARNING "RTnet: dropping packet in %s()\n", __FUNCTION__);
+	kfree_rtskb(skb);
+    }
+}
+
+EXPORT_SYMBOL_GPL(rtnetif_rx);
+
+
+#if IS_ENABLED(CONFIG_RTNET_DRV_LOOPBACK)
+#define __DELIVER_PREFIX
+#else /* !CONFIG_RTNET_DRV_LOOPBACK */
+#define __DELIVER_PREFIX static inline
+#endif /* CONFIG_RTNET_DRV_LOOPBACK */
+
+__DELIVER_PREFIX void rt_stack_deliver(struct rtskb *rtskb)
+{
+    unsigned short          hash;
+    struct rtpacket_type    *pt_entry;
+    unsigned long          context;
+    struct rtnet_device     *rtdev = rtskb->rtdev;
+    int                     err;
+    int                     eth_p_all_hit = 0;
+
+
+    rtcap_report_incoming(rtskb);
+
+    rtskb->nh.raw = rtskb->data;
+
+    raw_spin_lock_irqsave(&rt_packets_lock, context);
+
+#ifdef CONFIG_RTNET_ETH_P_ALL
+    eth_p_all_hit = 0;
+    list_for_each_entry(pt_entry, &rt_packets_all, list_entry) {
+	if (!pt_entry->trylock(pt_entry))
+	    continue;
+	raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+	pt_entry->handler(rtskb, pt_entry);
+
+	raw_spin_lock_irqsave(&rt_packets_lock, context);
+	pt_entry->unlock(pt_entry);
+	eth_p_all_hit = 1;
+    }
+#endif /* CONFIG_RTNET_ETH_P_ALL */
+
+    hash = ntohs(rtskb->protocol) & RTPACKET_HASH_KEY_MASK;
+
+    list_for_each_entry(pt_entry, &rt_packets[hash], list_entry)
+	if (pt_entry->type == rtskb->protocol) {
+	    if (!pt_entry->trylock(pt_entry)) 
+		continue;
+ 
+	    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+	    err = pt_entry->handler(rtskb, pt_entry);
+
+	    raw_spin_lock_irqsave(&rt_packets_lock, context);
+	    pt_entry->unlock(pt_entry);
+
+	    if (likely(!err)) {
+		raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+		return;
+	    }
+	}
+
+    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+    /* Don't warn if ETH_P_ALL listener were present or when running in
+       promiscuous mode (RTcap). */
+    if (unlikely(!eth_p_all_hit && !(rtdev->flags & IFF_PROMISC))) {
+#ifdef CONFIG_RTNET_RTIPV4_DEBUG
+	printk(KERN_WARNING "RTnet: no one cared for packet with layer 3 "
+		    "protocol type 0x%04x\n", ntohs(rtskb->protocol));
+#endif
+    }
+
+    kfree_rtskb(rtskb);
+}
+
+#if IS_ENABLED(CONFIG_RTNET_DRV_LOOPBACK)
+EXPORT_SYMBOL_GPL(rt_stack_deliver);
+#endif /* CONFIG_RTNET_DRV_LOOPBACK */
+
+
+static int rt_stack_mgr_task(void *arg)
+{
+    rtdm_event_t            *mgr_event = &((struct rtnet_mgr *)arg)->event;
+    struct rtskb            *rtskb;
+
+    while (!stop_rt_stack_mgr_task) {
+	if (rtdm_event_wait_one(mgr_event) < 0)
+	    break;
+
+	/* we are the only reader => no locking required */
+	while ((rtskb = __rtskb_fifo_remove(&rx.fifo)))
+	    rt_stack_deliver(rtskb);
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+
+/***
+ *  rt_stack_connect
+ */
+void rt_stack_connect (struct rtnet_device *rtdev, struct rtnet_mgr *mgr)
+{
+    rtdev->stack_event = &mgr->event;
+}
+
+EXPORT_SYMBOL_GPL(rt_stack_connect);
+
+
+/***
+ *  rt_stack_disconnect
+ */
+void rt_stack_disconnect (struct rtnet_device *rtdev)
+{
+    rtdev->stack_event = NULL;
+}
+
+EXPORT_SYMBOL_GPL(rt_stack_disconnect);
+
+
+/***
+ *  rt_stack_mgr_init
+ */
+int rt_stack_mgr_init (struct rtnet_mgr *mgr)
+{
+    int i;
+    struct sched_param mgr_task_param;
+    mgr_task_param.sched_priority = stack_mgr_prio;
+
+    raw_spin_lock_init(&rt_packets_lock);
+    
+    rtskb_fifo_init(&rx.fifo, CONFIG_RTNET_RX_FIFO_SIZE);
+
+    for (i = 0; i < RTPACKET_HASH_TBL_SIZE; i++)
+	INIT_LIST_HEAD(&rt_packets[i]);
+#ifdef CONFIG_RTNET_ETH_P_ALL
+    INIT_LIST_HEAD(&rt_packets_all);
+#endif /* CONFIG_RTNET_ETH_P_ALL */
+
+    rtdm_event_init(&mgr->event, 0);
+
+    stop_rt_stack_mgr_task = 0;
+    mgr->task = kthread_create(rt_stack_mgr_task, mgr, "rtnet-stack");
+    if (!mgr->task)
+            goto mgr_task_failed; 
+    sched_setscheduler(mgr->task, SCHED_FIFO, &mgr_task_param);
+    wake_up_process(mgr->task);
+    
+    return 0;
+
+mgr_task_failed:
+    rtdm_event_destroy(&mgr->event);
+    return -ENOMEM;
+}
+
+
+/***
+ *  rt_stack_mgr_delete
+ */
+void rt_stack_mgr_delete (struct rtnet_mgr *mgr)
+{
+    rtdm_event_destroy(&mgr->event);
+
+    stop_rt_stack_mgr_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(mgr->task);
+    /* release the task structure */
+    put_task_struct(mgr->task);
+}
diff -Naur a/net/sched/sch_api.c b/net/sched/sch_api.c
--- a/net/sched/sch_api.c	2020-11-23 13:48:37.093988075 +0200
+++ b/net/sched/sch_api.c	2021-07-14 15:39:12.946127521 +0300
@@ -1257,7 +1257,7 @@
 		rcu_assign_pointer(sch->stab, stab);
 	}
 	if (tca[TCA_RATE]) {
-		seqcount_t *running;
+		net_seqlock_t *running;
 
 		err = -EOPNOTSUPP;
 		if (sch->flags & TCQ_F_MQROOT) {
diff -Naur a/net/sched/sch_generic.c b/net/sched/sch_generic.c
--- a/net/sched/sch_generic.c	2020-11-23 13:48:37.109988396 +0200
+++ b/net/sched/sch_generic.c	2021-07-14 15:39:12.946127521 +0300
@@ -553,7 +553,11 @@
 	.ops		=	&noop_qdisc_ops,
 	.q.lock		=	__SPIN_LOCK_UNLOCKED(noop_qdisc.q.lock),
 	.dev_queue	=	&noop_netdev_queue,
+#ifdef CONFIG_PREEMPT_RT
+	.running	=	__SEQLOCK_UNLOCKED(noop_qdisc.running),
+#else
 	.running	=	SEQCNT_ZERO(noop_qdisc.running),
+#endif
 	.busylock	=	__SPIN_LOCK_UNLOCKED(noop_qdisc.busylock),
 	.gso_skb = {
 		.next = (struct sk_buff *)&noop_qdisc.gso_skb,
@@ -858,9 +862,15 @@
 	lockdep_set_class(&sch->busylock,
 			  dev->qdisc_tx_busylock ?: &qdisc_tx_busylock);
 
+#ifdef CONFIG_PREEMPT_RT
+	seqlock_init(&sch->running);
+	lockdep_set_class(&sch->running.lock,
+			  dev->qdisc_running_key ?: &qdisc_running_key);
+#else
 	seqcount_init(&sch->running);
 	lockdep_set_class(&sch->running,
 			  dev->qdisc_running_key ?: &qdisc_running_key);
+#endif
 
 	sch->ops = ops;
 	sch->flags = ops->static_flags;
diff -Naur a/net/socket.c b/net/socket.c
--- a/net/socket.c	2020-11-23 13:48:35.989965910 +0200
+++ b/net/socket.c	2020-12-17 18:23:08.377212700 +0200
@@ -1,5 +1,9 @@
 // SPDX-License-Identifier: GPL-2.0-or-later
 /*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
  * NET		An implementation of the SOCKET network access protocol.
  *
  * Version:	@(#)socket.c	1.1.93	18/02/95
@@ -105,6 +109,12 @@
 #include <net/busy_poll.h>
 #include <linux/errqueue.h>
 
+/* rtnet */
+#include <rtnet_socket.h>
+#include <ipv4/protocol.h>
+
+#define RTNET_PREALLOC_SIZE	64	
+
 #ifdef CONFIG_NET_RX_BUSY_POLL
 unsigned int sysctl_net_busy_read __read_mostly;
 unsigned int sysctl_net_busy_poll __read_mostly;
@@ -115,9 +125,13 @@
 static int sock_mmap(struct file *file, struct vm_area_struct *vma);
 
 static int sock_close(struct inode *inode, struct file *file);
+static int sock_close_rtnet(struct inode *inode, struct file *filp);
+static __poll_t sock_poll_rtnet(struct file *file,
+			      struct poll_table_struct *wait);
 static __poll_t sock_poll(struct file *file,
 			      struct poll_table_struct *wait);
 static long sock_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+static long sock_ioctl_rtnet(struct file *file, unsigned cmd, unsigned long arg);
 #ifdef CONFIG_COMPAT
 static long compat_sock_ioctl(struct file *file,
 			      unsigned int cmd, unsigned long arg);
@@ -165,6 +179,21 @@
 	.show_fdinfo =	sock_show_fdinfo,
 };
 
+static const struct file_operations socket_file_ops_rtnet = {
+	.owner =	THIS_MODULE,
+	.llseek =	no_llseek,
+	.read_iter =	NULL,
+	.write_iter =	NULL,
+	.poll =		sock_poll_rtnet,
+	.unlocked_ioctl = sock_ioctl_rtnet,
+	.mmap =		NULL,
+	.release =	sock_close_rtnet,
+	.fasync =	NULL,
+	.sendpage =	NULL,
+	.splice_write = NULL,
+	.splice_read =	NULL,
+};
+
 /*
  *	The protocol list. Each protocol is registered in here.
  */
@@ -401,6 +430,29 @@
  *	This function uses GFP_KERNEL internally.
  */
 
+struct file *sock_alloc_file_rtnet(struct rtsocket *rtsock, struct socket *sock, int flags, const char *dname)
+{
+	struct file *file;
+
+	if (!dname)
+		dname = sock->sk ? sock->sk->sk_prot_creator->name : "";
+
+	file = alloc_file_pseudo(SOCK_INODE(sock), sock_mnt, dname,
+				O_RDWR | (flags & O_NONBLOCK),
+				&socket_file_ops_rtnet); /* rtnet */
+	if (IS_ERR(file)) {
+		sock_release(sock);
+		return file;
+	}
+
+	sock->file = file;
+	rtsock->file = file;
+	file->private_data = rtsock;
+	stream_open(SOCK_INODE(sock), file);
+	return file;
+}
+EXPORT_SYMBOL(sock_alloc_file_rtnet);
+
 struct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)
 {
 	struct file *file;
@@ -423,6 +475,25 @@
 }
 EXPORT_SYMBOL(sock_alloc_file);
 
+static int sock_map_fd_rtnet(struct rtsocket *rtsock, struct socket *sock, int flags)
+{
+	struct file *newfile;
+	int fd = get_unused_fd_flags(flags);
+	if (unlikely(fd < 0)) {
+		sock_release(sock);
+		return fd;
+	}
+
+	newfile = sock_alloc_file_rtnet(rtsock, sock, flags, NULL);
+	if (!IS_ERR(newfile)) {
+		fd_install(fd, newfile);
+		return fd;
+	}
+
+	put_unused_fd(fd);
+	return PTR_ERR(newfile);
+}
+
 static int sock_map_fd(struct socket *sock, int flags)
 {
 	struct file *newfile;
@@ -449,7 +520,6 @@
  *
  *	On failure returns %NULL and assigns -ENOTSOCK to @err.
  */
-
 struct socket *sock_from_file(struct file *file, int *err)
 {
 	if (file->f_op == &socket_file_ops)
@@ -460,6 +530,16 @@
 }
 EXPORT_SYMBOL(sock_from_file);
 
+struct rtsocket *sock_from_file_rtnet(struct file *file, int *err)
+{
+	if (file->f_op == &socket_file_ops_rtnet)
+		return file->private_data;	/* set in sock_map_fd */
+
+	*err = -ENOTSOCK;
+	return NULL;
+}
+EXPORT_SYMBOL(sock_from_file_rtnet);
+
 /**
  *	sockfd_lookup - Go from a file number to its socket slot
  *	@fd: file handle
@@ -508,6 +588,23 @@
 	return NULL;
 }
 
+static struct rtsocket *sockfd_lookup_light_rtnet(int fd, int *err, int *fput_needed)
+{
+	struct fd f = fdget(fd);
+	struct rtsocket *rtsock;
+
+	*err = -EBADF;
+	if (f.file) {
+		rtsock = sock_from_file_rtnet(f.file, err);
+		if (likely(rtsock)) {
+			*fput_needed = f.flags & FDPUT_FPUT;
+			return rtsock;
+		}
+		fdput(f);
+	}
+	return NULL;
+}
+
 static ssize_t sockfs_listxattr(struct dentry *dentry, char *buffer,
 				size_t size)
 {
@@ -1201,6 +1298,21 @@
 	return err;
 }
 
+static long sock_ioctl_rtnet(struct file *file, unsigned cmd, unsigned long arg)
+{
+	struct rtsocket *rtsock;
+	void __user *argp = (void __user *)arg;
+
+	rtsock = file->private_data;
+	
+	if(rtsock->family == AF_INET)
+		return rt_udp_ioctl(rtsock, cmd, argp);
+	else if(rtsock->family == AF_PACKET)
+		return rt_packet_ioctl(rtsock, cmd, argp);
+	else
+		return -EINVAL;
+}
+
 /**
  *	sock_create_lite - creates a socket
  *	@family: protocol family (AF_INET, ...)
@@ -1265,6 +1377,31 @@
 	return sock->ops->poll(file, sock, wait) | flag;
 }
 
+static __poll_t sock_poll_rtnet(struct file *file, poll_table *wait)
+{
+	struct rtsocket *rtsock = file->private_data;
+	/* support only input for the moment */
+	__poll_t events = poll_requested_events(wait);
+	
+	/* model from linux/net/sock.h */
+	if (!poll_does_not_wait(wait)) {
+		poll_wait_rtnet(file, &rtsock->wq_head_rtnet, wait);
+
+		/* We need to be sure we are in sync with the
+		 * socket flags modification.
+		 *
+		 * This memory barrier is paired in the wq_has_sleeper_rtnet().
+		 */
+		smp_mb();
+	}
+	
+	if(rtskb_queue_empty(&rtsock->incoming) ||
+	   !(events & (EPOLLIN | EPOLLRDNORM | EPOLLRDBAND)))
+		return 0;
+	else
+		return EPOLLIN | EPOLLRDNORM | EPOLLRDBAND;
+}
+
 static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	struct socket *sock = file->private_data;
@@ -1278,6 +1415,31 @@
 	return 0;
 }
 
+static int sock_close_rtnet(struct inode *inode, struct file *filp)
+{
+	struct rtsocket *rtsock;
+	int err = 0;
+	
+	if(!filp) {
+		err = -EBADF;
+		goto release_regular_sock;
+	}
+	
+	rtsock = sock_from_file_rtnet(filp, &err);
+	if(!rtsock)
+		return err;
+	
+	if(rtsock->family == AF_INET)
+		rt_udp_close(rtsock);
+	else if(rtsock->family == AF_PACKET)
+		rt_packet_close(rtsock);
+
+release_regular_sock:
+	/* free the linux socket */
+	__sock_release(SOCKET_I(inode), inode);
+	return err;
+}
+
 /*
  *	Update the socket async list
  *
@@ -1529,6 +1691,59 @@
 	return __sys_socket(family, type, protocol);
 }
 
+int __sys_socket_rtnet(int family, int type, int protocol)
+{
+	int retval;
+	struct socket *sock;
+	int flags;
+	int fd;
+	struct rtsocket *rtsock;
+		
+	/* Check the SOCK_* constants for consistency.  */
+	BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC);
+	BUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK);
+	BUILD_BUG_ON(SOCK_CLOEXEC & SOCK_TYPE_MASK);
+	BUILD_BUG_ON(SOCK_NONBLOCK & SOCK_TYPE_MASK);
+
+	flags = type & ~SOCK_TYPE_MASK;
+	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
+		return -EINVAL;
+	type &= SOCK_TYPE_MASK;
+
+	if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
+		flags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;
+
+	/* force standard socket to UDP.
+	 * sock_create(family, type, protocol, &sock);
+	 */
+	retval = sock_create(AF_INET, SOCK_DGRAM, IPPROTO_UDP, &sock);
+
+	if (retval < 0)
+		return retval;
+
+	if ((family == AF_PACKET) && ((type == SOCK_DGRAM) || (type == SOCK_RAW)))
+		retval = rt_packet_socket(&rtsock, family, type, protocol);
+	else if ((family == AF_INET) && (type == SOCK_DGRAM) && 
+			 (protocol == IPPROTO_UDP))
+		retval = rt_inet_socket(&rtsock, family, type, protocol);
+	else
+		retval = -EINVAL;
+	if(retval < 0)
+		return retval;
+
+	fd = sock_map_fd_rtnet(rtsock, sock, flags & (O_CLOEXEC | O_NONBLOCK));
+	if(fd < 0)
+		return fd;
+	rtsock->fd = fd;
+
+	return fd;
+}
+
+SYSCALL_DEFINE3(socket_rtnet, int, family, int, type, int, protocol)
+{
+	return __sys_socket_rtnet(family, type, protocol);
+}
+
 /*
  *	Create a pair of connected sockets.
  */
@@ -1662,11 +1877,35 @@
 	return err;
 }
 
+int __sys_bind_rtnet(int fd, struct sockaddr __user *umyaddr, int addrlen)
+{
+	struct rtsocket *rtsock;
+	int err, fput_needed;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (rtsock) {
+		if(rtsock->family == AF_INET)
+			err = rt_udp_bind(rtsock, umyaddr, addrlen);
+		else if(rtsock->family == AF_PACKET)
+			err = rt_packet_bind(rtsock, umyaddr, addrlen);
+		else
+			err = -EINVAL;
+		fput_light(rtsock->file, fput_needed);
+	}
+	
+	return err;
+}
+
 SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
 {
 	return __sys_bind(fd, umyaddr, addrlen);
 }
 
+SYSCALL_DEFINE3(bind_rtnet, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
+{
+	return __sys_bind_rtnet(fd, umyaddr, addrlen);
+}
+
 /*
  *	Perform a listen. Basically, we allow the protocol to do anything
  *	necessary for a listen, and if that works, we mark the socket as
@@ -1997,6 +2236,70 @@
 	return err;
 }
 
+int __sys_sendto_rtnet(int fd, void __user *buff, size_t len, unsigned int flags,
+		 struct sockaddr __user *addr,  int addr_len)
+{
+	struct rtsocket *rtsock;
+	int err, fput_needed;
+	struct user_msghdr msg;
+	struct iovec iov;
+	char prealloc[RTNET_PREALLOC_SIZE];
+	struct sockaddr address;
+	
+	/* dirty way because iov_base is not in userspace */
+	if(likely(len <= RTNET_PREALLOC_SIZE))
+		iov.iov_base = prealloc;
+	else
+		iov.iov_base = kmalloc(len, GFP_ATOMIC);
+	if((err = copy_from_user(iov.iov_base, buff, len)))
+		goto out;
+	iov.iov_len = len;
+	msg.msg_iov = &iov;
+	msg.msg_iovlen = 1;
+	
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+
+	msg.msg_name = NULL;
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	msg.msg_namelen = 0;
+	if (addr) {
+		if(likely(addr_len <= sizeof(struct sockaddr)))
+			msg.msg_name = &address;
+		else
+			msg.msg_name = kmalloc(addr_len, GFP_ATOMIC);
+		if(!msg.msg_name) {
+			err = -ENOMEM;
+			goto out;
+		}
+		if((err = copy_from_user(msg.msg_name, addr, addr_len)))
+			goto out2;
+		msg.msg_namelen = addr_len;
+	}
+	if (rtsock->file->f_flags & O_NONBLOCK)
+		flags |= MSG_DONTWAIT;
+	msg.msg_flags = flags;
+	
+	if(rtsock->family == AF_INET)
+		err = rt_udp_sendmsg(rtsock, &msg, msg.msg_flags, 0);
+	else if(rtsock->family == AF_PACKET)
+		err = rt_packet_sendmsg(rtsock, &msg, msg.msg_flags, 0);
+	else
+		err = -EINVAL;
+	
+	fput_light(rtsock->file, fput_needed);
+
+out2:
+	if(msg.msg_name != &address)
+		kfree(msg.msg_name);
+out:
+	if(iov.iov_base != prealloc)
+		kfree(iov.iov_base);
+	return err;
+}
+
 SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
 		unsigned int, flags, struct sockaddr __user *, addr,
 		int, addr_len)
@@ -2004,6 +2307,13 @@
 	return __sys_sendto(fd, buff, len, flags, addr, addr_len);
 }
 
+SYSCALL_DEFINE6(sendto_rtnet, int, fd, void __user *, buff, size_t, len,
+		unsigned int, flags, struct sockaddr __user *, addr,
+		int, addr_len)
+{
+	return __sys_sendto_rtnet(fd, buff, len, flags, addr, addr_len);
+}
+
 /*
  *	Send a datagram down a socket.
  */
@@ -2060,6 +2370,73 @@
 	return err;
 }
 
+int __sys_recvfrom_rtnet(int fd, void __user *ubuf, size_t size, unsigned int flags,
+		   struct sockaddr __user *addr, int __user *addr_len)
+{
+	struct rtsocket *rtsock;
+	struct iovec iov;
+	struct user_msghdr msg;
+	struct sockaddr_storage address;
+	int err, err2;
+	int fput_needed;
+	char prealloc[RTNET_PREALLOC_SIZE];
+	
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock) {
+		printk(KERN_ERR "%s sockfd_lookup_light_rtnet returns null, err=%d\n",
+			   __func__, err);
+		return err;
+	}
+
+	if(size <= RTNET_PREALLOC_SIZE)
+		iov.iov_base = prealloc;
+	else
+		iov.iov_base = kmalloc(size, GFP_ATOMIC);
+	if(!iov.iov_base)
+		return -ENOMEM;
+	iov.iov_len = size;
+	msg.msg_iov = &iov;
+	msg.msg_iovlen = 1;
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	/* Save some cycles and don't copy the address if not needed */
+	msg.msg_name = addr ? (struct sockaddr *)&address : NULL;
+	/* We assume all kernel code knows the size of sockaddr_storage */
+	msg.msg_namelen = sizeof(struct sockaddr);
+	msg.msg_flags = 0;
+	if (rtsock->file->f_flags & O_NONBLOCK)
+		flags |= MSG_DONTWAIT;
+	if(rtsock->family == AF_INET)
+		err = rt_udp_recvmsg(rtsock, &msg, flags, 0);
+	else if(rtsock->family == AF_PACKET)
+		err = rt_packet_recvmsg(rtsock, &msg, flags, 0);
+	else
+		err = -EINVAL;
+	if(err < 0)
+		goto out;
+	else if (addr != NULL) {
+		err2 = move_addr_to_user(&address,
+					 msg.msg_namelen, addr, addr_len);
+		if (err2 < 0) {
+			err = err2;
+			goto out;
+		}
+	}
+
+	if(size > err)
+		size = err;
+	err = copy_to_user(ubuf, msg.msg_iov->iov_base, size);
+	if(!err)
+		err = size;
+
+	fput_light(rtsock->file, fput_needed);
+
+out:
+	if(iov.iov_base != prealloc)
+		kfree(iov.iov_base);
+	return err;
+}
+
 SYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,
 		unsigned int, flags, struct sockaddr __user *, addr,
 		int __user *, addr_len)
@@ -2067,6 +2444,13 @@
 	return __sys_recvfrom(fd, ubuf, size, flags, addr, addr_len);
 }
 
+SYSCALL_DEFINE6(recvfrom_rtnet, int, fd, void __user *, ubuf, size_t, size,
+		unsigned int, flags, struct sockaddr __user *, addr,
+		int __user *, addr_len)
+{
+	return __sys_recvfrom_rtnet(fd, ubuf, size, flags, addr, addr_len);
+}
+
 /*
  *	Receive a datagram from a socket.
  */
@@ -2444,11 +2828,41 @@
 	return err;
 }
 
+long __sys_sendmsg_rtnet(int fd, struct user_msghdr __user *msg, unsigned int flags,
+		   bool forbid_cmsg_compat)
+{
+	int fput_needed, err;
+	struct rtsocket *rtsock;
+
+	if (forbid_cmsg_compat && (flags & MSG_CMSG_COMPAT))
+		return -EINVAL;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+
+	if(rtsock->family == AF_INET)
+		err = rt_udp_sendmsg(rtsock, msg, flags, 1);
+	else if(rtsock->family == AF_PACKET)
+		err = rt_packet_sendmsg(rtsock, msg, flags, 1);
+	else
+		err = -EINVAL;
+
+	fput_light(rtsock->file, fput_needed);
+out:
+	return err;
+}
+
 SYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)
 {
 	return __sys_sendmsg(fd, msg, flags, true);
 }
 
+SYSCALL_DEFINE3(sendmsg_rtnet, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)
+{
+	return __sys_sendmsg_rtnet(fd, msg, flags, true);
+}
+
 /*
  *	Linux sendmmsg interface
  */
@@ -2656,12 +3070,41 @@
 	return err;
 }
 
+long __sys_recvmsg_rtnet(int fd, struct user_msghdr __user *msg, unsigned int flags,
+		   bool forbid_cmsg_compat)
+{
+	struct rtsocket *rtsock;
+	int err, fput_needed;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+	
+	if (rtsock->file->f_flags & O_NONBLOCK)
+		flags |= MSG_DONTWAIT;
+	if(rtsock->family == AF_INET)
+		err = rt_udp_recvmsg(rtsock, msg, flags, 1);
+	else if(rtsock->family == AF_PACKET)
+		err = rt_packet_recvmsg(rtsock, msg, flags, 1);
+	else
+		err = -EINVAL;
+	fput_light(rtsock->file, fput_needed);
+out:
+	return err;
+}
+
 SYSCALL_DEFINE3(recvmsg, int, fd, struct user_msghdr __user *, msg,
 		unsigned int, flags)
 {
 	return __sys_recvmsg(fd, msg, flags, true);
 }
 
+SYSCALL_DEFINE3(recvmsg_rtnet, int, fd, struct user_msghdr __user *, msg,
+		unsigned int, flags)
+{
+	return __sys_recvmsg_rtnet(fd, msg, flags, true);
+}
+
 /*
  *     Linux recvmmsg interface
  */
diff -Naur a/net/sunrpc/svc_xprt.c b/net/sunrpc/svc_xprt.c
--- a/net/sunrpc/svc_xprt.c	2020-11-23 13:48:36.165969444 +0200
+++ b/net/sunrpc/svc_xprt.c	2021-07-14 15:39:13.014127043 +0300
@@ -422,7 +422,7 @@
 	if (test_and_set_bit(XPT_BUSY, &xprt->xpt_flags))
 		return;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = svc_pool_for_cpu(xprt->xpt_server, cpu);
 
 	atomic_long_inc(&pool->sp_stats.packets);
@@ -446,7 +446,7 @@
 	rqstp = NULL;
 out_unlock:
 	rcu_read_unlock();
-	put_cpu();
+	put_cpu_light();
 	trace_svc_xprt_do_enqueue(xprt, rqstp);
 }
 EXPORT_SYMBOL_GPL(svc_xprt_do_enqueue);
diff -Naur a/net/xfrm/xfrm_state.c b/net/xfrm/xfrm_state.c
--- a/net/xfrm/xfrm_state.c	2020-11-23 13:48:37.301992252 +0200
+++ b/net/xfrm/xfrm_state.c	2021-07-14 15:39:13.226125557 +0300
@@ -44,7 +44,7 @@
  */
 
 static unsigned int xfrm_state_hashmax __read_mostly = 1 * 1024 * 1024;
-static __read_mostly seqcount_t xfrm_state_hash_generation = SEQCNT_ZERO(xfrm_state_hash_generation);
+static __read_mostly seqcount_spinlock_t xfrm_state_hash_generation;
 static struct kmem_cache *xfrm_state_cache __ro_after_init;
 
 static DECLARE_WORK(xfrm_state_gc_work, xfrm_state_gc_task);
@@ -139,6 +139,11 @@
 		return;
 	}
 
+	/* XXX - the locking which protects the sequence counter appears
+	 * to be broken here. The sequence counter is global, but the
+	 * spinlock used for the sequence counter write serialization is
+	 * per network namespace...
+	 */
 	spin_lock_bh(&net->xfrm.xfrm_state_lock);
 	write_seqcount_begin(&xfrm_state_hash_generation);
 
@@ -2589,6 +2594,8 @@
 	net->xfrm.state_num = 0;
 	INIT_WORK(&net->xfrm.state_hash_work, xfrm_hash_resize);
 	spin_lock_init(&net->xfrm.xfrm_state_lock);
+	seqcount_spinlock_init(&xfrm_state_hash_generation,
+			       &net->xfrm.xfrm_state_lock);
 	return 0;
 
 out_byspi:
